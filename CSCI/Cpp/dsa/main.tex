\documentclass{report}

\input{~/dev/latex/template/preamble.tex}
\input{~/dev/latex/template/macros.tex}

\title{\Huge{}}
\author{\huge{Nathan Warner}}
\date{\huge{}}
\fancyhf{}
\rhead{}
\fancyhead[R]{\itshape Warner} % Left header: Section name
\fancyhead[L]{\itshape\leftmark}  % Right header: Page number
\cfoot{\thepage}
\renewcommand{\headrulewidth}{0pt} % Optional: Removes the header line
%\pagestyle{fancy}
%\fancyhf{}
%\lhead{Warner \thepage}
%\rhead{}
% \lhead{\leftmark}
%\cfoot{\thepage}
%\setborder
% \usepackage[default]{sourcecodepro}
% \usepackage[T1]{fontenc}

% Change the title
\hypersetup{
    pdftitle={Data Structures and Algorithms}
}

\begin{document}
    % \maketitle
        \begin{titlepage}
       \begin{center}
           \vspace*{1cm}
    
           \textbf{Data Structures and Algorithms} \\
           In C++
    
           \vspace{0.5cm}
            
                
           \vspace{1.5cm}
    
           \textbf{Nathan Warner}
    
           \vfill
                
                
           \vspace{0.8cm}
         
           \includegraphics[width=0.4\textwidth]{~/niu/seal.png}
                
           Computer Science \\
           Northern Illinois University\\
           February 16, 2023 \\
           United States\\
           
                
       \end{center}
    \end{titlepage}
    \tableofcontents
    \pagebreak 
    \unsect{Selection Sort}
    \bigbreak \noindent 
    \begin{concept}
        The selection sort algorithm sorts an array by repeatedly finding the minimum element (if sorting in ascending order) from the unsorted part of the array and putting it at the end of the sorted part of the array. The algorithm maintains two subarrays in a given array:
    \end{concept}
    \begin{itemize}
        \item A subarray of already sorted elements.
        \item A subarray of elements that remain to be sorted.
    \end{itemize}
    At the start of the algorithm, the first subarray is empty. In each pass through the outer loop of the selection sort, the minimum element from the unsorted subarray is selected and moved to the end of the sorted subarray.
    \bigbreak \noindent 
    \subsection{Psuedocode}
    \begin{cppcode}
procedure selection_sort(array : list of sortable items, n : length of list)
    i := 0
    while i < n - 1
        min_index ← i
        j := i + 1
        while j < n
            if array[j] < array[min_index]
                min_index ← j
            end if
            j = j + 1
        end while
        swap array[i] and array[min_index]
        i = i + 1
    end while
end procedure
    \end{cppcode}

    \bigbreak \noindent 
    \subsection{Example}
    \begin{cppcode}
        int main(int argc, const char* argv[]) {
            int arr[] = {2,4,1,3,5}; int n = 5;

            for (int j=0; j <n-1; ++j) {
                int min = j;
                for (int k=j+1; k <n-1; ++k) {
                    if (arr[k] < arr[min]) {
                        min = k;
                    }
                }
                std::swap(arr[j], arr[min]);
            }
        }
    \end{cppcode}

    \pagebreak 
    \subsection{Complexity}
    \begin{itemize}
        \item \textbf{Time Complexity: $O(n^{2})$} 
        \item \textbf{Space Complexity: $O(1)$} 
    \end{itemize}
    \bigbreak \noindent 
    \nt{The primary advantage of selection sort is that it never makes more than $O(n)$ swaps, which can be useful if the array elements are large and copying them is a costly operation.}
    
    \pagebreak 
    \unsect{Insertion Sort}
    \bigbreak \noindent 
    \begin{concept}
        The insertion sort algorithm sorts a list by repeatedly inserting an unsorted element into the correct position in a sorted sublist. The algorithm maintains two sublists in a given array:

        \begin{itemize}
            \item A sorted sublist. This sublist initially contains a single element (an array of one element is always sorted).
            \item A sublist of elements to be inserted one at a time into the sorted sublist.
        \end{itemize}
    \end{concept}
    \bigbreak \noindent 
    \subsection{Psuedocode}
    \begin{cppcode}
procedure insertion_sort(array : list of sortable items, n : length of list)
    i ← 1
    while i < n
        j ← i
        while j > 0 and array[j - 1] > array[j]
            swap array[j - 1] and array[j]
            j ← j - 1
        end while
        i ← i + 1
    end while
end procedure
    \end{cppcode}

    \bigbreak \noindent 
    \subsection{Example}
    \bigbreak \noindent 
    \begin{cppcode}
        int main(int argc, const char* argv[]) {
            int arr[] = {2,4,1,3,5};
            int n = 5;

            for (int j=1; j<n; ++j) {
                for (int k=j; k>0; --k) {
                    if (arr[k-1] > arr[k]) {
                        std::swap(arr[k-1], arr[k]);
                    }
                }
            }
        }
    \end{cppcode}


    \pagebreak 
    \subsection{Optimizing Insertion Sort}
    \bigbreak \noindent 
    Performing a full swap of the array elements in each inner for loop iteration is not necessary. Instead, we save the value that we want to insert into the sorted subarray in temporary storage. In place of performing a full swap, we simply copy elements to the right. The saved value can then be inserted into its proper position once that has been located.
    \bigbreak \noindent 
    This alternative approach can potentially save a considerable number of assignment statements. If $N$ swaps are performed by the inner loop, the original version of insertion sort requires $N \cdot 3 $ assignment statements to perform those swaps. The improved version listed below only requires $N+2$ assignment statements to accomplish the same task.
    \subsubsection{Psuedocode}
    \bigbreak \noindent 
    \begin{cppcode}
procedure insertion_sort(array : list of sortable items, n : length of list)
    i ← 1
    while i < n
        temp ← array[i]
        j ← i
        while j > 0 and array[j - 1] > temp
            array[j] ← array[j - 1]
            j ← j - 1
        end while
        array[j] ← temp
        i ← i + 1
    end while
end procedure
    \end{cppcode}

    \bigbreak \noindent 
    \subsubsection{Example}
    \bigbreak \noindent 
    \begin{cppcode}
        int arr[] = {5,6,4,3,1};
        int n = 5;

        for (int j=1; j<n; ++j) {
            int tmp = arr[j];
            int k=j;
            for (; k>0; --k) {
                if (arr[k-1] > tmp) {
                    arr[k] = arr[k-1];
                } else {
                    break;
                }
            }
            arr[k] = tmp;
        }
    \end{cppcode}

    \bigbreak \noindent 
    \subsection{Complexity}
    \bigbreak \noindent 
    \begin{itemize}
        \item \textbf{Time Complexity: $O(n^{2})$}
        \item \textbf{Space Complexity: $O(1)$}
    \end{itemize}
    \bigbreak \noindent 
    \nt{The primary advantage of insertion sort over selection sort is that selection sort must always scan all remaining unsorted elements to find the minimum element in the unsorted portion of the list, while insertion sort requires only a single comparison when the element to be inserted is greater than the last element of the sorted sublist. When this is frequently true (such as if the input list is already sorted or partially sorted), insertion sort is considerably more efficient than selection sort. The best case input is a list that is already correctly sorted. In this case, insertion sort has O(n) complexity.}

    \pagebreak 
    \unsect{Bubble Sort}
    \bigbreak \noindent 
    \begin{concept}
       Bubble sort, sometimes referred to as sinking sort, is a simple sorting algorithm that repeatedly steps through the list, compares adjacent elements, and swaps them if they are in the wrong order. The pass through the list is repeated until the list is sorted.
    \end{concept}

    \bigbreak \noindent 
    \subsection{Psuedocode}
    \bigbreak \noindent 
    \begin{cppcode}
procedure bubble_sort(array : list of sortable items, n : length of list)
    do
        swapped ← false
        i ← 1
        while i < n
            if array[i - 1] > array[i]
                swap array[i - 1] and array[i]
                swapped ← true
            end if
            i ← i + 1
        end while
    while swapped
end procedure
    \end{cppcode}
    \bigbreak \noindent 
    \nt{If no items are swapped during a pass through the outer loop (i.e., the variable swapped remains false), then the array is already sorted and the algorithm can terminate.}

    \bigbreak \noindent 
    \subsection{Example}
    \bigbreak \noindent 
    \begin{cppcode}
        int arr[] = {5,6,4,3,1};
        int n = 5;

        bool swapped;
        do {
            swapped = 0;

            for (int i=0; i<n; ++i) { 
                if (arr[i-1] > arr[i]) {
                    std::swap(arr[i-1], arr[i]);
                    swapped = 1;
                }
            }

        } while (swapped);

    \end{cppcode}


    \bigbreak \noindent 
    \subsection{Optimizing Bubble Sort}
    \bigbreak \noindent 
    The bubble sort algorithm can be optimized by observing that the n-th pass finds the n-th largest element and puts it into its final place. Therefore the inner loop can avoid looking at the last $n-1$ items when running for the n-th time:
    \bigbreak \noindent 
    \subsubsection{Psuedocode}
    \bigbreak \noindent 
    \begin{cppcode}
procedure bubble_sort(array : list of sortable items, n : length of list)
    do
        swapped ← false
        i ← 1
        while i < n
            if array[i - 1] > array[i]
                swap array[i - 1] and array[i]
                swapped ← true
            end if
            i ← i + 1
        end while
        n ← n - 1
    while swapped
end procedure
    \end{cppcode}
    \bigbreak \noindent 
    It is common for multiple elements to be placed in their final positions on a single pass. In particular, after every pass through the outer loop, all elements after the position of the last swap are sorted and do not need to be checked again. Taking this into account makes it possible to skip over many elements, resulting in about a worst case 50\% improvement in comparison count (though no improvement in swap counts), and adds very little complexity because the new code subsumes the swapped variable:
    \bigbreak \noindent 
    \begin{cppcode}
     do
        last ← 0
        i ← 1
        while i < n
            if array[i - 1] > array[i]
                swap array[i - 1] and array[i]
                last ← i
            end if
            i ← i + 1
        end while
        n ← last
    while n > 1
    \end{cppcode}

    \pagebreak 
    \subsubsection{Example}
    \bigbreak \noindent 
    \begin{cppcode}
        int last;
        do  {
            last = 0;
            int j=1;

            for (; j<n; ++j) {
                if (arr[j-1] > arr[j]) {
                    std::swap(arr[j-1], arr[j]);
                    last = j;
                }
            }
            n = last;

        } while (n > 0);
    \end{cppcode}

    \bigbreak \noindent 
    \subsection{Complexity}
    \begin{itemize}
        \item \textbf{Time Complexity:} $O(n^{2})$
        \item \textbf{Space Complexity:} $O(1)$
    \end{itemize}
    \bigbreak \noindent
    \nt{Other $O(n^{2})$ sorting algorithms, such as insertion sort, generally run faster than bubble sort (even with optimizations) and are no more complex. Therefore, bubble sort is not a practical sorting algorithm.
        The only significant advantage that bubble sort has over most other sorting algorithms (but not insertion sort), is that the ability to detect that the list is sorted is built into the algorithm. When the list is already sorted (best-case), the complexity of bubble sort is only $O(n)$.
    }

    \pagebreak 
    \unsect{Two-Dimensional Array}

    \pagebreak 
    \unsect{Recursion}

    \pagebreak 
    \unsect{Complexity Analysis}
    \bigbreak \noindent 
    \subsection{Time Complexity}
    \bigbreak \noindent 
    \begin{concept}
        Time complexity in algorithms is a way to describe the efficiency of an algorithm in terms of the time it takes to run as a function of the length of the input. It gives us an idea of the growth rate of the runtime of an algorithm as the size of input data increases. Big O notation is a mathematical notation used to express this time complexity, focusing on the worst-case scenario or the upper limit of the algorithm's running time.
        \bigbreak \noindent 
        Big O notation describes the upper bound of the time complexity, ignoring constants and lower order terms which are less significant for large input sizes. Here are some common Big O notations and their meanings:
    \end{concept}
    \bigbreak \noindent 
    \subsubsection{Common time complexities}
    \bigbreak \noindent 
    The following is a list of the common time complexities, in order from best to worst
    \bigbreak \noindent 
    \begin{center}
        \begin{tabular}{lc}
            Notation & Name \\
        \hline
            $O(1)$ & Constant \\
            $O(\log{n}) $ & Logarithmic \\
            $O(n)$ & Linear \\
            $O(n\log_{}{n}) $ & Log-linear \\
            $O(n^{2}) $ & Quadratic \\
            $O(n^{3}) $ & Cubic \\
            $O(n^{k}) $ & Polynomial \\
            $O(2^{n}) $ & Exponential  \\
            $O(n!) $ & Factorial
        \end{tabular}
    \end{center}
    \bigbreak \noindent 
    You can also have multiple variables in your runtime. For example, the time required to paint a fence that is $w$ meters wide and $h$ meters high could be described as $O(wh)$. If you needed $p$ layers of paint, then you could say that the time is $O(whp)$

    \bigbreak \noindent 
    \subsubsection{Constant time}
    An O(1) time complexity, also known as constant time complexity, describes an algorithm where the time to complete does not depend on the size of the input data set. 
    \bigbreak \noindent 
    \subsubsection{Big O, Big Omega, and Big Theta}
    \bigbreak \noindent 
    Academics use big $O$, big $\Theta$, and big $\Omega$ to describe runtimes.
    \begin{itemize}
        \item \textbf{Big $\mathbf{O}$} notation (denoted as $O$) is widely used in academia to describe an upper bound on the time complexity of an algorithm. For instance, an algorithm that prints all the values in an array could be described as $O(N)$. However, it could also be described as $O(N^2)$, $O(N^3)$, or $O(2^N)$, among other possible Big O notations. The algorithm's execution time is at least as fast as each of these, making them upper bounds on the runtime. This relationship is akin to a less-than-or-equal-to relationship. For example, if Bob is $X$ years old (assuming no one lives past age 130), then it would be correct to say that $X \leq 130$. Similarly, it would also be correct, albeit less useful, to say that $X \leq 1,000$ or $X \leq 1,000,000$. While these statements are technically true, they are not particularly informative. Likewise, a simple algorithm to print the values in an array is $O(N)$, but it is also correct to describe it as $O(N^3)$ or any runtime larger than $O(N)$. This illustrates that while multiple Big O notations can technically describe the time complexity of an algorithm, the most informative description is the one that provides the tightest upper bound.
        \item \textbf{Big $\mathbf{\Omega}$}: In academia, $\Omega$ is the equivalent concept but for the lower bound. Printing the values in an array is $\Omega(N)$ as well as $\Omega(\log N)$ and $\Omega(1)$. After all, you know it won’t be faster than those runtimes. The $\Omega$ notation is used to describe the best-case scenario or the minimum amount of time an algorithm will take to complete. It ensures that the algorithm's execution time will not be less than the specified complexity, providing a guarantee on the lower limit of the algorithm's performance.
        \item \textbf{Big $\mathbf{\Theta}$}: In academia, $\Theta$ notation signifies that an algorithm's time complexity has both an upper and a lower bound. That is, an algorithm is $\Theta(N)$ if it is both $O(N)$ and $\Omega(N)$. $\Theta$ notation provides a tight bound on runtime, indicating that the algorithm's execution time grows at a rate directly proportional to the size of the input, neither faster nor slower. This precise characterization makes $\Theta$ especially useful for describing algorithms where the upper and lower bounds converge to the same complexity, offering a complete understanding of the algorithm's efficiency.
    \end{itemize}
    \bigbreak \noindent 
    \nt{In the industry, when people refer to big $O$ notation, they are likely talking about big $\mathbf{\Theta}$}

    \bigbreak \noindent 
    \subsubsection{Best Case, Worst Case, and Expected (or Average) Case}
    \bigbreak \noindent 
    We can actually describe our runtime for an algorithm in three different ways. Let’s look at this from the perspective of quick sort.
    \begin{itemize}
        \item \textbf{Best Case:} If all elements of the array are equal, then quick sort will, on average, just traverse through the array once. This is $\mathcal{O}(N)$. (This actually depends slightly on the implementation of quick sort. There are implementations that will run very quickly on a sorted array.)
        \item \textbf{Worst Case:} What if we get really unlucky and the pivot is repeatedly the biggest element in the array? (Actually, this can easily happen. If the pivot is chosen to be the first element in the subarray and the array is sorted in reverse order, we’ll have just this situation.) In this case, our recursion doesn’t divide the array in half and recursively sort each half, it just shrinks the subarray by one element. We end up with something similar to selection sort and the runtime degenerates to $\mathcal{O}(N^2)$.
        \item \textbf{Expected Case:} Usually, though, these wonderful or terrible situations won’t happen. Sure, sometimes the pivot will be very low or very high, but it won’t happen over and over again. We can expect a runtime of $\mathcal{O}(N \log N)$.
    \end{itemize}
    \bigbreak \noindent 
    We rarely discuss best case time complexity because it’s not a very useful concept. After all, wecould take essentially any algorithm, special case some input, and then get a O(1) runtime in thebest case.For many – probably most – algorithms, the worst case and the expected case are the same.Sometimes they’re different though and we need to describe both of the runtimes

    \bigbreak \noindent 
    \subsection{Space complexity}
    \bigbreak \noindent 
    Time is not the only thing that matters in an algorithm. We might also care about the amount of memory – or space – required by the algorithm. Space complexity is a parallel concept to time complexity. If we need to create an array of size $n$,this will require $O(n)$ space. If we need a two-dimensional array of size $n \times n$, this will require $O(n^{2})$ space.
    \bigbreak \noindent 
    \subsubsection{Constant time}
    \bigbreak \noindent 
    An algorithm has O(1) space complexity when the amount of memory it requires does not grow with the size of the input data set. This means the algorithm needs a constant amount of memory space, regardless of how large the input is.

    \bigbreak \noindent 
    \subsubsection{Space complexity in recursive algorithms}
    \bigbreak \noindent 
    Stack space in recursive calls counts too. For example, code like this would take $O(n)$ time and $O(n)$ space
    \bigbreak \noindent 
    \begin{cppcode}
    // Example 1
    int sum(int n) {
    if (n <= 0) 
        return 0;
    else
        return n + sum(n - 1);
    }
    \end{cppcode}
    \bigbreak \noindent 
    Each of these calls results in a stack frame with a copy of the variable n being pushed onto the program call stack and takes up actual memory

    \pagebreak \bigbreak \noindent 
    \subsection{Drop the constants}
    \bigbreak \noindent 
    It is entirely possible for $O(n)$ code to run faster than $O(1)$ code for specific inputs. Big O just describes the rate of increase, not the specific time required
    \bigbreak \noindent 
    For this reason, we drop the constants in runtimes. An algorithm that one might have described as \(O(2N)\) is actually \(O(N)\). If you’re going to try to count the number of instructions, then you’d have to go to the assembly level and take into account that multiplication requires more instructions than addition, how the compiler would optimize something, and all sorts of other details. 
    \bigbreak \noindent 
    That would be horrendously complicated, so don’t even start going down that road. Big O allows us to express how the runtime scales. We just need to accept that it doesn’t mean that \(O(N)\) is always better than \(O(N^2)\).

    \bigbreak \noindent 
    \subsection{Drop the non-dominant terms}
    \bigbreak \noindent 
    What do you do about an expression such as \(O(N^2 + N)\)? That second \(N\) isn’t exactly a constant. But it’s not especially important. We already said that we drop constants. \(O(N^2 + N^2)\) is \(O(2N^2)\), and therefore it would be \(O(N^2)\). If we don’t care about the latter \(N^2\) term, why would we care about \(N\)? We don’t.
    \bigbreak \noindent 
    We might still have a sum in a runtime. For example, the expression $O(B^{2} + A)$ cannot be reduced (without some special knowledge of $A$ and $B$)

    \bigbreak \noindent 
    \subsection{Multi-Part Algorithms: Add vs. Multiply}
    \bigbreak \noindent 
    Suppose you have an algorithm that has two steps. When do you multiply the runtimes and when do you add them?
    \bigbreak \noindent 
    \begin{cppcode}
    // Program 1

    for (int i=0; i<A; ++i) {
        cout << arrayA[i];
    }

    for (int i=0; i<B; ++i) {
        cout << arrayB[i];
    }

    // Program 2
    for (int i=0; i<A; ++i) {
        for (int j=0; j<B; ++j) {
            cout << arrayA[i] << ", " << arrayB[j];
        }
    }
    \end{cppcode}
    \pagebreak \bigbreak \noindent 
    In the first example, we do $A$ chunks of work then $B$ chunks of work. Therefore, the total amount of work is $O(A + B)$.In the second example, we do $B$ chunks of work for each element in $A$. Therefore, the total amount of work is $O(A * B)$

    \bigbreak \noindent 
    \subsection{Amortized Time}
    \bigbreak \noindent 
    A \texttt{C++} vector object allows you to have the benefits of an array while offering flexibility in size.
    You won’t run out of space in the vector since its capacity will grow as you insert elements.
    A vector is implemented with a dynamic array. When the number of stored in the array hits the
    array’s capacity, the vector class will create a new array with double the capacity and copy all of
    the elements over to the new array. The old array is then deleted.
    \bigbreak \noindent 
    How do you describe the runtime of insertion? This is a tricky question.
    The array could be full. If the array contains \(N\) elements, then inserting a new element will take
    \(O(N)\) time. You will have to create a new array of capacity \(2N\) and then copy \(N\) elements over. This
    insertion will take \(O(N)\) time.
    However, we also know that this doesn’t happen very often. The vast majority of the time, insertion
    will be in \(O(1)\) time.
    \bigbreak \noindent 
    We need a concept that takes both possibilities into account. This is what amortized time does. It
    allows us to describe that, yes, this worst case happens every once in a while. But once it
    happens, it won’t happen again for so long that the cost is “amortized.”
    \bigbreak \noindent 
    In this case, what is the amortized time?
    As we insert elements, we double the capacity when the size of the array is a power of 2. So after
    \(X\) elements, we double the capacity at array sizes 1, 2, 4, 8, 16, \(\ldots\), \(X\). That doubling takes,
    respectively, 1, 2, 4, 8, 16, 32, 64, \(\ldots\), \(X\) copies.
    \bigbreak \noindent 
    What is the sum of \(1 + 2 + 4 + 8 + 16 + \ldots + X\)? If you read this sum left to right, it starts with 1 and
    doubles until it gets to \(X\). If you read right to left, it starts with \(X\) and halves until it gets to 1.
    What then is the sum of \(X + X/2 + X/4 + X/8 + \ldots + 1\)? This is roughly \(2X\).
    (It's \(2X - 1\) to be exact, but this is big O notation, so we can drop the constant.).
    \bigbreak \noindent 
    Therefore, \(X\) insertions take \(O(2X)\) time. The amortized time for each insertion is therefore \(O(1)\).

    \bigbreak \noindent 
    \subsection{Log N Runtimes}
    \bigbreak \noindent 
    We commonly see O(log N) in runtimes. Where does this come from?
    \bigbreak \noindent 
    Let’s look at binary search as an example. In binary search, we are looking for an item search\_key in an N element sorted array. We first compare search\_key to the midpoint of the array. If search_key == array[mid], then we return. If search_key $<$ array[mid], then we search on the left side of the array. If search_key $>$ array[mid], then we search on the right side of the array.
    \bigbreak \noindent 
    We start off with with an N-element array to search. Then, after a single step, we’re down to N/2 elements. One more step, and we’re down to N/4 elements. We stop when we either find the value
    \bigbreak \noindent 
    or we’re down to just one element. The total runtime is then a matter of how many steps (dividing N by 2 each time) we can take until N becomes 1.
    \bigbreak \noindent 
    We could look at this in reverse (going from 1 to 16 instead of 16 to 1). How many times can we multiply N by 2 until we get N?
    \bigbreak \noindent 
    What is k in the expression $2^{k} = n$? This is exactly what log expresses.
    \begin{align*}
        &2^{k} = n \\
        &=\log_{2}{n} = k
    .\end{align*}

    \bigbreak \noindent 
    \subsection{Recursive runtime}
    \bigbreak \noindent 
    Here’s a tricky one. What’s the runtime of this code?
    \bigbreak \noindent 
    \begin{cppcode}
        int f(int n) {
            if (n <= 1) {
                return 1;
            } else {
                return f(n-1) + f(n-1);
            }
        }
            
    \end{cppcode}
    \bigbreak \noindent 
    let’s derive the runtime by walking through the code. Suppose we call f(4). This calls f(3) twice. Each of those calls to f(3) calls f(2), until we get down to f(1).
    \bigbreak \noindent 
    \fig{.8}{./figures/1.png}
    \bigbreak \noindent 
    How many calls are in this tree?
    \bigbreak \noindent 
    The tree will have depth N. Each node (i.e., function call) has two children. Therefore, each level will have twice as many calls as the one above it.
    \bigbreak \noindent 
    Therefore, there will be $2^{0} + 2^{1} + 2^{2} + 2^{3} + 2^{4} + \ldots + 2^{N}$ (which is $2^{N+1} – 1$) nodes.
    \bigbreak \noindent 
    Try to remember this pattern. When you have a recursive function that makes multiple calls, the runtime will often (but not always) look like $O(\text{branches}^{\text{depth}}) $ where branches is the number of time each recursive call branches. In this case, this gives us O(2N).
    \bigbreak \noindent 
    The space complexity of this algorithm will be $O(N)$. Although we have O(2N) function calls in the tree total, only $O(N)$ exist on the call stack at any given time. Therefore, we would only need to have $O(N)$ memory available.

    \pagebreak 
    \unsect{Shell sort}
    \bigbreak \noindent 
    \begin{concept}
        Shell sort is an advanced variant of insertion sort. It first sorts elements that are far apart from each other and successively reduces the interval (gap) between the elements to be compared. The idea is to arrange the list of elements into a sequence of incrementally more sorted arrays, which are then finally sorted with a simple insertion sort.
        \bigbreak \noindent 
        The key concept in Shell sort is the use of an interval to compare elements. Initially, elements far apart are compared and swapped if necessary. As the algorithm progresses, the interval decreases, making the array more and more sorted, until the interval is 1. At an interval of 1, the algorithm is essentially performing a standard insertion sort, but by this time, the array is partially sorted, making the insertion sort more efficient.
    \end{concept}
    \bigbreak \noindent 
    \subsection{Example}
    \bigbreak \noindent 
    \begin{cppcode}
    int arr[] = {6,3,2,1,8};
    int n = 5;

    for (int iv=n/2; iv>0; iv/=2) {

        for (int i=iv; i<n; ++i) {

            int j;
            int tmp = arr[i];
            for (j = i; j>=iv && arr[j-iv] > arr[j]; j-=iv) {
                arr[j] = arr[j-iv];
            }
            arr[j] = tmp;
        }
    }
    \end{cppcode}

    \pagebreak 
    \unsect{Quick Sort (Recursive)}
    \bigbreak \noindent 
    \begin{concept}
        The quicksort algorithm is a divide and conquer algorithm. Quicksort first divides a large array into two smaller sub-arrays: the low elements and the high elements. Quicksort can then recursively sort the sub-arrays. The steps are:
    \end{concept}
    \begin{itemize}
        \item Pick an element, called a pivot, from the array.
        \item Reorder the array so that all elements with values less than the pivot come before the pivot, while all elements with values greater than the pivot come after it (equal values can go either way). After this reordering, the pivot is in its final, sorted position. This reordering is called the partition operation.
        \item Recursively apply the above steps to the sub-array of elements with smaller values and separately to the sub-array of elements with greater values.
    \end{itemize}
    \bigbreak \noindent 
    \subsection{Base case}
    \bigbreak \noindent 
    The base case of the recursion is an array of size zero or one, which is in order by definition and requires no further sorting.
    \bigbreak \noindent 
    \subsection{Pivot selection}
    \bigbreak \noindent The pivot selection and partitioning steps can be done in several different ways; the choice of specific implementation schemes greatly affects the algorithm's performance.

    \bigbreak \noindent 
    \subsection{Psuedocode}
    \bigbreak \noindent 

    \subsubsection{What main calls}
    \bigbreak \noindent 
    \begin{cppcode}
    procedure quicksort(array : list of sortable items, n : length of list)
        quicksort(array, 0, n - 1)
    end procedure
    \end{cppcode}

    \pagebreak 
    \subsubsection{Recursive function}
    \bigbreak \noindent 
    \begin{cppcode}
    procedure quicksort(array : list of sortable items, start : first element of list,
    end : last element of list)
        if start < end
            pivot_point ← partition(array, start, end)
            quick_sort(array, start, pivot_point - 1)
            quick_sort(array, pivot_point + 1, end)
        end if
    end procedure
    \end{cppcode}

    \bigbreak \noindent 
    \subsubsection{Partition function}
    \bigbreak \noindent 
    \begin{cppcode}
procedure partition(array : list of sortable items, start : first element of list,
end : last element of list)
    mid ← (start + end) / 2
    swap array[start] and array[mid]

    pivot_index ← start
    pivot_value ← array[start]
    
    scan ← start + 1
    while scan <= end
        if array[scan] < pivot_value
            pivot_index ← pivot_index + 1
            swap array[pivot_index] and array[scan]
        end if
        scan ← scan + 1
    end while

    swap array[start] and array[pivot_index]

    return pivot_index
end procedure
    \end{cppcode}


    \pagebreak 
    \subsection{Examples}
    \bigbreak \noindent 
    \begin{cppcode}
int partition(int arr[], int start, int end) {
    int pivot_index, pivot_value, mid, scan;

    mid = (start + end) / 2;
    std::swap(arr[start], arr[mid]);

    pivot_index = start;
    pivot_value = arr[start];

    scan = start + 1;

    while (scan <= end) {
        if (arr[scan] < pivot_value) {
            ++pivot_index;
            std::swap(arr[pivot_index], arr[scan]);
        }
        ++scan;
    }
    std::swap(arr[start], arr[pivot_index]);

    return pivot_index;
}

void quicksort(int arr[], int start, int end) {
    int pivot_point;
    if (start < end) {
        pivot_point = partition(arr, start, end);
        quicksort(arr, start, pivot_point - 1);
        quicksort(arr, pivot_point + 1, end);
    }
}

void quicksort(int arr[], int n) {
    quicksort(arr, 0, n-1);
}

int main(int argc, const char* argv[]) {

    int arr[]  = {3,6,1,9,12,7,36,24,18,4};
    int n = 10;

    quicksort(arr,n);

    return EXIT_SUCCESS;
}
    \end{cppcode}

    \pagebreak 
    \subsection{Complexity}
    \bigbreak \noindent 
    \begin{itemize}
        \item \textbf{Time Complexity:} $O(n \log_{}{n})$
        \item \textbf{Space Complexity:} $O(\log_{}{n})$
    \end{itemize}

    \pagebreak 
    \unsect{Merge Sort Algorithm}
    \bigbreak \noindent 
    \begin{concept}
        Merge sort works as follows:
        \begin{itemize}
            \item Divide the unsorted list into n sublists, each containing one element. A list of one element is sorted by definition.
            \item Repeatedly merge sorted sublists (called "runs") to produce longer runs until there is only one run remaining. This is the sorted list
        \end{itemize}
    \end{concept}
    \bigbreak \noindent 
    \fig{.9}{./figures/1.svg}
    




    
    







    





    
    
\end{document}
