\documentclass{report}

\input{~/dev/latex/template/preamble.tex}
\input{~/dev/latex/template/macros.tex}

\title{\Huge{}}
\author{\huge{Nathan Warner}}
\date{\huge{}}
\fancyhf{}
\rhead{}
\fancyhead[R]{\itshape Warner} % Left header: Section name
\fancyhead[L]{\itshape\leftmark}  % Right header: Page number
\cfoot{\thepage}
\renewcommand{\headrulewidth}{0pt} % Optional: Removes the header line
%\pagestyle{fancy}
%\fancyhf{}
%\lhead{Warner \thepage}
%\rhead{}
% \lhead{\leftmark}
%\cfoot{\thepage}
%\setborder
% \usepackage[default]{sourcecodepro}
% \usepackage[T1]{fontenc}

% Change the title
\hypersetup{
    pdftitle={CS Complete}
}

\begin{document}
    % \maketitle
        \begin{titlepage}
       \begin{center}
           \vspace*{1cm}
    
           \textbf{Comprehensive CS}
    
           \vspace{0.5cm}
            
                
           \vspace{1.5cm}
    
           \textbf{Nathan Warner}
    
           \vfill
                
                
           \vspace{0.8cm}
         
           \includegraphics[width=0.4\textwidth]{~/niu/seal.png}
                
           Computer Science \\
           Northern Illinois University\\
           United States\\
           
                
       \end{center}
    \end{titlepage}
    \tableofcontents
    \pagebreak 
    \unsect{Theory of Computation}
    \bigbreak \noindent 
    \subsection{Natural Languages, Formal languages: Definitions and theorems}
    \begin{itemize}
        \item \textbf{G\"odel's incompleteness theorem}: 
        G\"odel's Incompleteness Theorems are two fundamental results in mathematical logic that state: 
        \begin{itemize}
            \item  Proved that for some axiomatic
                systems that there is no algorithm
                that will generate all true
                statements from those axioms.
            \item No such system can prove its own consistency.
        \end{itemize}
        This was the first indication that there are inherent limts on algorithms
        \item \textbf{Turing}: Alan Turing later provided formalism to the concepts of an “algorithm” and “computation”, he Invented definition for an abstract machine called the “universal algorithm machine”, he Provided means to formally (i.e., with mathematical rigor) explore the boundaries of what algorithms could, and could not, accomplish. Turing’s model for a universal abstract machine was the basis for the first computer – in fact, Turing was involved in the construction of the first computer.
        \item \textbf{Natural languages}: We communicate via a \textit{natural language}, Although we don’t often think about it, our language is
            guided by rules; spelling, grammar, punctuation
        \item \textbf{Formal language}:
            Formal languages, which are not intended for human-to-
            human communication, are similar to natural languages in
            that they too have rules that define “correct” words and
            statements, but they are also different than natural languages
            in two key ways;
            \begin{itemize}
                \item The rules that define a formal language are strictly enforced. There is no tolerance for misspellings, bad grammar, etc.
                \item For the purpose of determining if a word or statement is
                    acceptable in a formal language, meaning is ignored. Determining
                    if something is (or is not) part of a language is determined by the
                    language’s defining rules which do not attach meaning (i.e., no
                    definitions of words like in natural languages)
            \end{itemize}
            In short, formal languages is a game of symbols, not meaning
        \item \textbf{Formal Languge terminology}:
            \begin{itemize}
                \item \textbf{Symbol}: it is an abstract entity that is not formally defined – like a point or a line in geometry – but think of it as a single character like a letter, numerical digit, punctuation mark, or emoticon
                \item \textbf{String (or Word)}: A finite sequence (i.e., order matters) of zero or more symbols
                \item \textbf{Length}: The length of a string $w$ is denoted by $length(w)$ or $\abs{w}$ and is the number of symbols composing the string. Because strings, by definition, are finite then a string’s lengths is always defined (sometimes zero).
                \item \textbf{Prefix, suffix}: Any number of leading/trailing symbols of the string.
                \item \textbf{Concatenation}: The concatenation of two strings $w$ and $x$ is formed by writing the first string $w$ then the second string $x$
                    \bigbreak \noindent 
                    \textbf{Note:} For any string $w$, $\Lambda w = w\Lambda = w$  
                \item \textbf{Alphabet}: A finite set of symbols, typically denoted by the
                    Greek capital letter sigma $\Sigma$, for example
                    \begin{align*}
                        \Sigma = \{a,b,c\} \quad \Sigma = \{0,1\} \quad \Sigma = \varnothing \quad \text{(special case)}
                    .\end{align*}
            \end{itemize}
        \item \textbf{The empty string}:  A string with zero symbols is called the empty string
            and is denoted by the capital Greek letter lambda $\Lambda$, or sometimes
            lower case Greek letter epsilon $\epsilon$, where $\Lambda$ and $\epsilon$ are \textbf{not} symbols
            \bigbreak \noindent 
            Thus,
            \begin{align*}
                \abs{\Lambda} = 0
            .\end{align*}
        \item \textbf{Formal language definition}: A formal language is a et of strings from some \textbf{one} alphabet. Given an alphabet we generally define a formal language over that alphabet by
            specifying rules that either;
            \begin{enumerate}
                \item Tell us how to test a candidate word, or
                \item Tell us how to construct all words.
            \end{enumerate}
            For example, Given $\Sigma_{1}= \{x\} $, we can define languages
            \begin{align*}
                L_{1} &= \text{ any non empty string } = \{x, xx, xxx,...\} \\
                L_{2} &= \{X^{n}:\ x = 2k+1,\ k\in \mathbb{Z} \} = \{x,xxx,xxxxx,xxxxxxx,...\} \
                L_{3} &= \{x,xxxxxxxx\}
            .\end{align*}
        \item \textbf{The empty language}: The empty language $L = \varnothing$ is typically  denoted with the capital  greek letter phi $\Phi$. Thus, $L = \varnothing = \Phi $
        \item \textbf{Notes on formal languages}:
            \begin{itemize}
                \item All languages are defined over some alphabet; cannot define a language without an alphabet.
                \item Some languages are finite, some languages are infinite (remember, alphabets are always finite).
                \item Some languages include the empty string \(\Lambda\), some do not.
                \item Some languages are defined by rules, some are simply written completely (e.g., \(\Sigma_1 = \{x\}\), \(L_3 = \{x, \text{xxxxxxxxxx}\}\)).
                \item No matter what the alphabet \(\Sigma\) (even \(\Sigma = \emptyset\)), you can always define at least two languages; \(L_1 = \{\Lambda\}\) and \(L_2 = \emptyset\).
            \end{itemize}
        \item \textbf{Closure of an alphabet (closure of $\Sigma$) (Kleene closure)}:
            The language defined by the set of all strings (including the empty string $\Lambda$) over a fixed alphabet $\Sigma$.
            \begin{itemize}
                \item \textbf{Examples:}
                    \begin{align*}
                        \Sigma &= \{a\} & \Sigma^* &= \{\Lambda, a, aa, aaa, aaaa, \dots\} \\
                        \Sigma &= \{0, 1\} & \Sigma^* &= \{\Lambda, 0, 1, 00, 01, 10, 11, 000, \dots\} \\
                        \Sigma &= \emptyset & \Sigma^* &= \{\Lambda\}
                    \end{align*}
                    \bigbreak \noindent 
                    \textbf{Note:} If $\Sigma = \emptyset$ then $\Sigma^*$ is finite and $\Sigma^* = \{\Lambda\}$, otherwise $\Sigma^*$ is infinite.
            \end{itemize}
        \item \textbf{Positive closure}: $\Sigma^{+} = \Sigma^{*} - \{\Lambda\}$, you just take the empty string out of the kleene closure 
        \item \textbf{Recall: Power set}: The power set of any set $S$, written $\mathcal{P}(S)$ is the set of all subsets of $S$, including the empty set and the set $S$ itself.
            \bigbreak \noindent 
            In other words, given a set $S$, then its power set $\mathcal{P}(S)$ is a set of sets
            \begin{itemize}
                \item \textbf{Note:}
                    \begin{itemize}
                        \item If $S = \emptyset$, then then $\mathcal{P}(S) = \mathcal{P}(\emptyset) = \{\emptyset\} = \{\emptyset\}$ = a set with one element $=\emptyset$.
                        \item If $S$ is non-empty and finite with $n$ elements, then $\mathcal{P}(S)$ will be finite with $2^n$ elements.
                        \item If $S$ is infinite, then $\mathcal{P}(S)$ will be infinite.
                    \end{itemize}

                \item \textbf{Example:}

                    If $S = \{x, y, z\}$, then $\mathcal{P}(S)$ will have the following $2^3 = 8$ elements (each a set):
                    \[
                        \mathcal{P}(S) = \{\emptyset, \{x\}, \{y\}, \{z\}, \{x, y\}, \{x, z\}, \{y, z\}, \{x, y, z\}\}
                    \]
            \end{itemize}
        \item \textbf{Power set of the kleene closure $\mathcal{P}(\Sigma^{\star}) $}: Given some alphabet $\Sigma$ we can construct the set of all possible languages from $\Sigma$ as follows (assume non-empty $\Sigma$):
            \bigbreak \noindent 
            \fig{.6}{./figures/6.png}
        \item \textbf{From formal languages to computers}:
            \begin{itemize}
                \item Given an alphabet $\Sigma$ we can define many formal languages – the range of which is captured by $\mathcal{P}(\Sigma^*)$.

                \item We can define many formal languages verbally, but is there a way to define/express every language in any $\mathcal{P}(\Sigma^*)$ with some formal system or abstract machine?

                \item We search for a formal system or abstract machine with enough “power” to define any language in any $\mathcal{P}(\Sigma^*)$.

                \item \textbf{KEY POINT} \\
                    The abstract machines we discover along our search to cover $\mathcal{P}(\Sigma^*)$ turn out to be \textit{the theoretical basis for all computing}.

                \item In other words, by understanding the power (and limitations) of abstract machines that cover $\mathcal{P}(\Sigma^*)$, we are simultaneously discovering the same limits about all computing.
            \end{itemize}
    \end{itemize}

    \pagebreak 
    \subsection{Regular languages}
    \bigbreak \noindent 
    \textbf{Preface.} The first few subsubsections will be in the world of regular languages. In the context of computation theory, regular languages are a class of formal languages that can be recognized by finite automata. These languages are important because they are the simplest class of languages that can be described by a computational model. The characteristics of regular languages are as follows,
    \begin{itemize}
        \item \textbf{Finite Automata:} Regular languages can be recognized by deterministic or nondeterministic finite automata (DFA or NFA).
        \item \textbf{Regular Expressions:} Regular languages can be described using regular expressions.
        \item \textbf{Closure Properties:} Regular languages are closed under several operations, including:
            \begin{itemize}
                \item \textbf{Union:} The union of two regular languages is also regular.
                \item \textbf{Concatenation:} The concatenation of two regular languages is also regular.
                \item \textbf{Kleene Star:} The Kleene star operation, which involves repeating a regular language any number of times (including zero), results in a regular language.
                \item \textbf{Intersection and Difference:} Regular languages are also closed under intersection and difference.
            \end{itemize}
        \item \textbf{Decision Problems:} Certain decision problems are decidable for regular languages. For example, it is possible to determine whether a given string belongs to a regular language (membership problem), whether two regular languages are equivalent, or whether a regular language is empty.
    \end{itemize}
    \bigbreak \noindent 
    \subsubsection{Finite Automata}
    \begin{itemize}
        \item \textbf{Informal definition}: Described informally, a finite automaton (FA) is always associated with some alphabet $\Sigma$ and is an abstract machine which has 
            \begin{enumerate}
                \item A non-empty finite number of states, exactly one of which is designated as the “start state” and some number (possibly zero) of which are designated as “accepting states”.
                \item A transition table that shows how to move from one state to another based on symbols in the alphabet $\Sigma$
            \end{enumerate}
        \item \textbf{A simple example of a FA}:
            \bigbreak \noindent 
            \fig{.5}{./figures/23.png}
            \begin{itemize}
                \item Defined over alphabet $\Sigma = \{0, 1\}$.
                \item States are circles; transitions are directed edges (i.e., arrows) between states.
                \item Has exactly three states; \textbf{A}, \textbf{B}, and \textbf{C}.
                \item Every FA must have exactly one start state. In this example, the start state is \textbf{A} and denoted as the only state that has an edge coming to it from no other state.
                \item There is only one accepting state, \textbf{C}, and it is denoted by its \textit{double circle}. (We could have more than one but in this case we only have one)
                \item \textbf{Very important:}
                    \begin{itemize}
                        \item Each symbol in the alphabet has exactly one associated edge leaving every state.
                        \item In other words, every state must have exactly one edge leaving it for each symbol in the alphabet.
                    \end{itemize}
            \end{itemize}
        \item \textbf{How to use an FA}: The purpose of a FA is to define a language over its alphabet $\Sigma$. The FA provides the means by which to test a candidate string from $\Sigma$ and determine whether or not the string is in the language. It does this by “writing” the candidate string on an fictitious input tape and proceeding as follows:
        \begin{enumerate}
            \item Set the FA to the start state.
            \item If end-of-string then halt.
            \item Read next symbol on tape.
            \item Update the state according to the current state and the last symbol read.
            \item Goto step 2.
        \end{enumerate}
        When the process halts check which state the FA is in. If it is in any accepting state, then the string is in the language defined by the FA, otherwise the string is not in the language
    \item \textbf{Using the previous FA}: Let’s now try to use our FA to test whether or not the string 1001 is in the language
        \bigbreak \noindent 
        We start by writing the string on an input tape, placing the read head at the beginning of the tape, and placing the FA in its initial state, $A$
        \bigbreak \noindent 
        \fig{.8}{./figures/24.png}
        \bigbreak \noindent 
        Since the tape head is not at the end of the tape we
        \begin{enumerate}
            \item Read the next symbol from the tape.
            \item Follow the edge from the state we are currently in that corresponds to the symbol we just read to transition to the next state.
            \item Move the tape head
        \end{enumerate}
        \bigbreak \noindent 
        \fig{.8}{./figures/25.png}
        \bigbreak \noindent 
        In this case, we started in state $A$, read symbol 1, and followed the edge labeled 1 from $A$ which brought us back to $A$
        \bigbreak \noindent 
        We proceed in this way, read, change state, move tape head until we reach the end of the tape
        \bigbreak \noindent 
        Once the tape head reaches the end of the tape we simply look to see whether or not the FA ended in an accepting state.
        \bigbreak \noindent 
        In this case it ended in state $C$, which is an accepting state, which means that string 1001 is in the language.
        \bigbreak \noindent 
        \fig{.8}{./figures/26.png}
        \bigbreak \noindent 
        We deduce that the language has only strings with two consecutive zeroes somewhere.
        \pagebreak \bigbreak \noindent
    \item \textbf{FA Example Two: The set of all strings that do not contain a one ($\Sigma = \{0,1\})$}:
        \bigbreak \noindent 
        \fig{.5}{./figures/27.png}
        \bigbreak \noindent 
        This one is pretty simple. If we have a zero, stay in the accepting state, if we see a one, toss it to the other non-accepting state, its not coming back.
    \item \textbf{FA Example Three: The set of all strings that end in one ($\Sigma = \{0,1\})$}:
        \bigbreak \noindent 
        \fig{.5}{./figures/28.png}
    \item \textbf{FA Example Four: The set of all strings with an odd number of zeros ($\Sigma = \{0,1\})$}:
        \bigbreak \noindent 
        \fig{.5}{./figures/29.png}
    \item \textbf{FA Example Five: The set of all strings where the second to last symbol is one ($\Sigma = \{0,1\})$}:
        \bigbreak \noindent 
        \fig{.5}{./figures/30.png}
    \item \textbf{States are "memory"}: Consider the four FA we just created, in each instance the solution required us to design an FA that remembered at least part of what it had already read from the input tape. The type of memory that an FA has is very different than the RAM we find in
        contemporary computers, but the FA does have memory. Each time the FA enters a different state it is, in effect, redefining the memory of the
        entire FA. The FA can only be in a finite number of states, and that number can be arbitrarily
        large, but (as we will see) that difference in memory has a profound limiting effect in
        what FAs can compute.
        \bigbreak \noindent 
    \item \textbf{Limits of a FA}:
        \bigbreak \noindent 
        \textbf{Limited Memory:}
        \begin{itemize}
            \item \textbf{Finite State:} A finite automaton has a finite number of states. This means it can only "remember" a limited amount of information about the input it has processed. Once a finite automaton transitions to a new state, it forgets all previous information except for the current state.
            \item \textbf{No Stack or Tape:} Unlike more powerful models such as pushdown automata (which have a stack) or Turing machines (which have an infinite tape), finite automata cannot use any form of auxiliary memory to keep track of an unbounded number of items or to perform operations that require more complex memory management.
        \end{itemize}
        \bigbreak \noindent 
        \textbf{Inability to Count Unboundedly:}
        \begin{itemize}
            \item \textbf{No Arbitrary Counting}: Finite automata cannot count occurrences of symbols beyond the number of states they have. For example, a DFA with $n$ states can only count up to $n-1$ occurrences of a symbol reliably. Thus, they cannot recognize languages that require matching counts of different symbols if those counts are unbounded, such as $\{a^n b^n \mid n \geq 1\}$, where the number of 'a's must match the number of 'b's.
        \end{itemize}
    \item \textbf{FA Formal Definition}:
     We formally denote a \textit{finite automaton} by a 5-tuple $(Q, \Sigma, q_0, T, \delta)$, where
    \begin{itemize}
        \item $Q$ is a finite set of \textit{states}.
        \item $\Sigma$ is an alphabet of \textit{input symbols}.
        \item $q_0 \in Q$, is the \textit{start state}.
        \item $T \subseteq Q$, is the set of \textit{accepting states}.
        \item $\delta$ is the \textit{transition function} that maps a state in $Q$ and a symbol in $\Sigma$ to some state in $Q$. In mathematical notation, we say that $\delta: Q \times \Sigma \rightarrow Q$.
            With:
            \begin{itemize}
                \item $Q \times \Sigma$: The Cartesian product of the set of states $Q$ and the alphabet $\Sigma$. This represents all possible pairs of a state and an input symbol.
                \item $\rightarrow Q$: Indicates that the transition function maps each pair $(q, \sigma)$ (where $q \in Q$ and $\sigma \in \Sigma$) to a single state in $Q$.
            \end{itemize}
    \end{itemize}
    \item \textbf{Formally Specifying Our First FA}:
        \bigbreak \noindent 
        \fig{.5}{./figures/23.png}
        \bigbreak \noindent 
        Recall our first FA that accepts any string with two consecutive zeros somewhere.
        \bigbreak \noindent 
        We drew it as a Finite State diagram, but to formally define this FA we must specify each of the elements from the 5-tuple $(Q, \Sigma, q_0, T, \delta)$.
        \begin{itemize}
            \item $Q$ is a finite set of \textit{states}: \hspace{0.2cm} $Q = \{A, B, C\}$
            \item $\Sigma$ is an alphabet of \textit{input symbols}: \hspace{0.2cm} $\Sigma = \{0, 1\}$
            \item $q_0 \in Q$, is the \textit{start state}: \hspace{0.2cm} $q_0 = A$
            \item $T \subseteq Q$, is the set of \textit{accepting states}: \hspace{0.2cm} $T = \{C\}$
            \item $\delta$ is the \textit{transition function} $\delta: Q \times \Sigma \rightarrow Q$
        \end{itemize}
        \[
            \begin{array}{c|cc}
                \delta & \text{0} & \text{1} \\
                \hline
                A & B & C \\
                B & C & A \\
                C & C & C \\
            \end{array}
        \]
    \item \textbf{Unary}: consisting of or involving a single component or element.
    \item \textbf{Unary language}: One where the alphabet has only one symbol.
    \item \textbf{Binary}: Relating to, composed of, or involving two things.
    \item \textbf{Ternary}: Composed of three parts.
    \item \textbf{Dead state (trap state)}: This is a state that once entered, can never be left.
    \item \textbf{Deterministic finite automaton (DFA)}: The FA's we have looked at thus far have been DFA's. A DFA is a finite automaton where, for each state and each input symbol, there is exactly one transition to a new state. This means that given a current state and an input symbol, the next state is uniquely determined. In the future we will look at nondeterministic finite automaton (NFA). An NFA is a finite automaton where, for each state and input symbol, there can be multiple possible transitions to different states. Additionally, an NFA can have transitions that do not consume any input symbol ($\epsilon$-transitions).
    








    \end{itemize}

    \pagebreak 
    \subsubsection{Finite Automata: More examples}
    \begin{itemize}
        \item \textbf{$\Sigma = \{0,1\}$, all strings that start with 00}
        \item \textbf{$\Sigma = \{0,1\}$, all strings that end with 00}
            \bigbreak \noindent 
            \fig{.5}{./figures/31.png}
            \bigbreak \noindent 
            With:
            \begin{itemize}
                \item $Q = \{A,B,C\}$
                \item $\Sigma = \{0,1\}$
                \item $q_{0} = A$
                \item $T = C$
                \item $\delta:\ Q \times \Sigma \to  Q$ defined by                \begin{array}{c|cc}
                    $\delta$ & 0 & 1 \\
                    \hline
                    A & B & A\\
                    B & C & A\\
                    C & C & A
                \end{array}

            \end{itemize}
                
    \end{itemize}


    \pagebreak 
    \subsubsection{nondeterministic Finite automata (NFA)}
    \begin{itemize}
        \item \textbf{NFA definition}:
            \begin{itemize}
                \item If an automaton gets to a state where there is more than one possible transition corresponding to the symbol read from the tape, the automaton may  choose any of those paths. (nondeterminism) We say it \textbf{branches}
                \item if an automaton gets to a state where there is no transition for the symbol read from the tape, then that path of the automaton halts and rejects the string. We say it \textbf{dies}
                \item the automaton accepts the input string if and only if there exists a choice of transitions that ends in an accept state.
            \end{itemize}
            \bigbreak \noindent 
            \textbf{Example}: Consider this nondeterministic FA (NFA) over $\Sigma = \{0, 1\}$
            \bigbreak \noindent 
            \fig{.5}{./figures/32.png}
        \item \textbf{DFA or NFA?}:
            Consider the language $L$ over $\Sigma = \{a, b\}$ which is defined by
            \begin{align*}
                L = (a^{*}) + (ab)^{*}
            .\end{align*}
            \bigbreak \noindent 
            \fig{.5}{./figures/33.png}
        \item \textbf{NFA Formal definition}: We define an NFA $M(Q, \Sigma, q_{0}, T, \delta) $
            \begin{itemize}
                \item $Q$ is a finite set of states
                \item $\Sigma$ is an alphabet of input symbols
                \item $q_0 \in Q$ is the start state
                \item $T \subseteq Q$ is the set of accepting states
                \item $\delta$ is the transition function $\delta: Q \times \Sigma \to P(Q)$
            \end{itemize}
        \item \textbf{Transition function, DFA vs NFA}:
            \bigbreak \noindent 
            \fig{.5}{./figures/34.png}
        \item \textbf{NFA with $\epsilon$-transitions}: $\epsilon$-transitions allow the automaton to change state without
            consuming an input symbol
            \bigbreak \noindent 
            Changing states without consuming input symbols can go on arbitrarily long as there are $\epsilon$-transitions to traverse.
        \item \textbf{DFA or NFA with $\epsilon$-moves?}: Consider the language L over $\Sigma = \{a, b\}$ which is
            \begin{align*}
                L = (b^{*}a) + (a^{*}b)
            .\end{align*}
            \bigbreak \noindent 
            \fig{.5}{./figures/35.png}
        \item \textbf{NFA with $\epsilon$-transitions formal definition}: Everything is the same except for the transition function, we now have
            \begin{align*}
                \delta:\ Q \times (\Sigma \cup \{\epsilon\}) \to \mathcal{P}(Q)
            .\end{align*}
            \pagebreak \bigbreak \noindent 
        \item \textbf{$\delta$ – DFA, NFA, and NFA with $\epsilon$-moves}:
            \bigbreak \noindent 
            \fig{.35}{./figures/36.png}
        \item \textbf{DFA, NFA, or NFA with $\epsilon$ moves, who can define the most languages?}: We begin by noting, by definition, every DFA is an NFA. This means that any language you can define with a DFA can also be defined by an NFA. Thus,
            \begin{align*}
                \text{Languages defined by DFA} \subseteq \text{ Languages defined by NFA}
            .\end{align*}
            Also, by definition, every DFA is an NFA with $\epsilon$-moves, an NFA is an NFA with $\epsilon$ moves, even if it doesnt have any. Thus,
            \begin{align*}
                \text{Languages defined by DFA} \subseteq \text{ Languages defined by NFA with $\epsilon$-moves}
            .\end{align*}
            \bigbreak \noindent 
            But, by definition, every NFA is an NFA with $\epsilon$-moves. Thus,
            \begin{align*}
                \text{Languages defined by NFA} \subseteq \text{ Languages defined by NFA with $\epsilon$-moves}
            .\end{align*}
            \bigbreak \noindent 
            This tells us that
            \begin{itemize}
                
                \item NFAs are at least as powerful in defining languages as DFAs
                \item NFAs with $\epsilon$-moves are at least as powerful in defining languages as DFAs and NFAs.
            \end{itemize}
            \bigbreak \noindent 
            It turns out that these three are \textbf{equally} as powerful. We assert
            \begin{align*}
                &\text{Languages defined by DFA's} \\
                &=\text{Languages defined by NFA's} \\
                &=\text{Languages defined by NFA's with $\epsilon$-moves}
            .\end{align*}
            We prove this by showing an algorithm that converts any NFA with $\epsilon$-moves (or any NFA) to a DFA that accepts the exact same language
            \bigbreak \noindent 
            This means that there does not exist a language that can be defined by an NFA with $\epsilon$-moves (or NFA) that cannot also be defined by a DFA.
        \item \textbf{$\epsilon$-closure}: Before we can look at the algorithm we must first define the $\epsilon$-closure of a set of states 
            \bigbreak \noindent 
            Given:
            \begin{itemize}
                \item an NFA with $\epsilon$-moves $M(Q, \Sigma, q_{0}, T, \delta) $
                \item Some set of states $S \subseteq Q$
            \end{itemize}
            \bigbreak \noindent 
            \text{We define the } \varepsilon\text{-closure}(S) \text{ as the set of states that are reachable from the set of states } S \text{ using only zero or more } \varepsilon\text{-moves in } \delta.
            \bigbreak \noindent 
            \text{Note: it is always the case that } S \subseteq \varepsilon\text{-closure}(S)
            \bigbreak \noindent 
            The formal definition is
            \begin{align*}
                \epsilon-\text{closure}(q) = \{q\} \cup \{p:\ q \xrightarrow{\epsilon} p\}
            .\end{align*}
        \item \textbf{$\epsilon$-closure alternate notation}. 
            \begin{align*}
                \epsilon\text{-closure}(\{A\}) = \epsilon(\{A\}) = E(\{A\})
            .\end{align*}
        \item \textbf{$\epsilon$-closure of the empty set $\varnothing$}: The epsilon closure of the empty set is $\epsilon(\varnothing)  = \varnothing$
        \item \textbf{Algorithm: Converting NFA with $\epsilon$-moves to DFA}: The algorithm constructs a new DFA $M^{\prime}(Q^{\prime}, \Sigma, q_{0}^{\prime}, T^{\prime}, \delta^{\prime}) $ From an NFA with $\epsilon$-moves $M(Q, \Sigma, q_{0}, T, \delta) $. $\Sigma$ will remain the same
            \bigbreak \noindent 
            Things to note about the conversion:
            \begin{itemize}
                \item Same alphabet $\Sigma $
                \item Lose column $\epsilon$
                \item Lose all nondeterminism
                \item Lose all empty sets
                \item Cell values change from sets of states to states
            \end{itemize}
            \bigbreak \noindent 
            \textbf{Example}:
            \textbf{Consider the following NFA with $\varepsilon$-moves M(Q, $\Sigma$, $q_0$, T, $\delta$) over $\Sigma = \{0, 1\}$ and its associated transition table $\delta$: Q $\times$ ($\Sigma \cup \{\varepsilon\}$) $\rightarrow$ P(Q)}
            \bigbreak \noindent 
            \[
                \begin{array}{|c|c|c|c|}
                    \hline
                     & 0 & 1 & \varepsilon \\
                    \hline
                    X & \{Y\} & \{Y\} & \emptyset \\
                    \hline
                    Y & \{X, Z\} & \{Z\} & \{Z\} \\
                    \hline
                    Z & \emptyset & \{Y\} & \emptyset \\
                    \hline
                \end{array}
            \]
            \bigbreak \noindent 
            Start by computing the $\epsilon$-closure of the start state in $\delta$.
            \bigbreak \noindent 
            \fig{.4}{./figures/37.png}
            \bigbreak \noindent 
            There is a subtle - but very important - point to be made here …
            \bigbreak \noindent 
            we cannot simply take the $\epsilon$-closure (a set) and use it to create a row in $\delta^{\prime}$ (which needs to be a state). What we do is create a label for the new state in $\delta^{\prime}$ that represents the set of states from $\delta$ and then add that new state to $\delta^{\prime}$
            \bigbreak \noindent 
            In this instance we represented the set of states $\{X\}$ by a single state whose label is $X^{\prime}$
            \bigbreak \noindent 
            We continue by filling the columns of the start state for each symbol $\Sigma = \{0, 1\}$
            \bigbreak \noindent 
            Processing $\delta'$ state $X'$ which represents the set of states $\{X\}$ in $M$:
            \begin{itemize}
                \item Processing input symbol 0 (process each state in $\{X\}$ using $\delta$):
                    \begin{itemize}
                        \item Process $X$
                            \[
                                \delta(X, 0) = \{Y\}
                            \]
                            \[
                                \varepsilon\text{-closure}(\{Y\}) = \{Y, Z\}
                            \]
                    \end{itemize}
            \end{itemize}
    Since there are no more states in $\{X\}$ to process, we have finished processing the symbol 0 and have produced the set of states $\{Y, Z\}$.
    \bigbreak \noindent 
    We create a new state with label $Y'Z'$ (or $Z'Y'$, order does not matter) for $\delta'$ that represents $\{Y, Z\}$ in $M$ and define:
    \[
    \delta'(X', 0) = Y'Z'
    \]
    We note that $Y'Z'$ is a new state in $\delta'$ and so we create a new row for it in $\delta'$.
    \bigbreak \noindent 
    We continue this until we reach 
    \bigbreak \noindent 
    \fig{.8}{./figures/38.png}
    \bigbreak \noindent 
    Processing $\delta'$ state $Y'Z'$ which represents the set of states $\{Y, Z\}$ in $M$:
    \begin{itemize}
        \item Processing 0:
            \begin{itemize}
                \item Process $Y$
                    \[
                        \delta(Y, 0) = \{X, Z\}, \quad \varepsilon\text{-closure}(\{X, Z\}) = \{X, Z\}
                    \]
                \item Process $Z$
                    \[
                        \delta(Z, 0) = \emptyset, \quad \varepsilon\text{-closure}(\emptyset) = \emptyset
                    \]
            \end{itemize}
    \end{itemize}
    Here is our first instance of processing a state and symbol where the state in $\delta'$ represents multiple states in NFA $M$. When this happens, the set of states in NFA $M$ is computed by \textit{taking the union of the $\varepsilon$-closures}: $\{X, Z\} \cup \emptyset = \{X, Z\}$.
    \bigbreak \noindent 
    This produces a new label $X'Z'$ which we use to define:
    \[
        \delta'(X'Y', 0) = X'Z'
    \]
    and since $X'Z'$ is a new state, we add it to $\delta'$.
    \bigbreak \noindent 
    We continue this until we reach 
    \bigbreak \noindent 
    \fig{.8}{./figures/39.png}
    \bigbreak \noindent 
    A state in $M^{\prime}$ is an accepting state iff at least one of the states that it represents in $M$ is an accepting state … in this case $T^{\prime}= \{Y^{\prime}Z^{\prime}\}$.
    \bigbreak \noindent 
    We can now draw the new DFA 
    \bigbreak \noindent 
    \fig{.7}{./figures/40.png}
    \bigbreak \noindent 
    \textbf{Note:} If the closure or union of closures is the empty set, we do this
    \bigbreak \noindent 
    \fig{.7}{./figures/41.png}
    \bigbreak \noindent 
    This "emtpy" is a state and represents a garbage state, what goes does not leave.
\item \textbf{Kleene's theorem revisited}: The following are equivalent for a language $L$
    \begin{enumerate}
        \item There is a DFA for $L$
        \item There is an NFA for $L$
        \item There is an RE for $L$
    \end{enumerate}
    \item \textbf{Union of two DFA's (cartesian product construction)}:
        The process of finding the union of two deterministic finite automata (DFAs) involves creating a new DFA that accepts the union of the languages accepted by the original DFAs. This is done using a product construction (also called the Cartesian product construction), where you combine the states of both DFAs in a systematic way to ensure the resulting DFA accepts strings from either of the original DFAs.
        \bigbreak \noindent 
        Let’s say we have two DFAs:
        \[
            D_1 = (Q_1, \Sigma, \delta_1, q_1^{\text{start}}, F_1)
        \]
        that recognizes language \( L_1 \).
        \[
            D_2 = (Q_2, \Sigma, \delta_2, q_2^{\text{start}}, F_2)
        \]
        that recognizes language \( L_2 \).
        \bigbreak \noindent 
        \textbf{Create a New DFA State Set}:
        \bigbreak \noindent 
        \begin{itemize}
            \item The states of the new DFA are pairs of states, one from each of the original DFAs. The new state set will be the Cartesian product \(Q_1 \times Q_2\), meaning every possible combination of a state from \(D_1\) and a state from \(D_2\).
            \item If \(D_1\) has \(n\) states and \(D_2\) has \(m\) states, the new DFA will have \(n \times m\) states.
        \end{itemize}
        \bigbreak \noindent 
        \textbf{Define the New Start State}:
        \begin{itemize}
            \item The new start state is \((q_1^{\text{start}}, q_2^{\text{start}})\), where \(q_1^{\text{start}}\) is the start state of \(D_1\) and \(q_2^{\text{start}}\) is the start state of \(D_2\).
        \end{itemize}
        \textbf{Define the New Transition Function:}
        \begin{itemize}
            \item The transition function \(\delta\) for the new DFA operates by taking an input symbol and applying the transition functions of both original DFAs in parallel.
            \item For each input symbol \(a \in \Sigma\), the new DFA transitions from state \((q_1, q_2)\) to state \((\delta_1(q_1, a), \delta_2(q_2, a))\).
            \item In other words, if \(q_1\) moves to \(q_1'\) on input \(a\) in \(D_1\), and \(q_2\) moves to \(q_2'\) on input \(a\) in \(D_2\), the new DFA will move from \((q_1, q_2)\) to \((q_1', q_2')\).
        \end{itemize}
        \bigbreak \noindent 
        \textbf{Define the New Set of Accepting (Final) States}:
        The new DFA will accept a string if either of the original DFAs would accept it. Therefore, the set of final states \( F \) in the new DFA is defined as:
        \[
            F = \{ (q_1, q_2) \mid q_1 \in F_1 \ \text{or} \ q_2 \in F_2 \}
        \]
        This means that if either \( q_1 \) is a final state in \( D_1 \), or \( q_2 \) is a final state in \( D_2 \), the pair \( (q_1, q_2) \) is a final state in the new DFA.
        \bigbreak \noindent 
        \textbf{Note:} It is possible in the new DFA (constructed as the union of two DFAs) to have states that are unreachable—meaning there are states in the DFA that cannot be reached from the start state. This typically happens because, in the product construction, we generate all possible pairs of states from the two original DFAs, but not all of these pairs are necessarily reachable.
        \bigbreak \noindent 
        The union of two finite automata (FAs) is useful for constructing a new automaton that recognizes any string accepted by either of the two original automata. This has several practical applications in theoretical computer science and programming:
    \item \textbf{Finding the intersection of two DFA's}: The process is basically the same as finding the union, but it differs in how we define the accepting states in the new machine, the accepting states will be
        \begin{align*}
            T = \{(q_{1},q_{2}):\ q_{1} \in T_{1} \text{ and } q_{2} \in T_{2}\}
        .\end{align*}
        \bigbreak \noindent 
        \textbf{Note:} The intersection of two DFAs is useful in various practical applications where you need to accept only the strings that satisfy the conditions or rules of both automata
    \item \textbf{Concatenation of two DFA's}: The process is simple
        \bigbreak \noindent 
        For two machines $M_{1}(Q_{1}, \Sigma, q_{0_{1}}, T_{1}, \delta_{1})$, and $M_{2}(Q_{2}, \Sigma, q_{0_{2}}, T_{2}, \delta_{2})$ 
        \begin{enumerate}
            \item Connect the final states of the first machine to the start state of the second machine (With $\epsilon$-transitions)
            \item Clear $T_{1}$, There are no more final states in the first machine
            \item Convert $\epsilon$-NFA to DFA
        \end{enumerate}
        \textbf{Note:} The concatenation of two DFAs has practical uses in many scenarios where the language of interest is the concatenation of two sublanguages. Concatenating two DFAs allows you to recognize strings that can be divided into two parts, where the first part is recognized by one DFA and the second part is recognized by the other.
    \item \textbf{Finding the union of two NFA's}:
        taking the union of two nondeterministic finite automata (NFAs) involves constructing a new NFA that accepts any string that is accepted by either of the original NFAs. This process can be done by creating a new NFA that combines the two original NFAs. 
        \bigbreak \noindent 
        Given $M_{1}(Q_{1}, \Sigma, q_{0_{1}}, T_{1}, \delta_{1}) $, and $M_{2}(Q_{2}, \Sigma, q_{0_{2}}, T_{2}, \delta_{2}) $
        \bigbreak \noindent 
        \begin{enumerate}
            \item \textbf{New start state}: Start by defining a new start state $q^{\prime}_{0}$, this state will have $\epsilon$ transitions to the start states of both machines.
            \item \textbf{Define $Q^{\prime}$, the new set of states}: The new set of states will be the set of all states in $M_1$, and it will include all the states in $M_2$, along with the new start state. Thus,
                \begin{align*}
                    Q^{\prime} = Q_{1} \cup Q_{2} \cup \{q^{\prime}_{0}\}
                .\end{align*}
            \item \textbf{Define the transition function}: The transition function $\delta^{\prime}$ of the new NFA will include:
                \begin{itemize}
                    \item All the transitions of $M_{1}$ and $M_{2}$
                    \item Two $\epsilon$ transitions from the new start state to the start states of the two original machines $q_{0_{1}}$ and $q_{0_{2}}$. Thus,
                        \begin{align*}
                            \delta^{\prime}(q_{0}^{\prime}, \epsilon) = \{q_{0_{1}}, q_{0_{2}}\}
                        .\end{align*}
                \end{itemize}
            \item \textbf{Define the set of accepting states}: The set of accepting states will be
                \begin{align*}
                    T^{\prime} = T_{1} \cup T_{2}
                .\end{align*}
        \end{enumerate}
        \pagebreak \bigbreak \noindent 
        \textbf{Example:}
        \begin{figure}[ht]
            \centering
            \incfig{machine10}
            \label{fig:machine10}
        \end{figure}
        \bigbreak \noindent 
        $M_1 \cup M_2$ is then
        \begin{figure}[ht]
            \centering
            \incfig{machine11}
            \label{fig:machine11}
        \end{figure}
        \pagebreak \bigbreak \noindent 
    \item \textbf{Finding the intersection of two NFA's}: 
        For NFAs, intersection is more complex because NFAs are nondeterministic and don’t handle intersection naturally. Typically, you convert the NFAs to DFAs and then apply the DFA product construction
    \item \textbf{Concatenation of two NFA's}: The process is the same as with two DFA's (see above), but you don't need to convert to a DFA at the end.
    % \item \textbf{Convert DFA into RE}: To convert a DFA (Deterministic Finite Automaton) into a Regular Expression (RE), you can use the state elimination method or generalized transition automaton method. This process works by gradually reducing the DFA's states and transitions until only a regular expression representing the entire language remains.
    %     \bigbreak \noindent 
    %     Given a DFA $M(Q, \Sigma, \delta, q_{0}, F)$, the goal is to find a regular expression that represents the language recognized by this DFA.
    %     \bigbreak \noindent 
    %     \textbf{Process:}
    %     \begin{enumerate}
    %         \item \textbf{Add a new Start and accept state}:
    %             \begin{itemize}
    %                 \item Add a new start state \( q_s \) with an \(\epsilon\)-transition (empty string) to the original start state \( q_0 \). 
    %                     \bigbreak \noindent 
    %                     \textbf{Note:} Once you add the new start state \( q_s \) with an \(\epsilon\)-transition to the original start state \( q_0 \), \( q_0 \) is no longer considered the start state. Instead, \( q_0 \) becomes just another intermediate state in the automaton. The new start state is \( q_s \), and it immediately transitions to \( q_0 \) without consuming any input (via the \(\epsilon\)-transition).
    %                 \item Add a new accept state \( q_f \) and add \(\epsilon\)-transitions from each of the original accept states to this new accept state \( q_f \).
    %                     \bigbreak \noindent 
    %                     \textbf{Note:} Similarly, when you add the new accept state \( q_f \) and connect it via \(\epsilon\)-transitions from the original final states in \( F \), the original final states are no longer considered final states in the sense of marking the end of a string's acceptance. Now, the new final state \( q_f \) serves as the sole final state, and the automaton reaches \( q_f \) via \(\epsilon\)-transitions from the original final states.
    %             \end{itemize}
    %             These new states simplify the process because now there's exactly one start state and one accept state.
    %         \item \textbf{Eliminate States One by One}:
    %             \begin{itemize}
    %                 \item The idea is to progressively eliminate states from the DFA while updating the transitions between the remaining states with regular expressions.
    %                 \item Every time you eliminate a state $r$, you need to update the regular expressions on the transitions between the remaining states to account for the paths that go through $r$.
    %             \end{itemize}
    %             For any three states \( p \), \( r \), and \( q \), if there is a path from \( p \) to \( q \) that goes through \( r \), the new transition after eliminating \( r \) will include the regular expression:
    %             \[
    %                 R(p \rightarrow q) = R(p \rightarrow q) + R(p \rightarrow r) R(r \rightarrow r)^* R(r \rightarrow q)
    %             \]
    %             \pagebreak \bigbreak \noindent 
    %             Where:
    %             \begin{itemize}
    %                 \item \( R(p \rightarrow q) \) is the regular expression for the direct transition from \( p \) to \( q \).
    %                 \item \( R(p \rightarrow r) \) is the regular expression for the transition from \( p \) to \( r \).
    %                 \item \( R(r \rightarrow r) \) is the regular expression for the loop on state \( r \).
    %                 \item \( R(r \rightarrow q) \) is the regular expression for the transition from \( r \) to \( q \).
    %                 \item \( + \) represents union, and \( * \) represents the Kleene star (zero or more repetitions).
    %             \end{itemize}
    %             \bigbreak \noindent 
    %             After updating the transitions, remove the state \( r \).
    %             \bigbreak \noindent 
    %         \item \textbf{Repeat the Elimination Until Only Two States Remain}:
    %             Continue eliminating states and updating the transitions until only two states remain: the start state \( q_s \) and the new accept state \( q_f \).
    %             \bigbreak \noindent 
    %             At this point, the regular expression on the transition from \( q_s \) to \( q_f \) represents the language of the DFA.
    %     \end{enumerate}
    %     \bigbreak \noindent 
    %     \textbf{Example}: For the alphabet $\Sigma = \{0,1\}$, let's take the machine that accepts the strings with any number of ones, but the total number of zero's must be odd, and convert it to a RE.
    %     \begin{figure}[ht]
    %         \centering
    %         \incfig{machine1}
    %         \label{fig:machine1}
    %     \end{figure}
    %     \bigbreak \noindent 
    %     Let's start by making the new start and end states
    %     \pagebreak \bigbreak \noindent 
    %     \begin{figure}[ht]
    %         \centering
    %         \incfig{machine4}
    %         \label{fig:machine4}
    %     \end{figure}
    %     \bigbreak \noindent 
    %     Now we start eliminating states, note that it does not matter in which order we eliminate the states, but for this example we will begin by eliminating state $A$. To get from the start state $q_{0}$ to state $B$, we need to pass through $A$, to get from $A$ to $B$, we can have any number of $1's$ followed by a zero which takes us to be. Thus, the transition from $q_{0}$ to $B$ is the regular expression $1^{*}0$
    %     \bigbreak \noindent 
    %     We also have to consider the original transition from $B$ to $A$, and then back to $B$, for this we have the RE $01^{*}0$. Thus the machine becomes
    %     \bigbreak \noindent 
    %     \begin{figure}[ht]
    %         \centering
    %         \incfig{machine9}
    %         \label{fig:machine9}
    %     \end{figure}
    %     \bigbreak \noindent 
    %     To eliminate $B$, we need to consider the transitions through $B$ ie from $q_{0}$ to $q_{f}$. We know to get from $q_{0}$ to $B$ we have the RE $1^{*}0$, then from $B$ to $q_{f}$ we have $(1 + 01^{*}0)^{*} $. Thus, the transition for $q_{0}$ to $q_{f}$ is $1^{*}0(1 + 1^{*}01^{*}0)^{*} $. And the final machine with only one regular expression is 
    %     \begin{figure}[ht]
    %         \centering
    %         \incfig{machine8}
    %         \label{fig:machine8}
    %     \end{figure}
    %     \pagebreak 
%     \item \textbf{Convert RE to NFA}: Before we begin, recall order of operations (From highest to lowest )
%         \begin{enumerate}
%             \item Parenthesis  
%             \item Kleene star
%             \item Concatenation
%             \item Union (+ or \|)
%         \end{enumerate}
%         Let's consider the regular expression $aa(a+b)^{*}bb$
%         \begin{enumerate}
%             \item We start by defining simple NFA's for each symbol ($a$ and $b$)
%         \end{enumerate}
%         \begin{figure}[ht]
%             \centering
%             \incfig{machine12}
%             \label{fig:machine12}
%         \end{figure}
%         \bigbreak \noindent 
%         Then, by precedence, we design an NFA for inside the parenthesis, and then for the kleene star of the parenthesis. To make the NFA for $a + b$, we follow the rules for the union of two NFAs
%         \bigbreak \noindent 
%     \begin{figure}[ht]
%         \centering
%         \incfig{machine13}
%         \label{fig:machine13}
%     \end{figure}
%     \bigbreak \noindent 
%     In order to take the kleen star of this machine, we need to allow for zero or more repetitions. Thus,
%     \bigbreak \noindent 
% \begin{figure}[ht]
%     \centering
%     \incfig{machine14}
%     \label{fig:machine14}
% \end{figure}
% \bigbreak \noindent 
% Now we create two more NFA's, one for $aa$, and one for $bb$
% \bigbreak \noindent 
% \begin{figure}[ht]
%     \centering
%     \incfig{machine15}
%     \label{fig:machine15}
% \end{figure}
% \pagebreak \bigbreak \noindent 
% Now, we combine them all using the logic of concatenation. The final product is then
% \bigbreak \noindent 
% \begin{figure}[ht]
%     \centering
%     \incfig{machine17}
%     \label{fig:machine17}
% \end{figure}
% \bigbreak \noindent 
% Notice we added the extra epsilon transition (pink), this is to bypass the kleene star if the choice of zero occurences is executed.
%


    \pagebreak 
    \item \textbf{Properties of union, intersect, and concatenation for two FA's}: the properties of union, intersection, and concatenation for finite automata (FAs) are directly tied to the properties of regular languages.
        \bigbreak \noindent 
        \textbf{Union of two FA}
        \begin{itemize}
            \item \textbf{Closure}: The class of regular languages (those recognized by FA) is closed under union. This means the union of two regular languages is also regular, and there exists an FA that recognizes the union of the languages.
            \item \textbf{Commutative:} Union is commutative for FA, meaning the order of combining automata does not matter.
            \item \textbf{Associative:} Union is associative, so it doesn't matter how automata are grouped when performing multiple unions.
            \item \textbf{Distributive over Intersection} Union distributes over intersection for regular languages, just as with sets.
        \end{itemize}
        \textbf{Intersection of two FA}
        \begin{itemize}
            \item \textbf{Closure:} The class of regular languages is also closed under intersection, meaning there is always an FA (typically constructed as a DFA) that recognizes the intersection of two regular languages.
            \item \textbf{Commutative:} Intersection is commutative, meaning the order of combining automata doesn't matter.
            \item \textbf{Associative:} Intersection is associative, so the grouping doesn't matter.
            \item \textbf{Distributive over Union:} Intersection distributes over union for regular languages, just as with sets.
        \end{itemize}
        \bigbreak \noindent 
        \textbf{Concatenation}
        \begin{itemize}
            \item \textbf{Closure:} Regular languages are closed under concatenation.
            \item \textbf{Associativity:} Concatenation is associative. This means that the way in which you group the automata when performing concatenation doesn't matter. 
            \item \textbf{Identity Element:} The identity element for concatenation is the language that contains only the empty string,
                \begin{align*}
                    L(A)  \cdot \{\epsilon\} = L(A)
                .\end{align*}
            \item \textbf{Distributivity Over Union:} Concatenation distributes over union. This means:
                \begin{align*}
                    L(A) \cdot (L(B) \cup (L(C)) = L(A) \cdot L(B)) \cup (L(A) \cdot L(C))
                .\end{align*}
            \item \textbf{Concatenation with the Empty Set:} Concatenating any language with the empty set results in the empty set. This is because there are no strings to concatenate if one of the languages is empty:
        \end{itemize}
        \bigbreak \noindent 
        \textbf{Not commutative}






    \end{itemize}

        \pagebreak 
    \subsubsection{Regular expressions}
    \begin{itemize}
        \item \textbf{RE}: A RE corresponds to a set of strings; that is, a RE describes a language
        \item \textbf{RE three operations}:
            \begin{enumerate}
                \item Union (+)
                \item concatenation (xy)
                \item star (zero or more copies)
            \end{enumerate}
        \item \textbf{RE special symbols}
            \begin{align*}
                + \quad * \quad (\ \ )
            .\end{align*}
        \item \textbf{Grouping}: The parenthesis are used for grouping, 
        \item \textbf{Union}: the plus sign means \textbf{union}. Thus, writing
            \begin{align*}
                0 + 1
            .\end{align*}
            Means zero or one, we refer to + as "or"
        \item \textbf{Concatenation}: We concatenate simply by writing one expression after the other, with no spaces
            \begin{align*}
                (0+1)0
            .\end{align*}
            Is the pair of strings 00 and 10
        \item \textbf{Empty string}: We can also use the empty string $\epsilon$
            \begin{align*}
                (0 + 1)(0 + \epsilon)
            .\end{align*}
            corresponds to 00, 0, 10, and 1
        \item \textbf{Zero or more copies (star)}: Using the start indicates zero or more copies, thus
            \begin{align*}
                a*
            .\end{align*}
            corresponds to any string of a's: $\{\epsilon, a,aa,aaa,...\} $
        \item \textbf{More on union}:
            If you form an RE by the or of two REs, call them $R$ and $S$, then the resulting language is the union of the languages of $R$ and $S$.
            \bigbreak \noindent 
            Suppose $R = (0+1) = \{0, 1\}$, and $S=\{01(0+1)\}  = \{ 010,011\}$, then $R+S = (0+1) + (01(0+1))  = \{0,1,010, 011\}$
        \item \textbf{More on concatenation}: If you form an RE by the or of two REs, call them $R$ and $S$, then the resulting language consists of all strings that can be formed by taking one string from the language of $R$ and one string from the language of $S$ and concatenating them.
            \bigbreak \noindent 
            Suppose $R = (0+1) = \{0, 1\}$, and $S=\{01(0+1)\}  = \{ 010,011\}$, then $RS = (0+1)01(0+1) = \{0010,0011,1010,1011\}$
        \item \textbf{More on star}: If you for man RE by taking the star of an RE $R$, then the resulting language consists of all strings that can be formed by taking any number of strings from the language of $R$ (they need not be the same and they need not be different), and concatenating them.
            \bigbreak \noindent 
            Suppose $R = 01(0+1) = \{010, 011\}$, then $R^{*} = 01(0+1)* \{010, 010010, ..., 011,011011,... 010011, ...\} $
    \item \textbf{Precedence of the operations}
        \begin{enumerate}
            \item Star (*)
            \item Concatenation
            \item Union (+)
        \end{enumerate}
        \item \textbf{Recursive definition of the kleene star (closure) ($L^{*}$)}:
            \begin{enumerate}
                \item $\epsilon \in L^{*} $
                \item If $x \in L^{*}$ and $y\in L$, then $xy \in L^{*}$
            \end{enumerate}
            \bigbreak \noindent 
            \textbf{Base case:} The first rule provides a starting point by ensuring that the empty string \( \epsilon \) is in \( L^* \).
            \bigbreak \noindent 
            \textbf{Recursive step:} The second rule allows you to take any string \( x \) already in \( L^* \) and concatenate it with a string \( y \in L \) to produce a new string \( xy \in L^* \).
            \bigbreak \noindent 
            After using the second rule once to generate a new string \( xy \in L^* \), you can apply the rule again by concatenating this new string with another string from \( L \). This recursive process can continue indefinitely, generating all possible strings that can be formed by concatenating zero or more strings from \( L \).
        \item \textbf{Recursive definition of the kleene star (other)}
            \begin{enumerate}
                \item $L^{0} = \{\epsilon\}$ $\quad$ (Start with the empty string, always in the closure)
                \item $L^{i}=LL^{i-1}$ for $i>0$ $\quad$ (Start recursively building strings)
                \item $L^{*} = \bigcup_{i=0}^{\infty} L^{i}$ $\quad$ (the whole thing)
            \end{enumerate}
            \bigbreak \noindent 
            \textbf{Note:} We also define the positive closure of \( L \), denoted \( L^+ \), as \( L^* - \{\epsilon\} \) or
            \[
                L^+ = \bigcup_{i=1}^{\infty} L^i.
            \]
        \item \textbf{Closure of the empty language}: $\Phi^{*} = \{\epsilon\} $
        \item \textbf{Regular expression for the empty language}: $\Phi  = \varnothing$  is the regular expression for the empty language (empty set)
        \item \textbf{More on language composition operators}:
            The language composition operators were defined over any language and, in turn, generate new languages. As such, composition operators take any one or two languages from $P(\Sigma^{*})$ and can produce any language in $P(\Sigma^{*})$.
        \item \textbf{Regular languages (regular sets), regular expression limits}: Although regular expressions are based on language composition operators, their recursive definition (i.e., only regular expressions, therefore only languages defined by regular expressions) limits the languages that they can define.
            \bigbreak \noindent 
            \textbf{Note:} Regular expressions cannot produce all languages in $P(\Sigma^{*})$.
            \bigbreak \noindent 
            In fact, the set of languages that regular expressions can define have a special name – they are called regular languages (or sometimes regular sets).

        \item \textbf{Kleene's theorem}:
            There is an FA for a language if and only if there is an RE for the language
        \item \textbf{Regular expressions order of operations}: From highest to lowest precedence
            \begin{enumerate}
                \item Parenthesis  
                \item Kleene star
                \item Concatenation
                \item Union (+ or \|)
            \end{enumerate}
    \item \textbf{Properties of regular expressions}:
        \bigbreak \noindent 
        \textbf{Note:} Intersection is a operation not defined for regular expressions
        \bigbreak \noindent 
        \textbf{Union}
        \begin{itemize}
            \item \textbf{Commutative}:
                \[
                    R_1 \cup R_2 = R_2 \cup R_1
                \]

            \item \textbf{Associative}:
                \[
                    (R_1 \cup R_2) \cup R_3 = R_1 \cup (R_2 \cup R_3)
                \]

            \item \textbf{Identity Element}:
                \[
                    R_1 \cup \emptyset = R_1
                \]

            \item \textbf{Idempotent}:
                \[
                    R_1 \cup R_1 = R_1
                \]
        \end{itemize}
        \textbf{2. Concatenation (\(\cdot\))}
        \begin{itemize}
            \item \textbf{Non-commutative}:
                \[
                    R_1 \cdot R_2 \neq R_2 \cdot R_1
                \]

            \item \textbf{Associative}:
                \[
                    (R_1 \cdot R_2) \cdot R_3 = R_1 \cdot (R_2 \cdot R_3)
                \]

            \item \textbf{Identity Element}:
                \[
                    R_1 \cdot \epsilon = \epsilon \cdot R_1 = R_1
                \]

            \item \textbf{Concatenation with \(\emptyset\)}:
                \[
                    R_1 \cdot \emptyset = \emptyset \cdot R_1 = \emptyset
                \]
        \end{itemize}
        \bigbreak \noindent 
        \textbf{Kleene Star (\(*\))}
        \begin{itemize}
            \item \textbf{Kleene Star of \(\epsilon\)}:
                \[
                    \epsilon^* = \{\epsilon\}
                \]

            \item \textbf{Kleene Star of \(\emptyset\)}:
                \[
                    \emptyset^* = \{\epsilon\}
                \]

            \item \textbf{Idempotent}:
                \[
                    (R^*)^* = R^*
                \]
        \end{itemize}
        \bigbreak \noindent 
        \textbf{Distributive Properties}
        \begin{itemize}
            \item \textbf{Union over Concatenation}:
                \[
                    R_1 \cdot (R_2 \cup R_3) = (R_1 \cdot R_2) \cup (R_1 \cdot R_3)
                \]

            \item \textbf{Concatenation over Union}:
                \[
                    (R_1 \cup R_2) \cdot R_3 = (R_1 \cdot R_3) \cup (R_2 \cdot R_3)
                \]
        \end{itemize}
    \item \textbf{Language of a RE notation}: $L(RE)$ is the language defined by the regular expression $RE$, If we have an RE $R$, then the language $L(R)$ is the language defined by the RE $R$
    \item \textbf{When a regular expression is the empty set $\varnothing$}: When a regular expression (RE) represents the empty set it means that the RE matches no strings at all, not even the empty string.
        \bigbreak \noindent 
        The language is then
        \begin{align*}
            L(\varnothing) = \Phi        
        .\end{align*}
        Where $\Phi$ denotes the empty language
    \item \textbf{One or more occurences $RR^{*}$}: We denote this by plus instead of star, ie $RR^{*} = R^{+}$, but you also must redefine union as $\mid$ instead of $+$
    \item \textbf{Simplifying regular expressions (Some can also be found above in properties)}:
        \begin{itemize}
            \item \textbf{Concatenation of stars}: $(R^{*})^{*}  = R^{*}$
            \item \textbf{Concatenation of Repeated Expressions}: $R^{*}R^{*} = R^{*} $
            \item \textbf{Idempotence of Union}: $R\mid R = R$
            \item \textbf{Empty Set in Union and Concatenation:} $R \mid \varnothing  = R$, $R\varnothing = \varnothing $
            \item \textbf{Empty string in concatenation}: $\epsilon R = R\epsilon = R $
            \item \textbf{Union with the kleene star}: $R^{*}\mid R = R^{*}$
            \item \textbf{Distributive Property:} $R_{1}(R_{2} \mid R_{3}) = R_{1}R_{2} \mid R_{1}R_{3} $
            \item \textbf{Absorption:} $R\mid (RR^{*})  = RR^{*} = R^{+}$
        \end{itemize}
    \item \textbf{The RE operators with the empty language $\Phi$}:
        \begin{enumerate}
            \item $\varnothing r  = r\varnothing = \varnothing\varnothing = \varnothing$ for any regular expression $r$
            \item $r + \varnothing = \varnothing + r = r $
            \item $\varnothing + \varnothing = \varnothing $
            \item $\varnothing^{*} = \{\epsilon\} $
        \end{enumerate}
        These cases can also be represented with language notation
        \begin{enumerate}
            \item $\Phi L  = L\Phi = \Phi\Phi = \Phi$\ $\forall L$
            \item $L + \Phi = \Phi + L = L $
            \item $\Phi + \Phi = \Phi $
            \item $\Phi^{*} = \{\epsilon\} $
        \end{enumerate}
    \item \textbf{Convert RE to NFA-$\epsilon$}: The conversion algorithm starts by defining an NFA with $\epsilon$-moves for each of the three base cases from the recursive definition of a regular expressions over an alphabet $\Sigma$
        \begin{enumerate}
            \item $\varnothing$ is a regular expression and denotes the empty set (i.e., the empty language $\Phi$)
            \item $\epsilon$ is a regular expression and denotes the set $\{\epsilon\}$
            \item For each symbol $x \in \Sigma$, $x$ is a regular expression and denotes the set $\{x\} $.
        \end{enumerate}
        \bigbreak \noindent 
        Conditions on the NFAs with $\epsilon$-moves for This Algorithm
        \begin{enumerate}
            \item  There must be exactly one accepting state.
            \item No transitions (not even $\epsilon$-moves) may leave the one accepting state.
        \end{enumerate}
        \bigbreak \noindent 
        \textbf{Note:} If faced with an NFA with $\epsilon$-moves that has more than one accepting state and/or accepting states with transitions leaving it then simply modify the NFA with $\epsilon$-moves by
        \begin{enumerate}
            \item Adding a new accepting state.
            \item Add an $\epsilon$-move from each of the original accepting states to the newly added accepting state.
            \item Convert all of the original accepting states to non-accepting states.
        \end{enumerate}
        \bigbreak \noindent 
        The three base cases have the following nfa that satisfy the above criteria
        \bigbreak \noindent 
\begin{figure}[ht]
    \centering
    \incfig{crit2}
    \label{fig:crit2}
\end{figure}
\bigbreak \noindent 
We use those NFAs as the basic building blocks to iteratively build more complex NFA’s with $\epsilon$-moves (all the while honoring the accepting state conditions for this algorithm) as we apply the recursive part of the regular expression definition. Recall:
\bigbreak \noindent 
If $r$ and $s$ are regular expressions denoting the sets $R$ and $S$, respectively, then
\begin{enumerate}
    \item $r+s$ is a regular expression denoting the set $R + S$, (i.e,. union of languages),
    \item $rs$ is a regular expression denoting the set $RS$ (i.e., concatenating languages), and
\item $r*$ is a regular expression denoting the set $R^{*}$ (i.e., Kleene closure of a language).
\end{enumerate}
\bigbreak \noindent 
Once we define an NFA with $\epsilon$-moves for each of the base cases (which we have done) then when we address each recursive part of the definition (e.g., union above)
\begin{enumerate}
    \item We may assume that there already exists NFAs with $\epsilon$-moves for each of the regular expressions $r$ and $s$ (and that each also satisfies the acceptance state conditions of this algorithm) and 
    \item Then our job is to use those NFAs with $\epsilon$-moves to create a new NFA with $\epsilon$-moves that accepts $r+s$ and that also satisfies the acceptance state conditions of this algorithm.
\end{enumerate}
\bigbreak \noindent 
\textbf{The algorithm:}
\begin{itemize}
    \item \textbf{Handling union:} We start by assuming there already exists NFAs with $\epsilon$-moves $M_{1}$ and $M_{2}$ that accept regular expressions $r$ and $s$, respectively, and that both $M_{1}$ and $M_{2}$ satisfy the acceptance state conditions (i.e., one accepting state, no exit) of this algorithm.
        \bigbreak \noindent 
        \begin{figure}[ht]
            \centering
            \incfig{crit3}
            \label{fig:crit3}
        \end{figure}
        \bigbreak \noindent 
        \textbf{Note:} The details of the machine arn't important here, all we know is the machine has a start, does whatever else it needs to (repesented by the elipsis), and then accepts strings represented by $r$ in the top machine and $s$ in the bottom
        \bigbreak \noindent 
        We then use these machines $M_{1}$ and $M_{2}$ to create a new machine $M$ that accepts $r+s$
        \bigbreak \noindent 
        \begin{figure}[ht]
            \centering
            \incfig{crit4}
            \label{fig:crit4}
        \end{figure}
        \bigbreak \noindent 
        So what did we do here
        \begin{enumerate}
            \item Create new start state and add $\epsilon$-moves to the original start states
            \item Create new accepting state and add emoves from all the original accepting states.
            \item Change the original accepting states to non-accepting states.
        \end{enumerate}
    \item \textbf{Handle Concatenation}: We again start by assuming there already exists NFAs with $\epsilon$-moves $M_{1}$ and $M_{2}$ that accept regular expressions $r$ and $s$, respectively, and that both $M_{1}$ and $M_{2}$ satisfy the acceptance state conditions (i.e., one accepting state, no exit) of this algorithm.
        \bigbreak \noindent 
        \begin{figure}[ht]
            \centering
            \incfig{crit5}
            \label{fig:crit5}
        \end{figure}
        \bigbreak \noindent 
        We use $M_{1}$ and $M_{2}$ to construct new NFA with $\epsilon$-move $M$ that accepts $rs$.
        \bigbreak \noindent 
        \begin{figure}[ht]
            \centering
            \incfig{crit6}
            \label{fig:crit6}
        \end{figure}
        \bigbreak \noindent 
        \begin{enumerate}
            \item Add an $\epsilon$-move from $M_{1}$’s accepting state to $M_{2}$’s start state.
            \item Change $M_{1}$’s accepting state to a nonaccepting state.
        \end{enumerate}
        \pagebreak 
    \item \textbf{Handle Kleene closure}: We again start by assuming there already exists an NFA with $\epsilon$-moves $M_{1}$ that accepts regular expressions $r$ and that satisfies the acceptance state conditions (i.e., one accepting state, no exit) of this algorithm.
        \bigbreak \noindent 
        \begin{figure}[ht]
            \centering
            \incfig{crit7}
            \label{fig:crit7}
        \end{figure}
\end{itemize}
\bigbreak \noindent 
We use $M_{1}$ to construct new NFA with $\epsilon$-move $M$ that accepts $r^{*}$
\begin{figure}[ht]
    \centering
    \incfig{crit9}
    \label{fig:crit9}
\end{figure}
\bigbreak \noindent 
\begin{enumerate}
    \item Create new start and accepting states.
    \item Add $\epsilon$-move from new start to $M_{1}$ start, $M_{1}$ accepting to new accepting, and new start to new accepting.
    \item Add $\epsilon$-move from $M_{1}$ accepting to $M_{1}$ start. 
    \item Change $M_{1}$'s accepting state to a non accepting state.
\end{enumerate}

\item \textbf{RE to NFA conversion: Special cases}: Recall the special case with regular expressions, the empty language $\Phi$ – and how it behaved with the three regular expression operators;
    \begin{enumerate}
        \item $\Phi L  = L\Phi = \Phi\Phi = \Phi$\ $\forall L$
        \item $L + \Phi = \Phi + L = L $
        \item $\Phi + \Phi = \Phi $
        \item $\Phi^{*} = \{\epsilon\} $
    \end{enumerate}
    \bigbreak \noindent 
    \pagebreak \bigbreak \noindent 
    We can now check these operations using the algorithm with $\Phi $. First, we define the base case NFA's
    \bigbreak \noindent 
    \begin{figure}[ht]
        \centering
        \incfig{base}
        \label{fig:base}
    \end{figure}
    \bigbreak \noindent 
    \begin{enumerate}
        \item Confirming $L+\Phi = \Phi+L= L$ and $\Phi+\Phi = \Phi$:
            \bigbreak \noindent 
            \begin{figure}[ht]
                \centering
                \incfig{base2}
                \label{fig:base2}
            \end{figure}
            \pagebreak 
        \item Confirming $\Phi L = L\Phi = \Phi\Phi = \Phi$
            \bigbreak \noindent 
            \begin{figure}[ht]
                \centering
                \incfig{base6}
                \label{fig:base6}
            \end{figure}
        \item Confirming $\Phi^{*} = \{\epsilon\}$
            \bigbreak \noindent 
            \begin{figure}[ht]
                \centering
                \incfig{base7}
                \label{fig:base7}
            \end{figure}
    \end{enumerate}

    \item \textbf{Convert NFA-$\epsilon$ to RE}:
        If necessary, first modify the NFA with $\epsilon$-moves to satisfy these two conditions (i.e., conditions of this algorithm, not requirements of all NFA’s with $\epsilon$-moves);
        \begin{enumerate}[label=\alph*)]
            \item No transition may enter the start state – not even a loop. 
            \item  If there exists even one accepting state, then there can be only one accepting state and no transition may leave that accepting state.
        \end{enumerate}
        \bigbreak \noindent 
        \textbf{Note:} If there was no accepting state then do not create one, stop the algorithm, and output the regular expression $\varnothing$ to denote the empty language $\Phi$.
        \bigbreak \noindent 
        Then we start the algorithm. We need to convert the label on each transition to a regular expression until there is only two states, a start state and an accepting state, and all transitions between these two states are regular expressions. The final regular expression will be the union of all transitions.
        \begin{enumerate}
            \item  While there are more “middle” states (i.e., states that are neither the start state or accepting state)
                \begin{enumerate}[label=(\roman*)]
                    \item Select one of the remaining middle states.
                    \item Bypass the middle state creating new transitions as necessary annotating each new transition with a regular expression.
                    \item Remove the bypassed middle state.
                \end{enumerate}
            \item If there are any transitions between the start and accepting state, then the regular expression that accepts the same language as the original FA is the the union (i.e., “+”) of the regular expressions of all the transitions. 
                \bigbreak \noindent 
                If there are no transitions between the start and accepting state, then output the regular expression $\varnothing$ to denote the empty language $\Phi$.
        \end{enumerate}
        \bigbreak \noindent 
        Recall the following from the definition of regular expressions;
        \begin{enumerate}[label=(\roman*)]
            \item \relax [base case]: $L$ is a regular expression and denotes the set $\{L\}$
            \item  \relax [base case]: For each symbol $x\in\Sigma$, $x$ is a regular expression and denotes the set $\{x\}$
            \item  \relax [recursive case]: $r+s$ is a regular expression denoting the set $R + S$, (i.e,. union of languages).                       
        \end{enumerate}
        \bigbreak \noindent 
        We use those to covert every $\epsilon$ to a $\Lambda$, every symbol $x\in\Sigma$ to a regular expression of the same symbol, and every case comma-separated transition label to a regular expression with “+”.
        \bigbreak \noindent 
        After ensuring the FA abides by the start and end state conditions, and we convert every transition to the simple regular expressions, we begin eliminating states.
        \bigbreak \noindent 
        \textbf{Some notes:}
        \begin{enumerate}[label=(\alph*)]
            \item Regarding the middle states (states that are neither the start nor accepting state), it doesn't matter in which order we choose to eliminate them.
            \item For each state we are eliminating, we count the number of incoming and outgoing transitions (loops don't add to the count but we still need to take care of them with the regular expressions), there will be a new regular expression transition for all combinations of outgoing and incoming transitions. Ie pick a state to eliminate, then
                \begin{align*}
                    \text{New RE transitions} = N(\text{outgoing}) \times N(\text{incoming}) 
                .\end{align*}
                Not including the loops
        \end{enumerate}
        \pagebreak \bigbreak \noindent 
        \textbf{Example:} Consider the NFA-$\epsilon$
        \bigbreak \noindent 
        \begin{figure}[ht]
            \centering
            \incfig{re1}
            \label{fig:re1}
        \end{figure}
        \bigbreak \noindent 
        Before we begin eliminating states, we see that this FA does not obey the two constraints described above. So, we create a new accepting state such that there is only one accepting state. Each old accepting state has $\epsilon$ transitions to this new accepting state. This FA has no incoming transitions to the start state so nothing to fix there.
        \bigbreak \noindent 
        \begin{figure}[ht]
            \centering
            \incfig{re2}
            \label{fig:re2}
        \end{figure}
        \bigbreak \noindent 
        Now, we start eliminating states one at a time. We recall that it does not matter the order in which we eliminate them.
        \bigbreak \noindent 
        We start by eliminating state $X$. There is one incoming transition and one outgoing transition. Thus, there is $1\times 1 = 1$ new transition. To get from the start state, through $X$, to the accepting state, the regular expression is $aa^{*}\Lambda = aa^{*}$. Thus,
        \bigbreak \noindent 
\begin{figure}[ht]
    \centering
    \incfig{re3}
    \label{fig:re3}
\end{figure}
\bigbreak \noindent 
Next, we choose to eliminate state $Z$. We have one incoming and two outgoing transitionss. Thus, we have $1\times 2  = 2$ new regular expression transitions. To get from $Y$ through $Z$ to the accepting state, the RE is $b\epsilon =  b$. To go from $Y$ through $Z$ back to $Y$, we have $ba$. Thus
\bigbreak \noindent 
\begin{figure}[ht]
    \centering
    \incfig{re4}
    \label{fig:re4}
\end{figure}
\bigbreak \noindent 
Finally, we eliminate $Y$. To get from the start state, through $Y$, to the accepting state, the regular expression is $a(ba)^{*}b$. Thus, 
\bigbreak \noindent 
\begin{figure}[ht]
    \centering
    \incfig{re5}
    \label{fig:re5}
\end{figure}
\bigbreak \noindent 
The final regular expression is then 
\begin{align*}
    aa^{*} + \epsilon + a(ba)^{*}b
.\end{align*}
\blacksquare










    \end{itemize}

    \pagebreak 
    \subsubsection{Properties of regular languages}
    \begin{itemize}
        \item \textbf{Recall: Regular language}: Recall that we call a language a regular language if, and only if, the language is accepted by some regular expression. 
        \item \textbf{Recall: Recursive definition of regular expressions}: Recall also our recursive definition of regular expressions over some alphabet $\Sigma $
            \bigbreak \noindent 
            Let $\Sigma$ be an alphabet. The regular expressions over $\Sigma$ and the sets (i.e., languages) that they denote are defined recursively as follows:
            \bigbreak \noindent 
            \textbf{Base cases:}
            \begin{enumerate}
                \item $\varnothing$ is a regular expression and denotes the empty set (i.e., the empty language $\Phi$).
                \item $L$ is a regular expression and denotes the set $\{L\}$.
                \item For each symbol $x\in \Sigma$, $x$ is a regular expression and denotes the set $\{x\}$.
            \end{enumerate}
            \bigbreak \noindent 
            \textbf{Recursion:} If $r$ and $s$ are regular expressions denoting the sets $R$ and $S$, respectively, then
            \begin{enumerate}
                \item $r+s$ is a regular expression denoting the set $R + S$, (i.e,. union of languages),
                \item $rs$ is a regular expression denoting the set $RS$ (i.e., concatenating languages), and
                \item $r^{*}$ is a regular expression denoting the set $R^{*}$ (i.e., Kleene closure of a language).
            \end{enumerate}
        \item \textbf{Relationship between regular languages and the set of all possible languages $\mathcal{P}(\Sigma^{*})$}:
            We know that the set of regular languages must be a subset of $\mathcal{P}(\Sigma^{*})$ (it may be equal to $\mathcal{P}(\Sigma^{*})$), and so, we can start by creating that subset.
            \bigbreak \noindent 
            We can then start noting the languages that we know are regular based on the base cases from the definition of regular expressions and for some given alphabet, say $\Sigma = \{a, b\}$.
            \bigbreak \noindent 
            The recursion from the definition tells us that we can take any language, or pair of languages, from the already existing set of regular languages, and use it/them to create a new language this is also a regular language. And the recursion may be applied over and over (i.e., without limit), always taking only regular languages that have been previously created (original base case languages or languages subsequently derived), to create new languages. (i.e., the set of regular languages is infinite).
            \bigbreak \noindent 
        \item \textbf{Closure of regular languages and their operations}: Expanding on the item above, formally, we say that the set of regular languages is \textbf{closed} under the language composition operations union, concatentation, and Kleene star
        \item \textbf{Closure of complement and intersection}: In addition to the three operations that come from the definition of regular expressions (i.e., union, concatenation, and Kleene star), the set of regular languages is also closed under;
            \begin{itemize}
                \item Complement
                \item Intersection
            \end{itemize}
        \item \textbf{Complement of a Language}: Recall that every language is a set of strings – empty, finite, or infinite – that is always a subset of $\Sigma^{*}$
            \bigbreak \noindent 
            That is,
            \begin{itemize}
                \item For any alphabet $\Sigma$ we get $\Sigma^{*} $
                \item We define some language $L$ over that alphabet $\Sigma$
                \item Then $L$ is a subset of $\Sigma^{*}$; $L \subseteq \Sigma^{*}$
            \end{itemize}
            \bigbreak \noindent 
            We define a new language, the complement of $L$, denoted $L^{\prime}$ as the set of strings that are not in the language $L$ (i.e., $L^{\prime} = \Sigma^{*}  - L$).
        \item \textbf{Proof: Regular languages are closed under complement}: We assert that if you take the complement of a regular language, the resulting language is then regular
            \bigbreak \noindent 
            \textbf{Proof:}
            \begin{enumerate}
                \item A regular language is one that is accepted by some regular expression.
               \item By Kleene’s Theorem we know that any regular expression can be converted to a NFA with $\epsilon$-moves that accepts the same language, and vice versa.
                \item  We can convert any NFA with $\epsilon$-moves to a DFA that accepts the same language and every DFA is, by definition, an NFA with $\epsilon$-moves. 
            \end{enumerate}
            So $L$ is a regular language iff it is accepted by some DFA...
            \bigbreak \noindent 
            Consider some DFA $M(Q, \Sigma, q_{0}, T, \delta)$ that accepts regular language $L$.
            \bigbreak \noindent 
            We construct a new DFA $M^{\prime}(Q, \Sigma, q_{0}, T^{\prime}, \delta)$ from M by defining $T^{\prime} = Q - T$, that is, every accepting state in $M$ becomes a non- accepting state in $M^{\prime}$, and vice versa.
            \bigbreak \noindent 
            Since $M^{\prime}$ accepts $L^{\prime}$ and $M^{\prime}$ is a DFA, then $L^{\prime}$ is a regular language.
            \bigbreak \noindent 
            Since M was chosen arbitrarily, the complement of any regular language is also a regular language
            \bigbreak \noindent 
            \blacksquare
            \bigbreak \noindent 
            \textbf{Note:} Note: The proof must be based on DFA’s ... would not have worked for non-deterministic FA’s.
        \item \textbf{Intersection of Languages}:
            Given any two languages, $L_{1}$ and $L_{2}$, over some alphabet $\Sigma$ we can create a new language $L$ that is the intersection of the two sets $L_{1}$ and $L_{2}$.
            \bigbreak \noindent 
            That is, 
            \begin{align*}
                L = L_{1} \cap L_{2} = \{x:\ x \in L_{1} \land x\in L_{2} \}
            .\end{align*}
        % \item \textbf{Proof: Regular languages are closed under intersection}: This means that when you take the intersection of any two regular languages the language you produce is always regular. 
        \item \textbf{Proof: Regular languages are closed under intersection}: This means that when you take the intersection of any two regular languages, the resulting language is always regular.
            \bigbreak \noindent 
            Assume you have two regular languages \(L_1\) and \(L_2\).
            \bigbreak \noindent 
            We define a new language \(L\), which is the intersection of \(L_1\) and \(L_2\), namely
            \[
                L = \{x \mid x \in L_1 \land x \in L_2\},
            \]
            \begin{center}
                where ``\(\land\)'' means "logical and."
            \end{center}
            \bigbreak \noindent 
            Demorgan's law:
            \begin{align*}
                (a \land b) = \sim(\sim a \lor \sim b)
            .\end{align*}
            \bigbreak \noindent 
            To prove that \(L = L_1 \cap L_2\) is regular, we use the fact that regular languages are closed under complement and union. This leads us to apply De Morgan's Law:
            \[
                L_1 \cap L_2 = \sim (\sim L_1 \cup \sim L_2).
            \]
            \bigbreak \noindent 
            Here, \(\sim L_1\) and \(\sim L_2\) represent the complements of \(L_1\) and \(L_2\), respectively. Since \(L_1\) and \(L_2\) are regular, their complements \(\sim L_1\) and \(\sim L_2\) are also regular (because regular languages are closed under complement).
            \bigbreak \noindent 
            Next, since regular languages are closed under union, the language \(\sim L_1 \cup \sim L_2\) is also regular.
            \bigbreak \noindent 
            Finally, the complement of \(\sim L_1 \cup \sim L_2\), i.e., \((\sim L_1 \cup \sim L_2)'\), is also regular because regular languages are closed under complement. But \((\sim L_1 \cup \sim L_2)'\) is precisely \(L_1 \cap L_2\).
            \bigbreak \noindent 
            Thus, \(L = L_1 \cap L_2\) is regular, as required.
            \[
                L_1 \cap L_2 = \sim (\sim L_1 \cup \sim L_2) = \{x \mid x \in L_1 \land x \in L_2\}.
            \]
            \bigbreak \noindent 
            \(\blacksquare\)
        \item {Addings to regular expression recursive}: Thus, we add complement and intersection to recursive cases when building regular languages defined in $\mathcal{P}(\Sigma^{*})$
            \bigbreak \noindent 
            \textbf{Recursion:} If $r$ and $s$ are regular expressions denoting the sets $R$ and $S$, respectively, then
            \begin{enumerate}
                \item $r+s$ is a regular expression denoting the set $R + S$, (i.e,. union of languages),
                \item $rs$ is a regular expression denoting the set $RS$ (i.e., concatenating languages), and
                \item $r^{*}$ is a regular expression denoting the set $R^{*}$ (i.e., Kleene closure of a language).
            \end{enumerate}
            \bigbreak \noindent 
            or by taking the complement or intersection of one or two previously created regular languages.


            % \bigbreak \noindent 
            % Assume you have two regular languages \(L_1\) and \(L_2\).
            % \bigbreak \noindent 
            % We define a new language \(L\) that is the intersection of \(L_1\) and \(L_2\), namely \(L = \{x \mid x \in L_1 \land x \in L_2\}\),
            % \begin{center}
            %     where ``\(\land\)'' means ``logical and''.
            % \end{center}
            % \bigbreak \noindent 
            % By DeMorgan's Law (logic) we know that \((a \land b) \equiv (\sim a \lor \sim b)\),
            % \begin{center}
            %     where ``\(\sim\)'' means ``logical negation, or not'' and ``\(\lor\)'' means ``logical or''.
            % \end{center}
            % \bigbreak \noindent 
            % We can use DeMorgan's Law to re-write \(L\) as follows:
            % \[
            %     L = \{x \mid x \in L_1 \land x \in L_2\} = \sim \{x \mid x \notin L_1 \lor x \notin L_2\} = \sim(L_1' \cup L_2')'
            % \]
            % \bigbreak \noindent 
            % Since \(L_1\) and \(L_2\) are regular languages, then we know that their complements \(L_1'\) and \(L_2'\) are also regular languages.
            % \bigbreak \noindent 
            % Since \(L_1'\) and \(L_2'\) are regular languages, then we know that their union \(L_1' \cup L_2'\) is also a regular language.
            % \bigbreak \noindent 
            % Since \(L_1' \cup L_2'\) is a regular language, then we know its complement \((L_1' \cup L_2')' = L\) is also a regular language.
            % \bigbreak \noindent 
            % Since \(L_1\) and \(L_2\) were two arbitrarily chosen regular languages, then the intersection of any two regular languages is also a regular language.
            % \bigbreak \noindent 
            % It then follows from $L = \{x \mid x \in L_1 \land x \in L_2\} = \sim \{x \mid x \notin L_1 \lor x \notin L_2\} = \sim(L_1' \cup L_2')'$
            % \begin{align*}
            %     \sim(L^{\prime}_{1} \cup L^{\prime}_{2})     &= L_{1} \cap L_{2}
            % .\end{align*}
            %
            % \blacksquare
    \end{itemize}

    \pagebreak 
    \subsubsection{Applications of finite automata}
    \begin{itemize}
        \item \textbf{FA with output}:
            Thus far we have only considered finite automata as language acceptors (i.e., defining some regular language).
            \bigbreak \noindent 
            Finite automata can also serve another purpose. They can be used to process an input string to produce some output.
            \bigbreak \noindent 
            When used in this way, the finite automata do not define a language. In fact, they do not have any accepting states.
            \bigbreak \noindent 
            Their sole purpose is to process input to generate output
        \item \textbf{Moore machine}: A Moore machine is a deterministic finite automaton and is
            defined by a 6-tuple $(Q, \Sigma, q_{0}, \delta, \Gamma, O)$, where
            \begin{itemize}
                \item $\Gamma$ is an alphabet of output symbols.
                \item $O$ is the output function: $O:\ Q \to \Gamma$
            \end{itemize}
            \bigbreak \noindent 
            Each state is annotated with an output symbol. Output ”printed” upon entering state
            \bigbreak \noindent 
            \textbf{Note:} start state’s output symbol always “printed”, even on empty string $\epsilon$
        \item \textbf{Mealy Machine}: A Mealy machine is a deterministic finite automaton and is defined by a 5-tuple $(Q, \Sigma, q_{0}, \delta, \Gamma)$, where 
            \begin{itemize}
                \item $\Gamma$ is an alphabet of output symbols
            \end{itemize}
            \bigbreak \noindent 
            Each transition is annotated with an output symbol Output ”printed” when traversing edge
            \bigbreak \noindent 
            \textbf{Note:} The number of input and output symbols are always identical.
            \bigbreak \noindent 

    \end{itemize}

    \pagebreak 
    \subsection{Nonregular languages}
    \begin{itemize}
    \item \textbf{Intro to nonregular languages}: Suppose we have some alphabet $\Sigma$, we can then find $\Sigma^{*}$, which is the language consisting of all possible strings (including the empty string $\epsilon$) using the symbols from $\Sigma$. The we can take the power set of $\Sigma^{*}$ to get the set of all possible subsets of $\Sigma^{*}$. The alphabet $\Sigma$ is always finite, if $\Sigma$ is nonempty, then $\Sigma^{*}$ is always infinite. (If $\Sigma = \varnothing$, $\Sigma^{*} = \{\epsilon\} $). Since $\Sigma^{*}$ is infinite, $\mathcal{P}(\Sigma^{*})$ is also infinite. If $\Sigma = \varnothing \implies \Sigma^{*} = \{\epsilon\} \rightarrow \mathcal{P}(\Sigma^{*}) = \{\{\epsilon\}, \varnothing\}$. Note that for any set $S$, $\varnothing \subset S$.
        \bigbreak \noindent 
        What types of languages are contained in $\mathcal{P}(\Sigma^{*})$? 
        \begin{enumerate}
            \item If a language is finite, it is always regular. This is a consequence of Kleene's theorem, which asserts that if a regular expression expresses a language, the language is regular. If a language is finite, we can \textbf{always} make a regular expression for it. Simply take all strings in the language, and take the union. For example, if $L = \{a,ab,abc\}$, then a regular expression for the language is simply $a + ab + abc$ and the language is therefore regular. $\quad \blacksquare$ 
            \item We also know that there are infinitely many regular expressions than can express infinitely many languages from $\mathcal{P}(\Sigma^{*})$. This is a consequence of the recursive definition of regular expressions. Therefore there are infinitely many regular expressions to describe infinite languages.
        \end{enumerate}
        So, $\mathcal{P}(\Sigma^{*})$ is the infinite set of all possible languages, finite or infinite, that can be generated from a given language $\Sigma$. All Finite languages from this set are regular, and there are also infinitely many infinite languages from this set that are regular. But, are there any languages that are \textit{not} regular? Infinite languages that cannot be expressed as a regular expressions? 
        \bigbreak \noindent 
        It may seem unlikely that nonregular languages exist at all. To claim that a language is nonregular one must prove that no regular expression or FA that accepts the language exists
        \bigbreak \noindent 
        \begin{figure}[ht]
            \centering
            \incfig{p2}
            \label{fig:p2}
        \end{figure}
        \pagebreak 
    \item \textbf{The Pumping Lemma}: Before we continue our conversation about nonregular languages we first look at a lemma about regular languages
        \bigbreak \noindent 
        \textbf{Lemma.} Let $L$ be an infinite regular language, then
        \begin{align*}
            \exists\ x,y,z \in \Sigma^{*} \mid y\ne \epsilon \land xy^{n}z \in L\ \forall\ n> 0
        .\end{align*}
        In other words, all regular languages have the property that, there exists some strings $x,y,z$, where $y\ne \epsilon$ such that all the strings of the form $xy^{n}z \in L$ for all $n>0 $
        \bigbreak \noindent 
        This means that for some $x,y,z$, we can infintely pump in more copies of $y$, and the string remains in the language. For example,
        \begin{align*}
            xyz \quad xyyz \quad xyyyz \quad ...
        .\end{align*}
    \item \textbf{Pumping Lemma Proof}: Assume you have some regular language $L$ with infinitely many strings. Because $L$ is regular, there must exists some DFA $M(Q, \Sigma, q_{0}, T, \delta) $ that accepts $L$. Since FA's are required to have finite states, let's say it has $n$ states.
        \bigbreak \noindent 
        Because $L$ is infinite, we can always find strings that have length greater than $n$. Ie $\abs{w} \ge n$, where $w\in L$. Because $w$ has at least as many characters as there are states in $M$, as we process $w$ with $M$ we know that one or more of the states in $M$ must be revisited. We know that as we process $w$, it must traverse what we call the \textit{circuit}. which is a sequence of one or more edges that contains at least one state that is visited more than once.
        \bigbreak \noindent 
        Given that circuit and because we also selected $w \in L$, we note that we can modify $w$ to create a new word $w^{\prime}$ pumping into $w$ as many symbols as are needed and in just the right location in $w$ that would cause $M $ to traverse the circuit one more time than it did when processing $w$. We note that the new word $w^{\prime} \in L$.
        \bigbreak \noindent 
        In fact, given DFA $M$ with $n$ states and string $w \in L$, $|w| \geq n$, we can generate an infinite supply of new words by simply pumping into $w$, and $t$ the right location, more and more copies of the string that causes $M$ to traverse the circuit. We note that the new words created in this way are all in $L$.
        \bigbreak \noindent 
        This gives us the existence of the $x,y,z$ strings the Pumping Lemma needs as follows:
            \begin{itemize}
                \item $x$ is the prefix of $w$ that is consumed by $M$ as the DFA wanders up to the circuit (x may be $\Lambda$ and this sequence of states may be empty).
                \item $y$ is the substring of $w$ that is consumed by $M$ as the DFA traverses the circuit (since the circuit must visit at least one state more than once, it must consume some symbols, and so $y$ cannot be $\Lambda$).
                \item $z$ is the suffix of $w$ that is consumed by $M$ as the DFA leaves the circuit and goes to an accepting state (z may be $\Lambda$ and this sequence of states may be empty).
            \end{itemize}
            \bigbreak \noindent 
            Therefore, $L$ must contain all the strings of the form $xy^{n}z$ for all $n>0$.
            \bigbreak \noindent 
            \pagebreak \bigbreak \noindent 
\begin{figure}[ht]
    \centering
    \incfig{p3}
    \label{fig:p3}
\end{figure}
\bigbreak \noindent 
\item \textbf{The value of the pumping lemma}: The Pumping Lemma tells us something that is true of all regular infinite languages. 
    \bigbreak \noindent 
    The real value of The Pumping Lemma is to prove that some infinite language is nonregular, that is a language \textbf{cannot} be regular if it does not satisfy the claim in the pumping lemma. Thus, we can prove that a language is non-regular by contradiction.
    \bigbreak \noindent 
    \begin{enumerate}
        \item Assume the language is regular
        \item You show that it is not possible to find strings $x,y,z$ that satisfy the claim in the pumping lemma.
        \item We conclude that our assumption that the original language is a regular language must be false, and therefore, it must be nonregular.
    \end{enumerate}
\item \textbf{Note on the pumping lemma}: The pumping lemma gaurantees that for a large enough $w\in L$ ($\abs{w} \geq n$), where $n$ is the number of states in the assumed machine) we can find an $x,y,z$ such that $y\ne \epsilon$ and $xy^{n}z \in L\ \forall\ n > 0$. If the word you select is greater than or equal to $n$, and no such $y$ holds, then the language must be nonregular
    \bigbreak \noindent 
    The condition is $ \geq n$, because, for a machine with $n$ states, there are $n-1$ edges that must be traversed to reach the end. If a machine has four states, then there are $4-1 = 3$ edges to reach the end.
\item \textbf{The pumping language with length: Background}: In the case that you find an $x,y,z$ such that $y \ne \epsilon$ and $xy^{n}z \in L\ \forall\ n> 0$. This does \textbf{not} mean the language must be regular. We know that all regular languages do have this property, but that does not mean that simply possessing this property makes the language regular. In logic theory
    \begin{align*}
        a \rightarrow b 
        \not\implies b\rightarrow a
    .\end{align*}
    In words, if $a$ implies $b$, $b$ does not imply $a$
    \bigbreak \noindent 
    Thus, the pumping lemma described above is sometimes not enough, and we may need something more powerful.

\item \textbf{The pumping lemma with length}:
    Let $L$ be an infinite language accepted by a FA with $n$ states. Then, for all $w\in L$, where $\abs{w} \geq n$, there exists some three strings $x,y,z$ such that $w=xyz$, $y\ne \epsilon$, $\abs{xy} \leq n$, and all the strings of the form $xy^{i}z \in L$ for all $i>0$
    \bigbreak \noindent 
    The Pumping Lemma With Length adds that you must always be able to find $x,y,z$ in all sufficiently long words $w\in L$
    This means:
    \begin{itemize}
        \item For each word in $L$ that has length greater than $n$, where $n$ is the number of states in the assumed machine, there must be a composition $xyz$, where $x,y,z \in \Sigma^{*}$, $y\ne \epsilon$, and $\abs{xy} \leq n$. The $x,y,z$ need not be the same for every  word in $L$ with length greater than $n$, but a pair needs to exist for each word.
        \item Furthermore, for each word $w$, where $\abs{w} \geq n$, that has $x,y,z$ such that $w = xyz$. That same $x,y,z$ needs to have the property $xy^{i}z \in L \ \forall\ i>0$.
        \item Thus, to show that a language is not regular, we only need to show that such an $x,y,z$ does not exist for a single word. If we choose a word and $x,y,z$ exists, we need to keep looking.
    \end{itemize}
    If words keep leading to valid $x,y,z$ at some point we need to stop looking and start trying to prove that the language is actually regular. Whether by creating an RE or an FA. Keep in mind there will be infinite words to check.
    \end{itemize}

    \pagebreak 
    \subsubsection{Pumping lemma examples}
    \begin{itemize}
 \item \textbf{Pumping lemma example}: Use the pumping lemma to show that the language $L = a^{k}b^{k}\ \forall\ k > 0$ over $\Sigma = \{a,b\}$ is nonregular
    \bigbreak \noindent
    Suppose that $L$ is regular, then we should be able to find some $x,y,z$, where $y \ne \epsilon$ such that $xy^{n}z \in L\ \forall\ n > 0$. For simplicity, let's first examine the possible choices for $y$
    \begin{enumerate}
        \item $y = a^{\ell}$ or $y=b^{\ell}$ for some $\ell > 0$
        \item $y = a^{\ell}b^{\lambda}$ for some $\ell, \lambda > 0$
    \end{enumerate}
    In case one, pumping would lead to an imbalance in the number of $a$'s or $b$'s. In case two, pumping would lead to more than one $ab$ boundary.
    \bigbreak \noindent 
    Thus, no such $x,y,z$ exists and the language is nonregular $\quad \blacksquare $
    \bigbreak \noindent 
\item \textbf{Pumping lemma example:} Show that the language $L =  \{a^{t}:\ t \in \mathbb{P}\} $ over $\Sigma = \{a\}$ is nonregular.
    \bigbreak \noindent 
    Suppose $L$ is regular. Then we will find strings $x,y,z$ such that $y\ne \epsilon$ and $xy^{n}z \in L\ \forall\ n>0$.
    \bigbreak \noindent 
     The only choice of $y$ in this case is $y=a^{r},\ r>0$. 
     \bigbreak \noindent 
    First, define $w = a^{t} = xyz,\ t \in \mathbb{P}$. That is, since $w$ is a member of $L$, we can partition it into the form $a^{t} = xyz$. Furthermore, $xy^{n}z \in L\ \forall\ n>0$. Since this needs to hold \textbf{for all $n>0$}, showing that it doesn't work for a single $n>0$ breaks the argument. Let $n = t+1$. This leads us to some algebreic manipulations
    \begin{align*}
        xy^{n}z &= xy^{t+1}z = xyy^{t}z \\
                &=xyzy^{t} \quad \text{(Everything is $a$, we can commute)} \\
                &=a^{t}y^{t} \\
                &=a^{t}(a^{r})^{t} \\
                &= a^{t + rt} \\
                &= a^{t(1+r)}
    .\end{align*}
    This next assertion is one of number theory. We assert that if $t\in \mathbb{P}$, $t(r+1) \not\in \mathbb{P}$ (Since $r>0$, $r+1 > 0 $). 
    \bigbreak \noindent 
    Since we only have one choice of $y$ in this case, and we showed that it does not hold when $n=t+1$, the language must be irregular. $\quad \blacksquare $
 
\item \textbf{Using the Pumping Lemma With Length: Palindrome example}: Prove that the language \textit{Palindrome} over $\Sigma = \{a,b\} $ is a nonregular language.
    \bigbreak \noindent 
    Assume there exists an FA with $n$ states that accepts palindrome. Consider $w = a^{n+1}ba^{n+1} \in $ Palindrome
    \bigbreak \noindent 
    Since we assume $L$ is regular, then for $w=a^{n+1}ba^{n+1}$, which is clearly a palindrome with length $\geq n$, must have the property $w=xyz$, for some $x,y,z  \in \Sigma^{*}$, where $y\ne \epsilon$, and $\abs{xy} \leq n$. Thus, $y$ must be contained within $a^{n+1}$, which implies $xy$ must be contained within $a^{n+1}$. This leads to the conclusion that pumping more copies of $y$ will lead to an $a$ imbalance to the left of the $b$, which is \textbf{not} a palindrome. Thus, the language is nonregular. $\quad \blacksquare$.
\item \textbf{Pumping lemma with length example}: 
    Show with the pumping lemma that $L = a^{n}b^{m}c^{n+m},\ \forall\ m,n > 0$ is a nonregular language
    \bigbreak \noindent 
    The pumping lemma states that for an infinite regular language that has an FA with $k$ states, then
    \begin{align*}
        \forall w \in L,\ \abs{w} \ge n,\ \exists\ x,y,z\in \Sigma^{*} \mid w =xyz,\ \abs{xy} \le n \land xy^{i}z \in L \ \forall\ i > 0
    .\end{align*}
    Let $m,n = k$, then $w = a^{k}b^{k}c^{2k}$. Since $\abs{xy} \le n$, $y$ must be $a^{r},\ 0 < r \le k$. This implies $w = a^{r}a^{k-r}b^{k}c^{2k}$. Furthermore, $w^{\prime} = a^{ir}a^{k-r}b^{k}c^{2k} \in L\ \forall\ i > 0$. If $i = 2$, $w^{\prime} = a^{2r}a^{k-r}b^{k}c^{2k} = a^{k+r}b^{k}c^{2k}$. For $w^{\prime}$ to be in $L$, $2k$ must equal $k+r + k$. Since $2k \ne 2k + r$, we have a contradiction. Thus, pumping more copies of $y$ not satisfy $a^{n}b^{m} c^{n+m}$, as the number of $c$'s would not equal the number of $a$'s plus the number of $b$'s $\quad \blacksquare$
\item \textbf{Pumping lemma with length example}: 
    Over the alphabet $\Sigma = \{a,b,c\}$, show that the language that houses all strings that are not palindromes is nonregular.
    \bigbreak \noindent 
    Assume the language has an FA with $k$ states, then $\forall\ w \in L$, $\abs{w} \leq k,\ \exists \ x,y,z\in \Sigma^{*} \mid w = xyz, \abs{xy} \le k \ \land \ xy^{i}z \in L \ \forall \ i>0$.
    \bigbreak \noindent 
    An easy way to prove this is by showing that $L = \text{Palindrome}$ is nonregular, which is much simpler. Since regular languages are closed under complement, the complement of a regular language must be regular. Thus, the complement of a nonregular language must be nonregular.
    \bigbreak \noindent 
    Assume $L = \text{palindrome}$ is infinite and regular defined by an FA with $k$ states.
    \bigbreak \noindent 
    Let $w=a^{k}b^{k}a^{k}$, then $y=a^{r}, \ 0 < r \leq k$. Then $w = a^{r}a^{k-r}b^{k}a^{k}$ and $w^{\prime} = a^{ir}a^{k-r}b^{k}a^{k} \in L \ \forall \ i>0$. If $i=2$, $w^{\prime} = a^{2r}a^{k-r}b^{k}a^{k} = a^{k-r+2r}b^{k}a^{k} = a^{k + r}b^{k}a^{k}$. Since $r >0,\ k  + r > k \implies a^{k+r} > a^{k}$ and $w^{\prime}$ is not a palindrome. Thus, we have a contradiction  and $L$ must be nonregular. Since $L$ is nonregular, $L^{\prime}$ must be nonregular. Therefore, the language of all strings that are not palindromes is nonregular. $\quad \blacksquare$
\item \textbf{Pumping lemma with length example}: Show that the language $a^{t},\ t \in \mathbb{P}$ over the alphabet $\Sigma = \{a\}$ is nonregular.
    \bigbreak \noindent 
    Assume the language is regular defined by a FA with $n$ states. Choose $w=a^{\ell},\ \ell \in \mathbb{P},\ \ell \geq n$. Then, $y$ must be $a^{r},\ r>0$. From this, $w = xyz = a^{r}a^{\ell - r}$, and $w^{\prime} = xy^{i}z \in L \ \forall \ i > 0$. Thus, $w^{\prime} = a^{ir}a^{\ell - r} \in L \ \forall \ i > 0$. If we can show that some selection of $i$ yields a $w^{\prime} \not\in L$, then we have a contradiction and the language must be nonregular.
    \bigbreak \noindent 
    Choose $i = \ell  + 1$, which implies $w^{\prime} = a^{(\ell + 1)r}a^{\ell -r} = a^{\ell -r  + \ell r + r }  = a^{\ell + \ell r} = a^{\ell (1 + r)}$. Since $\ell \in \mathbb{P}$, and $r>0$, $\ell (1 + r)$ cannot be prime. Thus, we have a contradiction and the assumption does not hold for $L = a^{t},\ t\in \mathbb{P}$.
    \end{itemize}

    \pagebreak 
    \subsection{Context free grammers}
    \begin{itemize}

        \item A \textbf{Context Free Grammar} (CFG) is a 4-tuple \((V, \Sigma, S, P)\), where
            \begin{itemize}
                \item \(V\) is a non-empty finite set of \textit{variables}
                \item \(\Sigma\) is a finite alphabet of \textit{terminals}\\
                    (we assume \(V\) and \(\Sigma\) are disjoint)
                \item \(S \in V\), is the \textit{start variable}
                \item \(P\) is a finite set of \textit{productions} of the form \(A \rightarrow \alpha\) where
                    \begin{itemize}
                        \item \(A\) is a variable (i.e., \(A \in V\)) and
                        \item \(\alpha\) is a string of symbols from \((V \cup \Sigma)^*\) (i.e., \(\alpha \in (V \cup \Sigma)^*\)).
                    \end{itemize}
            \end{itemize}   
        \item \textbf{CFG notational convention}: 
            Variables are upper case letters with S always being the start variable.
            \bigbreak \noindent 
            Terminals are lower case letters, symbols, or constants, including $\epsilon$ to denote the empty symbol.
        \item \textbf{A simple CFG}: Consider the following CFG that has two productions
            \begin{align*}
                &S \to 0S1 \\
                &S \to \epsilon
            .\end{align*}
        \item \textbf{Derivations}:
            We say that a finite string $w$, consisting only of terminals, is generated by a CFG if, starting with the start variable $S$, you can apply a sequence of productions that result in $w$.
            \bigbreak \noindent 
            The sequence is called a derivation of $w$.
        \item \textbf{Derivation examples}: All derivations must start with the start variable $S$.
            \bigbreak \noindent 
            As long as there is at least one variable in our string we must continue the derivation by
            \begin{enumerate}
                \item Selecting a variable from our string,
                \item Selecting a production whose left side matches the variable we selected from our string, and
                \item Replacing the variable in our string with the right side of the production we selected.
            \end{enumerate}
            \bigbreak \noindent 
            With the two productions
            \begin{align*}
                &S \to 0S1 \\
                &S \to \epsilon
            .\end{align*}
            We select the first production and apply it 
            \begin{align*}
                S \Rightarrow 0S1 \Rightarrow 00S11 \Rightarrow 00\epsilon11 = 0011
            .\end{align*}
            This yields the string $w=0011$. Thus, we have derived this string from the grammer. And we also note that there are some other strings for which there is no derivation using our CFG
            \bigbreak \noindent 
            In short, we observe that a CFG defines a language over its alphabet of terminals $\Sigma$.
        \item \textbf{Context Free Languages}: A language is a context free language if it is generated by some context free grammar.
        \item \textbf{Is palindrome context free?}: Recall that the language Palindrome (i.e., the set of all strings that read the same forward as they do backward) is a non-regular language.
            \bigbreak \noindent 
            We can create a grammer for this language
            \begin{align*}
                &S \to aSa \\
                &S \to bSb \\
                &S \to a \\
                &S \to b \\
                &S \to \epsilon 
            .\end{align*}
            \bigbreak \noindent 
            The CFG builds the string from the middle always pushing outward by inserting the same pair of characters each time.
        \item \textbf{Notational convenience}: We can express the grammer above simply as
            \begin{align*}
                &S \to aSa \mid bSb \mid a \mid b \mid \epsilon
            .\end{align*}
            Where the pipe deliminates each production. Note that they are still separate productions, but since they share the same variable $S$, we can put them on the same line separated by a pipe.
    \end{itemize}




    \pagebreak 
    \unsect{DSA}
    \bigbreak \noindent 
    \subsection{C++ Stuff}
    \bigbreak \noindent 
    \subsubsection{Type declarations}
    \begin{itemize}
        \item \textbf{Discern any type}: Some rules,
            \begin{enumerate}
                \item Start with the variable name, we read from inside to out
                \item const, \%, *, and basic types go on the left
                \item const refers to what is immediately on the left (except for \texttt{const int*}), but the standard form of this is actually \texttt{int const*}. Thus, the exception to this is const is at the very left, then it refers to what is immediately right.
                \item arrays and functions go on the right, function args are type declaration sub-problems
            \end{enumerate}
            The Algorithm:
            \begin{itemize}
                \item Start with the variable name, or the implied name position 
                \item Read right until end or )
                \item Read left until end or (
                \item If something still left to read, move out one level of parenthesis and go to 2, else done.
            \end{itemize}
            \bigbreak \noindent 
            Thus, using parenthesis allows us to change direction, this will come in handy.
            \bigbreak \noindent 
            \textbf{Examples}:
            \begin{itemize}
                \item  $a$ is an int $\implies$ \texttt{int a}
                \item $a$ is a pointer to an int $\implies$ \texttt{int * a}
                \item $a$ is a pointer to a constant int $\implies$ \texttt{int const * a} (also \texttt{const int * a})
                \item $a$ is a constant pointer to an int $\implies$ \texttt{int * const a}
                \item $a$ is a constant pointer to a constant int $\implies$ \texttt{int const * const a} (also \texttt{const int * const a})
                \item $a$ is an array of 5 ints $\implies$ \texttt{int a[5]}
                \item $a$ is an array of 5 pointers to constant ints $\implies$ \texttt{int const * a[5]}
                \item $a$ is a pointer to an array of 5 constant ints $\implies$ \texttt{int const (* a)[5]}
            \end{itemize}
        \item \textbf{Multi dimensional arrays (matrices)}: Think of multi-dimensional arrays as arrays of arrays. More indicative of what’s happening internally. \texttt{float dat [3][4];} can be read as: "dat is an array of 3 arrays of 4 floats" (Using the algorithm from above).
            \bigbreak \noindent 
            \textbf{Examples}:
            \begin{itemize}
                \item \texttt{arg1} is a reference to an array of 25 constant pointers to arrays of 8 strings. $\implies$ \texttt{string (* const (\& arg1)[25])[8]}
                    \bigbreak \noindent 
                    \textbf{Note:} Notice how we use parenthesis to change direction
            \end{itemize}
        \item \textbf{Function Pointers}: Pointers point to bytes, which can be interpreted different ways. Pointers can point to bytes that can be interpreted as code, i.e. a function pointer.
            \bigbreak \noindent 
            \textbf{Examples}: 
            \begin{itemize}
                \item $f$ is a pointer to a function which takes an int and returns void. $\implies$ \texttt{void (* f) (int)}
            \end{itemize}
    \end{itemize}

   \pagebreak 
   \subsubsection{G++}
   \begin{itemize}
       \item \textbf{Compliation and linking}:
           Compilers turn source code into executable code.
           \begin{itemize}
               \item \textbf{Source code $\to$ object code (Compilation)}: Object code is almost executable. It contains pieces that it provides to other objects, and holes to be filled in. It is a slow process
               \item \textbf{Object code $\to$ executable (Linking)}: Connects pieces of object files together. This is a fast process
           \end{itemize}
           \bigbreak \noindent 
           \textbf{Note:}  Many “compilers” do both compiling and linking. Most programs are built in two stages:
           \begin{enumerate}
               \item Compile all the source code files
                \item Link the object code file into an executable
           \end{enumerate}
           This is the most efficient way to compile large projects.  Changing a single source code file requires a small number of compilations (slow), followed by linking (fast).
        \item \textbf{Standard unix c compiler}: The standard is GNU gcc
        \item \textbf{Standard unix cpp compiler}: The standard is GNU g++
        \item {g++ Options}: With no options, g++ will go from source to an executable named a.out
            \begin{itemize}
                \item \textbf{-o}: The -o option gives the name of the output file
                \item \textbf{-c}: The -c option makes the compiler stop after the compilation stage. No linking is done. The name of the object code file is the same as the source with the extension replaced with .o
                \item \textbf{-W[\textit{warning}]}: Tell the complier to look for a specific warning
                \item \textbf{-Wall (Warning all)}: There are many -W\textit{warning} options, which warn of various conditions. -Wall warns about all of them. The compiler keeps going through warnings
                    \bigbreak \noindent 
                    \textbf{Note:} A compiler warning is usually a bug waiting to happen. Do all you can to get rid of all warnings.
                \item \textbf{-Werror}: The -Werror option turns all warnings into errors. The compiler aborts on an error.
                \item \textbf{-g}: The -g option turns on debugging, and leaves much extra information in an object file. Executable is much larger, possibly slower.
                \item \textbf{-0}: The -O option turns on optimization. There are several different levels of optimization, e.g. -O0, -O1, -O2, -O3. 
                    \bigbreak \noindent 
                    \textbf{Note:} Optimization may break your code, and -O and -g don’t always work well together
                \item \textbf{-I[\textit{directory}]}: The -I option specifies an additional directory to search for include files. No space between -I and directory
                    \bigbreak \noindent 
                    Thus, 
                    \begin{cppcode}
                    #include "./dir/headerfile" // Without -I
                    #include "headerfile" // With -I : g++ -I./dir ...
                    \end{cppcode}
                \item \textbf{-L[\textit{directory}]}: The -L option specifies an additional directory to search for libraries. No space between -L and directory.
                    \bigbreak \noindent 
                    \textbf{Note:} This option is meant for linking only. It has no effect in compilation.
                \item \textbf{-l[\textit{libraryname}]}: The -l option specifies a library for linking. No space between -l and library name. The library name is related to the libray file name, but it is not identical. Library names start with “lib” and end with “.so.*” or “.a”. These are removed. For example
                    \begin{itemize}
                        \item The math library /lib/x86\_64-linux/gnu/libm.so.6 is linked as -lm
                        \item The X11 graphics library /usr/lib/x86\_64-linux-gnu/libX11.so is linked as -lX11
                    \end{itemize}
                    \bigbreak \noindent 
                    \textbf{Note:} This option is for linking only. It has no effect in compilation.  Libraries are the last things listed in a linking command.
                    \bigbreak \noindent 
                    If you're linking against a library that is located in a non-standard directory (a directory that is not automatically searched by the linker, such as ./libs), then you need to tell the linker where to find that library using the -L option. Thus, -L tells the compiler  where to look, -l specifies which one to grab.

            \end{itemize}
   \end{itemize}

   \pagebreak 
   \subsection{Elementary complexity theory}
   \begin{itemize}
       \item \textbf{Idea}: The same problem can frequently be solved with algorithms that differ in efficiency. The differences between the algorithms may be immaterial for processing a small number of data items, but these differences grow with the amount of data. To compare the efficiency of algorithms, a measure of the degree of difficulty of an algorithm called computational complexity was developed by Juris Hartmanis and Richard E. Stearns
           \bigbreak \noindent 
           Computational complexity indicates how much effort is needed to apply an algorithm or how costly it is. This cost can be measured in a variety of ways, and the particular context determines its meaning. This book concerns itself with the two efficiency criteria: time and space. The factor of time is usually more important than that of space, so efficiency considerations usually focus on the amount of time elapsed when processing data. However, the most inefficient algorithm run on a Cray computer can execute much faster than the most efficient algorithm run on a PC, so run time is always system-dependent. For example, to compare 100 algorithms, all of them would have to be run on the same machine. Furthermore, the results of run-time tests depend on the language in which a given algorithm is written, even if the tests are performed on the same machine. If programs are compiled, they execute much faster than when they are interpreted. A program written in C or Ada may be 20 times faster than the same program encoded in BASIC or LISP.
        \item \textbf{Units}: To evaluate an algorithm’s efficiency, real-time units such as microseconds and nanoseconds should not be used. Rather, logical units that express a relationship between the size $n$ of a file or an array and the amount of time $t$ required to process the data should be used
            \bigbreak \noindent 
            If there is a linear relationship between the size $n$ and time $t$, that is, $t_{1} = cn_{1}$, then an increase of data by a factor of 5 results in the increase of the execution time by the same factor. If $n_{2} = 5n_{1}$, then $t_{2}= 5t_{1} $
            \bigbreak \noindent 
            Similarly, if $t_1 = \log_2 n$, then doubling $n$ increases $t$ by only one unit of time. Therefore, if $t_2 = \log_2(2n)$, then $t_2 = t_1 + 1$.
        \item \textbf{Eliminating insignificant terms}: A function expressing the relationship between $n$ and $t$ is usually much more complex, and calculating such a function is important only in regard to large bodies of data; any terms that do not substantially change the function’s magnitude should  be eliminated from the function. The resulting function gives only an approximate measure of efficiency of the original function. However, this approximation is sufficiently close to the original, especially for a function that processes large quantities of data.
        \item \textbf{Asymptotic complexity}:  This measure of efficiency is called asymptotic complexity and is used when disregarding certain terms of a function to express the efficiency of an algorithm or when calculating a function is difficult or impossible and only approximations can be found
        \item \textbf{Big-O Notation}: The most commonly used notation for specifying asymptotic complexity—that is, for estimating the rate of function growth—is the big-O notation introduced in 1894 by Paul Bachmann.
            \bigbreak \noindent 
             Given two positive-valued functions $f$ and $g$, consider the following definition:
             \bigbreak \noindent 
             $f(n)$ is $O(g(n))$ if there exist positive numbers $c$ and $N$ such that $f(n) \leq c \cdot g(n)$ for all $n \geq N$.
             \begin{align*}
                 f(n) \text{ is } O(g(n)) \iff \exists\ c,N \in \mathbb{Z}^{+} \mid f(n) \le cg(n)\ \forall\ n \ge N
             .\end{align*}
             \bigbreak \noindent 
             Big-O notation says that for large enough $n$, the function $f(n)$ does not grow faster than a constant multiple of $g(n)$. So, $g(n)$ provides an upper bound on how fast $f(n)$ can grow as $n$ increases.
             \bigbreak \noindent 
             In other words, $f$ is big-O of $g$ if there is a positive number $c$ such that $f$ is not larger than $c \cdot g$ for sufficiently large $n$s; that is, for all $n$s larger than some number $N$. The relationship between $f$ and $g$ can be expressed by stating either that $g(n)$ is an upper bound on the value of $f(n)$ or that, in the long run, $f$ grows at most as fast as $g$.
             \bigbreak \noindent 
             The problem with this definition is that, first, it states only that there must exist
             certain $c$ and $N$, but it does not give any hint of how to calculate these constants. Second, it does not put any restrictions on these values and gives little guidance in situations when there are many candidates. In fact, there are usually infinitely many pairs
             of $c$'s and $N$'s that can be given for the same pair of functions $f$ and $g$.
             \bigbreak \noindent 
             For example, suppose 
             \begin{align*}
                 f(n) = 2n^{2} + 3n + 1 = O(n^{2})
             .\end{align*}
             Where $g(n) = n^{2}$. Candidate values for $c$ and $N$ are

             \[
                 \begin{array}{c|cccccccc}
                     c & \geq 6 & \geq 3^{\frac{3}{4}} & \geq 3^{\frac{1}{9}} & \geq 2^{\frac{13}{16}} & \geq 2^{\frac{16}{25}} & \cdots & \rightarrow & 2 \\
                     \hline
                     N & 1 & 2 & 3 & 4 & 5 & \cdots & \rightarrow & \infty \\
                 \end{array}
             \]
             \bigbreak \noindent 
             We obtain these values by solving the inequality:
             \begin{align*}
                 2n^{2}  + 3n  + 1 \leq cn^{2}
             .\end{align*}
             Or equivalently
             \begin{align*}
                 2 + \frac{3}{n}  + \frac{1}{n^{2}} \leq c
             .\end{align*}
             For different $n$'s
             \bigbreak \noindent 
             For large $n$, the terms $\frac{3}{n}$ and $\frac{1}{n^2}$ get smaller. Let's find $N$ such that for all $n \geq N$, the right-hand side stays bounded.
             \bigbreak \noindent 
             As $n$ gets larger, $\frac{3}{n}$ and $\frac{1}{n^2}$ approach zero. To simplify the analysis, choose $N=1$ initially and check how small $\frac{3}{n}$ and $\frac{1}{n^2}$ are:
             \[
                 2 + \frac{3}{1} + \frac{1}{1^2} = 2 + 3 + 1 = 6.
             \]
             From the inequality, at $N=1$, we have $6 \leq c$. Therefore, we can choose $c=6$. This ensures that for all $n \geq 1$, the inequality holds:
             \[
                 2 + \frac{3}{n} + \frac{1}{n^2} \leq 6.
             \]
             Thus, you can choose $c=6$ and $N=1$.
             \bigbreak \noindent 
             different pairs of constants $c$ and $N$ for the same function $g(= n^{2})$ can be determined.
         \item \textbf{Choosing the best $c$, $N$}: To choose the best $c$ and $N$, it should be determined for which N a certain term in $f$ becomes the largest and stays the largest.
             \bigbreak \noindent 
             In the example above, The only candidates for the largest term are \(2n^2\) and \(3n\);
             these terms can be compared using the inequality \(2n^2 > 3n\) that holds for \(n > 1.5\).
             Thus, \(N = 2\) and \(c \geq \frac{15}{4} = 3.75\).
         \item \textbf{Significance}: What is the practical significance of the pairs of constants just listed? All of them
             are related to the same function \(g(n) = n^2\) and to the same \(f(n)\). For a fixed \(g\), an infinite
             number of pairs of \(c\)'s and \(N\)'s can be identified. The point is that \(f\) and \(g\) grow at the same
             rate. The definition states, however, that \(g\) is almost always greater than or equal to \(f\) if it
             is multiplied by a constant \(c\). "Almost always" means for all \(n\)'s not less than a constant \(N\).
             The crux of the matter is that the value of \(c\) depends on which \(N\) is chosen, and vice
             versa.
         \item \textbf{Inherent imprecision: Choosing best $g(n)$}: The inherent imprecision of the big-O notation goes even further, because there 
             can be infinitely many functions \(g\) for a given function \(f\). For example, the \(f\) from 
             Equation 2.2 is big-O not only of \(n^2\), but also of \(n^3\), \(n^4\), \dots, \(n^k\), \dots for any \(k \geq 2\). 
             To avoid this embarrassment of riches, the smallest function \(g\) is chosen, \(n^2\) in this case.
        \item \textbf{Big-o as approximating terms}:  The approximation of function f can be refined using big-O notation only for
            the part of the equation suppressing irrelevant information. For example, in the equation below, the contribution of the third and last terms to the value of the function can
            be omitted
            \bigbreak \noindent 
            \begin{align*}
                f(n) &=n^{2} + 100n + \log(n) + 1000 \\
                \implies  f(n) &= n^{2} + 100n + O(\log(n))
            .\end{align*}
            \bigbreak \noindent 
            Similarly, 
            \begin{align*}
                f(n) &= 2n^{2} + 3n + 1 \\
                \implies f(n) &= 2n^{2} + O(n)
            .\end{align*}
            \bigbreak \noindent 
            This equation says that for large values of \(n\), the expression \(2n^2 + 3n + 1\) behaves like \(2n^2\) plus some terms that grow linearly or slower (captured by \(O(n)\)). The exact contributions of \(3n\) and \(1\) are not important for asymptotic analysis; what matters is that their growth is slower compared to \(2n^2\).
        




    \item \textbf{Algorithm analysis: Most common time complexities}: Ranked slowest to fastest growth
        \begin{itemize}
            \item \textbf{$O(1)$:} Constant time 
            \item \textbf{$O(\log(\log(n)))$}: Logarithmic time
            \item \textbf{$O(\log(n))$}: Logarthmic time
            \item \textbf{$O(n)$}: Linear time
            \item \textbf{$O(n\log(n))$}: Log-linear time
            \item \textbf{$O(n^{k}),\ k>1$}: Polynomial time
            \item \textbf{$O(a^{n}),\ a>1$}: Exponential time
            \item \textbf{$O(n!)$}: Factorial time
        \end{itemize}
    \item \textbf{Ranking complexities from slowest to fastest: Process}: Given 
        \begin{enumerate}[label=(\alph*)]
            \item $O(25) $
            \item $O(n^{\frac{1}{2}} + \log^{2}(n)) $
            \item $O(\log^{200}(n)) $
            \item $O(n^{3}\log^{4}(n)) $
            \item $O(n^{200} + 3^{n}) $
            \item $O(n\log^{40}(n)) $
            \item $O(4^{n}\log(n)) $
            \item $O(n^{3}\log(\log(n))) $
        \end{enumerate}
        How can we go about sorting these slowest to fastest. Well, to start, in the expressions with plus or minus, we can throw out the slower terms. Thus,
        \begin{enumerate}[label=(\alph*)]
            \item $O(n^{\frac{1}{2}}) $
            \item $O(25) $
            \item $O(\log^{200}(n)) $
            \item $O(n^{3}\log^{4}(n)) $
            \item $O(3^{n}) $
            \item $O(n\log^{40}(n)) $
            \item $O(4^{n}\log(n)) $
            \item $O(n^{3}\log(\log(n))) $
        \end{enumerate}
        In product terms, we disregard the slower term unless there are complexites with the same dominant term. For example, $O(n^{3}\log(\log(n)))$ grows slower than $O(n^{3}\log^{4}(n))$ because although they have the same dominant term $n^{3}$, $\log(\log(n))$ grows slower than $\log^{4}(n)$. Thus, the correct sequence is 
        \begin{enumerate}[label=(\alph*)]
            \item [(b)] $O(25)$
            \item [(c)] $O(\log^{200}(n)) $
            \item [(a)] $O(n^{\frac{1}{2}} + \log^{2}(n)) $
            \item [(f)] $O(n\log^{40}(n)) $
            \item [(h)] $O(n^{3}\log(\log(n))) $
            \item [(d)] $O(n^{3}\log^{4}(n)) $
            \item [(e)] $O(n^{200} + 3^{n})$
            \item [(g)] $O(4^{n}\log(n))$
        \end{enumerate}










   \end{itemize}
    


    \pagebreak 
    \unsect{Databases}

    \bigbreak \noindent 
    \subsection{Introduction to databases (db concepts)}
    \bigbreak \noindent 
    \subsubsection{Definitions and theorems}
    \begin{itemize}
        \item \textbf{What is a database?}: A database is a collection of stored operational data used by the application systems of some particular enterprise, better yet a collection of related data.
        \item \textbf{What is an enterprise?}: a generic term for any reasonably large-scale commercial, scientific, technical, or other application. Such as
            \begin{itemize}
                \item Manufacturing
                \item Financial
                \item Medical
                \item University
                \item Government
            \end{itemize}
        \item \textbf{Operational data}: Data maintained about the operation of an enterprise, such as
            \begin{itemize}
                \item Products
                \item Accounts
                \item Patients
                \item Students
                \item Plans
            \end{itemize}
            \bigbreak \noindent 
            \textbf{Note:} Notice that this DOES NOT include input/output data
        \item \textbf{Database Management System (DBMS)}: A Database Management System (DBMS) is a collection of programs that enables users to create and maintain a database. Ie a general-purpose software system that facilitates
            \begin{itemize}
                \item Definition of databases
                \item Construction of databases
                \item Manipulation of data within a database
                \item Sharing of data between users/applications
            \end{itemize}
        \item \textbf{Defining a database}: For the data being stored in the database, defining the database specifies
            \begin{itemize}
                \item The data types
                \item The structures
                \item The constraints
            \end{itemize}
        \item \textbf{Constructing a Database}: Constructing a database is the process of storing the data itself on some storage device
            \bigbreak \noindent 
            \textbf{Note:} The storage device is controlled by the DBMS
        \item \textbf{Manipulating a Database}
            \begin{itemize}
                \item retrieve specific information in a query
                \item update the database to include changes
                \item generate reports from the data
            \end{itemize}
            \bigbreak \noindent 
            Most likely already defined by whatever dbms you choose
        \item \textbf{Sharing a Database}: Sharing a database Allows multiple users and programs to access the database at the same time, any conflicts between applications are handled by the DBMS
        \item \textbf{Other Important Functions of a Database}: Other important functions provided by a DBMS include
            \begin{itemize}
                \item Protection, system protection, security protection
                \item Maintenence, allows updates to be performed easily
            \end{itemize}
        \item \textbf{Simplified Database System Environment}:
            \bigbreak \noindent 
            \fig{.5}{./figures/1.png}
        \item \textbf{Main characteristics of a database system are:}
            \begin{itemize}
                \item Self-describing nature of a database system
                \item Insulation between programs and data, and data abstraction
                \item Support for multiple views of the data
                \item Sharing of data and multi-user transaction processing
            \end{itemize}
        \item \textbf{Other Capabilities of DBMS Systems}: Support for at least one data model through which the user can view the data, There is at least one abstract model of data that allows the user to see the “information” in the database, Relational, hierarchical, network, inverted list, or object-oriented
            \bigbreak \noindent 
            Support for at least one data model through which the user can view the data
            \begin{itemize}
                \item efficient file access which allows us to “find the boss of Susie Jones”
                \item allows us to “navigate” within the data
                \item allows us to combine values in 2 or more databases to obtain “information”
            \end{itemize}
            \bigbreak \noindent 
            Support for high-level languages that allow the user to define the structure of the data, access that data, and manipulate it
            \begin{itemize}
                \item Data Definition Language (DDL)
                \item Data Manipulation Language (DML)
                \item Data Control Language (DCL)
                \item query language access data
                \item operations such as add, delete, and replace
            \end{itemize}
        \item \textbf{Transaction Management}: Transaction management is a feature that provides correct, concurrent access to the database, possibly by many users at the same time, ability to simultaneously manage large numbers of \textit{transactions}
        \item \textbf{Access Control}: Access control is the ability to limit access to data by unauthorized users along with the capability to check the validity of the data. This is to protect against loss when database crashes and prevent unauthorized access to portions of the data
        \item \textbf{Resiliency}: Resiliency is the ability to recover from system failures without losing data, Ideally, should be able to recover from any type of failure, such as 
            \begin{itemize}
                \item sabotage
                \item acts of God
                \item hardware failure
                \item software failure
                \item etc.
            \end{itemize}
            \bigbreak \noindent 
            \textbf{Note}: Obviously, some of these would require more than just software - offsite backups, etc
        \item \textbf{Use of Conceptual Modeling}:
            \bigbreak \noindent 
            \fig{.5}{./figures/2.png}
        \item \textbf{Leveled Architecture of a DBMS}:
            \bigbreak \noindent 
            \fig{.5}{./figures/3.png}
        \item \textbf{External level}: a view or sub-schema, a portion of the logical database, may be in a higher level language
        \item \textbf{Logical Level}: abstraction of the real world as it pertains to the users of the database. DBMS provides a data definition language (DDL) to describe the logical schema in terms of a specific data model such as relational, hierarchical, network, inverted list, etc.
        \item \textbf{Physical Level}: The collection of files and indices, the collection of files and indices, this is the actual data
        \item \textbf{Instance}: An instance of the database is the actual contents of the data, it could be 
            \begin{itemize}
                \item the extension of the database
                \item current state of the database
                \item a snapshot of the data at a given point in time
            \end{itemize}
        \item \textbf{Schema}: The schema of a database is the data about what the data represents. Such as,
            \begin{itemize}
                \item plan of the database
                \item logical plan
                \item physical plan
                \item the intention of the database
            \end{itemize}
        \item \textbf{Schema vs Instance}:
            \bigbreak \noindent 
            \fig{.5}{./figures/4.png}
        \item \textbf{Data Independence}: Data Independence is a property of an appropriately designed database system,  it has to do with the mapping of logical level to physical level, and logical to external
            \begin{itemize}
                \item \textbf{Physical data independence}:  Physical schema can be changed without modifying logical schema
                \item \textbf{Logical data independence}: logical schema can be changed without having to modify any of the external views
            \end{itemize}
        \item \textbf{DCL (Control), DDL (Definition), DML (Manipulation)}: may be completely separate (example is IMS), may be intermixed (DB2), or may be a host language, for example an  application program in which DML commands are embedded such as COBOL or PL/I
        \item \textbf{DBMS Components}:
            \bigbreak \noindent 
            \fig{.5}{./figures/5.png}
        \item \textbf{Overall DBMS Usage Scenario}: Database Administrator (DBA) define the conceptual, logical, and physical levels using DDL.  DBMS software stores instances of these in schemas.  User defines views (External Schema) in DDL. User accesses database using DML
        \item \textbf{Advantages of a Database}:
            \begin{itemize}
                \item Controlled redundancy
                \item Reduced inconsistency in the data
                \item Shared access to data
                \item Standards enforced
                \item Security restrictions maintained
                \item Integrity maintained more easily
                \item Provides capability for backup and recovery
                \item Permitting inferences and actions using rules
            \end{itemize}
        \item \textbf{Disadvantages of a Database}:
            \begin{itemize}
                \item Increased complexity needed to implement concurrency control
                \item Increased complexity needed for centralized access control
                \item Security needed to allow the sharing of data
                \item Necessary redundancies can cause complexity when updating
            \end{itemize}
        \item \textbf{Data vs Information}:
            \begin{itemize}
                \item \textbf{Data}: Data refers to raw, unprocessed facts, figures, and details. It represents basic elements that have not been interpreted or given any meaning.
                \item \textbf{Information}: Information is processed, organized, or structured data that is meaningful and useful. It is data that has been interpreted or analyzed to provide context, relevance, and purpose.
            \end{itemize}
    \end{itemize}

    \pagebreak 
    \subsection{Conceptual Modeling and ER Diagrams}
    \bigbreak \noindent 
    \subsubsection{Definitions and theorems}
    \begin{itemize}
        \item \textbf{Data Models}: A means of describing the structure of data, we typically have A set of operations that manipulate the data (for data models that are implemented)
        \item \textbf{Types of data models}:
            \begin{itemize}
                \item Conceptual data model
                \item  Logical data models - relational, network, hierarchical, inverted list, or object-oriented
            \end{itemize}
        \item \textbf{Conceptual Data Model}:
            \begin{itemize}
                \item Shows the structure of the data including how things are related
                \item Communication tool
                \item Independent of commercial DBMSes
                \item Relatively easy to learn and use
                \item Helps show the semantics or meaning of the data
                \item Graphical representation
                \item Entity-Relationship Model is very common
            \end{itemize}
        \item \textbf{Logical Data Models - Relational}: Data is stored in relations (tables). These tables have one value per cell. Based upon a mathematical model.
        \item \textbf{Logical Data Models - Network}: Data is stored in records (vertices) and associations between them (edges), Based upon a model called CODASYL
        \item \textbf{Logical Data Models - Hierarchical}: Data is stored in a tree structure with parent/child relationships
        \item \textbf{Logical Data Models - Inverted List}: Tabular representation of the data using indices to access the tables, Almost relational, but it allows for non-atomic data values \footnote{ "Non-atomic data values" refer to data structures or values that are composed of multiple components, as opposed to atomic data values, which are indivisible and represent a single value.}, which are not allowed in relations
        \item \textbf{Logical Data Models - Object Oriented}: Data stored as objects which contain
            \begin{itemize}
                \item Identifier
                \item Name
                \item Lifetime
                \item Structure
            \end{itemize}
        \item \textbf{Entity-Relationship Model}: Meant to be simple and easy to read. Should be able to convey the design both to database designers and unsophisticated users
        \item \textbf{Entities}: Principle objects about which information is kept - These are the *things* we store data about. If you look at the ER Diagram like a spoken language, the entities are nouns - Person, place, thing, event. When drawn on the ER diagram, entities are shown as rectangles with the name of the entity inside.
            \bigbreak \noindent 
            \fig{.5}{./figures/7.png}
        \item \textbf{Relationships}:  Relationships connect one or more entities together to show an association. A relationship \textit{cannot} exist without at least one associated entity.  Graphically represented as a diamond with the name of the relationship inside, or just beside it
            \bigbreak \noindent 
            \fig{.7}{./figures/8.png}
        \item \textbf{Attributes}: Characteristics of entities \textbf{OR} of relationships, Represent some small piece of associated data, Represented by either a rounded rectangle or an oval.
            \bigbreak \noindent 
            \fig{.5}{./figures/9.png}
        \item \textbf{Attributes on Entities}: When an attribute is attached to an entity, it is expected to have a value for every instance of that entity, unless it is
            allowed to be null. For instance, in the diagram above, Name was an attribute of Person. Every person
            that we store data about will have a value for Name.
        \item \textbf{Attributes on Relationships}: When an attribute is attached to a relationship, it is only expected to have a value when the entities involved in the
            relationship come together in the appropriate way.
            In the diagram from before, the Amount attribute is attached to the donates relationship, which connects the
            Person and Charity entities. Amount will have one value for each time a Person donates to a Charity, denoting how
            much that person donated to the charity. It will not necessarily have a value for a given person, or a given charity.
            This can be referred to as the \textbf{intersection data}.
        \item \textbf{Types of attributes}:
            \fig{.5}{./figures/10.png}
        \item \textbf{Degree of a Relationship}: The degree of a relationship is defined as how many entities it associates. If one entity is associated more than once
            (such as with a recursive relationship), then the degree counts each time it is referenced.
            \bigbreak \noindent 
            \fig{.5}{./figures/11.png}
            \bigbreak \noindent 
            \textbf{Note:} There is no limit to how many entities there can be in a relationship. After binary, and ternary, we start to call the relationships $n$-ary, where $n$ is the degree
        \item \textbf{Connectivity of a Relationship}:
            \begin{itemize}
                \item A constraint of the mapping of associated entities
                \item Written as (minimum, maximum).
                \item Minimum is usually zero or one.
                \item Maximum is a number (commonly one) or can be a letter denoting many.
                \item The actual number is called the cardinality.
            \end{itemize}
            \bigbreak \noindent 
            \fig{.5}{./figures/12.png}
            \bigbreak \noindent 
            Together (from the image) both sides make up the connectivity, to refer to a single side, we use the term "cardinality", ie the cardinality of a person is (1,1). If we hold Address constant (We know a specific address and are therefore refering to that), how many persons may live at that address, in this case (1,1)
        \item \textbf{Attributes on Relationships (revisited)}: Must be on a many-to-many relationship. (1-many and 1-to-1 relationships should have the attribute on one of
the entities involved.  Someone needs to know all of the associated entities to access the attribute.
        \item \textbf{Reading Cardinalities}: For binary relationships:
            \begin{itemize}
                \item For each Thing that smurfs, there are a minimum of $c$, and a maximum of $d$ Objects.
                \item For each Object that smurfs/is smurfed, there is a minimum of $a$ and a maximum of $b$ Things
            \end{itemize}
            \bigbreak \noindent 
            \fig{.5}{./figures/13.png}
        \item \textbf{Weak Entities}: Sometimes you may run into an entity that depends upon another entity for its existence. The weak entity is a tool you can use to represent this.:w
            \bigbreak \noindent 
            Weak entities are written like normal entities, except that they have a double rectangle outline. The relationship
that connects the weak entity to the strong entity it depends upon will be written with a double diamond. This
does not mean that the relationship is weak. It is just to indicate upon which entity the weak entity depends.
\bigbreak \noindent 
\fig{.5}{./figures/14.png}
        \item \textbf{Recursive Relationships}: It is possible for an entity to have a relationship with itself. This is called a recursive relationship. It makes more sense if you think of entities as collections of objects of their appropriate type
        \item \textbf{Recursive Relationships - Many-To-Many}: A many-to-many recursive relationship means that the objects are arranged in a network structure, Notice that the minimum is 0 on both sides. This is important.
            \bigbreak \noindent 
            \fig{.5}{./figures/15.png}
        \item \textbf{Recursive Relationships - One-To-Many}: A one-to-many recursive relationship means that the objects are arranged in a tree structure, Notice that the minimum is still 0 on both sides. This is important.
            \bigbreak \noindent 
            \fig{.4}{./figures/16.png}
        \item \textbf{Entity or Attribute?}: 
            Sometimes it isn’t clear whether something should be an entity or an attribute of some other entity. Usually the
decision will come down to how complicated it is to store the data, and how important it is. If it ends up being used
in multiple places, it might be a clue that you should use an entity
        \item \textbf{Inheritance}: Two types of inheritance available
            \begin{itemize}
                \item ”is a” inheritance. This shows that the subtype IS a member of the supertype.
                \item ”is part of ” inheritance. This shows that the supertype contains, or is made up of members of the subtypes.
            \end{itemize}
            \bigbreak \noindent 
            All attributes of the supertype entity are inherited by the subtype entities. The identifier of the subtypes will be the same as the supertype
            \bigbreak \noindent 
        \item \textbf{IS A Inheritance}:  This type of inheritance happens when you have a supertype and one or more subtypes that are members
            of the supertype. Denoted by an upside-down triangle, with the supertype on top, and the subtypes coming out the bottom.
            \bigbreak \noindent 
            \fig{.5}{./figures/17.png}
        \item \textbf{Defining IS-A inheritance}: There are two things you need to choose when using IS-A inheritance:
            \begin{itemize}
                \item \textbf{Generalization (no) vs. specialization (yes)}: can the supertype occur without being a member of the specified subtypes?
                \item \textbf{Overlapped (yes) vs. disjoint subtypes (no)}: is it possible for a single occurrence of the supertype to be a member of more than one subtype?
            \end{itemize}
            \bigbreak \noindent 
            They are mutually exclusive so you need to pick one of each, ie. GO, GD, SO, SD
        \item \textbf{IS-A inheritance - Generalization}:  Supertype is the union of all of the subtypes, This means that an instance of the supertype CANNOT EXIST without belonging to at least one subtype.
        \item \textbf{IS-A inheritance - Specialization}: The subtype entities specialize the supertype, This means that an instance of the supertype CAN exist without being related to any of the subtypes
        \item \textbf{IS-A inheritance - Overlapping Subtypes}: It is possible for an instance of the supertype to be related to more than one of the subtypes
        \item \textbf{IS-A inheritance - Disjoint Subtypes}: the subtype entities are mutually exclusive, it is not possible for an instance of the supertype to be related to more than one subtype.
        \item \textbf{IS-PART-OF Inheritance}: ”Is part of ” inheritance indicates that the
supertype is constructed from instances of the
subtypes. It is shown on an ER diagram as a circle,
with the supertype on the top, and subtypes on
the bottom.
\bigbreak \noindent 
\fig{.5}{./figures/18.png}
    \item \textbf{Warning about IS-PART-OF}:  The IS PART OF inheritance operator does have its uses, but it is not very commonly used, If you see something involving a certain number of things being present, there are several possibilities
        \begin{itemize}
            \item Sometimes a number is specified that isn’t actually important for what we are modeling. This won’t even be represented on an ER Diagram. This is the case when changing the number wouldn’t have any effect on the necessary structure of a database.
            \item If you need a certain number of items for a relationship to hold, you should explore using the connectivity of the relationship to express that.
            \item Finally, this IS PART OF inheritance might be useful. It is almost never necessary, however.
        \end{itemize}
    \item \textbf{Are you actually representing what you want to?}: Let’s say you’re running a business selling used cars. A simple ER diagram for the sales might look like the following:
        \bigbreak \noindent 
        \fig{.4}{./figures/19.png}
        \bigbreak \noindent 
        The resulting database would have one entry for each time a specific person buys a specific car. If the same person
buys the same car more than once (obviously selling it to someone else at some point), this model would no longer
be appropriate.
\bigbreak \noindent 
Adding a new entity to the relationship for the date/time of the purchase can fix this problem.
\bigbreak \noindent 
\fig{.5}{./figures/20.png}
\bigbreak \noindent 
Notice that the connectivities can change when you add new entities to the relationship.
\item \textbf{Weak Entities - Introduction}: So far, all of the entities we have used have been things that stand on their own. There are some situations where
we are modeling an object for which we certainly need to store data, but the items exist only in the context of some
other entity. Many of these examples can occur
\bigbreak \noindent 
One example of a time that an entity depends on another would be the idea of a city. Within a state, we can
generally be assured that cities will have unique names. If we were working only at that level, the City could be an
entity as we saw above. A good identifier for it would be the name of the city, so we would see the following:
\bigbreak \noindent 
\fig{.5}{./figures/21.png}
\bigbreak \noindent 
In some situations, this would be valid. The Name attribute can serve, in those circumstances, as an appropriate
identifier.
\bigbreak \noindent 
To indicate this sort of dependency, we can make the dependent entity a “weak” entity. This is drawn with a
double-edged rectangle, shown below.
\bigbreak \noindent 
\fig{.4}{./figures/22.png}
\bigbreak \noindent 
Notice that the City entity is now drawn as a weak entity, with a double border. The relationship between the weak
entity and the strong entity is also drawn with a double border. The relationship is not weak, per se, but it is used to
indicate which strong entity the weak entity depends upon.
    \item \textbf{Discriminant (partial key)}:  The discriminant, also known as the partial key, is an attribute (or a set of attributes) within the weak entity that can uniquely identify the weak entity, but only in combination with the primary key of the strong entity it is associated with. In other words, the discriminant helps to distinguish instances of the weak entity when they are tied to a particular instance of the strong entity.
    \item \textbf{Schema}: In databases, a schema is the structural definition of how data is organized in a database. It outlines the way data is stored

    \end{itemize}

    \pagebreak 
    \subsection{The Relational Model}
    \begin{itemize}
        \item \textbf{Basic Structure}: 
            \begin{itemize}
                \item \textbf{Relations}: In the relational data model, our database is made up of one or more \textbf{relations} (tables). Each relation should have a unique name.
                \item \textbf{Schema}: The schema of a relation is written as \textbf{Relation\_Name}($A_{1}, A_{2}, ...,A_{n} $), Where $A_{1}, A_{2}, ...,A_{n}$ are placeholders for the attribute names
                \item \textbf{Column headers (attributes)}: The attributes becomes the column headers of the relation.
                \item \textbf{Instance data, tuples}: When there is instance data, it will come in the form of \textbf{tuples} (rows), which have a value for each attribute, as shown below
            \end{itemize}
            \bigbreak \noindent 
            \textbf{Note:} No field may contain than one value.

            \begin{table}[h!]
                \centering
                \begin{tabular}{|>{\centering\arraybackslash}m{2cm}|>{\centering\arraybackslash}m{2cm}|>{\centering\arraybackslash}m{2cm}|c|>{\centering\arraybackslash}m{2cm}|}
                    \hline
                    \multicolumn{5}{|c|}{\textbf{Relation\_Name}} \\ \hline
                    $A_1$ & $A_2$ & $A_3$ & $\dots$ & $A_n$ \\ \hline
                    $x_1$ & $x_2$ & $x_3$ & $\dots$ & $x_n$ \\ \hline
                    $y_1$ & $y_2$ & $y_3$ & $\dots$ & $y_n$ \\ \hline
                    $\dots$ & $\dots$ & $\dots$ &  & $\dots$ \\ \hline
                \end{tabular}
            \end{table}
        \item \textbf{The domain of an attribute}: Each attribute becomes a column heading
            \bigbreak \noindent 
            Each attribute (column) also has an associated \textbf{domain}. The domain of an attribute is the set of all valid values for it.  The domain may be looked at as a data type, but may have additional constraints.
        \item \textbf{The domain of a set of attributes}: The domain of a set of attributes is the set of all possible combinations of values for the attributes in the set.
        \item \textbf{Tuples (Rows)}: A tuple is a special type of (mathematical) set containing values for each attribute within the relation. Tuples are shown as rows in the table, with the value for each attribute under the appropriate column
        \item \textbf{Atomic tuples}:  The values are required to be atomic; there can be only one value per tuple per attribute
        \item \textbf{Relation vs relationship}: Though they have similar names, A relation (table) and a relationship (from an ER diagram) \textbf{ARE NOT} the same thing.
            \begin{itemize}
                \item \textbf{Degree of relation}: The degree of a relation is the number of attributes present.
                \item \textbf{Cardinality of a Relation}: The cardinality of a relation is the number of tuples present.
            \end{itemize}
        \item \textbf{Keys}: Speaking generally, the purpose of a key is to uniquely identify a tuple in some relation.
            \begin{itemize}
                \item \textbf{Super keys}: A super key within a relation is an attribute or set of attributes whose values can uniquely identify any tuple within that relation
                \item \textbf{The trivial key}: Every relation has at least one - the set of all attributes in the relation 
                \item \textbf{Candidate Keys}: A candidate key is a minimal super key – the minimum set of attributes necessary to uniquely identify a tuple within the relation
                \item \textbf{Primary Key}: The primary key for a relation is chosen by the database designer from among the relation’s candidate keys. It becomes the “official” key that is used to reference tuples within the relation. There can be only one
                \item \textbf{Prime, non-prime attributes}: Once a primary key is chosen, each of the attributes in the relation will be either \textbf{prime} or \textbf{non-prime} with respect to the relation. A prime attribute is one of the attributes that can be found in any of the candidate keys. A non-prime attribute is one of the attributes not found in any of the candidate keys
                    \bigbreak \noindent 
                    Once a primary key is chosen for it, the schema of a relation is written with the primary key’s attributes underlined
                \item \textbf{Foreign Keys}: A foreign key is a tool used to link relations within a database. Since every relation has a primary key that uniquely identifies each tuple, the values of those key attributes can be used from another relation to reference individual tuples.
                    \bigbreak \noindent 
                    The relation whose primary key is being used is the \textbf{home relation}
            \end{itemize}
        \item \textbf{Order Independence}: In relations, the order things appear doesn’t matter. There are ways to force them to sort later when we’re working with SQL, but the relation itself has no order for either rows or attributes...
        \item \textbf{Order Independence - Attributes}: It doesn’t matter what order the attributes appear in, if two relational schemas have the same name, the same attributes, and the same primary key, then they are equivalent.
        \item \textbf{Order Independence - Tuples}: Tuples are stored unordered. If you need to have them appear in some order later, you will be able to sort based on the values inside of them using SQL.
        \item \textbf{Constraints}: Constraints are limits imposed on the domains of various attributes. These can come from the system your database is modeling
        \item \textbf{Entity Integrity Constraint}: The entity integrity constraint applies to all relations. It states that no tuple may exist within a relation that has null value for any of attributes that make up the primary key. This is a consequence of the primary key being a candidate key, which is minimal and cannot do its job with any less data.
        \item \textbf{Referential Integrity Constraint}: It constrains the values of foreign keys in relations to values that actually exist as primary keys for tuples within the home relation. If the foreign key is otherwise allowed to be NULL, then that is also an acceptable value.
        \item \textbf{Summary: Terms}:
            \begin{itemize}
                \item \textbf{Relations}: Tables
                \item \textbf{Colums}: Attributes
                \item \textbf{Tuples}: The rows in the relation that holds the instance datae
                \item \textbf{Domain of an attribute}: Set of all possible values for the attribute
                \item \textbf{Domain of a set of attributes}: Set of all possible combinations of values for the attributes in the set
                \item \textbf{Degree of relation:} The degree of a relation is the number of attributes present.
                \item \textbf{Cardinality of a Relation:} The cardinality of a relation is the number of tuples present.
            \end{itemize}
    \end{itemize}

    \pagebreak 
    \subsection{Relational Model Normalization}
    \begin{itemize}
        \item \textbf{Designing Relational Databases}: There are a large number of possible ways to represent each problem with using relations. Some choices will perform better than others for various reasons. The option chosen should be the best one, but how do we know which one that is?
            \bigbreak \noindent 
            We should study:
            \begin{itemize}
                \item Problems that can come up
                \item How to avoid them
                \item Desirable properties
                \item How to guarantee them
            \end{itemize}
        \item \textbf{Basic Example}: If our database is a single relation with schema \textbf{SP}(\underline{SuppName}, SuppAddr, \underline{Item}, Price) with the instance data:
            \bigbreak \noindent 
            \begin{center}
                \begin{tabular}{|l|l|l|l|}
                    \hline
                    \textbf{SuppName} & \textbf{SuppAddr} & \textbf{Item} & \textbf{Price} \\ \hline
                    John    & 10 Main   & Apple   & \$2.00  \\ \hline
                    John    & 10 Main   & Orange  & \$2.50  \\ \hline
                    Jane    & 20 State  & Grape   & \$1.25  \\ \hline
                    Jane    & 20 State  & Apple   & \$2.25  \\ \hline
                    Frank   & 30 Elm    & Mango   & \$6.00  \\ \hline
                \end{tabular}
            \end{center}
            \bigbreak \noindent 
            There are some common things that we might want to do that would cause issues
            \begin{itemize}
                \item \textbf{Insertion Anomaly}: Let’s say we want to add a new vendor, “Sally”, and store her address, “40 Pine”, but she is not selling anything yet. Can this be inserted into the relation SP?
                    \bigbreak \noindent 
                    \textbf{NO}. The primary key is (SuppName, Item), but we only have SuppName. The entity integrity constraint is violated if we try to insert the data as a tuple in this relation. It cannot fit. We call this an insertion anomaly.
                \item \textbf{Deletion Anomaly}: This time, let’s say that Frank no longer sells Mango. We want to take that out of the database so nobody can order a mango that is not available. Can this tuple remain in the relation with the Mango information removed?
                    \bigbreak \noindent 
                    \textbf{NO}. The primary key is (SuppName, Item), and the Item is going away. The entity integrity constraint is violated if we remove the data from the tuple in this relation. We can either keep the whole tuple, advertising fake mango, or delete the whole tuple and lose the information on Frank, which doesn’t exist in any other tuples. We call this a deletion anomaly.
                \item \textbf{Update Anomaly}: Next, let’s say that John is moving to a different address. We would have to change it once for every item John is selling. This isn’t a big deal with only two items, but as John’s list of supplied items grows, so does the amount of database work that needs to be done every time he moves. If any of the SuppAddr values for John don’t agree, then it may not be clear which is the right address for John. This is an update anomaly.
                \item \textbf{Redundancy}: Redundancy is when values are repeated.
                    \bigbreak \noindent 
                    It can be
                    \begin{itemize}
                        \item \textbf{Good:} If you have an off-site backup of your entire database, the redundancy is useful, and can be used to restore in case of a failure.
                        \item \textbf{Bad:} Redundancy on the same physical device is unnecessary. It wastes space and comes with the potential for update anomalies.
                    \end{itemize}
                \item \textbf{Note:} The good redundancy is something the DBA/IT department should handle. When we talk about redundancy in the design of our database, we will be talking about the bad kind.
            \end{itemize}
        \item \textbf{Anomolies summarized}:
            \bigbreak \noindent 
            \textbf{Insertion anomalies}:
            \begin{itemize}
                \item When a piece of data cannot be inserted because it violates some constraint of the relation.
                \item Usually this is the entity integrity constraint being violated, but not always. See the Sally example
            \end{itemize}
            \bigbreak \noindent 
            \textbf{Deletion anomalies}: 
            \begin{itemize}
                \item When deleting some piece of data, a deletion anomaly is when more data is lost than intended
                \item Usually this is caused when the data removed is part of the primary key, which would cause a violation of the entity integrity constraint. See the Frank example
            \end{itemize}
            \bigbreak \noindent 
            \textbf{Update anomalies}:
            \begin{itemize}
                \item When updating a single value requires changes to multiple tuples, this is an update anomaly. See the John example.
                \item This is caused by unnecessary redundancies in the data.
                \item These cause inefficiency, and potential inconsistencies.
            \end{itemize}
            \bigbreak \noindent 
        \item \textbf{Decomposition}: There is no rule that says that a relational database must be made up of a single relation. The way we will solve these anomalies is to add new relations to our database and change the old ones. This is called decomposition.
            \bigbreak \noindent 
            Using the example from above, we can remove the anomalies by decomposing the database into two relations.
            \bigbreak \noindent 
            \textbf{SP}(\underline{SuppName}, \underline{Item}, Price)
            \bigbreak \noindent 
            \begin{center}
                \begin{tabular}{|l|l|l|}
                    \hline
                    \textbf{SuppName} & \textbf{Item} & \textbf{Price} \\ \hline
                    John    & Apple   & \$2.00  \\ \hline
                    John    & Orange  & \$2.50  \\ \hline
                    Jane    & Grape   & \$1.25  \\ \hline
                    Jane    & Apple   & \$2.25  \\ \hline
                \end{tabular}
            \end{center}
            \bigbreak \noindent 
            \textbf{S}(SuppName, SuppAddr)
            \bigbreak \noindent 
            \begin{center}
                \begin{tabular}{|l|l|}
                    \hline
                    \textbf{SuppName} & \textbf{SuppAddr} \\ \hline
                    John    & 10 Main   \\ \hline
                    Jane    & 20 State  \\ \hline
                    Frank   & 30 Elm    \\ \hline
                    Sally   & 40 Pine   \\ \hline
                \end{tabular}

            \end{center}
        \item \textbf{When to decompose}: One way of designing a database could be to list all of the possible anomalies and then decompose to fix each of them. The problem with this is that any anomalies you don’t see coming will not be fixed.
            \bigbreak \noindent 
            We will look at a systematic method of identifying the potential for anomalies. This method is called normalization
        \item \textbf{Normalization}: Normalization involves making sure that each of your relations follows certain rules. Depending on which rules are followed, each of the relations in your database will be in one or more normal forms. These rules are based on functional dependencies
        \item \textbf{Functional Dependencies}: A functional dependency is a statement about which attributes can be inferred from other attributes. If we take $X$ and $Y$ as sets of attributes, we can write:
            \begin{align*}
                X \to Y
            .\end{align*}
            \bigbreak \noindent 
            Which means, if, whenever unique values for \textbf{all} of the attributes in $X$ are known, unique values for \textbf{each} of the attributes of $Y$ are guaranteed to be possible to look up or to infer using those values.
            \bigbreak \noindent 
            This is read either as:
            \begin{itemize}
                \item $X$ functionally determines $Y$
                \item $Y$ is functionally dependent upon $X$
            \end{itemize}
        \item \textbf{Functional Dependencies: Real-life Examples}: 
            \begin{itemize}
                \item \textbf{ZID $\to$ StudentFirstName, StudentLastName, Birthday}: If I identify a student using their ZID, that student has one first name, last name, and birthday
                \item \textbf{StudentFirstName \not \to ZID}: The first name is not enough to determine a single ZID, as there are multiple students with the same first name
                \item \textbf{ZID, CourseID, Semester \to Grade}: If I know which student, which course, and which semester, I can find a single grade
            \end{itemize}
        \item \textbf{Functional Dependencies: Keep In Mind}: FDs are constraints present within the operational data your database models. They don’t necessarily describe how things work in the real world, but they do have to accurately describe any data you will store in your database
            \bigbreak \noindent 
            FDs \textbf{must} hold for all possible data values. Attempts to add data that does not obey the FDs will result in anomalies.
            \bigbreak \noindent 
            FDs can be enforced during insertion if the database is set up properly
        \item \textbf{Armstrong’s Axioms}: Armstrong’s Axioms are a set of rules for operations that are permissible when manipulating functional dependencies
            \begin{itemize}
                \item \textbf{Reflexivity}: If $Y \subseteq X$, then $X \to Y $
                \item \textbf{Augmentation}: If $X \to Y$, the $XZ \to YZ$ for any $Z$
                \item \textbf{Transitivity}: If $X \to Y$ and $Y \to Z$, then $X \to Z $
                \item \textbf{Decomposition}: If $X \to YZ$, then $X \to Y$ and $X \to Z $
                \item \textbf{Composition}: If $X \to Y$ and $A \to B$, then $XA \to YB $
                \item \textbf{Union (Notation)}: If $X\to Y$ and $Y \to Z$, then $X \to YZ $
                \item \textbf{Pseudo-transitivity}: If $X \to Y$ and $YZ \to W$, then $XZ \to W $
                \item \textbf{Self-determination}: $I \to I $ for any $I$
            \end{itemize}
        \item \textbf{Functional Dependencies: Keys Revisited}: Now that we know about functional dependencies (FDs), we can assert:
            \bigbreak \noindent 
            \begin{center}
                The attributes of a superkey must functionally determine all of the attributes of the relation.
            \end{center}
            \bigbreak \noindent 
            Candidate keys and primary keys are superkeys, so this is true of them as well, and they also satisfy additional requirements.
            \bigbreak \noindent 
            \textbf{Example:} As an example, say we have the relation \textbf{R}(\underline{a},b,c,d,e,f). We can say
            \begin{align*}
                a &\to a,b,c,d,e,f \\
                \implies a&\to b,c,d,e,f
            .\end{align*}
        \item \textbf{First Normal Form (1NF)}: You should recall from the introduction to relations that all of the values in a tuple with a relation must be atomic. This means that there is a maximum of one value per attribute per tuple
            \bigbreak \noindent 
            The requirement for a relation to be in First Normal Form (1NF) is this same requirement that all of the values must be atomic
            \bigbreak \noindent 
            What this usually looks like is a table with mutltiple values in a single cell. A non-1NF relation would not even technically count as a relation.
            \bigbreak \noindent 
            Given the table:
            \bigbreak \noindent 
            \begin{center}
                \begin{tabular}{|c|c|c|}
                    \hline
                    X  & Y  & Z  \\ \hline
                    x1 & y1 & z1 \\ 
                       &    & z2 \\ 
                       &    & z3 \\ \hline
                    x2 & y2 & z4 \\ \hline
                    x3 & y2 & z5 \\ \hline
                \end{tabular}
            \end{center}
            \bigbreak \noindent 
            It looks like $X$ would have been the primary key, but it’s not doing its job of uniquely determining $Z$, which is showing as a repeating group so $X$ can’t be a key
            \bigbreak \noindent 
            What usually causes this is not having the correct primary key
            \bigbreak \noindent 
            The table above has the following function dependencies:
            \begin{align*}
                X &\to Y \\
               X,Z &\to Z
            .\end{align*}
            \bigbreak \noindent 
            To move this pseudo-relation into an actual relation that doesn’t violate 1NF, we need to choose a real primary key that meets the requirements. We do that using the FDs. In this case, ($X$ , $Z$) works.
            \bigbreak \noindent 
            Changing the primary key yields: - $R$($X$, $Y$ , $Z$))
            \bigbreak \noindent 
            \begin{center}
                \begin{tabular}{c|c|c}
                    X  & Y  & Z  \\ \hline
                    x1 & y1 & z1 \\ \hline
                    x1 & y1 & z2 \\ \hline
                    x1 & y1 & z3 \\ \hline
                    x2 & y2 & z4 \\ \hline
                    x3 &y2 & z5 
                \end{tabular}
            \end{center}
        \item \textbf{Pseudo-relation}: The notation for a “pseudo-relation” like the one above would be to use inner parenthesis on the repeating group, ie. \textbf{R}($X$, $Y$ , ($Z$))
        \item \textbf{Second Normal Form (2NF)}: Second Normal Form (2NF) has to do with the concept of full dependence.
            \bigbreak \noindent 
            Given two sets of attributes, $X$ and $Y$ , we can say that $Y$  is fully dependent on $X$, if (and only if)
            \begin{align*}
                X \to Y
            .\end{align*}
            And no subset of $X$ determines $Y$
            \bigbreak \noindent 
            A relation is in 2NF if:
            \begin{itemize}
                \item It already meets the requirements of 1NF, and
                \item All non-prime attributes of the relation are fully dependent upon the entire primary key
            \end{itemize}
            \bigbreak \noindent 
            What breaks 2NF is when attributes are dependent upon only part of the primary key. To fix 2NF violations once we’re in 1NF, decomposition is the solution.
            \bigbreak \noindent 
            \textbf{Example:} Going back to our earlier example: \textbf{EmpProj}(\underline{EmpID}, \underline{Project}, Supv, Dept, Case)
            \bigbreak \noindent 
            \begin{center}
            \begin{tabular}{|c|c|c|c|c|}
                \hline
                EmpID & Project & Supv & Dept & Case \\ \hline
                e1    & p1      & s1   & d1   & c1   \\ \hline
                e2    & p2      & s2   & d2   & c2   \\ \hline
                e1    & p3      & s1   & d1   & c3   \\ \hline
                e3    & p3      & s1   & d1   & c3   \\ \hline
            \end{tabular}
        \end{center}
        \bigbreak \noindent 
            \textbf{Functional Dependencies}:
            \begin{center}
                EmpID, Project $\to$ Supv, Dept, Case \\
                EmpID $\to$ Supv, Dept \\
                Supv $\to$ Dept
            \end{center}
            \bigbreak \noindent 
            A quick glance confirms all of the values are atomic, so 1NF is confirmed.
            \bigbreak \noindent 
            There is a 2NF violation caused by (EmpID $\to$ Supv, Dept) because the primary key is (EmpID, Project), but only EmpID is on the LHS.
            \bigbreak \noindent 
            Observing the instance data, you should easily see that the attributes of the RHS cause update anomalies in this
            table. We also can’t insert a new employee with no project (insertion anomaly), and removing e2 from p2 would
            remove e2 from the database entirely (deletion anomaly). These are symptoms of the 2NF violation.
            \bigbreak \noindent 
            \textbf{Decomposition Pattern}: There is a pattern to follow for the decomposition. Start with the original relation, and the FD that causes the violation.
            \begin{align*}
                &\text{\textbf{EmpProj}(\underline{EmpID}, \underline{Project}, Supv, Dept, Case)} \\
                &\text{\textbf{EmpID} $\to$ Supv, Dept}
            .\end{align*}
            \bigbreak \noindent 
            The attributes on the RHS of the FD are removed from the original relation and placed into a newly created relation that has the FD’s LHS as its primary key. A foreign key links the attribute from the LHS in the original table (the LHS is not removed) to the corresponding tuple in the new table, where it is the primary key.
            \bigbreak \noindent 
            \begin{align*}
                &\text{\textbf{EmpProj}(EmpID, Project, Case)} \\
                &\text{\textbf{Employee}(EmpID, Supv, Dept)}
            .\end{align*}
            \bigbreak \noindent 
            \textbf{Instance of 2NF Version}:
            \bigbreak \noindent 
            \textbf{EmpProj}(EmpID, Project, Case)                           
            \begin{center}
                \begin{tabular}{|c|c|c|}
                    \hline
                    EmpID & Project & Case \\ \hline
                    e1    & p1      & c1   \\ \hline
                    e2    & p2      & c2   \\ \hline
                    e1    & p3      & c3   \\ \hline
                    e3    & p3      & c3   \\ \hline
                \end{tabular}
            \end{center}
            \bigbreak \noindent 
            \textbf{Employee} (EmpID, Supv, Dept
            \bigbreak \noindent 
            \begin{center}
                \begin{tabular}{|c|c|c|}
                    \hline
                    EmpID & Supv & Dept \\ \hline
                    e1    & s1   & d1   \\ \hline
                    e2    & s2   & d2   \\ \hline
                    e3    & s1   & d1   \\ \hline
                \end{tabular}
            \end{center}
            \bigbreak \noindent 
        \item \textbf{Third Normal Form (3NF)}: To be in Third Normal Form (3NF), a relation must
            \begin{enumerate}
                \item already qualify to be in 2NF
                \item none of the non-prime attributes may be transitively dependent upon the primary key
            \end{enumerate}
            \bigbreak \noindent 
            By definition, all non-prime attribute are functionally dependent upon the primary key. What makes a transitive dependency is that there is also some non-prime attribute (which also depends on the key) that also functionally determines the attribute.
            \bigbreak \noindent 
            To quickly identify the transitive dependencies from the list of FDs, look on the LHS for attributes that are non-prime in the context of the current relation.
            \bigbreak \noindent 
            \textbf{Example:}
            \begin{align*}
                &\text{\textbf{EmpProj}(EmpID, Project, Case)} \\
                &\text{\textbf{Employee} (EmpID, Supv, Dept} \\
                &\text{EmpID, Project $\to$ Supv, Dept, Case} \\
                &\text{EmpID $\to$ Supv, Dept} \\
                &\text{Supv $\to$ Dept}
            .\end{align*}
            \bigbreak \noindent 
            In this case, the FD that causes our relations to violate 3NF is (Supv → Dept), and the violation happens in the
            Employee relation. If you refer back to the instance data of that in the 2NF solution, you can see that the
            violation can cause anomalies, so we want to fix it.
            \bigbreak \noindent 
            Just like 2NF, we fix 3NF by decomposing using the FD that causes the violation to occur. \textbf{AT NO POINT DO WE CHANGE THE FDs}
            \bigbreak \noindent 
            \textbf{Decomposition Pattern}: We follow the same pattern for decomposition in 3NF as we did in 2NF. Start with the relation that has the violation, and the FD that causes the violation to occur.
            \begin{align*}
                &\text{\textbf{Employee} (EmpID, Supv, Dept)} \\
                &\text{Supv $\to$ Dept}
            .\end{align*}
            \bigbreak \noindent 
            The attributes on the RHS of the FD are removed from the violating relation and placed into a newly created
            relation that has the FD’s LHS as its primary key. A foreign key links the attribute from the LHS in the original table
            (the LHS is not removed) to the corresponding tuple in the new table, where it is the primary key.
            \begin{align*}
                &\text{\textbf{Employee}(EmpID, Supv)} \\
                &\text{\textbf{SupvDept}(Supv, Dept)}
            .\end{align*}
            \bigbreak \noindent 
            The RHS (Dept) that was a violation when it was in Employee because the LHS (Supv) was non-prime is no longer
            there to cause the problem. It is in the new relation where the LHS (Supv) is the primary key, and therefore we
            don’t have a transitive dependency. These two relations no longer have the 3NF violation.
        \item \textbf{Summary of the normalization forms}:
            \bigbreak \noindent 
            \textbf{First Normal Form (1NF):}
            \begin{itemize}
                \item No repeating groups. All values are atomic.
                \item A primary key must have been chosen, and this primary key must be a proper superkey – it needs to be able to functionally determine every attribute in the relation.
            \end{itemize}
            1NF violations are fixed by choosing an appropriate primary key
            \bigbreak \noindent 
            \textbf{Second Normal Form (2NF) - To be in Second Normal Form, a relation must conform to 1NF and:}
            \begin{itemize}
                \item All of the non-prime attributes must be fully dependent upon the entire primary key.
                \item No non-prime attribute may be functionally determined by any subset of the primary key.
                \item No partial key dependencies
            \end{itemize}
            \bigbreak \noindent 
            2NF violations are fixed by decomposition.
            \bigbreak \noindent 
            \textbf{Third Normal Form (3NF) - To be in Third Normal Form, a relation must conform to 2NF and:}
            \begin{itemize}
                \item There may be no transitive dependencies.
                \item No non-prime attribute may functionally determine another non-prime attribute.
            \end{itemize}
            \bigbreak \noindent 
            3NF violations are fixed by decomposition.




    \end{itemize}

    \pagebreak 
    \subsection{ERD to Relations (Conceptual to logical)}
    \begin{itemize}
        \item \textbf{The basic outline (steps)}
            \begin{enumerate}
                \item Handle all of the entities
                \item Handle all of the relationships
            \end{enumerate}
        \item \textbf{Entity handling}: We will start with entities, because they can stand on their own, unlike relationships or attributes. In general, each entity will get its own relation. The attributes of the entity will become attributes in the schema of the relation created. There are some special cases to take into account, which will be handled from most independent to least, so:
            \begin{enumerate}[label=\alph*.]
                \item Strong (non-weak) entities that are not subtypes
                \item Strong.(non-weak) entities that are subtypes
                \item Weak entities
            \end{enumerate}
        \item \textbf{Entities like date}: there is no reason to make a relation for a “Date” entity or similar. The single value for the date is enough to determine it, and any other data associated with it is generally happening through a relationship anyway. Think about what data would go into such a table and how little use there would be for storing it separately.
        \item \textbf{Handling strong, non subtype entities}: Make a new relation, whose name will be the same as the name of the entity ▶ The primary key of the relation will be all of the identifier attributes, taken together ▶ All attributes of the entity become attributes of the relation ▶ Every instance of the entity gets the relevant values put into a new tuple in the relation
            \bigbreak \noindent 
            \textbf{Example:} Suppose we had an entity $A$ with attributes $\underline{ID}$, and other:
            \bigbreak \noindent 
            Then, we would make a relation $A(\underline{ID}, other)$
        \item \textbf{Handling strong, subtype entities}: Suppose
            \bigbreak \noindent 
            \fig{.5}{./figures/42.png}
            \bigbreak \noindent 
            \textbf{Employee} is a supertype (not subtype) so it gets handled in the previous step
            \begin{center}
                \textbf{Employee}(\underline{EmpId}, name)
            \end{center}
            \bigbreak \noindent 
            \textbf{Hourly} and \textbf{Salaried} are each strong, but they are subtypes (each is a type of Employee), so they are handled here
            \bigbreak \noindent 
            This type of inheritance means that the subtypes are types of the supertype, so they are identified by \textbf{Employee’s} EmpID
            \bigbreak \noindent 
            There are two methods of handling these.
            \begin{enumerate}
                \item \textbf{Big table}: The first method involves putting the attributes of the subtypes into the relation made for the supertype. So, the original relation:
                    \begin{center}
                        \textbf{Employee}(\underline{EmpID}, Name)
                    \end{center}
                    Would become something like:
                    \begin{center}
                        \textbf{Employee}(\underline{EmpID}, Name,Wage, Salary)
                    \end{center}
                    but it would need to be modified to indicate which subtypes a given employee belongs to. Let’s examine that on the next page.
                    \bigbreak \noindent 
                    The big table method needs a way to know which of the subtypes the current instance of the supertype belongs to, which is handled differently depending on the IS-A’s configuration.
                    \bigbreak \noindent 
                    For \textbf{disjoint subtypes}, where an instance of the supertype can only be one of the subtypes at a timei, we can add an attribute, EmpType that has a value indicating which type this employee is.:
                    \begin{center}
                        \textbf{Employee}(\underline{EmpID}, Name, EmpType,Wage, Salary)
                    \end{center}
                    \bigbreak \noindent 
                    For generalization, EmpType would not allow NULL. For specialization, it would be allowed.
                    \bigbreak \noindent 
                    For \textbf{overlapping subtypes}, it is possible to be more than one at a time, so we need an individual true/false answer for each type:
                    \bigbreak \noindent 
                    \begin{center}
                        \textbf{Employee}(\underline{EmpID}, Name, IsHourly,Wage, IsSalaried, Salary)
                    \end{center}
                    \bigbreak \noindent 
                    In this case, nothing about the schema would indicate generalization vs. specialization
                \item \textbf{New relation}: Method 2 involves creating a new relation for the subtype entity
                    The name of new relation would be the same as the name of the entity.
                    \bigbreak \noindent 
                    The primary key of the new relation would be the same as the primary key for the supertype’s relation.
                    \bigbreak \noindent 
                    The primary key is also a foreign key to the existing table.
                    \bigbreak \noindent 
                    An instance of the supertype entity will only have a tuple in the subtype relation if it is a member of that subtype, so we will not need any extra attributes like we did in method 1.
                    \bigbreak \noindent 
                    The foreign key can be used to look up any of the attributes that are being inherited from the supertype
                    \bigbreak \noindent 
                    Thus, we would have 
                    \begin{align*}
                        &\text{\textbf{Employee}(\underline{EmpID}, Name} \\
                        &\text{\textbf{Hourly}(\underline{EmpID\dag},Wage)} \\
                        &\text{\textbf{Salaried}(\underline{EmpID\dag}, Salary)}
                    .\end{align*}
                    \bigbreak \noindent 
                    \textbf{Note:} The (\dag) (dagger symbol) will be used in these slides to indicate that the attribute is part of a foreign key (and, in this example, the whole thing).
            \end{enumerate}
        \item \textbf{Handling weak entities}:  Suppose
            \bigbreak \noindent 
            \fig{.5}{./figures/43.png}
            \bigbreak \noindent 
            The strong entity would already have a relation. 
            \begin{center}
                \textbf{Strong}(\underline{id}, x)
            \end{center}
            \bigbreak \noindent 
            The weak entity gets its own relation. The primary key will be the concatenation of the weak entity’s discriminator with the strong entity’s identifier. The other attributes of the entity are brought in as non-prime attributes.
            \begin{center}
                \textbf{Weak}(\underline{id}\dag, \underline{disc}, y)
            \end{center}
            \bigbreak \noindent 
            The \underline{id} portion is a foreign key to the Strong relation
        \item \textbf{Entities: Functional Dependencies}: The only functional dependencies introduced by the entities of an ER diagram are the ones introduced when the identifiers become primary keys. Remember that a primary key has to functionally determine all of the other attributes in a relation
        \item \textbf{Handle relationships}: The relationships will be handled in order from lowest degree to highest degree, and within that, from simplest cardinality (one-to-one) to more complicated cardinalities (many-to-many, etc.).
            \bigbreak \noindent 
            The purpose of a relationship is to form connections between entities. We know that we are using relations to represent our entities, so we will need to use a tool that can link those relations to each other.
            \bigbreak \noindent 
            The tool best suited to linking tuples from relations together is the foreign key.
            \bigbreak \noindent 
            Every relationship we model in the relational model will have one or more foreign key involved. Where we put these foreign keys will depend on the cardinality, and the decisions are motivated by the normal forms we discussed.
            \begin{enumerate}
                \item \textbf{Binary one-to-one Relationships}: In a binary relationship, we will already have made a relation for each of the entities involved.
                    \bigbreak \noindent 
                    \fig{.5}{./figures/44.png}
                    \begin{center}
                        \textbf{A}(\underline{a},p) $\quad$ \textit{and} $\quad$ \textbf{B}(\underline{b}, q)
                    \end{center}
                    \bigbreak \noindent 
                    Since each instance of B will have one of A, and each instance of A will have one of B through C, we can represent this one-to-one relationship by putting a new foreign key into the entity for either side. Choose either:
                    \begin{center}
                        \textbf{A}(\underline{a},p,b\dag) $\quad$ \textit{or} $\quad$ \textbf{B}(\underline{b}, q, a\dag)
                    \end{center}
                    \bigbreak \noindent 
                    The relationship implies the functional dependencies:
                    \begin{align*}
                        a \to b \\
                        b \to a
                    .\end{align*}
                \item \textbf{Binary one-to-many Relationships}: In a binary relationship, we will already have made a relation for each of the entities involved.
                    \begin{center}
                        \textbf{A}(\underline{a},p) $\quad$ \textit{and} $\quad$ \textbf{B}(\underline{b}, q)
                    \end{center}
                    \bigbreak \noindent 
                    For this one-to-many relationship, there can be many instances of B for each of A, so we can’t have the foreign key in the A table (wouldn’t be atomic, so 1NF would be violated). We still do have the option of putting a foreign key in the B table pointing to the corresponding A, so our only option is: \begin{center}
                        \textbf{B}(\underline{b},q,a\dag)
                    \end{center}
                    \bigbreak \noindent 
                    The only FD is 
                    \begin{align*}
                        b \to a 
                    .\end{align*}
                    \bigbreak \noindent 
                \item \textbf{Binary many-to-many Relationships}: In a binary relationship, we will already have made a relation for each of the entities involved.
                    \begin{center}
                        \textbf{A}(\underline{a},p) $\quad$ \textit{and} $\quad$ \textbf{B}(\underline{b}, q)
                    \end{center}
                    \bigbreak \noindent 
                    There are no new functional dependencies introduced by the relationship, and putting a foreign key into either relation would not be atomic (1NF violation). The many-to-many relationship requires a new relation. Its foreign key will be the concatenation of the primary keys of each of the entity relations, which will be used as foreign keys to the corresponding tables. Any intersection data is put into this new relation as a non-prime attribute.
                    \bigbreak \noindent 
                    \begin{center}
                        \textbf{C}(\underline{a}\dag, \underline{b}\dag, x)
                    \end{center}
                \item \textbf{Relationships Greater than Binary: one-to-one-to-one}:
                    \bigbreak \noindent 
                    \fig{.7}{./figures/45.png}
                    \bigbreak \noindent 
                    So we have 
                    \begin{center}
                        \textbf{A}(\underline{a}, p) and \textbf{B}(\underline{b}, q) and \textbf{C}(\underline{c},r
                    \end{center}
                    \bigbreak \noindent 
                    Each of the “one legs” represents a functional dependency, and each of them gives us a potential relation to choose from for our relation.
                    \bigbreak \noindent 
                    \begin{table}[h!]
                        \centering
                        \begin{tabular}{ll}
                            \toprule
                            \textbf{Functional Dependency} & \textbf{Potential Relation for \textbf{D}} \\ 
                            \midrule
                            $a, b \rightarrow c$ & $\mathbf{D} \left( a^\dagger, b^\dagger, c^\dagger, x \right)$ \\[8pt]
                            $b, c \rightarrow a$ & $\mathbf{D} \left( a^\dagger, b, c^\dagger, x \right)$ \\[8pt]
                            $a, c \rightarrow b$ & $\mathbf{D} \left( a^\dagger, b, c^\dagger, x \right)$ \\ 
                            \bottomrule
                        \end{tabular}
                    \end{table}
                    \textbf{Note:} If we have say only two ones, like a one to one to many relationship, we would just have less functional dependencies and therefore less options to choose from (see table above)
                \item \textbf{Greater than Binary without any “ones”}:
                    No functional dependencies are implied by this relationship. To stay in 3NF, the relation we must use is:
                    \begin{center}
                        \textbf{D}(\underline{a}\dag, \underline{b}\dag, \underline{c}\dag, x)
                    \end{center}
                \item \textbf{Date entities (and similar)}: For relationships that have a “Date” entity (or the equivalent), recall that we did not make a relation for that entity. The only change necessary for your relationship involving that entity is that the date value is used instead of a foreign key, and that attribute will not be a foreign key, because the home relation would not exist
                    \bigbreak \noindent 
                    As an example, if the C entity in the ternary relationship with no “ones” ER diagram were a Date entity, we would not create the C relation for it, and the relation to represent the relationship would be modified. Notice that the 𝑐 attribute is still part of the primary key, but no longer a foreign key.
                    \begin{align*}
                        &\text{From: \textbf{D}(\underline{a}\dag, \underline{b}\dag, \underline{c}\dag, x)} \\
                        &\text{To: \textbf{D}(\underline{a}\dag, \underline{b}\dag, \underline{c}, x)} \\
                   .\end{align*}
                \item \textbf{Recursive Relationships: one-to-many}: Recursive relationships will be handled as if they were normal relationships of the same degree and cardinality. The practical difference is that the entity that is linked multiple times will still only have one relation, so multiple foreign keys might go to the same table.
                    \bigbreak \noindent 
                    Suppose: 
                    \bigbreak \noindent 
                    \fig{.7}{./figures/46.png}
                    \bigbreak \noindent 
                    There should obviously only be one relation for the entity Department, because it is only a single entity.
                    \begin{center}
                        \textbf{Department}(\underline{DeptNo}, Name)
                    \end{center}
                    \bigbreak \noindent 
                    With a non-recursive one-to-many binary relationship, we would have put a foreign key to the relation for the one
                    side into the relation on the one side. In this version, we only have one table, so the decision is easy. We will need
                    to come up with another name for the foreign key, as we cannot have two attributes with the same name inside the
                    same relation. Thus, we grow Department into the following:
                    \begin{center}
                        \textbf{Department}(\underline{DeptNo}, Name, ReportsToDept\dag)
                    \end{center}
                    \bigbreak \noindent 
                    Where the home relation for the new attribute, ReportsToDept, is that same relation, Department. The tuple of
                    the department that the current department reports to will be have a DeptNo that equals the ReportsToDept in
                    the current tuple. Alternatively, ReportsToDept can be NULL if the department does not report to another.
                \item \textbf{Recursive Relationships, many-to-many}: Suppose 
                    \bigbreak \noindent 
                    \fig{.87}{./figures/47.png}
                    \bigbreak \noindent 
                    Like non-recursive many-to-many relationships, we will need to create a new relation. Unlike the non-recursive
                    version, we only have one home relation for our two foreign keys. As in the one-to-many version, we will need to
                    choose a new name for at least one copy of the foreign key, since they can’t share the same name. The relation for
                    our Person entity would be \textbf{Person}(\underline{ID}, Name)
                    \bigbreak \noindent 
                    The new relation created to represent the relationship would be in the following form:
                    \begin{center}
                        \textbf{Friends}(\underline{activeFriend}\dag, \underline{passiveFriend}\dag)
                    \end{center}
                    \bigbreak \noindent 
                    ActiveFriend and PassiveFriend are foreign keys to the tuple in Person with data for the person that is taking part in
                    the relationship. This can be done in a directed or undirected way, and you probably want to put a comment
                    somewhere about which way you intend to use it.
                    \bigbreak \noindent 
                    directed: (Person1, Person2) would not imply (Person2, Person1)
                    \bigbreak \noindent 
                    undirected: (Person1, Person2) does imply (Person2, Person1)
                    \bigbreak \noindent 
                    \fig{.6}{./figures/48.png}
            \end{enumerate}
        \item \textbf{Summary}:
            \begin{enumerate}
                \item Strong, non-subtype entities
                    \begin{itemize}
                        \item New relation, PK is entities identifiers
                    \end{itemize}
                \item Sub-type entities
                    \begin{itemize}
                        \item New relations, PK is supertype identifiers, which are foreign keys to supertype relation
                    \end{itemize}
                \item Weak entities
                    \begin{itemize}
                        \item New relation, PK is concatenation of strong identifier and discriminator. Strong Id from the concat is FK to strong relation.
                    \end{itemize}
                \item \textbf{Relationships: Binary 1-1}
                    \begin{itemize}
                        \item Put foreign key in either side
                    \end{itemize}
                \item \textbf{Binary 1-m}
                    \begin{itemize}
                        \item Put foreign key to one side in the many side                     
                    \end{itemize}
                \item \textbf{Binary m-m}
                    \begin{itemize}
                        \item New relation, PK is concatenation of both entities keys, which also serves as foreign keys to entities. 
                    \end{itemize}
                \item \textbf{n-ary 1-1-...-1 (all ones)}
                    \begin{itemize}
                        \item New relation, choose n-1 entities for PK, put remaining entity ID as non-prime, but foreign.
                    \end{itemize}
                \item \textbf{n-ary 1-1-...-m (Two ones)}
                    \begin{itemize}
                        \item New relation, choose all many legs and one of the one legs for PK, put remaining one leg as non-prime but foreign
                    \end{itemize}
                \item \textbf{n-ary 1-m-...--m (Single one)}
                    \begin{itemize}
                        \item New relation, choose all many legs for PK, put remaining one leg as non-prime but foreign 
                    \end{itemize}
                \item \textbf{n-ary m-m-...-m (No ones)}
                    \begin{itemize}
                        \item New relation, all legs are PK
                    \end{itemize}
                \item \textbf{Handle date entities, and things of that nature}
                    \begin{itemize}
                        \item Since we do not create relations for these types of entities, we cannot make them foreign keys, because the home relation will not exist. They can still be part of the PK.
                    \end{itemize}
                \item \textbf{Recursive relationships}
                    \begin{itemize}
                        \item We handle these the same, but the foregin key will link to the same relation. Make sure to put a comment somewhere to specify directed or undirected.
                    \end{itemize}
            \end{enumerate}

    \end{itemize}

    \pagebreak 
    \subsection{MariaDB, SQL}
    \begin{itemize}
        \item \textbf{DDL}:
            \begin{itemize}
                \item CREATE TABLE
                \item ALTER TABLE
                \item DROP TABLE
            \end{itemize}
        \item \textbf{DML}:
            \begin{itemize}
                \item INSERT 
                \item UPDATE
                \item DELETE
            \end{itemize}
        \item \textbf{MariaDB navigation}:
            \begin{itemize}
                \item \textbf{USE <x>}: select the database <x>
                \item \textbf{SHOW TABLES} list all of the tables in the current database
                \item \textbf{DESCRIBE <x>} show the properties of each column of table <x>
                \item \textbf{SHOW CREATE TABLE} <x> show a CREATE TABLE statement that can be used to reconstruct table <x>
            \end{itemize}
        \end{itemize}
        \pagebreak \bigbreak \noindent 
        \subsubsection{DDL}
        \begin{itemize}
        \item \textbf{Creating a new table with CREATE TABLE}: The basic format of a CREATE TABLE statement. []’s and <>’s are not to be typed. [] indicates that the contents are optional, and the <>’s indicate placeholders:
            \bigbreak \noindent 
            \begin{sqlcode}
                CREATE TABLE <table_name> (
                    <attribute> <type> [NOT NULL] [UNIQUE] [PRIMARY KEY], [ ... ]
                    [PRIMARY KEY(<pkattrs>),]
                    [FOREIGN KEY(<attr_here>) REFERENCES <home_table>(<attr_home>)]
                );
            \end{sqlcode}
            \begin{itemize}
                \item <table\_name> name of the table
                \item <attribute> name of the current attribute
                \item <type> data type of the current attribute
                \item <pkattrs> comma-separated list of the attributes makeing up the table’s primary key
                \item <attr\_here> comma-separated list of attributes in the current table forming a foreign key
                \item <home\_table> name of the home table
                \item <attr\_home> comma-separated list of attributes in the home table, matching the attributes in <attr\_here>
            \end{itemize}
        \item \textbf{Table / Column names}: When choosing a name for a table or a column, we can use the following characters:
            \begin{itemize}
                \item any of the normal upper or lower case letters (regexp: [A-Za-z]
                \item an underscore – \_
                \item a dollar sign – \$
                \item digits, but only after the first character
            \end{itemize}
            \bigbreak \noindent 
            The following limits are in place:
            \begin{itemize}
                \item Table names must be unique within the database. They share the same namespace with views.
                \item Attribute/column names must be unique with each table.
                \item Unless quoted properly with backticks, reserved keywords cannot be used as identifier
            \end{itemize}
            \bigbreak \noindent 
            \textbf{Note:} These identifiers may or may not be case sensitive, depending on the locale setting of the server.
            \bigbreak \noindent 
            Generally, the maximum length of an identifier is 64 characters.
        \item \textbf{Data types}
            \begin{itemize}
                \item \textbf{INT/INTEGER}: integer values
                \item \textbf{FLOAT}: single precision floating point numbers
                \item \textbf{DOUBLE/REAL}: double precision floating point numbers
                \item \textbf{DECIMAL(i,j)}: decimal numbers, i digits total, j after the decimal point .
                \item \textbf{CHAR(n)}: character string exactly n characters long
                \item \textbf{VARCHAR(n)}: variable-length character string up to n characters long
                \item \textbf{DATE}: date in 'YYYY-MM-DD' format
                \item \textbf{TIME}: time in 'HH:MM:SS' format
                \item \textbf{DATETIME}: date/time in 'YYYY-MM-DD HH:MM:SS' format, no timezone conversion
                \item \textbf{TIMESTAMP}: date/time in 'YYYY-MM-DD HH:MM:SS' format, timezone conversion
            \end{itemize}
        \item \textbf{Column Options}: Here are some common options that can be applied to a column/attribute. They are written right after the type when defining a new column in a CREATE TABLE statement
            \begin{itemize}
                \item \textbf{NULL}: allows NULL to be stored as the value for this attribute (default)
                \item \textbf{NOT NULL}: prevents NULL from being stored as the value for this attribute
                \item \textbf{UNIQUE}: ensures that no two tuples have the same value for this attribute
                \item \textbf{PRIMARY KEY}: declares this attribute to be the entire primary key
                \item \textbf{AUTO\_INCREMENT}: next-available value auto-assigned for this attribute when not provided
                \item \textbf{DEFAULT <x>}: sets the default value of the attribute to <x> when not supplied
            \end{itemize}
        \item \textbf{Setting the Primary Key}: There are two ways to set the primary key:
            \begin{enumerate}
                \item For single-attribute primary keys, you can use the PRIMARY KEY column option. The option may only be used once, and proclaims that the single attribute is the entirety of the primary key.
                \item If you have multiple attributes in the primary key, the only way is to add the separate constraint:
                    \begin{center}
                        \texttt{PRIMARY KEY(<x>,<y>,<z>,<etc>)}
                    \end{center}
                    \bigbreak \noindent 
                    This can also be used for single attribute primary keys.
                    \bigbreak \noindent 
                    \textbf{Note:} It should be obvious that only one primary key can be set.
            \end{enumerate}
        \item \textbf{Comments}: MariaDB supports the following comment syntax
            \begin{enumerate}
                \item \textbf{Pound (#)}:
                \item \textbf{Double hypen (\texttt{--})}: This is the standard style
                \item \textbf{C-style multiline comments (\textbackslash * ... *\textbackslash)}
            \end{enumerate}
        \item \textbf{Quotes}: There are two types of quotes that you may encounter in SQL.
            \begin{enumerate}
                \item \textbf{Quotes for values – single quotes 'value'}: not necessary for numeric values, but can be used without breaking them, always required for string values. If it is ambiguous whether something is a value or an identifier, use these quotes
                \item \textbf{Quotes for identifiers – backticks `identifier}: not necessary for identifiers that follow the rules from above, but can be used anyway, can allow identifier names to contain characters not otherwise allowed.  Can allow identifiers to use names that would normally be reserved keywords
            \end{enumerate}
            \bigbreak \noindent 
            \textbf{Note:} Notice that identifier in the SQL context is a different thing than an identifier in an ER diagram. Here, identifier will mean the name of some table, column, variable, etc.
        \item \textbf{An example of CREATE TABLE}: Let’s go ahead and make the SQL CREATE TABLE statement to create a table for the relation:
            \begin{center}
                \textbf{Person}(\underline{SSN}, FNAME, LNAME, PHONE)
            \end{center}
            \bigbreak \noindent 
            \begin{sqlcode}
                CREATE TABLE Person(
                    SSN CHAR(9) PRIMARY KEY, # SSN BAD IDEA, PK on same line (1)
                    FNAME CHAR(20) NOT NULL, # First name
                    LNAME CHAR(20) NOT NULL, # Last name
                    PHONE CHAR(10) # Phone number
                ); 
            \end{sqlcode}
            \bigbreak \noindent 
            The relational schema we started with does not have information on data types or column options other than PRIMARY KEY, so we choose them while creating the table.
        \item \textbf{Setting up a foreign key}: A foreign key links the current table to another table, which we call the home relation.
            \begin{enumerate}
                \item The foreign key must contain all of the attributes of the primary key of the home relation.
                \item They may have different names in each of the tables, but there needs to be a match for each.
                \item Each of these attributes must have the exact same data type as its counterpart in the home table.
            \end{enumerate}
            \bigbreak \noindent 
            If a table is to contain a foreign key, we include a constraint in our CREATE TABLE statement like the following:
            \begin{sqlcode}
                FOREIGN KEY (<localnames>) REFERENCES <home_table>(<homenames>)
            \end{sqlcode}
            This can be done for multiple foreign keys, filling in the placeholders <localnames>, <home\_table>, and <homenames> appropriately for each.
    \item \textbf{Table with foreign key example}: Let’s make a table for a subtype of Person, Student:
        \begin{center}
            \textbf{Student}(\underline{SSN}\dag, CLSYEAR, GPA, TOTALHRS)
        \end{center}
        \bigbreak \noindent 
        \begin{sqlcode}
            CREATE TABLE Student (
                SSN CHAR(9) NOT NULL, -- SSN is BAD IDEA
                CLSYEAR CHAR(9), -- fresh/soph/junior/senior
                GPA DECIMAL(4.3), -- 4.000, we hope
                TOTALHRS INT,

                PRIMARY KEY (SSN), -- set up the primary key separately (2)
                FOREIGN KEY (SSN) REFERENCES Person(SSN) -- a Student is a Person
            );
        \end{sqlcode}
        \bigbreak \noindent 
        \textbf{Note:} We need to use SHOW CREATE TABLE to show the get information of the foreign keys of a table.
    \item \textbf{Change existing table schema: ALTER TABLE}: An ALTER TABLE statement will allow you to have the DBMS make changes to the schema of a table that has already been created. It works with various subcommands. The three we will cover are:
        \begin{enumerate}
            \item ALTER TABLE ADD
            \item ALTER TABLE MODIFY
            \item ALTER TABLE DROP
        \end{enumerate}
    \item \textbf{ALTER TABLE ADD}: The ALTER TABLE ADD command can be used to add a new column or new columns to the schema of an existing table.
        \bigbreak \noindent 
        To add a single column/attribute
        \bigbreak \noindent 
        \begin{sqlcode}
            ALTER TABLE <table_name> ADD <attribute> <type>; 
        \end{sqlcode}
        \bigbreak \noindent 
        To add multiple columns/attributes:
        \bigbreak \noindent 
        \begin{sqlcode}
            ALTER TABLE <table_name> ADD (<attribute> <type>, ...);
        \end{sqlcode}
    \item \textbf{ALTER TABLE MODIFY}: The ALTER TABLE MODIFY command can be used to change properties of a column/attribute (including type, length, and other column options) in a table that already exists.
        \bigbreak \noindent 
        \begin{sqlcode}
            ALTER TABLE <table_name> MODIFY <col_name> <new_options>;
        \end{sqlcode}
    \item \textbf{ALTER TABLE DROP}: The ALTER TABLE DROP command can be used to remove a column/attribute from the schema of a table.
        \bigbreak \noindent 
        \begin{sqlcode}
            ALTER TABLE <table_name> DROP <col_name>;
        \end{sqlcode}
    \item \textbf{SHOW TABLES}: In MariaDB/MySQL, if you want to see a list of the tables present in the current database, you can use the command:
        \bigbreak \noindent 
        \begin{sqlcode}
            SHOW TABLES;
        \end{sqlcode}
    \item \textbf{DROP TABLE}: To remove a table from the database, we can use the DROP TABLE command.
        \bigbreak \noindent 
        \begin{sqlcode}
        DROP TABLE <table_name>;
        \end{sqlcode}
    \item \textbf{Termination of commands (;)}: Notice in all sql code examples we have a semi colon after the command / line, this is needed to execute the command.








    \end{itemize}

    \pagebreak 
    \subsubsection{DML except SELECT}
    \begin{itemize}
        \item \textbf{DML Introduction}: The Data Manipulation Language (DML) is the language used to work with the instance data. In SQL, this means doing things with the rows contained by tables, rather than to the tables themselves. We have
            \begin{itemize}
                \item \textbf{INSERT}: Add a new row to a table
                \item \textbf{UPDATE}: Change values in an existing row
                \item \textbf{DELETE}: Remove rows from the table
                \item \textbf{SELECT}: Display the data stored in rows (In the next subsection)
            \end{itemize}
        \item \textbf{INSERT}:
            \begin{sqlcode}
                INSERT INTO <table_name>
                    VALUES (<value_list>);

                INSERT INTO <table_name>
                    (<attr_list>)
                    VALUES (<value_list>);

                INSERT INTO <table_name>
                    <another_query>;
            \end{sqlcode}
            \bigbreak \noindent 
            Where 
            \begin{itemize}
                    \item \textbf{<table\_name>}: The name of the table where the row should be added.
                    \item \textbf{<value\_list>}: A list of values for the new row. If no <attr\_list> is given, then the values are for each of the columns of the table, in order.
                    \item \textbf{<attr\_list>}: A list of names of attributes that match up with the values in <value\_list>. This allows us to omit optional columns or change the order.
                    \item \textbf{<another\_query>}: A query that returns rows, like a SELECT statement. The rows returned are inserted into the table.
            \end{itemize}
            \bigbreak \noindent 
            \textbf{Notes}: Without the attribute list, there must be a value in the VALUES() for every column, and they have to be in the same order as they had in the table.
            \bigbreak \noindent 
            Columns not in the attribute list are set to their default value if possible. This is why PHONE is NULL. This version of the INSERT statement is better if you’re making SQL that needs to be in a script that is to be run later, as it tolerates more changes to the table schema than the other version.
        \item \textbf{The WHERE clause}:
            \bigbreak \noindent 
            \begin{sqlcode}
            ... WHERE <expression> ...
            \end{sqlcode}
            \bigbreak \noindent 
            When working with DML statements, it will be desirable to be able to work only with specific rows. This can be accomplished using a WHERE clause.
            \bigbreak \noindent 
            The WHERE clause is the keyword WHERE followed by an expression that evaluates to either true or false. It is included in an SQL query to control which rows are affected by the query
            \bigbreak \noindent 
            The expression after WHERE is evaluated one time per row. Rows where the expression evaluates as true are included in the operation. Rows where the expression evaluates to false are excluded from the operation
            \bigbreak \noindent 
            WHERE clauses are generally used in UPDATE, DELETE, and SELECT statements.
        \item \textbf{UPDATE}:
            \bigbreak \noindent 
            \begin{sqlcode}
                UPDATE <table_name>
                    SET <attr> = <value> [, <attr> = <value> ...]
                    [ WHERE <expression> ];
            \end{sqlcode}
            \bigbreak \noindent 
            Where
            \begin{itemize}
                \item \textbf{<attr>}: name of a column to change
                \item \textbf{<value>}: value to assign to <attr>
                \item \textbf{<expression>}: expression evaluated for each row to determine if the row is affected
            \end{itemize}
        \item \textbf{DELETE}: To delete the rows without getting rid of the table, use a DELETE statement.
            \bigbreak \noindent 
            \begin{sqlcode}
            DELETE FROM <table_name>
                [ WHERE <expression> ];
            \end{sqlcode}
            \bigbreak \noindent 
            It is important to realize that all rows are affected by default, so if a WHERE clause is not supplied, all of the rows will be deleted.
        \item \textbf{Views in SQL}: A view in SQL is a virtual table. It does not store its own data, but rather derives it from the other tables (or views) via a query that is a part of its definition.
            \bigbreak \noindent 
                Views do not contain their own data. They dynamically grab their data from the base tables on demand. Thus, changes to the data in the base tables will be reflected in the views that derive from them automatically
        \item \textbf{CREATE VIEW}:
            \bigbreak \noindent 
            \begin{sqlcode}
                CREATE VIEW <view_name>
                    [( <view_col_name> [, <view_col_name>]...)] # can rename columns here
                    AS SELECT <attr_name> [, <attr_name>] ...
                        FROM <source_table_or_view> [, ...]
                        WHERE <condition>;
                    \end{sqlcode}
                    \bigbreak \noindent 
                    The portion after the AS keyword is a SELECT statement, part of the DML that is used to ask the DBMS to show
                    portions of instance data (rows from tables). 
                    \bigbreak \noindent 
                    Once the view is created, it supports DML queries in most of the same ways a non-virtual table can be. Writing to a
                    view is sometimes possible, but depends on how the SELECT statement that constructed it was formulated. It is
                    generally a better idea to write directly to the base tables.
            \bigbreak \noindent 
            \textbf{Example:}
            \bigbreak \noindent 
            \begin{sqlcode}
                CREATE VIEW dekalb_people
                    (SSN, first_name, last_name) # control the names of the columns as seen in the view
                    AS SELECT SSN, FNAME, LNAME # control which columns are returned by SELECT
                        FROM Person # get rows from the Person table
                        WHERE ZIP = '60115'; # control which rows make it into the view
            \end{sqlcode}
        \item \textbf{DROP VIEW}: Although tables and views share the same namespace (so it is not possible to have a view and a table with the same name) and work the same in a lot of queries, DROP TABLE is one of the exceptions and will not work to delete a view. It will give you an error message
            \bigbreak \noindent 
            Instead, use DROP VIEW, which has generally the same syntax:
            \bigbreak \noindent 
            \begin{sqlcode}
            DROP VIEW <viewname>;
            \end{sqlcode}
        \item \textbf{Advantages of Views}:
            \begin{itemize}
                \item Base tables should always be designed in Third Normal Form or better. Views allow us to access them in possibly more convenient ways while still having the benefits of 3NF.
                \item Views can free users from complicated DML operations, such as joins.
                \item Users can be denied direct access to base tables, but given access to portions of them through the views. This enhances security
            \end{itemize}
    \end{itemize}


    \pagebreak 
    \subsubsection{DML SELECT}
    \begin{itemize}
        \item \textbf{SELECT Statement Format}: Two versions of the basic format of a SELECT statement follow.
            \bigbreak \noindent 
            \begin{sqlcode}
                SELECT [DISTINCT|ALL] <column_list> # most common, show row data
                    FROM <table_list>
                    [ WHERE <where_exp> ]
                    [ GROUP BY <group_key> ]
                    [ HAVING <having_exp> ]
                    [ ORDER BY <sortcols> ] ;

                SELECT <anyexpression> ; # show results of the supplied expression
            \end{sqlcode}
            Where
            \begin{itemize}
                \item \textbf{<column\_list>}: comma separated list of the columns to show in the results, * for all columns
                \item \textbf{<where\_exp>}: boolean expression evaluated once per row to determine whether the row is included
                \item \textbf{<group\_key>}: comma-separated list of the columns to use when grouping the rows
                \item \textbf{<having\_exp>}: boolean expression evaluated once per group to determine whether the group is included
                \item \textbf{<sortcols>}: comma-separated list of the columns to sort by (most important comes first)
                \item \textbf{<anyexpression>}: the expression whose results should be displayed
            \end{itemize}
        \item \textbf{Example data}: Here we have a simple database to use for the examples that follow. It tracks suppliers and the parts they supply.
            \bigbreak \noindent 
            \fig{.5}{./figures/49.png}
            \begin{align*}
                &\text{Supplier Info S(\underline{S}, SNAME, STATUS, CITY)} \\
                &\text{Part Info P(\underline{P}, PNAME, COLOR,WEIGHT)} \\
                &\text{Supplied Parts SP(\underline{S}$^{\dag}$, \underline{P}$^{\dag}$, QTY)}
            .\end{align*}
            \bigbreak \noindent 
            The S table contains the information on the suppliers themselves.
            \bigbreak \noindent 
            \begin{center}
                \begin{tabular}{c|c|c|c}
                    S &SNAME &STATUS &CITY \\
                    \hline
                    S1 &Smith &20 &London \\
                    S2 &Jones &10 &Paris \\
                    S3 &Blake &30 &Paris \\
                    S4 &Clark &20 &London \\
                    S5 &Adams &30 &Athens  
                \end{tabular}
            \end{center}
            \bigbreak \noindent 
            The P table contains information on parts.
            \bigbreak \noindent 
            \begin{center}
                \begin{tabular}{c|c|c|c}
                    P &PNAME &COLOR &WEIGHT \\
                    \hline
                    P1 &Nut &Red &12 \\
                    P2 &Bolt &Green &17 \\
                    P3 &Screw &Blue &17 \\
                    P4 &Screw &Red &14 \\
                    P5 &Cam &Blue &12 \\
                    P6 &Cog &Red &19 
                \end{tabular}
            \end{center}
            \bigbreak \noindent 
            The SP table contains information on which suppliers supply which parts, and how many.
            \bigbreak \noindent 
            \begin{center}
                \begin{tabular}{c|c|c}
                    S& P& QTY \\
                    \hline
                    S1 &P1 &300 \\
                    S1 &P2 &200 \\
                    S1 &P3 &400 \\
                    S1 &P4 &200 \\
                    S1 &P5 &100 \\
                    S1 &P6 &100 \\
                    S2 &P1 &300 \\
                    S2 &P2 &400 \\
                    S3 &P2 &200 \\
                    S4 &P2 &200 \\
                    S4 &P4 &300 \\
                    S4 &P5 &400
                \end{tabular}
            \end{center}
        \item \textbf{Example query}: 
            \bigbreak \noindent 
            Get supplier numbers and status for suppliers in Paris.
            \bigbreak \noindent 
            \begin{sqlcode}
            SELECT S,STATUS
                FROM S
                WHERE CITY = 'paris';
            \end{sqlcode}
            \bigbreak \noindent 
            Get part numbers for all parts supplied
            \bigbreak \noindent 
            \begin{sqlcode}
            SELECT P FROM SP;
            \end{sqlcode}
            \bigbreak \noindent 
            Adding the DISTINCT keyword can prevent duplicate output rows from being shown.
            \bigbreak \noindent 
            \begin{sqlcode}
            SELECT DISTINC P
                FROM SP;
            \end{sqlcode}
            \bigbreak \noindent 
            List the full details of all suppliers.
            \bigbreak \noindent 
            \begin{sqlcode}
            SELECT * FROM S;
            \end{sqlcode}
            \bigbreak \noindent 
            List supplier numbers for all suppliers in Paris with a STATUS greater than 20.
            \bigbreak \noindent 
            \begin{sqlcode}
            SELECT * FROM S 
                WHERE CITY = 'Paris' AND 
                    STATUS > 20;
            \end{sqlcode}
        \item \textbf{Relational Operators in SQL}:
            \begin{itemize}
                \item $=$ is equal to
                \item $<$ less than
                \item $<=$ less than or equal to
                \item $>$ greater than
                \item $>=$ greater than or equal to
                \item $<>$ or $!=$ not equal to
            \end{itemize}
        \item \textbf{Compound Logical Operators}:
            \begin{itemize}
                \item \textbf{AND}
                \item \textbf{OR}
                \item \textbf{NOT}
            \end{itemize}
        \item \textbf{The ORDER BY clause}: Adding the ORDER BY clause allows us to enforce a sorting order upon our results.
            \bigbreak \noindent 
            \begin{sqlcode}
            ORDER BY <attrs>
            \end{sqlcode}
            \bigbreak \noindent 
            Where <attrs> is a comma-separated list of the attributes to base our sorting upon.
            \bigbreak \noindent 
            After each attribute, you have the option to add either DESC (for descending) or ASC (for ascending) to affect the sort direction for each attribute. The default sort direction is ascending, if not specified.
            \bigbreak \noindent 
            The first attribute listed is the most important, and any subsequent attributes is only sorted upon if there are multiple rows in which the values for the previous attributes before them were all the same.
    \item \textbf{ORDER BY example}: List the supplier numbers and status for suppliers in Paris in descending order of status.
        \bigbreak \noindent 
        \begin{sqlcode}
            SELECT S,STATUS FROM S
                WHERE CITY = 'Paris'
                ORDER BY STATUS DESC;
        \end{sqlcode}
    \item \textbf{Cartesian Product in SQL}: For two sets $A = \{a,b,c\}$, and $B=\{d,e,f\} $
        \begin{align*}
            A \times B = \{(a, d), (a, e), (a, f), (b , d), (b , e), (b ,f), (c, d), (c, e), (c, f)\}
        .\end{align*}
        \bigbreak \noindent 
        This is relevant because the Cartesian Product is used in SQL when we SELECT from multiple tables. When this happens, the sets (like $A$ and $B$) to be combined are the tables, and the items inside of them are the tuples/rows they contain.
        \bigbreak \noindent 
        When the Cartesian Product is done on two tables,
        \begin{itemize}
            \item The width of the result is the sum of the widths (in columns) of both of the tables.
            \item The length (in rows) of the result will be the product of the lengths of both of the tables.
        \end{itemize}
        \bigbreak \noindent 
        \textbf{Note:} The Cartesian Product is an associative operation
    \item \textbf{Aliases and the dot operator}: When we select certain relations, we can give aliases to them and then reference their attributes with a dot (similar to how you access C++ member functions)
        \bigbreak \noindent 
        This is important because if we take the cartesian product of the same relations, we need to give them aliases (they would otherwise have the same name)
    \item \textbf{Aliases with AS}: We can also use AS to assign aliases
        \bigbreak \noindent 
        \begin{sqlcode}
        SELECT col AS c1
            FROM relation1 AS r1
            ...
        \end{sqlcode}
    \item \textbf{Using the dot in general}: In general, even without aliases, we can use the dot to refer to relations attributes, this is crucial if we select on two relations with matching attribute names.
    \item \textbf{Cartesian Product Example}: S has 5 rows of 4 columns. The Cartesian product, SELECT * FROM S T1, S T2; returns 25 rows, each with 2 sets of the columns in S, for a total of 8 columns. They don’t all fit on the page here.
        \bigbreak \noindent 
        \begin{center}
            \begin{tabular}{c|c|c|c|c|c|c|c}
                T1.S &T1.SNAME &T1.STATUS &T1.CITY &T2.S &T2.SNAME &T2.STATUS &T2.CITY \\
                \hline
                S1 &Smith &20 &London &S1 &Smith& 20 &London \\
                S2 &Jones &10 &Paris &S1 &Smith &20 &London \\
                S3 &Blake &30 &Paris &S1 &Smith &20 &London \\
                S4 &Clark &20 &London &S1 &Smith &20& London \\
                S5 &Adams &30 &Athens &S1 &Smith &20& London \\
                S1 &Smith &20 &London &S2 &Jones &10& Paris \\
                S2 &Jones &10 &Paris &S2 &Jones &10 &Paris \\
                S3 &Blake &30 &Paris &S2 &Jones &10 &Paris \\
                S4 &Clark &20 &London &S2& Jones &10& Paris \\
                S5 &Adams &30 &Athens &S2& Jones &10& Paris \\
                S1 &Smith &20 &London &S3& Blake &30& Paris \\
                S2 &Jones &10 &Paris &S3 &Blake &30 &Paris \\
                S3 &Blake &30 &Paris &S3 &Blake &30& &Paris \\
                S4 &Clark &20 &London &S3& Blake &30 &Paris \\
                S5 &Adams &30 &Athens &S3& Blake &30 &Paris 
            \end{tabular}
        \end{center}
    \item \textbf{Cartesian product example}: For each part supplied, get the part number and names of all the cities supplying the part. (This is a join which pulls together data from multiple tables.)
        \bigbreak \noindent 
        \begin{sqlcode}
        SELECT DISTINC P, CITY
            FROM SP,S
            WHERE SP.S = S.S
        \end{sqlcode}
        \bigbreak \noindent 
        List the supplier numbers for all pairs of suppliers such that two suppliers are located in the same city.
        \bigbreak \noindent 
        \begin{sqlcode}
            SELECT T1.S, T2.S /* one S from each side */
                FROM S T1, S T2 /* cartesian product of S with S, giving name to each side */
                WHERE T1.CITY = T2.CITY /* same city for both suppliers */
                    AND T1.S < T2.S; /* avoid duplicate pairs; lower S on left */
        \end{sqlcode}
    \item \textbf{Multiple-row subqueries}: Multiple-row subqueries are nested queries that have the potential to return more than one row of results to
        the parent query. Most commonly used in WHERE and HAVING clauses
        \bigbreak \noindent 
        \textbf{Note:} Must be used  with multiple-row operators.
    \item \textbf{SQL Sets}: In an SQL statement, we can denote a set with a list of values inside parentheses.
    \item \textbf{Multiple Row Subqueries: IN Set Operator}: IN is a set operator used to test membership.
        \bigbreak \noindent 
        The IN operator will have value on its left, and a set on its right. It will evaluate to true if the value from the left hand side is present in the set provided on the right.
        \bigbreak \noindent 
        \begin{center}
            \begin{center}
                \begin{tabular}{c|c}
                    Example & Evaluates to \\
                    \hline
                    'S1' IN ('S2','S3','S1') & true \\
                    'S1' IN ('S2','S3','S4') &false \\
                    4 IN (2,1,6,4,5) &true \\
                    3 IN (1,5,6,10) &false
                \end{tabular}
            \end{center}
        \end{center}
        \bigbreak \noindent 
        When a multiple-row subquery is evaluated, its results are inserted into its parent query as a set. We can use set operations like IN to fit those results into our query
    \item \textbf{Multiple-row subqueries: Example}: List the supplier names for suppliers who supply part P2. (This time using a subquery.)
        \bigbreak \noindent 
        \begin{sqlcode}
            SELECT SNAME
                FROM S
                WHERE S IN              # IN operator used to check current S against the list
                    ( SELECT S          # this is the subquery
                    FROM SP             # which returns a list (set)
                    WHERE P = 'P2' );   # containing all the suppliers that supply part P2.
        \end{sqlcode}
        \bigbreak \noindent 
        \textbf{Note:} The innermost subqueries are the first to run. It returns
        \bigbreak \noindent 
        \begin{center}
            \begin{tabular}{c}
                S \\
                \hline
                S1 \\
                S2 \\
                S3 \\
                S4
            \end{tabular}
            \end{center}
            \bigbreak \noindent 
            Which is inserted into the parent query as ('S1', 'S2', 'S3', 'S4'), in the position where the subquery that returned the results was found.
            \bigbreak \noindent 
            Thus, after the subquery is run, the outer query effectively becomes:
            \bigbreak \noindent 
            \begin{sqlcode}
                SELECT SNAME
                    FROM S
                    WHERE S IN                  # IN operator used to check current S against the list
                        ('S1', 'S2', 'S3', 'S4');   # <-- results of subquery inserted in place
            \end{sqlcode}
            \bigbreak \noindent 
            Which would have the following results:
            \bigbreak \noindent 
            \begin{center}
                \begin{tabular}{c|c}
                    SNAME \\
                    \hline
                    Smith \\
                    Jones \\
                    Blake \\
                    Clark  \\
                \end{tabular}
            \end{center}
        \item \textbf{Set Operators: ALL and ANY}: The ALL and ANY operators modify the normal relational (in the comparison sense) operators to work on sets.
            \bigbreak \noindent 
            If we want to compare a value with every item in the set and reduce the answers to a single true/false using the AND operation, we can use ALL
            \bigbreak \noindent 
            \begin{sqlcode}
            <value> <relop> ALL (set)
            \end{sqlcode}
            \bigbreak \noindent 
            If we want to compare a value with every item in the set and reduce the answers to a single true/false using the OR operation, we can use ANY.
            \bigbreak \noindent 
            \begin{sqlcode}
            <value> <relop> ANY (set)
            \end{sqlcode}
        \item \textbf{Set Operator: EXISTS}: The EXISTS operator is a unary operator working on sets that is used to determine whether the set supplied is non-empty. Once again <set> is either an explicitly written set or a multi-row subquery
            \bigbreak \noindent 
            \begin{sqlcode}
            EXISTS (set)
            \end{sqlcode}
            \begin{itemize}
                \item Evaluates to true if the set is non-empty (contains at least one element)
                \item Evaluates to false if the set is empty (no elements inside)
            \end{itemize}
        \item \textbf{Set operator: NOT EXISTS}: EXISTS, when used in conjunction with the logical inversion operator, NOT, enables two types of queries that were difficult before
            \begin{itemize}
                \item Queries involving the set difference operation $\{a,b,c,d,e\} - \{b,c\} = \{a,d,e\}$
                \item Queries that involve the concept of every
            \end{itemize}
            \bigbreak \noindent 
            only include rows where the subquery is EMPTY
        \item \textbf{Union}: The UNION operator causes two sets to be merged, the set union.
            \bigbreak \noindent 
            \begin{sqlcode}
                SELECT P
                    FROM P
                    WHERE WEIGHT > 18 # first SELECT returns only P6
                UNION
                    SELECT P
                    FROM SP
                    WHERE S = 'S2'; # second query returns P1, P2
            \end{sqlcode}
        \item \textbf{Union caveat}: You should be careful in situations where the domain of a column matters, as UNION will put rows together whether the columns match in type/purpose or not.
        \item \textbf{Group Functions}: Group functions are sometimes referred to as aggregate or multiple-row functions. They take a list of columns as an argument, with an optional DISTINCT or ALL inside before those columns are listed.
            \begin{itemize}
                \item \textbf{SUM(<x>)}: add up the value of column <x> in all of the rows of each group
                \item \textbf{AVG(<x>)}: find the average value of column for each group
                \item \textbf{COUNT(<x>)}: count how many rows there are (usually <x> is a * here.)
                \item \textbf{MAX(<x>)}: returns the maximum value of column <x> for each group
                \item \textbf{MIN(<x>)}: returns the minimum value of column for each group
                \item \textbf{STDDEV(<x>)}: returns the standard deviation of column <x> for each group
                \item \textbf{VARIANCE(<x>)}: returns the variance of column for each group
            \end{itemize}
            All of these functions will return a single value for each group present.
            \bigbreak \noindent 
            If no GROUP BY clause is included, then there is only a single group, which contains all the rows of the query. The GROUP BY clause will allow that to be divided into subgroups.
        \item \textbf{Group function example: COUNT}: Find out the number of suppliers
            \bigbreak \noindent 
            \begin{sqlcode}
                SELECT COUNT(*) FROM S;
            \end{sqlcode}
        \item \textbf{DISTINCT with group functions}: Get the total number of suppliers currently supplying parts
            \bigbreak \noindent 
            If you want to count only distinct values, we can do that with DISTINCT
            \bigbreak \noindent 
            \begin{sqlcode}
            SELECT COUNT(DISTINCT S) FROM SP;
            \end{sqlcode}
            \bigbreak \noindent
        \item \textbf{WHERE clause with group functions}: The WHERE clause is evaluated BEFORE any groups are formed.
            \bigbreak \noindent 
            \begin{sqlcode}
                SELECT COUNT(*)
                    FROM SP
                    WHERE P = 'P2'; # the value of P is known before grouping, so WHERE works
            \end{sqlcode}
        \item \textbf{The GROUP BY clause}: The GROUP BY clause in a SELECT statement takes the following form:
            \bigbreak \noindent 
            \begin{sqlcode}
            GROUP BY <attrs>
            \end{sqlcode}
            \bigbreak \noindent 
            It will cause the SELECT statement to examine the rows in its result set, and gather the ones that match on their values for the columns in <attrs> into subgroups.
            \bigbreak \noindent 
            \begin{sqlcode}
            SELECT SUM(QTY) FROM SP
                GROUP BY P; # make a subgroups for each part

            SELECT P, SUM(QTY) FROM SP # added P to be shown
                GROUP BY P; # make a subgroup for each part
            \end{sqlcode}
        \item \textbf{Group by caveat}: However, if we try to display columns that aren’t part of the <attrs> of the GROUP BY and aren’t calculated by a group function, we begin to have problems.
            \bigbreak \noindent 
            \begin{sqlcode}
            SELECT P, S, QTY, SUM(QTY) FROM SP GROUP BY P; # P is good, but look at S and QTY
            \end{sqlcode}
            \bigbreak \noindent 
            \begin{center}
                \begin{tabular}{c|c|c|c}
                    P &S &QTY &SUM(QTY) \\
                    \hline
                    P1 &S1 &300 &600 \\
                    P2 &S1 &200 &1000 \\
                    P3 &S1 &400 &400 \\
                    P4 &S1 &200 &500 \\
                    P5 &S1 &100 &500 \\
                    P6 &S1 &100 &100 
                \end{tabular}
            \end{center}
            \bigbreak \noindent 
            What do the values of S and QTY mean in this grouped context? Nothing! They are not relevant or correct. Is S1 the
            only supplier for all of the groups? The SP table indicates no. Is the QTY there valid for P1? No, the correct answer
            for total P1 supplied is in the SUM(QTY).
            \bigbreak \noindent 
            There is a distinct value of S and a value of QTY for every row in each subgroup. That is many values, and only one
            place to show them in – it’s not atomic. Unfortunately the DBMS is just choosing one to show anyway, but it has no
            meaning, and such situations should be avoided.
        \item \textbf{HAVING clause}: Just as the WHERE clause could be used to filter individual rows based on whether they evaluated true for its expression, the HAVING clause allows us to filter out groups based on values that pertain to the group.
            \bigbreak \noindent 
            \begin{sqlcode}
            HAVING <expr>
            \end{sqlcode}
            \bigbreak \noindent 
            For each group in the results, the HAVING expression, <expr> is evaluated, and only groups where <expr> is true will be included in the final output.
            \bigbreak \noindent 
            The reason HAVING is necessary is that the WHERE clause is evaluated BEFORE the groups are formed, and is not able to work with values that don’t exist until after it has already finished.
        \item \textbf{Example with HAVING}: List the part numbers for all parts supplied by more than one supplier.
            \bigbreak \noindent 
            \begin{sqlcode}
                SELECT P
                    FROM SP
                    GROUP BY P
                    HAVING COUNT(*) > 1;
            \end{sqlcode}
        \item \textbf{Single-Row Subqueries}: Single-row subqueries are subqueries that return a single value (ONE ROW with ONE COLUMN).
            \bigbreak \noindent 
            Like the multiple-row subqueries, they are evaluated and then their results are used in the parent query that contained them.
            \bigbreak \noindent 
            They don’t need to use the multiple-row operators to work.
            \bigbreak \noindent 
        \item \textbf{Single-Row Subquery as a column}: SELECT Title, Retail, (SELECT AVG(Retail) FROM Books) # third column will have result 'Overall Average' # with a changed title
            \bigbreak \noindent 
            \begin{center}
                \begin{tabular}{c|c|c}
                    Title &Retail &Overall &Average \\
                    \hline
                    The Princess Bride &39.99 &42.00  \\
                    The Life of Pi &3.14 &42.00 \\
                    The Hitchhiker’s Guide &29.50 &42.00 \\
                    $\cdots$ & $\cdots$ & $\cdots $
                \end{tabular}
            \end{center}
            \bigbreak \noindent 
            Having the call to the group function AVG would normally reduce the results to a single row per group, but it happened inside a subquery, so it did not change the outer query. This can be useful when you really want to know an aggregate value but don’t want to condense your rows.
        \item \textbf{Single-Row Subquery in a WHERE clause}: Let’s use a bookstore as an example. If you knew the ISBN of a book and wanted to run a query to find all of the books that are more expensive than it, you could use a subquery to find out the cost of the book with that ISBN and then compare that value with its result.
            \bigbreak \noindent 
            \begin{sqlcode}
            SELECT Title, Cost
                FROM Books
                WHERE Cost > # compare the cost of current row with result of subquery
                    (SELECT Cost # only the Cost returned -- single column
                    FROM Books
                    WHERE ISBN = '1328948854'); # ISBN is PK -- single row
            \end{sqlcode}
        \item \textbf{Single-Row Subquery in a HAVING clause}: Since the result of the subquery is inserted in place, it will work anywhere a single value makes sense. This includes use as part of a HAVING clause. Using the same book database from the previous slide:
            \bigbreak \noindent 
            \begin{sqlcode}
                SELECT Category,
                    AVG(Retail - Cost) 'Average Profit' # calculate average profit of all books, change the label
                    FROM Books
                    GROUP BY Category
                    HAVING AVG(Retail - Cost) > # compare cost of each group with result of the subquery
                        ( SELECT AVG(Retail - Cost) # finds the average profit for books in LIT
                        FROM Books
                        WHERE Category = 'LIT' );
            \end{sqlcode}
        \item \textbf{Single-Row Subquery example 1}: List the supplier numbers for suppliers who are located in the same city as supplier S1
            \bigbreak \noindent 
            \begin{sqlcode}
                SELECT S
                    FROM S
                    WHERE CITY = # compare each row with result of subquery
                        ( SELECT CITY # find out which city S1 is in
                        FROM S
                        WHERE S = 'S1' );
                    \end{sqlcode}
        \item \textbf{The LIKE operator}: So far, all of the string comparisons we’ve done have been with the = operator, which tests for strict equality. (Locale
            settings determine whether it’s a case sensitive or case insensitive match.)
            \bigbreak \noindent 
            Using just =, we’d have to have a lot of OR’s strung together to have any kind of flexibility.
            \bigbreak \noindent 
            If we have a pattern to be matched, we generally won’t use =, but rather the LIKE operator.
            \bigbreak \noindent 
            \begin{sqlcode}
                <val> LIKE <pattern>
            \end{sqlcode}
            The LIKE operator will return true when <val> matches the pattern specified in <pattern>.
        \item \textbf{Patterns with LIKE}: The patterns that LIKE uses to check your values against are defined using these special characters. 
            \begin{itemize}
                \item \% $\quad$ any zero or more characters can fit here without breaking the match
                \item \_ $\quad$ any single character can fit here without breaking the match
                \item \textbackslash $\quad$escape the next character
                \item \% $\quad$escaped \%, so only match the actual \% character here
                \item \textbackslash\_ $\quad$ escaped \textbackslash\_, so only match the actual \_ character here
                \item \textbackslash\textbackslash $\quad $ escaped \textbackslash, so only match the actual \textbackslash character here
            \end{itemize}
            \bigbreak \noindent 
            Any characters not in this list will only match themselves.
            \bigbreak \noindent 
        \item \textbf{LIKE: Character classes and union (or)}: You can specify a list or range of characters with square brackets.
            \bigbreak \noindent 
            \begin{sqlcode}
            SELECT ...
                WHERE ... LIKE "_[abc]%"
                WHERE ... LIKE "_[a-z]%"
            \end{sqlcode}
        \item \textbf{Negating character classes:} To invert a character class, we can use !
            \bigbreak \noindent 
            \begin{sqlcode}
            SELECT ...
                WHERE ... LIKE "_[!abc]%"
            \end{sqlcode}
        \item \textbf{List suppliers whose name starts with the letter 'S'}:
            \bigbreak \noindent 
            \begin{sqlcode}
                SELECT * FROM S
                    WHERE SNAME LIKE `S%`;
            \end{sqlcode}
        \item \textbf{Single-valued (non-group) functions}: Unlike the aggregate functions, these functions won’t make your results collapse based on groups. They are evaluated, and their value is inserted in place.
            \begin{itemize}
                \item \textbf{LOWER(<str>)}: Returns copy of <str> but all lowercase
                \item \textbf{UPPER(<str>)}: Returns a copy of <str> but all uppercase
                \item \textbf{SUBSTR(<str>, <pos>, <len>)}: Returns a copy of the substring of <str> starting at its <pos>th position, <len> characters long
                \item \textbf{LENGTH(<str>)}: Returns the length in characters of the string, <str>
                \item \textbf{LPAD(<str>, <len>, <sp>)}: Returns <str> fit into <len> characters, padding with <sp> on the left if necessary
                \item \textbf{RPAD(<str>, <len>, <sp>)}: Returns <str> fit into <len> characters, padding with <sp> on the right if necessary
                \item \textbf{ROUND(<num>,<pos>)}: Returns the number <num>, rounded to <pos> digits after the decimal point
                \item \textbf{CONCAT(<str>, [...])} Returns a the concatenation of the strings <str>, in order.
                \item \textbf{SOUNDEX(<str>)}: Returns a string containing a code that can be used to compare how <str> sounds like other strings.
            \end{itemize}
            \bigbreak \noindent 
            These functions can be nested however you’d like. Just like C++, they’re evaluated from the inside out.
        \item \textbf{JOINS}: We’ve seen joins in some of our examples already.
            \bigbreak \noindent 
            A join is an operation that takes information from separate tables and combines it into one set of results.
            \bigbreak \noindent 
            There are two basic types of join:
            \begin{enumerate}
                \item \textbf{Inner join}
                \item \textbf{Outer join}
            \end{enumerate}
            For either of these types of join, it is possible to join a table with itself, in which case we call it a self join.
        \item \textbf{Change to S}: To make things interesting in these joins, let’s add a new supplier, S7, to our S table. They won’t supply anything yet so leave P and SP unchanged.
            \bigbreak \noindent 
            We have already done a few inner joins in the earlier examples,
            \bigbreak \noindent 
        \item \textbf{Inner join}: With an inner join, only lines that match up with each other in both tables will be a part of the result. Earlier, we accomplished this by putting together two tables with the Cartesian Product, and then using a WHERE clause to make sure only things that matched on the foreign key were retained.
            \bigbreak \noindent 
            \begin{sqlcode}
                SELECT S.S,P,SNAME,QTY
                    FROM S,SP # Cartesian product of S with SP
                    WHERE S.S = SP.S; # only keep rows matching S=S
            \end{sqlcode}
            \bigbreak \noindent 
            Can be written as
            \bigbreak \noindent 
            \begin{sqlcode}
            SELECT S.S,P,SNAME,QTY
                FROM S JOIN SP # replace the comma with the keyword JOIN
                ON S.S = SP.S; # WHERE becomes ON for the foreign key
            \end{sqlcode}
        \item \textbf{Outer Join}: An outer join can be more flexible than an inner join. It will contain everything that the inner join contained, but one or both of the two tables involved will be special, and will have at least one row in the results whether it matched the other side or not. The values for the missing side will be filled with NULL since there is no relevant value.
            \bigbreak \noindent 
            Here we have the same query from before, but as an outer join instead of an inner join.
            \bigbreak \noindent 
            \begin{sqlcode}
                SELECT S.S,P,SNAME,QTY
                    FROM S LEFT JOIN SP # LEFT means table on LHS of JOIN is the strong one
                    ON S.S=SP.S;
            \end{sqlcode}
            \bigbreak \noindent 
            LEFT means the table on the left-hand side of the JOIN keyword is the strong one. RIGHT would mean the RHS is strong. In some dialects of SQL, you can use FULL to make both strong, but this does not work in MariaDB. You can accomplish something similar with a UNION if needed.
        \item \textbf{The LIMIT Clause}: the LIMIT clause is used to restrict the number of rows returned by a query. When you specify LIMIT 50, it tells the database to return only the first 50 rows of the result set.
            \bigbreak \noindent 
            \begin{sqlcode}
            SELECT * from sometable LIMIT 50; // Queries the first 50 rows
            \end{sqlcode}
            \bigbreak \noindent 
            \textbf{Note:} Redundant if the number of rows in the relation is less than the limit restriction
        \item \textbf{IS NULL and IS NOT NULL}: A field with a NULL value is a field with no value.
            \bigbreak \noindent 
            If a field in a table is optional, it is possible to insert a new record or update a record without adding a value to this field. Then, the field will be saved with a NULL value.
            \bigbreak \noindent 
            It is not possible to test for NULL values with comparison operators, We will have to use the IS NULL and IS NOT NULL operators instead.
            \bigbreak \noindent 
            \begin{sqlcode}
            SELECT ...
                WHERE ... IS NULL;

            SELECT ...
                WHERE ... IS NOT NULL;
            \end{sqlcode}
        \item \textbf{BETWEEN operator}: the BETWEEN operator is used to filter the result set within a specified range. It works for numbers, dates, and text values.
            \bigbreak \noindent 
            \begin{sqlcode}
            SELECT ...
                WHERE ... BETWEEN val1 AND val2;
            \end{sqlcode}
            \bigbreak \noindent 
            We can of course negate this with NOT
            \bigbreak \noindent 
            \begin{sqlcode}
            SELECT ...
                WHERE ... NOT BETWEEN val1 AND val2;
            \end{sqlcode}






    \end{itemize}






\end{document}
