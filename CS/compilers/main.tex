\documentclass{report}

\input{~/dev/latex/template/preamble.tex}
\input{~/dev/latex/template/macros.tex}

\title{\Huge{}}
\author{\huge{Nathan Warner}}
\date{\huge{}}
\fancyhf{}
\rhead{}
\fancyhead[R]{\itshape Warner} % Left header: Section name
\fancyhead[L]{\itshape\leftmark}  % Right header: Page number
\cfoot{\thepage}
\renewcommand{\headrulewidth}{0pt} % Optional: Removes the header line
%\pagestyle{fancy}
%\fancyhf{}
%\lhead{Warner \thepage}
%\rhead{}
% \lhead{\leftmark}
%\cfoot{\thepage}
%\setborder
% \usepackage[default]{sourcecodepro}
% \usepackage[T1]{fontenc}

% Change the title
\hypersetup{
    pdftitle={Comilers}
}

\begin{document}
    % \maketitle
        \begin{titlepage}
       \begin{center}
           \vspace*{1cm}
    
           \textbf{Compilers}
    
           \vspace{0.5cm}
            
                
           \vspace{1.5cm}
    
           \textbf{Nathan Warner}
    
           \vfill
                
                
           \vspace{0.8cm}
         
           \includegraphics[width=0.4\textwidth]{~/niu/seal.png}
                
           Computer Science \\
           Northern Illinois University\\
           United States\\
            
                
       \end{center}
    \end{titlepage}
    \tableofcontents
    \pagebreak 
    \unsect{Introduction}
    \begin{itemize}
        \item \textbf{Compilers}: In its most general form, a compiler is a program that accepts as input a program text in a certain language and produces as output a program text in another language, while preserving the meaning of that text
        \item \textbf{Translation, source language, target language, and implementation language}: This process is called translation, as it would be if the texts were in natural languages. Almost all compilers translate from one input language, the source language, to one output language, the target language, only. One normally expects the source and target language to differ greatly: the source language could be C and the target language might be machine code for the Pentium processor series. The language the compiler itself is written in is the implementation language.
            \bigbreak \noindent 
            To obtain the translated program, we run a compiler, which is just another program whose input is a file with the format of a program source text and whose output is a file with the format of executable code
        \item \textbf{Bootstrapping}: When the source language is also the implementation language and the source text to be compiled is actually a new version of the compiler itself, the process is called bootstrapping. 
        \item \textbf{Front and back end}: The part of a compiler that performs the analysis of the source language text is called the front-end, and the part that does the target language synthesis is the back-end
            \bigbreak \noindent 
            If the compiler has a very clean design, the front-end is totally unaware of the target language and the back-end is totally unaware of the source language, the only thing they have in common is knowledge of the semantic representation
        \item \textbf{Parse tree / syntax tree}: The \textbf{syntax tree} of a program text is a data structure which shows precisely how the various segments of the program text are to be viewed in terms of the grammar. The syntax tree can be obtained through a process called “parsing”
            \bigbreak \noindent 
            \textbf{Parsing} is the process of structuring a text according to a given grammar. For this reason, syntax trees are also called \textbf{parse trees}; we will use the terms interchangeably, with a slight preference for “parse tree” when the emphasis is on the actual parsing. Conversely, parsing is also called syntax analysis
        \item \textbf{Abstract syntax tree (AST)}: The exact form of the parse tree as required by the grammar is often not the most convenient one for further processing, so usually a modified form of it is used, called an abstract syntax tree, or AST. Detailed information about the semantics can be attached to the nodes in this tree through annotations, which are stored in additional data fields in the nodes; hence the term annotated abstract syntax tree. Since unannotated ASTs are of limited use, ASTs are always more or less annotated in practice, and the abbreviation “AST” is used also for annotated ASTs.
        \item \textbf{Lexical analysis}: Usually the grammar of a programming language is not specified in terms of input characters but of input “tokens”. Input tokens may be and sometimes must be separated by white space, which is otherwise ignored. So before feeding the input program text to the parser, it must be divided into tokens. Doing so is the task of the lexical analyzer; the activity itself is sometimes called “to tokenize”, but the literary value of that word is doubtful.
        \item \textbf{Example grammer, parse tree, and AST}: Consider the grammer
            \begin{align*}
                E &\to E + T \ \mid \ E - T \ \mid \ T \\
                T &\to T * F\ \mid \ T / F \ \mid \ F \\
                F &\to \text{identifier} \ \mid \ \text{ constant } \ \mid \ (E)
            ,\end{align*}
            where $E$ is an expression, $T$ is a term, and $F$ is a factor. The parse tree for the expression $b*b-4*a*c$ would look something like
            \bigbreak \noindent 
            \fig{.6}{./figures/1.png}
            \bigbreak \noindent 
            Whereas the AST would be 
            \bigbreak \noindent 
            \fig{.6}{./figures/2.png}
            \bigbreak \noindent 
            The annotated AST would be
            \bigbreak \noindent 
            \fig{.6}{./figures/3.png}
        \item \textbf{Narrow and broad compilers}: A narrow compiler reads a small part of the program, typically a few tokens, processes the information obtained, produces a few bytes of object code if appropriate, discards most of the information about these tokens, and repeats this process until the end of the program text is reached.
            \bigbreak \noindent 
            A broad compiler reads the entire program and applies a series of transformations to it (lexical, syntactic, contextual, optimizing, code generating, etc.), which eventually result in the desired object code. This object code is then generally written to a file
        \item \textbf{$N$-pass compilers}: 
            Since the “field of vision” of a narrow compiler is, well, narrow, it is possible that it cannot manage all its transformations on the fly. Such compilers then write a partially transformed version of the program to disk and, often using a different program, continue with a second pass; occasionally even more passes are used. Not surprisingly, such a compiler is called a 2-pass (or $N$-pass) compiler, or a 2-scan ($N$-scan) compiler. If a distinction between these two terms is made, “2-scan” often indicates that the second pass actually re-reads (re-scans) the original program text, the difference being that it is now armed with information extracted during the first scan.
        \item \textbf{Portable programs}: A program is considered portable if it takes a limited and reasonable effort to make it run on different machine types. What constitutes “a limited and reasonable effort” is, of course, a matter of opinion, but today many programs can be ported by just editing the makefile to reflect the local situation and recompiling.
        \item \textbf{Grammers (CFG's)}: Grammars, or more precisely context-free grammars, are the essential formalism for describing the structure of programs in a programming language. In principle the grammar of a language describes the syntactic structure only, but since the semantics of a language is defined in terms of the syntax, the grammar is also instrumental in the definition of the semantics
            \bigbreak \noindent 
            There are other grammar types besides context-free grammars, but we will be mainly concerned with context-free grammars. We will also meet regular grammars, which more often go by the name of “regular expressions” and which result from a severe restriction on the context-free grammars; and attribute grammars, which are context-free grammars extended with parameters and code. Other types of grammars play only a marginal role in compiler construction. The term “contextfree” is often abbreviated to CF. We will give here a brief summary of the features of CF grammars
            \bigbreak \noindent 
            A “grammar” is a recipe for constructing elements of a set of strings of symbols. When applied to programming languages, the symbols are the tokens in the language, the strings of symbols are program texts, and the set of strings of symbols is the programming language. The string
            \begin{center}
                BEGIN print ("Hi!") END
            \end{center}
            consists of 6 symbols (tokens) and could be an element of the set of strings of symbols generated by a programming language grammar, or in more normal words, be a program in some programming language. This cut-and-dried view of a programming language would be useless but for the fact that the strings are constructed in a structured fashion; and to this structure semantics can be attached.
            \bigbreak \noindent 
            the six tokens produced by a lexical analyzer are:
            \begin{itemize}
                \item \textbf{BEGIN:} keyword
                \item \textbf{print:} identifier (or keyword, depending on the language specification)
                \item \textbf{(:} left parenthesis
                \item \textbf{"Hi!":} string literal
                \item \textbf{):} right parenthesis
                \item \textbf{END:} keyword
            \end{itemize}
        \item \textbf{The form of a grammar}: A \textbf{grammar} consists of a set of production rules and a start symbol. Each production rule defines a named syntactic construct. A \textbf{production rule} consists of two parts, a left-hand side and a right-hand side, separated by a left-to-right arrow. The \textbf{left-hand side} is the name of the syntactic construct; the \textbf{right-hand side} shows a possible form of the syntactic construct. An example of a production rule is
            \begin{align*}
                \text{expression } \to '('\text{expression operator expression})'
            .\end{align*}
        \item \textbf{Terminal and non-terminal symbols}: The right-hand side of a production rule can contain two kinds of symbols, terminal symbols and non-terminal symbols. As the word says, a \textbf{terminal symbol} (or \textbf{terminal} for short) is an end point of the production process, and can be part of the strings produced by the grammar. A \textbf{non-terminal symbol} (or \textbf{non-terminal} for short) must occur as the left-hand side (the name) of one or more production rules, and cannot be part of the strings produced by the grammar. Terminals are also called \textbf{tokens}, especially when they are part of an input to be analyzed. Non-terminals and terminals together are called \textbf{grammar symbols}. The grammar symbols in the righthand side of a rule are collectively called its \textbf{members}; when they occur as nodes in a syntax tree they are more often called its “children”
            \begin{itemize}
                \item \textbf{Non-terminals} are denoted by capital letters, mostly $A$, $B$, $C$, and $N$.
                \item \textbf{Terminals} are denoted by lower-case letters near the end of the alphabet, mostly $x$, $y$, and $z$.
                \item \textbf{Sequences of grammar symbols} are denoted by Greek letters near the beginning of the alphabet, mostly $\alpha$ (alpha), $\beta$ (beta), and $\gamma$ (gamma).
                \item \textbf{Lower-case letters near the beginning of the alphabet} ($a$, $b$, $c$, etc.) stand for themselves, as terminals.
                \item \textbf{The empty sequence} is denoted by $\varepsilon$ (epsilon).
            \end{itemize}
        \item \textbf{Sentential form, production tree,and production step}: The central data structure in the production process is the sentential form. It is usually described as a string of grammar symbols, and can then be thought of as representing a partially produced program text. For our purposes, however, we want to represent the syntactic structure of the program too. The syntactic structure can be added to the flat interpretation of a sentential form as a tree positioned above the sentential form so that the leaves of the tree are the grammar symbols. This combination is also called a production tree
            \bigbreak \noindent 
            A string of terminals can be produced from a grammar by applying so-called production steps to a sentential form, as follows. The sentential form is initialized to a copy of the start symbol. Each production step finds a non-terminal $N$ in the leaves of the sentential form, finds a production rule $N \to \alpha$ with $N$ with N as its lefthand side, and replaces the $N$ in the sentential form with a tree having $N$ as the root and the right-hand side of the production rule, $\alpha$, as the leaf or leaves. When no more non-terminals can be found in the leaves of the sentential form, the production process is finished, and the leaves form a string of terminals in accordance with the grammar.
            \bigbreak \noindent 
            Using the conventions described above, we can write that the production process replaces the sentential form $\beta N \gamma$ by $ \beta \alpha \gamma $
        \item \textbf{More on sentential forms and production steps}: A production step is a single application of a grammar rule. If a grammar has a rule
            \begin{align*}
                N  \to \alpha
            \end{align*}
            then a sentential of the form
            \begin{align*}
                \beta N \gamma
            \end{align*}
            can be written as
            \begin{align*}
                \beta \alpha \gamma
            .\end{align*}
            \begin{itemize}
                \item $N$: A non-terminal to be expanded
                \item $\alpha$: The replacement string
                \item $\beta, \gamma$: Unchanged context
            \end{itemize}
            When we write
            \begin{align*}
                \beta N \gamma \implies \beta \alpha \gamma
            \end{align*}
            we are describing one production step
            \begin{itemize}
                \item $N$ — a non-terminal that we are going to expand
                \item $\alpha $ — the right-hand side of a grammar rule (what replaces N)
                \item $\beta$ — everything to the left of $N$
                \item $\gamma$ — everything to the right of $N$
            \end{itemize}

        \item \textbf{Derivation}: The steps in the production process leading from the start symbol to a string of terminals are called the derivation of that string. Suppose our grammar consists of the four numbered production rules:
            \begin{enumerate}
                \item expression $\to$ ’(’ expression operator expression ’)’
                \item expression $\to$ ’1’
                \item operator $\to$ ’+’
                \item operator $\to$ ’ *’
            \end{enumerate}
            in which the terminal symbols are surrounded by apostrophes and the non-terminals are identifiers, and suppose the start symbol is expression. Then the sequence of sentential forms shown below
            \bigbreak \noindent 
            \fig{.6}{./figures/4.png}
            \bigbreak \noindent 
            forms the derivation of the string $(1*(1+1))$. More in particular, it forms a \textbf{leftmost derivation}, a derivation in which it is always the leftmost non-terminal in the sentential form that is rewritten
            \bigbreak \noindent 
            An indication $R@P$ shows that grammar rule $R$ is used to rewrite the non-terminal at position $P$. The resulting parse tree is 
            \bigbreak \noindent 
            \fig{.6}{./figures/5.png}
            \bigbreak \noindent 
            We see that recursion—the ability of a production rule to refer directly or indirectly to itself—is essential to the production process; without recursion, a grammar would produce only a finite set of strings
            \bigbreak \noindent 
            The production process is kind enough to produce the program text together with the production tree, but then the program text is committed to a linear medium (paper, computer file) and the production tree gets stripped off in the process. Since we need the tree to find out the semantics of the program, we use a special program, called a “parser”, to retrieve it
        \item \textbf{Extended forms of grammars}: The single grammar rule format
            \begin{align*}
                \text{non-terminal } \to \text{ zero or more grammar symbols}
            \end{align*}
            used above is sufficient in principle to specify any grammar, but in practice a richer notation is used
            \bigbreak \noindent 
            The format described so far is known as BNF, which may be considered an abbreviation of Backus–Naur Form or of Backus Normal Form. It is very suitable for expressing nesting and recursion, but less convenient for expressing repetition and optionality, although it can of course express repetition through recursion. To remedy this, three additional notations are introduced, each in the form of a postfix operator:
            \begin{itemize}
                \item $R^{+}$: Indicates the occurrence of one or more $R$s, to express repetition
                \item $R^{?}$: Indicates the occurrence of zero or one $R$s, to express optionality
                \item $R^{*}$: Indicates the occurrence of zero or more $R$s, to express optional repetition
            \end{itemize}
            Parentheses may be needed if these postfix operators are to operate on more than one grammar symbol. The grammar notation that allows the above forms is called EBNF, for Extended BNF. An example is the grammar rule
            \begin{align*}
                \text{paramater\_list } \to \text{ ('IN' \; $\mid$ \; 'OUT')}^{?} \text{ identitfier (',' identifier)}^{*}
            .\end{align*}
        \item \textbf{Properties of grammars}: 
            \begin{itemize}
                \item \textbf{Left-recursive non-terminal}: A non-terminal $N$ is left-recursive if, starting with a sentential form $N$, we can produce another sentential form starting with $N$
                \item \textbf{Left-recursive grammar}:  By extension, a grammar that contains one or more left-recursive rules is itself called left-recursive
                \item \textbf{Right-recursive}: The right version of left-recursive, not as important.
                \item \textbf{Nullable non-terminal}: A non-terminal $N$ is nullable if, starting with a sentential form $N$, we can produce an empty sentential form $\epsilon$
                    \bigbreak \noindent 
                    A grammar rule for a nullable non-terminal is called an $\epsilon$-rule.
                \item \textbf{Useless non-terminal}: A non-terminal $N$ is useless if it can never produce a string of terminal symbols: any attempt to do so inevitably leads to a sentential that again contains $N$. 
                \item \textbf{Ambiguous grammar}: A grammar is ambiguous if it can produce two different production trees with the same leaves in the same order. 
                    \bigbreak \noindent 
                    That means that when we lose the production tree due to linearization of the program text we cannot reconstruct it unambiguously; and since the semantics derives from the production tree, we lose the semantics as well. So ambiguous grammars are to be avoided in the specification of programming languages, where attached semantics plays an important role
            \end{itemize}
        \item \textbf{Symbols}: The basic unit in formal grammars is the symbol. The only property of these symbols is that we can take two of them and compare them to see if they are the same. In this they are comparable to the values of an enumeration type. Like these, symbols are written as identifiers, or, in mathematical texts, as single letters, possibly with subscripts. Examples of symbols are $N$, $x $, procedure\_body, assignment\_symbol, $t_{k} $.
        \item \textbf{Production rule}. Given two sets of symbols $V_{1}$ and $V_{2}$, a production rule is a pair
            \begin{align*}
                (N, \alpha) \text{ such that } N \in V_{1}, \alpha \in V_{2}^{*}
            ,\end{align*}
            in which $X^{*} $ means a sequence of zero or more elements of the set $X$. This means that a production rule is a pair consisting of an $N$, which is an element of $V_1$, and a sequence $\alpha$ of elements of $V_2$. We call $N$ the \emph{left-hand side} and $\alpha$ the \emph{right-hand side}. We do not normally write this as a pair $(N, \alpha)$, but rather as
            \[
                N \to \alpha,
            \]
            although technically it is a pair. The $V$ in $V_1$ and $V_2$ stands for \emph{vocabulary}.

        \item \textbf{CFGs}: A context-free grammar $G$ is a 4-tuple
            \begin{align*}
                G = (V_{N}, V_{T}, S, P)
            \end{align*}
            where $V_{N}$ and $V_{T}$ are sets of symbols. $S$ is a symbol, and $P$ is a set of production rules. The elements of $V_{N}$ are the \textbf{non-terminal symbols}, and elements of $V_{T} $ are the \textbf{terminal symbols}. $S$ is called the \textbf{start symbol}.
        \item \textbf{Context conditions}: The previous paragraph defines only the context-free form of a grammar. To make it a real, acceptable grammar, it has to fulfill three context condition
            \begin{enumerate}
                \item $V_{N} \cap V_{T} = \varnothing $
                \item $S \in V_{N}$
                \item $P \subseteq \{(N,\alpha)} \ \mid \ N \in V_{N}, \alpha \in (V_{N} \cup V_{T})^{*} $
            \end{enumerate}
        \item \textbf{Strings}: Sequences of symbols are called strings. 
        \item \textbf{Directly derivable}: A string may be derivable from another string in a grammar. More precisely, a string $\beta$ is said to be \emph{directly derivable} from a string $\alpha$, written as
            \[
                \alpha \Rightarrow \beta,
            \]
            if and only if there exist strings $\delta_1$, $\delta_2$, and $\gamma$, and a non-terminal
            $N \in V_N$, such that
            \[
                \alpha = \delta_1 N \delta_2, \quad
                \beta = \delta_1 \gamma \delta_2, \quad
                (N, \gamma) \in P.
            \]
        \item \textbf{Derivable}: 
            A string $\beta$ is said to be \emph{derivable} from a string $\alpha$, written as
            \[
                \alpha \xRightarrow{*} \beta,
            \]
            if and only if either $\alpha = \beta$, or there exists a string $\gamma$ such that
            \[
                \alpha \xRightarrow{*} \gamma \quad \text{and} \quad \gamma \Rightarrow \beta.
            \]
            This means that a string is derivable from another string if we can reach the second string from the first through zero or more production steps. 
        \item \textbf{Sentential form}: A sentential form of a grammar $G$ is defined as
            \begin{align*}
                \alpha \ \mid \ S \xRightarrow{*} \alpha
            \end{align*}
            which is any string that is derivable from the start symbol S of G. Note that α may be the empty string.
        \item \textbf{Terminal production}:  A terminal production of a grammar $G$ is defined as a sentential form that does not contain non-terminals:
            \begin{align*}
                \alpha \ \mid \ S \xRightarrow{*} \alpha \land \alpha \in V_{T}^{*}
            .\end{align*}
        \item \textbf{Language generated by a grammar $G$}: The language $\mathcal{L}$ generated by a grammar $G$ is defined as 
            \begin{align*}
                \mathscr{L}(G) = \{\alpha \ \mid \ S \xRightarrow{*} \alpha \land \alpha \in V_{T}^{*}\}
            .\end{align*}
            So, the set of all terminal productions of $G$. 
            \bigbreak \noindent 
            If $G$ is a grammar for a programming language, then $\mathscr{L}(G)$ is the set of all programs in that language that are correct in a context-free sense.
        \item \textbf{Sentences}: These terminal productions are called \textbf{sentences} in the language $\mathscr{L}(G) $
        \item \textbf{Intro to closure algorithms}: Quite a number of algorithms in compiler construction start off by collecting some basic information items and then apply a set of rules to extend the information and/or draw conclusions from them. These “information-improving” algorithms share a common structure which does not show up well when the algorithms are treated in isolation; this makes them look more different than they really are. We will therefore treat here a simple representative of this class of algorithms, the construction of the calling graph of a program, and refer back to it from the following chapters
        \item \textbf{Calling graph}: The calling graph of a program is a directed graph which has a node for each routine (procedure or function) in the program and an arrow from node $A$ to node $B$ if routine $A$ calls routine $B$ directly or indirectly. Such a graph is useful to find out, for example, which routines are recursive and which routines can be expanded in-line inside other routines
            \bigbreak \noindent 
            The initial calling graph is, however, of little immediate use since we are mainly interested in which routine calls which other routine directly or indirectly. For example, recursion may involve call chains from A to B to C back to A. To find these additional information items, we apply the following rule to the graph: If there is an arrow from node $A$ to node $B$ and one from $B$ to $C$, make sure there is an arrow from $A$ to $C$.
            \bigbreak \noindent 
            Consider the program
            \bigbreak \noindent 
            \begin{cppcode}
                void P(void) { ... Q(); ... S (); ... }
                void Q(void) { ... R(); ... T (); ... }
                void R(void) { ... P (); }
                void T(void) { ... }
                void S(void) { ... }
            \end{cppcode}
            \bigbreak \noindent 
            The calling graph is then
            \bigbreak \noindent 
            \fig{.6}{./figures/6.png}
        \item \textbf{Transitive closure}: If we consider this rule as an algorithm (which it is not yet), this set-up computes the transitive closure of the relation “calls directly or indirectly”. The transitivity axiom of the relation can be written as:
            \begin{align*}
                A \subseteq B \land B \subseteq C \rightarrow A \subseteq C
            ,\end{align*}
            where the operator $\subseteq$ should be read as "calls directly or indirectly". Now, $A$ is recursive is equivalent to $A \subseteq A$.
            \bigbreak \noindent 
            Adding this rule to the figure above, we get
            \bigbreak \noindent 
            \fig{.6}{./figures/7.png}
            \bigbreak \noindent 
            We see that the recursion of the routines $P$, $Q$, and $R$ has been brought into the open.
        \item \textbf{Components of a closure algorithm}: 
            \begin{itemize}
                \item \textbf{Data definitions}: definitions and semantics of the information items; these derive from the nature of the problem.
                \item \textbf{Initializations}: one or more rules for the initialization of the information items; these convert information from the specific problem into information items.
                \item \textbf{Inference rules}: one or more rules of the form: “If information items I1,I2,... are present then information item J must also be present”. These rules may again refer to specific information from the problem at hand.
            \end{itemize}
            The rules are called inference rules because they tell us to infer the presence of information item $J$ from the presence of information items $I_{1},I_{2},....$ When all inferences have been drawn and all inferred information items have been added, we have obtained the closure of the initial item set. If we have specified our closure algorithm correctly, the final set contains the answers we are looking for. For example, if there is an arrow from node $A$ to node $A$, routine $A$ is recursive, and otherwise it is not. Depending on circumstances, we can also check for special, exceptional, or erroneous situations
        \item \textbf{Recursion detection as a closure algorithm}: 
            \begin{itemize}
                \item \textbf{Data definitions:}
                    \begin{enumerate}
                        \item $G$, a directed graph with one node for each routine. The information items are arrows in $G$.
                        \item An arrow from a node $A$ to a node $B$ means that routine $A$ calls routine $B$ directly or indirectly.
                    \end{enumerate}
                \item \textbf{Initializations:} If the body of a routine $A$ contains a call to routine $B$, an arrow from $A$ to $B$ must be present.
                \item \textbf{Inference rules:}: If there is an arrow from node $A$ to node $B$ and one from $B$ to $C$, an arrow from $A$ to $C$ must be present.
            \end{itemize}
            Two things must be noted about this format. The first is that it does specify which information items must be present but it does not specify which information items must not be present; nothing in the above prevents us from adding arbitrary information items. To remedy this, we add the requirement that we do not want any information items that are not required by any of the rules: we want the smallest set of information items that fulfills the rules in the closure algorithm. This constellation is called the \textbf{least fixed point} of the closure algorithm.
            \bigbreak \noindent 
            The second is that the closure algorithm as introduced above is not really an algorithm in that it does not specify when and how to apply the inference rules and when to stop; it is rather a declarative,
        \item \textbf{Transitive closure algorithms}: General closure algorithms may have inference rules of the form “If information items $I_{1},I2_{2},...$ are present then information item $J$ must also be present”, as explained above. If the inference rules are restricted to the form “If information items $(A,B)$ and $(B,C)$ are present then information item $(A,C)$ must also be present”, the algorithm is called a transitive closure algorithm





    \end{itemize}


    \pagebreak 
    \unsect{Lexical vs Syntactic analysis}
    \begin{itemize}
        \item \textbf{Lexical analysis}: First phase of compilation, its purpose is to convert the raw input text (source code) into a sequence of tokens. 
            \begin{itemize}
                \item Reads characters from the source program
                \item Groups characters into tokens
                \item Removes whitespace and comments
                \item Classifies lexemes into categories such as:
                \begin{itemize}
                    \item keywords
                    \item identifiers
                    \item literals
                    \item operators
                \end{itemize}
            \end{itemize}
            Consider
            \begin{cppcode}
            x = 10 + y;
            \end{cppcode}
            \bigbreak \noindent 
            The tokens are
            \begin{center}
                IDENTIFIER ASSIGN NUMBER PLUS IDENTIFIER SEMICOLON
            \end{center}
        \item \textbf{Syntactic analysis}: Syntactic analysis (parsing) is the second phase of compilation. Its purpose is to determine whether the token sequence follows the grammar of the language.
            \begin{itemize}
                \item Takes tokens from the lexer
                \item Checks grammatical structure
                \item Builds a parse tree or syntax tree
                \item Detects syntax errors
            \end{itemize}
            Checks whether 
            \bigbreak \noindent 
            \begin{cppcode}
            x = 10 + y
            \end{cppcode}
            \bigbreak \noindent 
            matches the grammar rule
            \bigbreak \noindent 
            \begin{align*}
                \text{assignment } \to \text{ identifier } = \text{ expression};
            .\end{align*}
            The component that performs syntactic analysis is called the parser.
        \item \textbf{Interpreters and compilers}
            \begin{itemize}
                \item \textbf{Interpreters}: Lexical analysis $\to$ parsing $\to$ execute
                \item \textbf{Compilers}: Lexical analysis $\to$ parsing $\to$ code generation $\to$ executable
            \end{itemize}
        \item \textbf{Chomsky hierarchy}: The Chomsky Hierarchy is a classification of formal grammars based on their generative power—that is, the types of languages they can describe and the computational models required to recognize them.
            \bigbreak \noindent 
            It consists of four levels, ordered from most restrictive to most powerful.
            \begin{itemize}
                \item \textbf{Type 3 - Regular grammars}: Of the form
                    \begin{align*}
                        A \to a B \quad \text{or} \quad A \to a 
                    .\end{align*}
                    Recognized by finite automata (DFA / NFA). Examples are regular languages, programming language tokens, and identifiers, numbers, keywords
                \item \textbf{Type 2 -  CFGs}: Of the form
                    \begin{align*}
                        A \to \alpha
                    ,\end{align*}
                    where
                    \begin{itemize}
                        \item $A$ is a single non-terminal
                        \item $\alpha$ is any string of terminals and non-terminals
                    \end{itemize}
                    \begin{itemize}
                        \item Allows recursion
                        \item Can represent nested structures
                        \item Most programming language syntax is CFG-based
                    \end{itemize}
                    Recognized by push down automata (PDA). Examples are
                    \begin{itemize}
                        \item Arithmetic expressions
                        \item Balanced parentheses
                        \item Programming language syntax
                    \end{itemize}
                \item \textbf{Type 1 - CSGs}: Of the form
                    \begin{align*}
                        \alpha A \beta \to \alpha \gamma \beta
                    \end{align*}
                    with 
                    \begin{align*}
                        \left\lvert \gamma \right\rvert \geq 1
                    .\end{align*}
                    Characteristics are
                    \begin{itemize}
                        \item Rules depend on surrounding context
                        \item Length of strings never decreases
                        \item More powerful than CFGs
                    \end{itemize}
                    Recognized by Linear bounded automata (LBA)
                \item \textbf{Type 0 - Unrestricted grammars}: Of the form
                    \begin{align*}
                        \alpha \to \beta
                    ,\end{align*}
                    where $\alpha$ contains at least one non-terminal. Characteristics are
                    \begin{itemize}
                        \item No restrictions on production rules
                        \item Most powerful grammar type
                        \item Can generate all computable languages
                    \end{itemize}
                    Recognized by turing machines. Example is an REL.
            \end{itemize}
        \item \textbf{Regular grammars / regular languages}: A regular grammar is the most restrictive class in the Chomsky hierarchy. It generates exactly the regular languages, which are the languages recognized by finite automata.
            \bigbreak \noindent 
            A grammar is regular if all of its productions are of one of the following forms:
            \begin{itemize}
                \item \textbf{Right regular}:
                    \begin{align*}
                        A \to a B \quad \text{or} A \to a
                    .\end{align*}
                \item \textbf{Left regular}:
                    \begin{align*}
                        A \to Ba \quad \text{or} A \to a
                    .\end{align*}
            \end{itemize}
            A grammar must be either entirely right-regular or entirely left-regular — never mixed.
            \bigbreak \noindent 
            Consider a mixed grammar
            \begin{align*}
                S &\to \varepsilon \; \mid \; aA \\
                A &\to Sb
            .\end{align*}
            This would allow
            \begin{itemize}
                \item Non-terminals on both sides
                \item Multiple derivation directions
                \item Power beyond regular languages
            \end{itemize}
            Such grammars can generate non-regular languages, which breaks the definition.
            \bigbreak \noindent 
            Notice that the above grammar could generate the language $\mathscr{L} = \{a^{n}b^{n} \ : \ n \geq 0\}$, which is famously nonregular. Observe that 
            \begin{align*}
                S &\to \varepsilon \; \mid \; aA \\
                A &\to bS
            \end{align*}
            yields $\mathscr{L} = \{(ab)^{n} \ : \ n \geq 0 \} $, which is regular. Notice that all productions are right-linear
        \item \textbf{Regular languages and FAs in lexical analysis}: Lexical analysis is the first phase of compilation. Its purpose is to convert a stream of characters into a stream of tokens.
            \bigbreak \noindent 
            This process is based almost entirely on finite automata. Lexical structure of programming languages is:
            \begin{itemize}
                \item Regular
                \item Pattern-based
                \item Non-nested
                \item Locally decidable
            \end{itemize}
            All of these can be described by regular expressions, which implies regular languages, which implies FAs.
        \item \textbf{Token specification}: Each token class is defined using an regular expression. For example,
            \begin{align*}
                \text{identifier}  &\to \text{letter(letter | digit)}^{*} \\
                \text{number}      &\to \text{digit}^{+} \\
                \text{whitespace}  &\to \text{(space | tab | newline)}^{+}
            .\end{align*}
            Each regular expression is converted into an NFA using standard constructions like Thompson's construction with $\epsilon$-transitions allowed.
            \bigbreak \noindent 
            \textbf{Note:} Note efficient to execute directly, we can instead convert the NFA to a DFA, which is possible.
        \item \textbf{How the DFA is used (maximal munch / longest match rule)}: 
            \begin{enumerate}
                \item Start at the initial state
                \item Read input character by character
                \item Follow transitions
                \item Track the last accepting state
                \item When no transition is possible:
                    \begin{itemize}
                        \item Backtrack to last accepting state
                        \item Emit the corresponding token
                        \item Restart from next input position
                    \end{itemize}
            \end{enumerate}
            \textbf{Note:} You use a finite automaton to extract the tokens. The FA operations on raw characters.
            \begin{align*}
                \text{Char stream } \to \text{ FA } \to \text{ tokens} 
            .\end{align*}
            So the FA’s job is to recognize token boundaries, not to consume tokens. The outputs of the FA is the tokens.
        \item \textbf{Lexical analyzer example}: We will build a lexer for the following token types
            \begin{align*}
                \text{ID }    &$\to$ [a-zA-Z][a-zA-Z0-9]^{*} \\
                \text{NUM }   &$\to$ [0-9]^{+} \\
                \text{PLUS }  &$\to$ + \\
                \text{WS }    &$\to$ [ \t\n]^{+}
            .\end{align*}
            We will use the following transition table
            \begin{center}
                \begin{tabularx}{\textwidth}{@{}XXXXX@{}}
                    \toprule
                    \textbf{Current}	&\textbf{Letter}	&\textbf{Digit}	&+	&\textbf{Whitespace} \\
                    \midrule
                    q0	&q1	&q2	&q3	&q0 \\[2ex]
                    q1	&q1	&q1	&—	&accept\\[2ex]
                    q2	&—	&q2	&—	&accept\\[2ex]
                    q3	&—	&—	&—	&accept \\
                    \bottomrule
                \end{tabularx}
            \end{center}
            Consider the stream
            \begin{cppcode}
                sum1 + 42
            \end{cppcode}
            \bigbreak \noindent 
            Then, reading the stream gives
            \begin{center}
                \begin{tabularx}{\textwidth}{@{}XXX@{}}
                    \toprule
                    \textbf{Input Read} &	\textbf{State} &	\textbf{Action} \\
                    \midrule
                    s&	q1	&continue\\[2ex]
                    u&	q1	&continue\\[2ex]
                    m&	q1	&continue\\[2ex]
                    1&	q1	&continue\\[2ex]
                    space& —	&emit ID(sum1)\\[2ex]
                    +&	q3	&emit PLUS\\[2ex]
                    space& —	&ignore\\[2ex]
                    4& q2	&continue\\[2ex]
                    2& 	q2	&continue\\[2ex]
                    EOF&	—	&emit NUM(42) \\
                    \bottomrule
                \end{tabularx}
            \end{center}
            So, the output tokens are
            \bigbreak \noindent 
            \begin{cppcode}
                <ID, "sum1">
                <PLUS, "+">
                <NUM, "42">
            \end{cppcode}
        \item \textbf{Invalid tokens}: A token is invalid if:
            \begin{itemize}
                \item The input character sequence does not match any token pattern
                \item The DFA reaches a state with no valid transition
                \item No accepting state was reached before failure
            \end{itemize}
            In this case, a lexical error is reported.
        \item \textbf{Where do the tokens go}:  The output of the lexical analyzer goes directly to the parser. The lexer produces a stream of tokens, and these tokens are consumed one-by-one by the parser. The lexer does not store the full list of tokens permanently — it typically supplies them on demand to the parser.
        \item \textbf{String to int conversion}
            \bigbreak \noindent 
            \begin{cppcode}
                int string_to_int(const string& s) {
                    int i = 0;
                    bool negative = false;

                    if (s[0] == '-') negative = true;

                    for (const auto& c : s)  {
                        if (isdigit(c)) { i = (i * 10) + (c - '0'); }
                        else return -1;
                    }

                    return negative ? -i : i;
                }
            \end{cppcode}
        \item \textbf{Hex string to int conversion}:
            \bigbreak \noindent 
            \begin{cppcode}
             int hex_stoi(const string& s) {
                int i = 0;
                for (const auto& c : s) {
                    if (isdigit(c)) {
                        i = i * 16 + (c - '0');
                    } else if (c >= 97 && c <= 102) {
                        i = i * 16 + (c - 'a') + 10;
                    } else if (c >= 65 && c <= 70) {
                        i = i * 16 + ((c - 'A') + 10);
                    } else return -1;
                }

                return i;
            }
            \end{cppcode}
        \item \textbf{UTF-8}
        \item \textbf{Recursion in CFGs}: It is important to note that when constructing CFGs, we should use only left-recursion or only right-recursion. If a grammar is is both left and right recursive, 
            \begin{itemize}
                \item Ambiguous parse trees
                \item Infinite recursion in top-down parsers
                \item No fixed associativity
                \item Impossible to assign precedence cleanly
                \item AST construction becomes ill-defined
            \end{itemize}
        \item \textbf{Associativity in grammars}: Consider the grammar
            \begin{align*}
                E &\to E + T\ |\ E - T \ |\ T \\
                T &\to \text{var }\ | \ \text{int}
            .\end{align*}
            For an expression like $a-b-c$, there are two interpretations,
            \begin{itemize}
                \item \textbf{Left-associative}
                    \begin{align*}
                        (a-b)-c
                    .\end{align*}
                \item \textbf{Right-associative}
                    \begin{align*}
                        a-(b-c)
                    .\end{align*}
            \end{itemize}
            Note that most arithmetic operators are left-associative. If we notice the rule $E \to E - T$, the recursive call to $E$ is on the left-hand side of the production. This is called left recursion. Let's derive 
            \begin{align*}
                a-b-c
            .\end{align*}
            The derivation is 
            \begin{align*}
                E \implies (E - T)
            .\end{align*}
            Then, expand the left $E$ again,
            \begin{align*}
                E - T \implies ((E - T) - T) \implies ((T-T)-T) \implies ((a-b)-c)
            .\end{align*}
            This associativity is forced by the grammar. In order to derive 
            \begin{align*}
                (a-(b-c))
            \end{align*}
            we would need the rule 
            \begin{align*}
                E \to T - E
            .\end{align*}
        \item \textbf{What's the point of a parse tree}: A parse tree gives you the complete syntactic structure of an input string as dictated by a grammar. More precisely, it shows how the grammar derives the string, step by step.
            \bigbreak \noindent 
            A parse tree is a concrete representation of a derivation in a context-free grammar. It tells you:
            \begin{itemize}
                \item Which grammar rules were applied
                \item In what order they were applied
                \item How the input string is grouped syntactically
                \item How associativity and precedence are enforced
            \end{itemize}
            Every internal node corresponds to a nonterminal, every leaf corresponds to a terminal symbol.
            \bigbreak \noindent 
            A parse tree proves that a string:
            \begin{itemize}
                \item belongs to the language
                \item is generated by the grammar
                \item If a parse tree exists → the string is syntactically valid.
            \end{itemize}
        \item \textbf{Parsing expressions with grammars}: 
            In compiler theory, a grammar serves three closely related goals:
            \begin{itemize}
                \item Define the legal syntax of expressions.
                \item Guide parsing to produce a parse tree (concrete syntax tree).
                \item Enable construction of an AST, which is used for semantic analysis and code generation.
            \end{itemize}
            The grammar must encode:
            \begin{itemize}
                \item Operator precedence
                \item Operator associativity
                \item Grouping rules (parentheses)
            \end{itemize}
            Consider the simple grammar
            \begin{align*}
                E &\to E + T\ |\ E - T \ |\ T \\
                T &\to \text{var }\ | \ \text{int}
            .\end{align*}
            Consider the expression $a - b - c$, the derivation is
            \begin{align*}
                E &\implies E - T \implies E - T - T \implies T - T - T \\
                  &\implies \text{var (a)} - \text{ var (b)} - \text{ var (c)}
            ,\end{align*}
            thus yielding $a - b  - c$. The parse tree from this derivation is then
            \bigbreak \noindent 
            \fig{.6}{./figures/7.png}
            \bigbreak \noindent 
            Note that in parse trees, the children of a node are implicitly grouped as if surrounded by parentheses, Even if no parentheses appear in the source code, the tree structure itself acts as parentheses. A parse tree represents how the grammar groups subexpressions.
            \begin{itemize}
                \item Each node = an operation
                \item Its children = operands
                \item Subtrees = grouped expressions
            \end{itemize}
            Note that in derivations, when we expand a non-terminal, that expansion is implicitly grouped by parenthesis, even if we do not write the parenthesis in the derivation. In fact, we shouldn't. Every application of a production rule implicitly creates a grouped subtree, the grouping exists in the tree, not in the derivation string.
            \bigbreak \noindent 
            Suppose that we try to extend this grammar for multiplication and division, so we might extend our grammar to
            \begin{align*}
                E &\to E + T\ |\ E - T\ | \ E*T \ | \ E/T \ |\ T \\
                T &\to \text{var }\ | \ \text{int}
            .\end{align*}
            The problem is that this grammar treats all operators as equal precedence. So,
            \begin{align*}
                a + b * c
            \end{align*}
            can be derived either as $((a+b) * c)$ or $(a + (b*c))$. For the first one, the derivation is 
            \begin{align*}
                E \implies (E*T) \implies ((E+T) * T) \implies ((T+T)*T) \implies ((a+b) * c)
            .\end{align*}
            The second one is derived from
            \begin{align*}
                E \implies (E+T) \implies ((E*T)+T) \implies ((T*T) + T) \implies ((b*c) + a) 
            ,\end{align*}
            which is the same as 
            \begin{align*}
                (a + (b*c))
            .\end{align*}
            Both are valid derivations, which yield different parse trees for the same expression. This implies an ambiguous grammar, which is unacceptable for a complier.
            \bigbreak \noindent 
            Compilers must:
            \begin{itemize}
                \item Produce exactly one meaning
                \item Generate deterministic code
                \item Respect mathematical precedence
            \end{itemize}
            Ambiguity causes:
            \begin{itemize}
                \item Undefined behavior
                \item Multiple possible ASTs
                \item Impossible semantic analysis
            \end{itemize}
            So we need a grammar that forces precedence structurally, not by rules outside the grammar.
            \bigbreak \noindent 
            The solution is to separate precedence levels into different nonterminals.
        \item \textbf{Structural layers}: In order to enforce precedence, we must build structural layers in our grammar, where higher precedence operators appear on lower levels. Consider
            \begin{itemize}
                \item \textbf{E:} Expression, lowest precedence
                \item \textbf{T:} Term, medium precedence
                \item \textbf{F:} Factor, highest precedence
            \end{itemize} 
            Each layer corresponds to how tightly operators bind. Precedence is enforced by how deep an operator appears in the grammar. Lower in the grammar enforces 
            \begin{itemize}
                \item more deeply nested in the parse tree
                \item evaluated earlier
                \item higher precedence
            \end{itemize}
            The grammar
            \begin{align*}
                E &\to E + T \ \mid \ E - T \ \mid \ T \\
                T &\to T * F \ \mid \ T / F \ \mid \ F \\
                F &\to \text{int} \ \mid \ \text{var}
            .\end{align*}
            Provides correct operator precedence. Consider the expression $a + b * c$. The derivation is then
            \begin{align*}
                E &\implies E + T \implies T + T \implies T + T * F \implies T + T * \text{var (c)} \\
                  &\implies F + F + \text{var (c)} \implies \text{var (a) } + F + \text{ var (c)}  \\
                  &\implies \text{var (a) } + \text{var (b)} + \text{ var (c)}
            .\end{align*}
            Recall that the parenthesis are not required in the derivation, as they are implicit and expressed by the parse tree. Derivations describe rule application order. Parse trees describe grouping.
            \bigbreak \noindent 
            The parse tree for the above derivation would be
            \bigbreak \noindent 
            \fig{1}{./figures/8.png}
            \bigbreak \noindent 
            With this knowledge its easy to see that if we switched the precedence of +,-,*,/ so that +,- appears on a deeper level than *,/, then $a+b$ would be grouped and evaluated before multiplying by $c$.
        \item \textbf{Explicit Parenthesis, custom order of operations}: We can create a new terminal $(E)$, so our grammar would become
            \begin{align*}
                E &\to E + T \ \mid \ E - T \ \mid \ T \\
                T &\to T * F \ \mid \ T / F \ \mid \ F \\
                F &\to (E) \ \mid \ \text{int} \ \mid \ \text{var}
            .\end{align*}
            Note that parenthesis has highest precedence, so it must appear on the deepest level. Now, suppose we want the expression
            \begin{align*}
                (a + ((b-c)*d))
            .\end{align*}
            Note that most of these parenthesis are implicit i.e we do not need them, the only explicit parenthesis are
            \begin{align*}
                a + (b-c) * d
            .\end{align*}
            However, even if parenthesis are usually implicit, but still added in the input string, we must use $(E)$ to derive incorporate those parenthesis. For $a + (b-c) * d$ the derivation is 
            \begin{align*}
                E &\implies E + T \implies E + T * F \implies E + F * F \implies E + (E) * F  \\
                &\implies \cdots \implies a + (E) * d \implies a + (E-T) * d \implies \cdots \implies a + (b-c) * d
            ,\end{align*}
            which has parse tree
            \bigbreak \noindent 
            \fig{1}{./figures/9.png}
            \bigbreak \noindent 
            If the input string was instead 
            \begin{align*}
                (a + ((b-c) * d))
            ,\end{align*}
            the derivation is 
            \begin{align*}
                E &\implies T \implies F \implies (E) \implies (E + T) \implies (E + F) \implies (E + (E)) \\
                  &\implies (E + (T)) \implies (E + (T * F)) \implies (E + (F * F)) \implies (E + ((E) * F)) \\
                  &\implies (E + ((E - T) * F)) \implies \cdots \implies (a + ((b-c) * d))
            .\end{align*}
        \item \textbf{AST's, parse tree to AST}: Turning a parse tree into an Abstract Syntax Tree (AST) is one of the most important conceptual steps in a compiler. The key idea is: A parse tree represents syntax. An AST represents meaning.
            \bigbreak \noindent 
            A parse tree includes
            \begin{itemize}
                \item Every nonterminal $(E, T, F)$
                \item Every production rule used
                \item Parentheses as grammar artifacts
                \item Structural nodes that carry no semantic meaning
            \end{itemize}
            The parse tree for $a+b*c$ is
            \bigbreak \noindent 
            \fig{1}{./figures/10.png}
            \bigbreak \noindent 
            This tree
            \begin{itemize}
                \item Is correct
                \item Is verbose
                \item Contains nodes that exist only to enforce grammar rules
            \end{itemize}
            But a compiler does not need most of this information. An AST
            \begin{itemize}
                \item Removes grammar-specific nodes
                \item Keeps only semantic structure
                \item Represents computation directly
                \item Is independent of parsing strategy
            \end{itemize}
            For $a+b*c$, the AST is simply
            \bigbreak \noindent 
            \fig{1}{./figures/11.png}
            \bigbreak \noindent 
            If a node exists only to enforce grammar structure, remove it. If a node represents an operation or value, keep it.
            \bigbreak \noindent 
            To convert an parse tree to AST, we follow the following steps
            \begin{enumerate}
                \item \textbf{Remove Non-semantic Nodes}: Nodes like $E,T,F$, parenthesis, and single child chains are removed.
                    \begin{align*}
                        E \to T \to F \to a
                    \end{align*}
                    becomes simply $a$.
                \item \textbf{Promote operators}: Nodes like
                    \begin{align*}
                        E &\to E + T, \\
                        T &\to T * F
                    \end{align*}
                    become
                    \bigbreak \noindent 
                    \fig{1}{./figures/12.png}
                    \bigbreak \noindent 
                    \fig{1}{./figures/13.png}
                \item \textbf{Preserve hierarchy}: Because the grammar enforced precedence, the tree already has correct nesting, no additional rules are needed.
                    \bigbreak \noindent 
            \end{enumerate}
            A parse tree answers “how was this derived?” An AST answers “what does this compute?”
            \bigbreak \noindent 
            In essence, to build an AST, remove grammar-only nodes from the parse tree and retain only operators and operands arranged according to the tree’s structure.
            \bigbreak \noindent 
            Algorithmically, 
            \begin{enumerate}
                \item Visit parse tree node
                \item If node is:
                    \begin{itemize}
                        \item Operator $\to $ create AST node
                        \item Literal $\to $ create leaf
                        \item Grammar-only node $\to $ skip
                    \end{itemize}
                \item Recursively process children
                \item Return AST node upward
            \end{enumerate}
            Notice that the parse tree
            \bigbreak \noindent 
            \fig{.6}{./figures/9.png}
            \bigbreak \noindent 
            becomes the AST
            \bigbreak \noindent 
            \fig{1}{./figures/14.png}
            If the expression $a+b-c*d$ had no explicit parenthesis, the parse tree would be
            \bigbreak \noindent 
            \fig{.8}{./figures/15.png}
            \bigbreak \noindent 
            So, the AST would be 
            \bigbreak \noindent 
            \fig{1}{./figures/16.png}
        \item \textbf{Evaluating ASTs}: A post-order traversal of an expression AST produces Reverse Polish Notation (RPN).
            \bigbreak \noindent 
            This is not a coincidence; it is a direct consequence of how ASTs represent computation.
            \bigbreak \noindent 
            In an AST:
            \begin{itemize}
                \item Internal nodes = operators
                \item Leaves = operands
                \item Children = arguments to the operator
            \end{itemize}
            Post-order traversal visits:
            \begin{itemize}
                \item Left subtree
                \item Right subtree
                \item Node itself
            \end{itemize}
            This is precisely
            \begin{center}
                operand operand operator
            \end{center}
            which is reverse polish notation.
        \item \textbf{Negation and exponentiation}: To introduce unary negation and exponentiation correctly, we must extend the grammar without breaking precedence or associativity. This requires adding one new level and being careful about recursion direction. Negation binds tighter than multiplication or division, but looser than exponentiation. Exponentiation binds looser than parenthesis. So, the grammar becomes
            \begin{align*}
                E &\to E + T \mid E - T \mid T \\ 
                T &\to T * U \mid T / U \mid U \\ 
                U &\to -U \mid P \\ 
                P &\to F\land P  \mid F \\ 
                F &\to (E) \mid \text{int} \mid \text{var}
            .\end{align*}
            Note that exponentiation is right-associative, so right recursion works here. 
            \begin{align*}
                a \land b \land c = a \land (b \land c) 
            .\end{align*}


    \end{itemize}

    \pagebreak 
    \unsect{Intel x86-64 architecture and code generation}
    \begin{itemize}
        \item \textbf{General purpose registers}: General-purpose registers store integers, pointers, loop counters, and intermediate results.
            \bigbreak \noindent 
            The 32-bit registers are 
            \begin{itemize}
                \item EAX – Accumulator (frequently used for arithmetic and return values)
                \item EBX – Base register
                \item ECX – Counter (loops, shifts)
                \item EDX – Data register (multiplication/division)
                \item ESI – Source index (string operations)
                \item EDI – Destination index (string operations)
                \item EBP – Base pointer (stack frame)
                \item ESP – Stack pointer
            \end{itemize}
            The 64-bit registers are
            \begin{itemize}
                \item RAX, RBX, RCX, RDX, RSI, RDI, RBP, RSP
                \item R8–R15 (additional registers introduced in 64-bit mode)
            \end{itemize}
            Each 64-bit register has accessible sub-registers. For example, 
            \begin{align*}
                \text{AX } \to \text{ EAX } \to \text{ AX }\to \text{ AH/AL}
            .\end{align*}
            This backward compatibility is fundamental to x86 design.
        \item \textbf{3-bit register specifier}: Many x86 instructions include a 3-bit register specifier, which implicitly numbers the registers:
            \begin{center}
                \begin{tabularx}{\textwidth}{@{}XX@{}}
                    \toprule
                    \textbf{Encoding}&	\textbf{Register} \\
                    \midrule
                    000	&EAX\\[2ex]
                    001	&ECX\\[2ex]
                    010	&EDX\\[2ex]
                    011	&EBX\\[2ex]
                    100	&ESP\\[2ex]
                    101	&EBP\\[2ex]
                    110	&ESI\\[2ex]
                    111	&EDI\\
                    \bottomrule
                \end{tabularx}
            \end{center}
            \textbf{Note:} Exists only at the binary encoding level
        \item \textbf{Instruction pointer (RIP)}: 
            \begin{itemize}
                \item \textbf{EIP (32-bit) / RIP (64-bit)}: Holds the address of the next instruction to execute.
            \end{itemize}
        \item \textbf{The return instruction (C3)}: The ret (return) instruction transfers control back to the calling function by restoring the instruction pointer from the stack. It is the logical inverse of the call instruction.
        \item \textbf{Dynamic code execution basic example (CPP) (JIT execution)}: We will 
            \begin{itemize}
                \item Allocate memory at runtime
                \item Write machine code (Intel x86/x86-64 instructions) into that memory
                \item Mark the memory as executable
                \item Cast the address to a function pointer
                \item Call it
            \end{itemize}
            This is exactly what a JIT compiler does at a minimal level. Consider 
            \bigbreak \noindent 
            \begin{cppcode}
                unsigned char prog[50000];
                prog[0] = 0xC3; // ret instruction

                int value;
                // Cast to a function pointer, then call, store returned value from call in value
                value = ((int(*)(void))prog)();
            \end{cppcode}
            \bigbreak \noindent 
            If we run this code, we will get a seg fault. This is because our memory block is only read/write, we need a way to mark the memory as executable. We can do this by using sys/mman.h to allocate memory through the operating system, and errno to check for errors.
        \item \textbf{sys/mman.h}:  <sys/mman.h> declares the memory-mapping API. It allows a process to request, control, and manage virtual memory regions directly from the operating system.
            \bigbreak \noindent 
            The function \textbf{mmap} maps a region of virtual memory.
            \bigbreak \noindent 
            \begin{cppcode}
                void* mmap(
                    void* addr,
                    size_t length,
                    int prot,
                    int flags,
                    int fd,
                    off_t offset
                );
            \end{cppcode}
            \bigbreak \noindent 
            fd is an open file descriptor referring to a file or device, it declares “this mapping is backed by the contents of this file.” It must be opened with permissions compatible with prot. The kernel uses it as the source of bytes for the mapping. offset specifies where in the file the mapping starts, in bytes.
            \bigbreak \noindent 
            Returns a pointer to the mapped region. On failure, it returns MAP\_FAILED.
            \bigbreak \noindent 
            \textbf{munmap} Unmaps a previously mapped region.
            \bigbreak \noindent 
            \begin{cppcode}
                int munmap(void* addr, size_t length);
            \end{cppcode}
            \bigbreak \noindent 
            After munmap, any access to the region is invalid.
            \bigbreak \noindent 
            \textbf{mprotect} Changes access permissions on an existing mapping.
            \bigbreak \noindent 
            \begin{cppcode}
                int mprotect(void* addr, size_t len, int prot);
            \end{cppcode}
            \bigbreak \noindent 
            Commonly used to transition memory:
            \begin{itemize}
                \item \textbf{RW:} Write code
                \item \textbf{RX:} Execute code
            \end{itemize}
            This is critical for enforcing $W \oplus X$ (write xor execute) security.
            \bigbreak \noindent 
            The protection flags (prot) are
            \begin{itemize}
                \item \textbf{PROT\_READ}: Readable
                \item \textbf{PROT\_WRITE}: Writable
                \item \textbf{PROT\_EXEC}: Executable
                \item \textbf{PROT\_NONE}: No access
            \end{itemize}
            These specify what the CPU is allowed to do with the memory
            \bigbreak \noindent 
            The mapping flags (flags) are
            \begin{itemize}
                \item \textbf{MAP\_PRIVATE}: Copy-on-write
                \item \textbf{MAP\_SHARED}: Shared between processes
                \item \textbf{MAP\_ANONYMOUS}: Not backed by a file
                \item \textbf{MAP\_FIXED}: Place mapping at a fixed address (dangerous)
            \end{itemize}
        \item \textbf{errno}: errno is a thread-local integer that reports why a system call failed. It is declared in 
            \bigbreak \noindent 
            \begin{cppcode}
                <errno.h>
            \end{cppcode}
            \bigbreak \noindent 
            System calls signal failure via return values, they set errno to a symbolic error code. The value persists until overwritten
            \bigbreak \noindent 
            \begin{cppcode}
                void* p = mmap(...);
                if (p == MAP_FAILED) {
                    perror("mmap");
                }
            \end{cppcode}
            \bigbreak \noindent 
            Only meaningful immediately after failure, not reset automatically. The common errno values with mmap are
            \begin{itemize}
                \item \textbf{ENOMEM}:	Insufficient memory or address space
                \item \textbf{EACCES}:	Permission denied (e.g., PROT\_EXEC disallowed)
                \item \textbf{EINVAL}:	Invalid flags, alignment, or parameters
                \item \textbf{EBADF}:	Invalid file descriptor
                \item \textbf{ENODEV}:	Unsupported mapping type
            \end{itemize}
            To convert errno to text, use perror or strerror
            \bigbreak \noindent 
            \begin{cppcode}
                perror(const char*)
                strerror(errno)
            \end{cppcode}
            \bigbreak \noindent 
            perror prints a textual description of the error code currently stored in the system variable errno to stderr. It's parameter is a pointer to a null-terminated string with explanatory message
        \item \textbf{DCE with mmap and errno}: 
            \bigbreak \noindent 
            \begin{cppcode}
const int prog_size = 50000;
unsigned char* prog;

prog = mmap(0, prog_size, PROT_EXEC |$\mid$| PROT_READ |$\mid$| PROT_WRITE, MAP_PRIVATE |$\mid$| MAP_ANONYMOUS, -1, 0);

if (errno) {
    perror("mmap");
    return;
}
            \end{cppcode}
            \bigbreak \noindent 
            Having fd=-1 means “This mapping is not backed by any file.” This is only valid when used together with MAP\_ANONYMOUS. Without it, fd=-1 is an error.
            \bigbreak \noindent 
            Having addr=0 means that the memory placement is decided by the operating system.
            \bigbreak \noindent 
            Now, prog points to the start of our block of memory that is readable, writable, and executable. Remember, to free the memory we must call \texttt{munmap}
            \bigbreak \noindent 
            \begin{cppcode}
            munmap(prog, prog_size);
            \end{cppcode}
            \bigbreak \noindent 
            Now, with
            \bigbreak \noindent 
            \begin{cppcode}
                value = ((int(*)(void))prog)();
            \end{cppcode}
            \bigbreak \noindent 
            We can output the value of value, which will give us the current value in the accumulator RAX. The return value of a function is defined to be placed in the accumulator register
            \begin{itemize}
                \item prog points to executable memory
                \item (int(*)(void))prog reinterprets that address as a function
                \item () performs a call instruction
                \item The CPU jumps to prog
                \item Your machine code executes
                \item A ret instruction returns control to the caller
                \item The caller reads the return value from the accumulator
                \item That value is assigned to value
            \end{itemize}
            Remember that we have 
            \bigbreak \noindent 
            \begin{cppcode}
                prog[0] = 0xC3
            \end{cppcode}
            \bigbreak \noindent 
            I.e the ret instruction, which is the only instruction currently in our program.
            \bigbreak \noindent 
            \begin{cppcode}
                cout << "Memory at: " << (int*) prog << '\n';
            \end{cppcode}
            \bigbreak \noindent 
            Note that if we left our program pointer as char*, C++ uses the overload for outputing C-style strings. Since we just want the address of the first byte of our program, we must cast it to a different pointer, in this case a pointer to an integer. A better way to go about doing this is 
            \bigbreak \noindent 
            \begin{cppcode}
            cout << "Memory at: " << static_cast<void*>(prog) << '\n';
            \end{cppcode}
        \item \textbf{The ModR/M byte}: The ModR/M byte is a mandatory operand-encoding byte used by many x86 instructions. It specifies which registers are used and whether an operand refers to a register or to memory, including the basic addressing mode. 
            \bigbreak \noindent 
            The ModR/M byte is one byte (8 bits), divided into three fields
            \begin{itemize}
                \item \textbf{r /m}: Bits 0-2 
                \item \textbf{reg}: Bits 3-5
                \item \textbf{mod}: Bits 6-7
            \end{itemize}
            mod determines how the r/m field is interpreted:
            \begin{itemize}
                \item \textbf{00}:	Memory operand, no displacement (except special cases)
                \item \textbf{01}:	Memory operand + 8-bit displacement
                \item \textbf{10}:	Memory operand + 32-bit displacement (16-bit in 16-bit mode)
                \item \textbf{11}:	Register operand (no memory access)
            \end{itemize}
            The reg field usually selects a register operand. 
            \begin{itemize}
                \item \textbf{000}:	EAX
                \item \textbf{001}:	ECX
                \item \textbf{010}:	EDX
                \item \textbf{011}:	EBX
                \item \textbf{100}:	ESP
                \item \textbf{101}:	EBP
                \item \textbf{110}:	ESI
                \item \textbf{111}:	EDI
            \end{itemize}
            In /r encodings, reg is a real operand. In /0–/7 encodings, reg is an opcode extension, not a register
            \bigbreak \noindent 
            The r/m field selects:
            \begin{itemize}
                \item A register if mod = 11
                \item A memory addressing mode if mod $\ne $ 11
            \end{itemize}
            In the second case, r/m selects a base addressing form. In 32/64-bit mode:
            \begin{itemize}
                \item 000	[EAX]
                \item 001	[ECX]
                \item 010	[EDX]
                \item 011	[EBX]
                \item 100	SIB byte follows
                \item 101	[disp32] if mod=00, otherwise [EBP]+disp
                \item 110	[ESI]
                \item 111	[EDI]
            \end{itemize}
            If r/m = 100 and mod $\ne $ 11, a SIB (Scale–Index–Base) byte follows. This allows
            \begin{align*}
                [\text{base } + \text{ index } \cdot \text{ scale } + \text{ displacement} ]
            .\end{align*}
        \item \textbf{Word sizes in Intel architecture}: 
            \begin{itemize}
                \item \textbf{Byte}:	8 bits
                \item \textbf{Word}:	16 bits
                \item \textbf{Doubleword (dword)}:	32 bits
                \item \textbf{Quadword (qword)}:	64 bits
                \item \textbf{Double quadword (dqword / xmmword)}:	128 bits
                \item \textbf{YMM word}:	256 bits
                \item \textbf{ZMM word}:	512 bits
            \end{itemize}
        \item \textbf{MOV instructions: Putting a value into the accumulator}: For this we use the instruction
            \bigbreak \noindent 
            \begin{cppcode}
            MOV     dst,src
            \end{cppcode}
            \bigbreak \noindent 
            Many forms of the MOV instruction can be found in the Intel manual. There are different forms based on whether dst, src is memory or a register, and how many bits these fields are. For example, we could use
            \bigbreak \noindent 
            \begin{cppcode}
            MOV    r/m8,r8
            \end{cppcode}
            \bigbreak \noindent 
            where r/m8,r8 means that the destination operand can be either an 8-bit memory location, or an 8-bit register, and the source must be an 8-bit register. Conversely,
            \bigbreak \noindent 
            \begin{cppcode}
            MOV    r8,r/m8
            \end{cppcode}
            \bigbreak \noindent 
            is the opposite. In the first form, the opcode is 88 /r, where /r denotes that the encoding requires a modR/M byte. 
            \bigbreak \noindent 
            Note that there are forms for 8, 16, 32, and 64 bit. There are also immediate byte forms, for example
            \bigbreak \noindent 
            \begin{cppcode}
            MOV    rk,immk 
            \end{cppcode}
            \bigbreak \noindent 
            where $k\in \{8,16,32,64\} $. Consider the form
            \bigbreak \noindent 
            \begin{cppcode}
            MOV r32,imm32
            \end{cppcode}
            \bigbreak \noindent 
            This form has opcode $B8 + \text{ rd id} $, where the $d$ stands for \textit{double word} 32-bit.  If our instruction is
            \bigbreak \noindent 
            \begin{cppcode}
                MOV    EAX,4
            \end{cppcode}
            \bigbreak \noindent 
            where 4 is the immediate byte, then the opcode is
            \begin{align*}
                B804000000
            .\end{align*}
            Note that EAX is 000, so B8 remains the same. Further note that our immediate byte is 32-bits, since we used the double word version, and those bytes are in little endian, so the bytes are reversed. In BE it would be
            \begin{align*}
                00000004
            .\end{align*}
            Another note is that there are no memory to memory moves, we must go memory to register, then register to memory. 
            \bigbreak \noindent 
            Let's add this MOV instruction to our code, we add to the program buffer
            \bigbreak \noindent 
            \begin{cppcode}
            size_t p_offset = 0;

            prog[p_offset++] = 0xb8; // MOV EAX,4
            prog[p_offset++] = 0x04;
            prog[p_offset++] = 0x00;
            prog[p_offset++] = 0x00;
            prog[p_offset++] = 0x00;
            prog[p_offset++] = 0xc3; // RET to caller
            \end{cppcode}
            \bigbreak \noindent 
            Now, the value returned from the call to prog gives the value $4$, as $4$ is the value in the accumulator.
        \item \textbf{A note}: The 64-bit architecture is called IA-32e, since it is an extension of the 32-bit architecture, not a 64-bit rework.
        \item \textbf{Two's complement}: IA uses twos complement, so to put $-7$ into the accumulator, we use
            \begin{align*}
                f9ffffff
            .\end{align*}
            as the immediate byte. Notice the LE. 
        \item \textbf{Finite width arithmetic}: An $n$-digit base-$b$ register  can store exactly
            \begin{align*}
                \{0,1,\dots, b^{n}-1\}
            ,\end{align*}
            nothing else. Any computation that exceeds this range wraps. The hardware drops the carry out of the top digit. This behavior is precisely arithmetic modulo $b^{n}$.
        \item \textbf{Twos complement}: This system comes from the question... On an $n$-digit base $b$-machine, what value should represent $-x$ so that subtraction can be done by addition.
            \bigbreak \noindent 
            So, with finite-width arithmetic, we want a value $y$ such that
            \begin{align*}
                x + y \equiv 0 \pmod{b^{n}}
            .\end{align*}
            So, $y$ is the additive inverse of $x$ modulo $b^{n}$. Notice that since $x+y \equiv 0 \pmod{b^{n}} $, 
            \begin{align*}
                x+y = kb^{n}
            \end{align*}
            for some integer $k$. However, since 
            \begin{align*}
                0 \leq x,y \leq b^{n}-1
            \end{align*}
            implies that 
            \begin{align*}
                0 \leq x + y \leq 2(b^{n}-1) = 2b^{n}-2
            ,\end{align*}
            the only multiples of $b^{n}$ that $x+y$ can equal are $0$ and $b^{n}$, so we can safely say that $k = 1$. Thus, our equation becomes
            \begin{align*}
                x+y =b^{n}
            .\end{align*}
            From this, we can solve for $y$
            \begin{align*}
                y = b^{n}-x
            .\end{align*}
            Therefore, 
            \begin{align*}
                x + (b^{n}-x) = b^{n}
            ,\end{align*}
            since $x + y = b^{n}$, and $y = b^{n}-x$. Thus,
            \begin{align*}
                x + (b^{n}-x) \equiv 0 \pmod{b^{n}}
            .\end{align*}

        \item \textbf{Why twos complement works}: Consider an $n$ digit system (base $b$), and a number $x$ in that system. If we subtract by $b^{n}$, we get
            \begin{align*}
                b^{n} - x
            .\end{align*}
            Now, we can add zero to this subtraction
            \begin{align*}
                b^{n} - 1 - x + 1 = (b-1)(b^{n-1}+b^{n-2}+\dots+b^{0}) - x + 1
            .\end{align*}
            Notice that $b^{n}-1$ becomes $(b-1)(b^{n-1} + b^{n-2} + \dots + b^{0}) $. Further notice that $b^{n}-1$ is the number with all digits equal to $b-1$. In base $2$, this would be all bits set to 1.
            \bigbreak \noindent 
            This algebraic trick reveals that
            \begin{align*}
                b^{n}-1 = (b - 1)(b^{n-1} + \dots + b^{0})
            .\end{align*}
            From this, subtracting $x$ gives the digitwise complement of $x$. In base two, this would mean flipping all the bits. Thus,
            \begin{align*}
                b^{n}-1 - x + 1
            \end{align*}
            is the digitwise complement plus one, where the plus one is a carry. This is the step that turns the digitwise complement into the true additive inverse in base $b$.  We now understand that
            \begin{align*}
                \bar{x} = (b^{n}-1)-x
            .\end{align*}
            This is the digitwise (radix-$b-1$) complement. So, we know the complement and the additive inverse. Notice that all we need to do to turn the complement into the additive inverse is add one to the complement. 
        \item \textbf{A cleaned up twos complement argument}: 
            Consider a base-$b$, $n$-digit system, note that our context is finite width arithmetic, since this is how registers work. Thus, all arithmetic is taken modulo $b^{n}$, since
            \begin{align*}
                0 \leq x \leq b^{n}-1
            \end{align*}
            for all $x$ in our system. Now consider 
            \begin{align*}
                b^{n}-x
            \end{align*}
            Now, add zero to this expression
            \begin{align*}
                b^{n}-1-x+1
            .\end{align*}
            Consider the quantity $b^{n}-1$, we can factor out a $b-1$ to get
            \begin{align*}
                (b^{n}-1) = (b-1)(b^{n-1} + b^{n-2} + \dots + b^{1} + b^{0})
            .\end{align*}
            Observe that this quantity is a number in our system with all digits equal to $b-1$. For example, if $b=2$, then this is precisely the expression that gives us an $n$-bit binary number with all bits set to one. 
            \bigbreak \noindent 
            So, subtracting $x$ from this quantity gives the radix $b-1$ (digitwise) complement. In base two this would mean flipping all bits in $x$. So,
            \begin{align*}
                b^{n}-1-x+1
            \end{align*}
            is the digitwise complement plus one. Our goal is to derive the additive inverse of $x$ using this digitwise complement. The additive inverse of $x$ in unbounded width arithmetic would be the number $y$ such that
            \begin{align*}
                x+y = 0
            .\end{align*}
            However, we are dealing with finite width arithmetic, so all expressions equal to zero must be congruent to zero when taken modulo $b^{n}$. So, we must instead consider
            \begin{align*}
                x+y \equiv 0 \pmod{b^{n}}
            .\end{align*}
            Using modular arthritic, this means that
            \begin{align*}
                x+y = kb^{n}
            \end{align*}
            for some $k\in \mathbb{Z}$. But, notice that since
            \begin{align*}
                0 \leq x,y \leq b^{n}-1
            ,\end{align*}
            it must be that
            \begin{align*}
                0 \leq x + y \leq 2(b^{n} - 1) = 2b^{n}-2
            .\end{align*}
            Since the maximum in our system is $b^{n}$, the only possible multiple of $b^{n}$ is $b^{n}$ itself, so $k=1$ (actually $k\in \{0,1\}$, but $k=0$ only if $x=y=0$, otherwise $k=1$). Thus,
            \begin{align*}
                x+y = b^{n}
            .\end{align*}
            Now we see that the additive inverse $y$ is 
            \begin{align*}
                y = b^{n} - x 
            .\end{align*}
            Notice that the digitwise complement ($b^{n}-1-x$) and the additive inverse ($b^{n}-x$) look very similar. All we need to do to get from the digitwise complement to the additive inverse is add one to the complement, which is precisely what twos complement does.
            \bigbreak \noindent 
            The story of the sign bit (in base two) arises once you define the signed interpretation map
            \begin{align*}
                \phi(u) = \begin{cases}
                    u, \quad 0 \leq u < 2^{n}-1     \\
                    u-2^{n}, \quad 2^{n-1} \leq u < 2^{n}
                \end{cases}
            .\end{align*}
            Notice that 
            \begin{align*}
                \phi(2^{n}-u) = (2^{n}-u)-2^{n} = -u = -\phi(u)
            .\end{align*}
            Any value greater than or equal to $2^{n-1}$, which corresponds to MSB=1 is negative, hence the sign bit.
            \bigbreak \noindent 
            In addition, consider performing twos complement twice on any number $x$,
            \begin{align*}
                2^{n}-(2^{n}-x) = x
            \end{align*}
            so we get back $x$. This is why we don't have to reverse the process of twos complement to undo it.

        \item \textbf{ADD instructions: Adding the value from a register into the accumulator}: Suppose we want to add $+3$ into ECX, then add that value to the accumulator. We can use the ADD instruction version
            \bigbreak \noindent 
            \begin{cppcode}
            ADD    r32, r/m32
            \end{cppcode}
            \bigbreak \noindent 
            with opcode 03 /r and the same immediate byte move instruction
            \bigbreak \noindent 
            \begin{cppcode}
            MOV    ECX, 3 
            \end{cppcode}
            \bigbreak \noindent 
            This has encoding 
            \begin{align*}
                B903000000
            .\end{align*}
            The ADD instruction is 
            \bigbreak \noindent 
            \begin{cppcode}
            ADD    EAX,ECX
            \end{cppcode}
            The ModR/M byte for the ADD instruction is $C1$, since mod=11, reg=000 (ECX), and r/m = 001  (EAX).  Thus, the encoding is
            \begin{align*}
                03C1
            .\end{align*}
        \item \textbf{Note on instruction length}: An x86 instruction can be anywhere from 1 to 15 bytes long. There is no fixed instruction width. The length depends on which optional components are present.
        \item \textbf{/0 - /7}: In Intel x86 architecture, the symbols /0 through /7 refer to opcode extensions encoded in the ModR/M byte of an instruction. They are not registers themselves; rather, they select which operation an instruction performs when multiple operations share the same primary opcode. 
            \bigbreak \noindent 
            Many x86 instructions reuse a single opcode and rely on the ModR/M byte to disambiguate the operation. When documentation writes /0, /1, …, /7, it means “The reg/opcode field of the ModR/M byte must contain this value.” That 3-bit field can encode values from 0 to 7, hence /0–/7.
            \bigbreak \noindent 
            Therefore, the 3-bit reg field has exactly one meaning per instruction:
            \begin{itemize}
                \item Either it selects a register operand
                \item Or it selects an opcode extension (/0–/7)
            \end{itemize}
        \item \textbf{Substraction instructions}: Suppose we want to subtract ECX from EAX,
            \bigbreak \noindent 
            \begin{cppcode}
            SUB    EAX, ECX
            \end{cppcode}
            \bigbreak \noindent 
            Suppose ECX has value 3, and $EAX$ has value $-7$. The opcode for this SUB instruction
            \bigbreak \noindent 
            \begin{cppcode}
             SUB r/m32, r32
            \end{cppcode}
            \bigbreak \noindent 
            Is 29 /r. The encoding is therefore $29C8$, since the ModR/M byte is C8 (11 001 000).
        \item \textbf{Multiplication instructions}: We have two primary instructions, MUL (unsigned multiplication), and IMUL (signed multiplication). Regarding IMUL, there are four one operand instructions, one for each register/memory size. Consider
            \bigbreak \noindent 
            \begin{cppcode}
            IMUL    r/m32
            \end{cppcode}
            \bigbreak \noindent 
            It should be noted that the operand is multiplied against the value in the accumulator, and the DX/AX registers are combined to make one 64-bit register split into two 32-bit sections, 32-bits per half. The DX register is the high order 32-bits, and the AX register is the low order 32-bits. Since we will use the 32-bit version, EDX and EAX are our register pairs.
            \bigbreak \noindent 
            The opcode for this instruction is f7 /5, so we use $101$ in the 3-bit reg field in the ModR/M byte. 
            \bigbreak \noindent 
            If our instruction was
            \bigbreak \noindent 
            \begin{cppcode}
            IMUL    ECX
            \end{cppcode}
            \bigbreak \noindent 
            ,this byte would be 11 101 001, which is $E9$ in hex. Thus, the encoding is $F7E9$.
        \item \textbf{Division and converting instructions}: We have DIV for unsigned division, and IDIV for signed division. This operation divides the value in the accumulator by the value in the supplied operand, and again the result is split into the DX / AX pair. The quotient of the operation will be housed in AX, and the remainder in DX.
            \bigbreak \noindent 
            Before we can proceed with the division, we must extend the sign bit of the accumulator into the DX register. For this, we need CWD, CDQ, CQO instructions. CWD converts a word to a double word, and CDQ converts a double word to a quadword. In particular, the version of these instructions that extend the sign bit of AX into DX are given by opcode 99, and they take no operands. Thus,
            \bigbreak \noindent 
            \begin{cppcode}
            CDQ
            \end{cppcode}
            \bigbreak \noindent 
            Extends the sign bit of EAX (32-bit doubleword) into EDX, thus converting to a quadword (64-bit). This operation must be done before division.
            \bigbreak \noindent 
            Note that the encoding for CDQ is 99, and the encoding for IDIV is f7 /7, where the instruction is
            \bigbreak \noindent 
            \begin{cppcode}
            IDIV   r/m32
            \end{cppcode}
        \item \textbf{XOR instruction, Clearing registers}: The XOR instruction is
            \bigbreak \noindent 
            \begin{cppcode}
            XOR   r /m32 r32 
            \end{cppcode}
            \bigbreak \noindent 
            and variants. The opcode for this particular instruction is $31 /r$. To clear a register, we therefore use the same register for both operands 
            \bigbreak \noindent 
        \item \textbf{R8-R15, the REX prefix}: The REX prefix is a one-byte instruction prefix introduced with x86-64 to extend the original x86 encoding so that it can address 64-bit operands and additional registers. 
            \bigbreak \noindent 
            The REX prefix is one byte, with the following fixed structure:
            \bigbreak \noindent 
            \begin{cppcode}
            0100WRXB
            \end{cppcode}
            \bigbreak \noindent 
            The high nibble 0100 identifies it as a REX prefix. The low nibble contains four 1-bit flags.
            \begin{center}
                \begin{tabularx}{\textwidth}{@{}XXX@{}}
                    \toprule
                    \textbf{Bit}&	\textbf{Name}&	\textbf{Purpose}
                    \midrule
                    W	&REX.W	&64-bit operand size\\[2ex]
                    R	&REX.R	&Extends ModR/M reg field\\[2ex]
                    X	&REX.X	&Extends SIB index field\\[2ex]
                    B	&REX.B	&Extends ModR/M r/m or SIB base \\
                    \bottomrule
                \end{tabularx}
            \end{center}
            REX.W = 1 forces a 64-bit operand size. Requires REX.W Without it, the instruction would operate on 32-bit registers.
            \bigbreak \noindent 
            REX.R, REX.X, and REX.B extend 3-bit register fields into 4-bit fields.
            \begin{itemize}
                \item \textbf{REX.R}: Extends the reg field of the modR/m byte. Place the high order bit of the non r /m register mask into this bit. For example, for R8 = 1000, set R=1.
                \item \textbf{REX.X}: Extends the index field in the SIB byte, only meaningful if a SIB byte is present, otherwise leave zero
                \item \textbf{REX.B}: Extends the r /m field of the modr /m byte. Place the high order bit of the r / m register mask into this bit.
            \end{itemize}
        \item \textbf{Recall: Fast exponentiation}: Uses the identity that
            \begin{align*}
               a^{b} = \begin{cases}
                    (a^{2})^{\frac{b}{2}} \quad b = 2k \\
                    a\cdot a^{b-1} \quad b=2\ell  + 1
               \end{cases}
            .\end{align*}
            \bigbreak \noindent 
            \begin{cppcode}
            e = 1
            // b!=0 and b>=0 (nonnegative b) implies b>0
            while (b>0) {
                if (b & 0x01) {
                    e = e * a
                }
                a = a*a
                b>>=1 // Divide by two
            }
            \end{cppcode}
        \item \textbf{$k$ child trees to binary trees}: When creating abstract syntax trees, it is likely the case that a binary tree is too restrictive, we may need to have trees that can have $k$ children, for $k$ not restricted to $2$. Thus, it is helpful to know the way in which we can convert such a tree into a binary tree.
            \bigbreak \noindent 
            The idea is that for each parent node, we take its first (leftmost) child and make that its child in the binary tree. Then, for any other children (siblings of the first child), we attach those in a link to the first child. So, if $A$ has children $B,C,D$, and $B$ has children $E,F$, then the binary tree would have connections
            \begin{align*}
                A &\to B, \\
                B &\to C,E \\
                C &\to D, \\
                E &\to F
            .\end{align*}
        \item \textbf{The POP and PUSH instructions}: The PUSH instruction pushes a word, doubleword, or quadword onto the stack. Decrements the stack pointer and then stores the source operand on the top of the stack.
            \bigbreak \noindent 
            \begin{cppcode}
            PUSH    r32
            \end{cppcode}
            \bigbreak \noindent 
            The POP instruction loads the value from the top of the stack to the location specified with the destination operand
            \bigbreak \noindent 
            \begin{cppcode}
            POP     r32
            \end{cppcode}
        \item \textbf{The ECHG (exchange) instruction}: Exchanges register / memory with register
        \item \textbf{Machine execution model (MEM)}: There is a problem that arises when trying to generate machine code. We have many registers, but when generating code, which registers do we use? And if we have to go to memory if those registers are in use, which addresses do we go to? And how does this access of memory to store values work? 
            \bigbreak \noindent 
            Since this is outside the scope of our current progress in compiler theory, we will start with the use of a stack based model.
            \bigbreak \noindent 
            In this model, we will have the top of the stack always be the current value in the accumulator.
            \bigbreak \noindent 
            First, consider a generic tree 
            \bigbreak \noindent 
            \fig{1}{./figures/18.png}
            \bigbreak \noindent 
            where $T_{1}$ and $T_{2}$ are subtrees. When $T_{1}$ completes its duties, its computed value will be in the accumulator. So, we push this value onto the stack. Then, we can generate code for $T_{2}$ and its value will now be in the accumulator. From here, we need to perform the addition with these two values. So, we pop the top of the stack into ECX, then write the instruction
            \bigbreak \noindent 
            \begin{cppcode}
            ADD    EAX,ECX
            \end{cppcode}
            \bigbreak \noindent 
            Consider the expression $6-5-10$, which generates the AST 
            \bigbreak \noindent 
            \fig{1}{./figures/17.png}
            \bigbreak \noindent 
            The code will look something like
            \bigbreak \noindent 
            \begin{cppcode}
            // Computes 6-5 and places result on the top of the stack
            MOV   EAX,6
            PUSH  EAX
            MOV   EAX,5
            POP   ECX
            XCHG  EAX, ECX
            SUB   EAX, ECX
            PUSH  EAX

            // Computes 6-5-10, 6-5 is on the top of the stack at this point
            MOV   EAX,10
            POP   ECX
            XCHG  EAX,ECX
            SUB   EAX,ECX
            \end{cppcode}

        \item \textbf{The TEST instruction}: The TEST instruction in IA-32e (the 64-bit extension of the x86 architecture) performs a bitwise logical AND between two operands without storing the result. Its sole purpose is to update the condition flags in RFLAGS. Given
            \bigbreak \noindent 
            \begin{cppcode}
            TEST  op1, op2
            \end{cppcode}
            \bigbreak \noindent 
            The processor computes
            \bigbreak \noindent 
            \begin{cppcode}
            temp := op1 AND op2
            \end{cppcode}
            \bigbreak \noindent 
            The value of temp is not written back to any operand. Instead, only flags are updated. After execution:
            \begin{itemize}
                \item \textbf{ZF (Zero Flag)}: Set if temp == 0, otherwise cleared.
                \item \textbf{SF (Sign Flag)}: Set according to the most significant bit of temp.
                \item \textbf{PF (Parity Flag)}: Set according to parity of the low byte of temp.
                \item \textbf{CF (Carry Flag)}: Cleared to 0
                \item \textbf{OF (Overflow Flag)}: Cleared to 0
                \item \textbf{AF (Auxiliary Flag)}: Undefined
            \end{itemize}
            This is identical to AND except the result is discarded.
        \item \textbf{Common use cases of TEST}:
            \begin{enumerate}
                \item \textbf{Checking if a register is zero}:
                    \bigbreak \noindent 
                    \begin{cppcode}
                    TEST   R,R

                    // Jump on ZF zero
                    jz
                    \end{cppcode}
                \item \textbf{Testing specific bits}:
                    \bigbreak \noindent 
                    \begin{cppcode}
                    test rax, 1
                    \end{cppcode}
                    \bigbreak \noindent 
                    This tests if the LSB is set (check ZF).
            \end{enumerate}
        \item \textbf{Code generation for the fast exponentiation algorithm}: We aim to compute the quantity $e = a^{b}$. For this, we will place 
            \begin{align*}
                e \to r8, \quad a \to r9, \quad b \to r10
            .\end{align*}
            Assume the stack has $b$ on the bottom, and $a$ at the top. The code is then
            \begin{cppcode}
                // Clear out r8
                XOR   r8,r8

                // Place |$a$| in r9
                POP   r9

                // Place |$b$| in r10
                POP   r10

                TEST r10,r10
            \end{cppcode}










            

    \end{itemize}

















    
\end{document}
