\documentclass{report}

\input{~/dev/latex/template/preamble.tex}
\input{~/dev/latex/template/macros.tex}

\title{\Huge{}}
\author{\huge{Nathan Warner}}
\date{\huge{}}
\fancyhf{}
\rhead{}
\fancyhead[R]{\itshape Warner} % Left header: Section name
\fancyhead[L]{\itshape\leftmark}  % Right header: Page number
\cfoot{\thepage}
\renewcommand{\headrulewidth}{0pt} % Optional: Removes the header line
%\pagestyle{fancy}
%\fancyhf{}
%\lhead{Warner \thepage}
%\rhead{}
% \lhead{\leftmark}
%\cfoot{\thepage}
%\setborder
% \usepackage[default]{sourcecodepro}
% \usepackage[T1]{fontenc}

% Change the title
\hypersetup{
    pdftitle={Formal DSA in C++}
}

\begin{document}
    % \maketitle
        \begin{titlepage}
       \begin{center}
           \vspace*{1cm}
    
           \textbf{Formal DSA in C++}
    
           \vspace{0.5cm}
            
                
           \vspace{1.5cm}
    
           \textbf{Nathan Warner}
    
           \vfill
                
                
           \vspace{0.8cm}
         
           \includegraphics[width=0.4\textwidth]{~/niu/seal.png}
                
           Computer Science \\
           Northern Illinois University\\
           United States\\
           
                
       \end{center}
    \end{titlepage}
    \tableofcontents
    \pagebreak 
    \unsect{Elementary complexity theory}
       \pagebreak 
   \subsection{Elementary complexity theory}
   \begin{itemize}
       \item \textbf{Idea}: The same problem can frequently be solved with algorithms that differ in efficiency. The differences between the algorithms may be immaterial for processing a small number of data items, but these differences grow with the amount of data. To compare the efficiency of algorithms, a measure of the degree of difficulty of an algorithm called computational complexity was developed by Juris Hartmanis and Richard E. Stearns
           \bigbreak \noindent 
           Computational complexity indicates how much effort is needed to apply an algorithm or how costly it is. This cost can be measured in a variety of ways, and the particular context determines its meaning. This book concerns itself with the two efficiency criteria: time and space. The factor of time is usually more important than that of space, so efficiency considerations usually focus on the amount of time elapsed when processing data. However, the most inefficient algorithm run on a Cray computer can execute much faster than the most efficient algorithm run on a PC, so run time is always system-dependent. For example, to compare 100 algorithms, all of them would have to be run on the same machine. Furthermore, the results of run-time tests depend on the language in which a given algorithm is written, even if the tests are performed on the same machine. If programs are compiled, they execute much faster than when they are interpreted. A program written in C or Ada may be 20 times faster than the same program encoded in BASIC or LISP.
        \item \textbf{Units}: To evaluate an algorithm’s efficiency, real-time units such as microseconds and nanoseconds should not be used. Rather, logical units that express a relationship between the size $n$ of a file or an array and the amount of time $t$ required to process the data should be used
            \bigbreak \noindent 
            If there is a linear relationship between the size $n$ and time $t$, that is, $t_{1} = cn_{1}$, then an increase of data by a factor of 5 results in the increase of the execution time by the same factor. If $n_{2} = 5n_{1}$, then $t_{2}= 5t_{1} $
            \bigbreak \noindent 
            Similarly, if $t_1 = \log_2 n$, then doubling $n$ increases $t$ by only one unit of time. Therefore, if $t_2 = \log_2(2n)$, then $t_2 = t_1 + 1$.
        \item \textbf{Eliminating insignificant terms}: A function expressing the relationship between $n$ and $t$ is usually much more complex, and calculating such a function is important only in regard to large bodies of data; any terms that do not substantially change the function’s magnitude should  be eliminated from the function. The resulting function gives only an approximate measure of efficiency of the original function. However, this approximation is sufficiently close to the original, especially for a function that processes large quantities of data.
        \item \textbf{Asymptotic complexity}:  This measure of efficiency is called asymptotic complexity and is used when disregarding certain terms of a function to express the efficiency of an algorithm or when calculating a function is difficult or impossible and only approximations can be found
        \item \textbf{Big-O Notation}: The most commonly used notation for specifying asymptotic complexity—that is, for estimating the rate of function growth—is the big-O notation introduced in 1894 by Paul Bachmann.
            \bigbreak \noindent 
             Given two positive-valued functions $f$ and $g$, consider the following definition:
             \bigbreak \noindent 
             $f(n)$ is $O(g(n))$ if there exist positive numbers $c$ and $N$ such that $f(n) \leq c \cdot g(n)$ for all $n \geq N$.
             \begin{align*}
                 f(n) \text{ is } O(g(n)) \iff \exists\ c,N \in \mathbb{Z}^{+} \mid f(n) \le cg(n)\ \forall\ n \ge N
             .\end{align*}
             \bigbreak \noindent 
             Big-O notation says that for large enough $n$, the function $f(n)$ does not grow faster than a constant multiple of $g(n)$. So, $g(n)$ provides an upper bound on how fast $f(n)$ can grow as $n$ increases.
             \bigbreak \noindent 
             In other words, $f$ is big-O of $g$ if there is a positive number $c$ such that $f$ is not larger than $c \cdot g$ for sufficiently large $n$s; that is, for all $n$s larger than some number $N$. The relationship between $f$ and $g$ can be expressed by stating either that $g(n)$ is an upper bound on the value of $f(n)$ or that, in the long run, $f$ grows at most as fast as $g$.
             \bigbreak \noindent 
             The problem with this definition is that, first, it states only that there must exist
             certain $c$ and $N$, but it does not give any hint of how to calculate these constants. Second, it does not put any restrictions on these values and gives little guidance in situations when there are many candidates. In fact, there are usually infinitely many pairs
             of $c$'s and $N$'s that can be given for the same pair of functions $f$ and $g$.
             \bigbreak \noindent 
             For example, suppose 
             \begin{align*}
                 f(n) = 2n^{2} + 3n + 1 = O(n^{2})
             .\end{align*}
             Where $g(n) = n^{2}$. Candidate values for $c$ and $N$ are

             \[
                 \begin{array}{c|cccccccc}
                     c & \geq 6 & \geq 3^{\frac{3}{4}} & \geq 3^{\frac{1}{9}} & \geq 2^{\frac{13}{16}} & \geq 2^{\frac{16}{25}} & \cdots & \rightarrow & 2 \\
                     \hline
                     N & 1 & 2 & 3 & 4 & 5 & \cdots & \rightarrow & \infty \\
                 \end{array}
             \]
             \bigbreak \noindent 
             We obtain these values by solving the inequality:
             \begin{align*}
                 2n^{2}  + 3n  + 1 \leq cn^{2}
             .\end{align*}
             Or equivalently
             \begin{align*}
                 2 + \frac{3}{n}  + \frac{1}{n^{2}} \leq c
             .\end{align*}
             For different $n$'s
             \bigbreak \noindent 
             For large $n$, the terms $\frac{3}{n}$ and $\frac{1}{n^2}$ get smaller. Let's find $N$ such that for all $n \geq N$, the right-hand side stays bounded.
             \bigbreak \noindent 
             As $n$ gets larger, $\frac{3}{n}$ and $\frac{1}{n^2}$ approach zero. To simplify the analysis, choose $N=1$ initially and check how small $\frac{3}{n}$ and $\frac{1}{n^2}$ are:
             \[
                 2 + \frac{3}{1} + \frac{1}{1^2} = 2 + 3 + 1 = 6.
             \]
             From the inequality, at $N=1$, we have $6 \leq c$. Therefore, we can choose $c=6$. This ensures that for all $n \geq 1$, the inequality holds:
             \[
                 2 + \frac{3}{n} + \frac{1}{n^2} \leq 6.
             \]
             Thus, you can choose $c=6$ and $N=1$.
             \bigbreak \noindent 
             different pairs of constants $c$ and $N$ for the same function $g(= n^{2})$ can be determined.
         \item \textbf{Choosing the best $c$, $N$}: To choose the best $c$ and $N$, it should be determined for which N a certain term in $f$ becomes the largest and stays the largest.
             \bigbreak \noindent 
             In the example above, The only candidates for the largest term are \(2n^2\) and \(3n\);
             these terms can be compared using the inequality \(2n^2 > 3n\) that holds for \(n > 1.5\).
             Thus, \(N = 2\) and \(c \geq \frac{15}{4} = 3.75\).
         \item \textbf{Significance}: What is the practical significance of the pairs of constants just listed? All of them
             are related to the same function \(g(n) = n^2\) and to the same \(f(n)\). For a fixed \(g\), an infinite
             number of pairs of \(c\)'s and \(N\)'s can be identified. The point is that \(f\) and \(g\) grow at the same
             rate. The definition states, however, that \(g\) is almost always greater than or equal to \(f\) if it
             is multiplied by a constant \(c\). "Almost always" means for all \(n\)'s not less than a constant \(N\).
             The crux of the matter is that the value of \(c\) depends on which \(N\) is chosen, and vice
             versa.
         \item \textbf{Inherent imprecision: Choosing best $g(n)$}: The inherent imprecision of the big-O notation goes even further, because there 
             can be infinitely many functions \(g\) for a given function \(f\). For example, the \(f\) from 
             Equation 2.2 is big-O not only of \(n^2\), but also of \(n^3\), \(n^4\), \dots, \(n^k\), \dots for any \(k \geq 2\). 
             To avoid this embarrassment of riches, the smallest function \(g\) is chosen, \(n^2\) in this case.
        \item \textbf{Big-o as approximating terms}:  The approximation of function f can be refined using big-O notation only for
            the part of the equation suppressing irrelevant information. For example, in the equation below, the contribution of the third and last terms to the value of the function can
            be omitted
            \bigbreak \noindent 
            \begin{align*}
                f(n) &=n^{2} + 100n + \log(n) + 1000 \\
                \implies  f(n) &= n^{2} + 100n + O(\log(n))
            .\end{align*}
            \bigbreak \noindent 
            Similarly, 
            \begin{align*}
                f(n) &= 2n^{2} + 3n + 1 \\
                \implies f(n) &= 2n^{2} + O(n)
            .\end{align*}
            \bigbreak \noindent 
            This equation says that for large values of \(n\), the expression \(2n^2 + 3n + 1\) behaves like \(2n^2\) plus some terms that grow linearly or slower (captured by \(O(n)\)). The exact contributions of \(3n\) and \(1\) are not important for asymptotic analysis; what matters is that their growth is slower compared to \(2n^2\).
        




    \item \textbf{Algorithm analysis: Most common time complexities}: Ranked slowest to fastest growth
        \begin{itemize}
            \item \textbf{$O(1)$:} Constant time 
            \item \textbf{$O(\log(\log(n)))$}: Logarithmic time
            \item \textbf{$O(\log(n))$}: Logarthmic time
            \item \textbf{$O(n)$}: Linear time
            \item \textbf{$O(n\log(n))$}: Log-linear time
            \item \textbf{$O(n^{k}),\ k>1$}: Polynomial time
            \item \textbf{$O(a^{n}),\ a>1$}: Exponential time
            \item \textbf{$O(n!)$}: Factorial time
        \end{itemize}
    \item \textbf{Ranking complexities from slowest to fastest: Process}: Given 
        \begin{enumerate}[label=(\alph*)]
            \item $O(25) $
            \item $O(n^{\frac{1}{2}} + \log^{2}(n)) $
            \item $O(\log^{200}(n)) $
            \item $O(n^{3}\log^{4}(n)) $
            \item $O(n^{200} + 3^{n}) $
            \item $O(n\log^{40}(n)) $
            \item $O(4^{n}\log(n)) $
            \item $O(n^{3}\log(\log(n))) $
        \end{enumerate}
        How can we go about sorting these slowest to fastest. Well, to start, in the expressions with plus or minus, we can throw out the slower terms. Thus,
        \begin{enumerate}[label=(\alph*)]
            \item $O(n^{\frac{1}{2}}) $
            \item $O(25) $
            \item $O(\log^{200}(n)) $
            \item $O(n^{3}\log^{4}(n)) $
            \item $O(3^{n}) $
            \item $O(n\log^{40}(n)) $
            \item $O(4^{n}\log(n)) $
            \item $O(n^{3}\log(\log(n))) $
        \end{enumerate}
        In product terms, we disregard the slower term unless there are complexites with the same dominant term. For example, $O(n^{3}\log(\log(n)))$ grows slower than $O(n^{3}\log^{4}(n))$ because although they have the same dominant term $n^{3}$, $\log(\log(n))$ grows slower than $\log^{4}(n)$. Thus, the correct sequence is 
        \begin{enumerate}[label=(\alph*)]
            \item [(b)] $O(25)$
            \item [(c)] $O(\log^{200}(n)) $
            \item [(a)] $O(n^{\frac{1}{2}} + \log^{2}(n)) $
            \item [(f)] $O(n\log^{40}(n)) $
            \item [(h)] $O(n^{3}\log(\log(n))) $
            \item [(d)] $O(n^{3}\log^{4}(n)) $
            \item [(e)] $O(n^{200} + 3^{n})$
            \item [(g)] $O(4^{n}\log(n))$
        \end{enumerate}
    \item \textbf{Properties of Big-O notation}
        \begin{enumerate}
            \item \textbf{Transitivity}: $\text{ If } f(n) \text{ is } O(g(n)) \text{ and } g(n) \text{ is } O(h(n)), \text{ then } f(n) \text{ is } O(h(n)).$ \\
            \bigbreak \noindent 
            \textbf{Proof:} According to the definition, $f(n)$ is $O(g(n))$ if there exist positive numbers $c_1$ and $N_1$ such that $f(n) \leq c_1 g(n)$ for all $n \geq N_1$, and $g(n)$ is $O(h(n))$ if there exist positive numbers $c_2$ and $N_2$ such that $g(n) \leq c_2 h(n)$ for all $n \geq N_2$. Hence, $c_1 g(n) \leq c_1 c_2 h(n)$ for $n \geq N$ where $N$ is the larger of $N_1$ and $N_2$. If we take $c = c_1 c_2$, then $f(n) \leq ch(n)$ for $n \geq N$, which means that $f$ is $O(h(n))$.
        \item \textbf{Addition}: If $f(n)$ is $O(h(n))$ and $g(n)$ is $O(h(n))$, then $f(n) + g(n)$ is $O(h(n))$.
            \bigbreak \noindent 
            \textbf{Proof}: If $f(n) \leq c_{1}h(n)$, and $g(n) \leq c_{2}h(n)$, then $f(n) + g(n) \leq c_{1}h(n) + c_{2}h(n) \leq (c_1 + c_2) h(n)$. $\ell$et $c = c_1 + c_2$, then $f(n) + g(n) \leq ch(n)$ and $f(n) + g(n)$ is $O(h(n)) $
        \item \textbf{Polynomial bounds}: The function $an^{k}$ is $O(n^{k})$
            \bigbreak \noindent 
            \textbf{Proof}: $an^{k} \leq cn^{k}$ for $c \geq a$. Since we can always find some constant $c \geq a$, $an^{k}$ is $O(n^{k})$
            \bigbreak \noindent 
            \textbf{Observation}: For $an^{k} \leq cn^{k}$ to hold, $c \geq a$ is necessary
        \item \textbf{Domination of higher-degree polynomials}: $n^{k}$ is $O(n^{k+j}) \ \forall \ j > 0 $
            \bigbreak \noindent 
            This statement holds if $c = N = 1$
            \bigbreak \noindent 
            It follows from all these facts that every polynomial is big-O of $n$ raised to the largest power, or
            \begin{align*}
                f(n) = a_k n^k + a_{k-1} n^{k-1} + \cdots + a_1 n + a_0 \text{ is } O(n^k)
            .\end{align*}
        \item \textbf{Logs}: The function $\log_a n$ is $O(\log_b n)$ for any positive numbers $a$ and $b \neq 1$.
            \bigbreak \noindent 
            This correspondence holds between logarithmic functions. The fact above states that regardless of their bases, logarithmic functions are big-O of each other; that is, all these functions have the same rate of growth.
            \bigbreak \noindent 
            \textbf{Proof}: Let $\log_{a}(n) = x$, and $\log_{b}(n) = y $, then $a^{x} = n,\ b^{y} =n$. Take the natural log of both sides
            \begin{align*}
                \ln{(a^{x})} = \ln{(n)} &\quad \ln{(b^{y})} = \ln{(n)} \\
                \implies x\ln{(a)} = \ln{(n)} &\quad y\ln{(b)} = \ln{(n)} \\
                \implies x\ln{(a)} &= y\ln{(b)}
            .\end{align*}
            Since $x = \log_{a}{(n)}$, and $y=\log_{b}{(n)}$, then we have
            \begin{align*}
                \ln{(a)}\log_{a}{(n)} &= \ln{(b)}\log_{b}{n} \\
                \log_{a}{(n)} &= \frac{\ln{(b)}}{\ln{(a)}} \log_{b}{(n)}
            .\end{align*}
            $\ell$et $c=\frac{\ln{(b)}}{\ln{(a)}}$, then $\log_{a}{(n)} = c\log_{b}{(n)}$, which proves that $\log_{a}(n)$ and $\log_{b}{(n)}$ are multiples of each other. Thus, $\log_{a}{(n)}$ is $O(\log_{b}{n})$
            \bigbreak \noindent 
            \textbf{Note:} Because the base of the logarithm is irrelevant in the context of big-O notation, we can always use just one base.
            \bigbreak \noindent 
            \begin{align*}
               \therefore \log_{a}{(n)} \text{ is } O(\text{lg}n)
            .\end{align*}
            For any positive $a\ne 1$, where $\text{lg}(n)$ is $\log_{2}{(n)}$
    \end{enumerate}
\item \textbf{Big-$\Omega$}. The function $f(n)$ is $\Omega(g(n))$ iff $\exists\ c,N \in \mathbb{R}^{+}\ \mid \ f(n) \geq cg(n) \ \forall \ n \geq N$.
    \bigbreak \noindent 
    In other words, $cg(n)$ is a lower bound on the size of $f(n)$, or, in the long run, $f$ grows at least at the rate of $g$
    \bigbreak \noindent 
    There is an interconnection between these two notations expressed by the equivalence
    \begin{align*}
        f(n) \text{ is } \Omega(g(n)) \text{ iff } g(n) is O(f(n))
    .\end{align*}
    \bigbreak \noindent 
    There are an infinite number of possible lower bounds for the function $f$; that is, there is an infinite set of $g$s such that $f(n)$ is $\Omega(g(n))$ as well as an unbounded number of possible upper bounds of $f$. This may be somewhat disquieting, so we restrict our attention to the smallest upper bounds and the largest lower bounds. Note that there is a common ground for big-O and $\Omega$ notations indicated by the equalities in the definitions of these notations: Big-O is defined in terms of ``$\leq$'' and $\Omega$ in terms of ``$\geq$''; ``$=$'' is included in both inequalities. This suggests a way of restricting the sets of possible lower and upper bounds. 
\item \textbf{Big-$\Theta$}: $f(n)$ is $\Theta(g(n))$ iff $\exists\ c_{1}, c_{2}, N \in \mathbb{R}^{+} \ \mid \ c_{1}g(n) \leq f(n) \leq c_{2}g(n) \ \forall \ n \geq N$
    \bigbreak \noindent 
    We see that $f(n)$ is $\Theta(g(n))$ if $f(n)$ is $O(g(n))$ and $f(n)$ is $\Omega(g(n))$.
    \bigbreak \noindent 
    When applying any of these notations, do not forget that they are approximations that hide some detail that in many cases may be considered important.
\item \textbf{Double $O$ notation}: $f$ is $OO(g(n))$ if it is $O(g(n))$ and the constant $c$ is too large to have practical significance. Thus, $10^{8}n$ is $OO(n)$. However, the definition of “too large” depends on the particular application.
\item \textbf{Using asymptotic complexity to estimate time}:
    If an algorithm is $O(n^2)$, the time to process $n$ elements is proportional to $n^2$. 
    \bigbreak \noindent 
    Let $T(n)$ represent the time, so $T(n) = k \cdot n^2$ where $k$ is a constant.
    \bigbreak \noindent 
    To find the time for 1 million elements ($n = 10^6$):
    \[
        T(10^6) = k \cdot (10^6)^2 = k \cdot 10^{12}
    \]
    For example, if processing $1000$ elements takes $1$ second, then:
    \bigbreak \noindent 
    \[
        T(1000) = k \cdot 1000^2 = k \cdot 10^6 \implies k = \frac{1}{10^6}
    \]
    Now, for $n = 10^6$:
    \[
        T(10^6) = \frac{1}{10^6} \cdot (10^6)^2 = 10^6 \text{ seconds} = 1,000,000 \text{ seconds} \approx 11.57 \text{ days}.
    \]
\item \textbf{Finding asymptotic complexites}: Asymptotic bounds are used to estimate the efficiency of algorithms by assessing the
amount of time and memory needed to accomplish the task for which the algorithms
were designed. This section illustrates how this complexity can be determined.
In most cases, we are interested in time complexity, which usually measures the
number of assignments and comparisons performed during the execution of a program. For now let's focus on assignments
\bigbreak \noindent 
Consider a simple loop to calculate the sum of numbers in an array
\bigbreak \noindent 
\begin{cppcode}
    for (i = sum = 0; i < n; i++)
        sum += a[i];
\end{cppcode}
\bigbreak \noindent 
First, two variables are initialized, then the for loop iterates $n$ times, and during each iteration, it executes two assignments, one of which updates sum and the other of which updates $i$. Thus, there are $2 + 2n$ assignments for the complete run of this for loop; its asymptotic complexity is $O(n)$.
\bigbreak \noindent 
Complexity usually grows if nested loops are used, as in the following code, which outputs the sums of all the subarrays that begin with position 0:
\bigbreak \noindent 
\begin{cppcode}
    for (i = 0; i < n; i++) {
        for (j = 1, sum = a[0]; j <= i; j++)
            sum += a[j];
        cout<<"sum for subarray 0 through "<< i <<" is "<<sum<<endl;
    }
\end{cppcode}
\bigbreak \noindent 
Before the loops start, $i$ is initialized. The outer loop is performed $n$ times, executing in each iteration an inner \texttt{for} loop, print statement, and assignment statements for $i$, $j$, and \texttt{sum}. The inner loop is executed $i$ times for each $i \in \{1, \ldots, n-1\}$ with two assignments in each iteration: one for \texttt{sum} and one for $j$. Therefore, there are
\[
1 + 3n + \sum_{i=1}^{n-1} 2i = 1 + 3n + 2(1 + 2 + \cdots + n - 1) = 1 + 3n + n(n - 1)
\]
$= O(n) + O(n^2) = O(n^2)$ assignments executed before the program is completed.
\item \textbf{Amortized complexity}: amortized analysis can be used
to find the average complexity of a worst case sequence of operations















   \end{itemize}
    





    \pagebreak 
    \unsect{Linked lists}
    \bigbreak \noindent 
    \subsection{Singly-linked lists}
    \bigbreak \noindent 
    If a node contains a data member that is a pointer to another node, then many nodes
    can be strung together using only one variable to access the entire sequence of nodes.
    Such a sequence of nodes is the most frequently used implementation of a linked list,
    which is a data structure composed of nodes, each node holding some information
    and a pointer to another node in the list. If a node has a link only to its successor in
    this sequence, the list is called a singly linked list
    \bigbreak \noindent 
    Each node resides on the heap
    \bigbreak \noindent 
    Linked lists can easily grow and shrink in size without reallocating memory or moving elements. Adding or removing nodes (especially at the beginning or middle) is more efficient compared to arrays, as no shifting of elements is required. Memory is allocated as needed, avoiding wasted space typical in arrays with fixed sizes.
    \bigbreak \noindent 
    However, each node requires extra memory for the pointer to the next node. Accessing elements requires traversal from the head, making lookups slower (O(n)) compared to arrays, which offer O(1) access via indexing.  Nodes are scattered in memory, leading to poor cache performance compared to arrays, which have contiguous memory locations.



    \bigbreak \noindent 
    \subsubsection{Structure of the node}
    \bigbreak \noindent 
    The node structure is typically implemented in the following way
    \bigbreak \noindent 
    \begin{cppcode}
        struct node {
            node* next = nullptr;
            T data = 0; 

            node() = default;
            node(data) : data(data) {}
            node(next, data) : next(next), data(data) {}
        }
    \end{cppcode}
    \bigbreak \noindent 
    A node includes two data members: info and next. The info member is used
    to store information, and this member is important to the user. The next member is
    used to link nodes to form a linked list. It is an auxiliary data member used to maintain the linked list. It is indispensable for implementation of the linked list, but less
    important (if at all) from the user’s perspective. Note that node is defined
    in terms of itself because one data member, next, is a pointer to a node of the same
    type that is just being defined. Objects that include such a data member are called
    self-referential objects.

    \bigbreak \noindent 
    \subsubsection{The list class/struct}
    \bigbreak \noindent 
    We also implement the list structure as a class or struct.
    \bigbreak \noindent 
    \begin{cppcode}
        class single_list {
            node* head = nullptr;
        public:
            ...
        };
    \end{cppcode}


    \bigbreak \noindent 
    \subsubsection{Interface of a singly linked list stack}
    \bigbreak \noindent 
    The interface typically includes the following operations:
    \begin{enumerate}
        \item \textbf{Insert:} Add a node at the beginning, end, or a specific position in the list.
        \item \textbf{Delete:} Remove a node from the beginning, end, or a specific position.
        \item \textbf{Search:} Find a node with a given value.
        \item \textbf{Traverse:} Iterate through the list to access or print each node's data.
        \item \textbf{IsEmpty:} Check if the list is empty.
        \item \textbf{Size:} Return the number of nodes in the list. The first node is called the head, and the last node points to nullptr (indicating the end of the list).
    \end{enumerate}

    \pagebreak 
    \subsubsection{Traversing}
    \bigbreak \noindent 
    Traversing a list is simple.
    \bigbreak \noindent 
    \begin{cppcode}
    node* curr = head;

    while (curr) {
        curr = curr->next;
        ...
    }
    \end{cppcode}

    \pagebreak 
    \subsubsection{Printing}
    \bigbreak \noindent 
    Now that we can traverse, we can print each node
    \bigbreak \noindent 
    \begin{cppcode}
    node* curr = head;
    while (curr) {
        cout << curr->data;
        curr=curr->next;
    }
    \end{cppcode}

    \pagebreak 
    \subsubsection{Printing in reverse}
    \bigbreak \noindent 
    Printing in reverse requires creating a stack.
    \bigbreak \noindent 
    \begin{cppcode}
    if (!head) return; // noop, dont even bother creating a vector.

    vector<node*> stack;
    node* curr = head;

    while (curr) {
        stack.push_back(curr);
        curr=curr->next;
    }

    for (int i=(int)stack.size()-1; i>=0; --i) {
        cout << stack[i]->data << " ";
    }
    cout << endl;
    \end{cppcode}

    \pagebreak 
    \subsubsection{Getting the length}
    \bigbreak \noindent 
    While we traverse, just increment a counter.
    \bigbreak \noindent 
    \begin{cppcode}
        size_t len() {
            size_t len = 0;
            for (node* curr = head; curr; curr=curr->next, ++len);
            return len;
        }
    \end{cppcode}

    \pagebreak 
    \subsubsection{Clearing}
    \bigbreak \noindent 
    \begin{cppcode}
        void clear() {
            node* curr=head, *prev=nullptr;

            while (curr) {
                prev=curr;
                curr=curr->next;
                delete prev;
            }
            head = nullptr;
        }
    \end{cppcode}

    \pagebreak 
    \subsubsection{Reversing}
    \bigbreak \noindent 
    Reversing is pretty straight forward
    \bigbreak \noindent 
    \begin{cppcode}
        void reverse() {
            node* prev=nullptr, *curr=head, *next=nullptr;

            while(curr) {
                next=curr->next;
                curr->next = prev;
                prev = curr;
                curr=next;
            }

            head = prev;
        }
    \end{cppcode}
    \bigbreak \noindent 
    In each iteration, next temporarily holds the next node so you don’t lose track of it when reversing the link.
    \bigbreak \noindent 
    The curr->next pointer is set to prev, effectively reversing the link.
    \bigbreak \noindent 
    Prev is then updated to curr, and curr is updated to next to continue the process.


    \pagebreak 
    \subsubsection{Pushing}
    \bigbreak \noindent 
    \begin{cppcode}
        void push(int element) {
            if (!head) {
                head = new node(element);
                return;
            }

            node* curr = head;
            while (curr->next) {
                curr=curr->next;
            }
            curr->next = new node(element);
        }
    \end{cppcode}

    \pagebreak 
    \subsubsection{Inserting}
    \bigbreak \noindent 
    \begin{cppcode}
        void insert(int pos, int element) {
            if (!head || pos == 0) {
                node* new_node = new node(element);
                new_node->next = head;
                head = new_node;
                return;
            }
            node* curr = head;

            int count=0;
            while (count != pos-1 && curr->next) {
                curr=curr->next;
                ++count;
            }
            node* new_node = new node(element);

            new_node->next = curr->next;
            curr->next = new_node;
        }
    \end{cppcode}
    \bigbreak \noindent 
    \begin{enumerate}
        \item \textbf{Check if the list is empty or inserting at the head (position 0):}
            \begin{itemize}
                \item If head is nullptr (meaning the list is empty) or pos == 0 (you want to insert at the beginning), a new node is created with the given element.
                \item The new node's next pointer is set to the current head (which could be nullptr if the list is empty), and then head is updated to point to this new node.
                \item This handles the case where the new node becomes the first node in the list.
            \end{itemize}
        \item \textbf{Traverse to the correct position:}
            \begin{itemize}
                \item If you are inserting somewhere other than the head, the function uses a loop to find the node just before the desired position (pos - 1).
                \item It starts at the head and moves along the list until it reaches the node right before where the new node will be inserted.
            \end{itemize}
        \item \textbf{Insert the new node:}
            \begin{itemize}
                \item Once the loop finds the right place (curr points to the node before the insertion position), a new node is created.
                \item The new node’s next pointer is set to curr->next (the node currently in the target position).
                \item Then, curr->next is updated to point to the new node, effectively inserting the new node into the list.
            \end{itemize}

    \end{enumerate}


    \pagebreak 
    \subsubsection{Popping}
    \bigbreak \noindent 
    \begin{cppcode}
        void pop() {
            if (!head) return;
            if (!head->next) {
                delete head;
                head=nullptr;
                return;
            }

            node* prev=nullptr, *curr = head;
            while (curr->next) {
                prev=curr;
                curr=curr->next;
            }
            delete curr;
            prev->next=nullptr;
        }
    \end{cppcode}
    \bigbreak \noindent 
    \begin{enumerate}
        \item \textbf{Empty List Check:} If the list is empty (head == nullptr), it does nothing.
        \item \textbf{Single Node Case:} If the list has only one node, it deletes the head and sets head to nullptr.
        \item \textbf{Multiple Nodes:} It traverses to the last node using two pointers (prev and curr), deletes the last node (curr), and sets the second-to-last node's next pointer (prev->next) to nullptr to mark the new end of the list.
    \end{enumerate}

    \pagebreak 
    \subsubsection{Erasing}
    \bigbreak \noindent 
    \begin{cppcode}
        void erase(int element) {
            if (!head) return;

            while (head->data == element) {
                if (head->next && head->data == element) {
                    node* tmp = head;
                    head = head->next;
                    delete tmp;
                }
            }

            node* prev=nullptr, *curr=head;

            while (curr) {
                if (curr->data == element) {
                    node* tmp = curr;
                    prev->next = curr->next;
                    curr=curr->next;
                    delete tmp;
                } else {
                    prev=curr;
                    curr=curr->next;
                }
            }
        }
    \end{cppcode}
    \bigbreak \noindent 
    This erase function removes all nodes with a specific value (element) from the list:
    \begin{itemize}
        \item \textbf{Empty List Check:} If the list is empty (head == nullptr), it returns immediately.
        \item \textbf{Head Node Deletion:} If the head contains the target value, it deletes the head and updates it to the next node. We keep doing this until the head node no longer contains the data we want to remove
        \item \textbf{Traverse and Delete:} It iterates through the list, and for each node with the target value, it removes the node by adjusting the next pointer of the previous node and deleting the current node.
    \end{itemize}


    \pagebreak 
    \subsubsection{Searching}
    \bigbreak \noindent 
    \begin{cppcode}
        node* search(int element) {
            node* curr = head;
            while (curr) {
                if (curr->data == element) {
                    return curr;
                }
            }
            return nullptr;
        }
    \end{cppcode}




    \pagebreak 
    \unsect{Recursion}
    \bigbreak \noindent 
    \subsection{Recursion vs iteration}
    \bigbreak \noindent 
    In theory, any problem that can be solved recursively can be solved iteratively. This also means that any problem that can be solved iteratively can also be solved recursively.
    \bigbreak \noindent 
    The question is, for any problem that can be solved, which method can be used such that the problem is easier to solve.

    \pagebreak \bigbreak \noindent 
    \subsection{Elementary recursion}
    \bigbreak \noindent 
    A recursive definition consists of two parts. In the first part, called the anchor or
    the ground case, the basic elements that are the building blocks of all other elements
    of the set are listed. In the second part, rules are given that allow for the construction
    of new objects out of basic elements or objects that have already been constructed.
    These rules are applied again and again to generate new objects. For example, to construct the set of natural numbers, one basic element, 0, is singled out, and the operation of incrementing by 1 is given as:
    \begin{enumerate}
        \item $0 \in \mathbb{N}$
        \item If $n\in \mathbb{N},\ then (n+1) \in \mathbb{N}$
        \item There are no other objects in the set $\mathbb{N}$
    \end{enumerate}
    It is more convenient to use the following definition, which encompasses the whole range of Arabic numeric heritage:
    \begin{enumerate}
        \item $0, 1,2,3,4,5,6,7,8,9 \in \mathbb{N} $
        \item If $n\in \mathbb{N}$, then $n0, n1,n2,n3,n4,n5,n6,n7,n8,n9 \in \mathbb{N}$
        \item These are the only natural numbers
    \end{enumerate}
    \bigbreak \noindent 
    Recursive definitions serve two purposes: generating new elements, as already
    indicated, and testing whether an element belongs to a set. In the case of testing, the
    problem is solved by reducing it to a simpler problem, and if the simpler problem is
    still too complex it is reduced to an even simpler problem, and so on, until it is reduced to a problem indicated in the anchor

    \bigbreak \noindent 
    \subsection{Base cases}
    \bigbreak \noindent 
    In recursion, a base case is a condition that stops further recursive calls and provides a direct answer without further recursion
    \bigbreak \noindent 
    If there were no base case, there would be nothing to stop the recursion. Thus, it would go on until the program crashes. For this reason, all recursive functions must have at least one base case.
    \bigbreak \noindent 
    If a base case in a recursive function returns a value, then every recursive call leading up to that base case should also return a value. This is necessary to ensure that the result of the recursion is propagated back up the call stack.
    \bigbreak \noindent 
    In a recursive function, the base case stops the recursion, and if the base case returns something (e.g., a node pointer, integer, etc.), the recursive calls that occur before reaching the base case need to return that result so it can propagate back to the original caller.

    \bigbreak \noindent 
    \subsubsection{Factorials}
    \bigbreak \noindent 
    \begin{align*}
        n! = 
        \begin{cases}
            1 & \text{if } n = 0 \\
            n(n-1)! & \text{if } n \ne 0
        \end{cases}
    .\end{align*}
    \bigbreak \noindent 
    \begin{cppcode}
        int factorial(int n) {
            if (n == 0) return 1;
            return n * factorial(n-1);

            // Expands to 
            // n * n-1 * n-2 * ... * 1
        }
    \end{cppcode}

    \bigbreak \noindent 
    \subsubsection{Powers}
    \bigbreak \noindent 
    Consider the recursive definition for a power of $x$
    \bigbreak \noindent 
    \begin{align*}
        x^{n} = 
        \begin{cases}
            1 & \text{if } n=0 \\\
            x\cdot x^{n-1} &\text{if } n>0
        \end{cases}
    .\end{align*}
    \bigbreak \noindent 
    \begin{cppcode}
        constexpr int power(int x, int n) {
            if (n == 0) return 1;
            return x * power(x,n-1);
        }
    \end{cppcode}
    \bigbreak \noindent 
    \bigbreak \noindent 
    The function power() can be implemented differently, without using any recursion, as in the following loop:
    \bigbreak \noindent 
    \begin{cppcode}
        int power2(int x, int n) {
            int res = 1;

            for (res = x; n > 1; --n) {
                res*=x;
            }
            return res;
        }
    \end{cppcode}
    \bigbreak \noindent 
    Do we gain anything by using recursion instead of a loop? The recursive version
seems to be more intuitive because it is similar to the original definition of the power
function. The definition is simply expressed in C++ without losing the original structure of the definition. The recursive version increases program readability, improves
self-documentation, and simplifies coding. In our example, the code of the nonrecursive
version is not substantially larger than in the recursive version, but for most recursive
implementations, the code is shorter than it is in the nonrecursive implementations

    \pagebreak 
    \subsection{Tail recursion}
    \bigbreak \noindent 
    Tail recursion is a type of recursion where the recursive call is the last thing the function does before returning a result. This means there are no more computations or operations to perform after the recursive call.
    \bigbreak \noindent 
    Because of this, tail recursion can be optimized by some compilers or interpreters to avoid adding new frames to the call stack, making it more memory-efficient than regular recursion.
    \bigbreak \noindent 
    In simple terms, if a recursive function calls itself, and after that call there’s nothing left to do, it's tail recursion. This allows the function to reuse the same memory space, preventing stack overflow in cases with deep recursion.
    \bigbreak \noindent 
    the recursive call is not only the last statement but there are no earlier recursive calls, direct or indirect. For example, the function tail() defined as
    \bigbreak \noindent 
    \begin{cppcode}
        void tail(int i) {
            if (i > 0) {
                cout << i << '';
                tail(i-1);
            }
        }
    \end{cppcode}
    \bigbreak \noindent 
    Is an example of a function with tail recursion, whereas the function nonTail() defined as
    \bigbreak \noindent 
    \begin{cppcode}
        void nonTail(int i) {
            if (i > 0) {
                nonTail(i-1);
                cout << i << '';
                nonTail(i-1);
            }
        }
    \end{cppcode}
    \bigbreak \noindent 
    Is not. Tail recursion is simply a glorified loop and can be easily replaced by one. In this example, it is replaced by substituting a loop for the if statement and decrementing the variable i in accordance with the level of recursive call. In this way, tail() can be expressed by an iterative function:
    \bigbreak \noindent 
    \begin{cppcode}
        void iterativeEquivalentOfTail(int i) {
            for ( ; i > 0; i--)
            cout << i << '';
        }
    \end{cppcode}
    \bigbreak \noindent 
    Is there any advantage in using tail recursion over iteration? For languages such as C++, there may be no compelling advantage, but in a language such as Prolog, which has no explicit loop construct (loops are simulated by recursion), tail recursion acquires a much greater weight. In languages endowed with a loop or its equivalents, such as an if statement combined with a goto statement, tail recursion should not be used.
    \bigbreak \noindent 
    Another problem that can be implemented in recursion is printing an input line in reverse order. Here is a simple recursive implementation:
    \bigbreak \noindent 
    \begin{cppcode}
        void reverse() {
            char ch;
            cin.get(ch);
            if (ch != '\n') {
                reverse();
                cout.put(ch); 
            }
        }
    \end{cppcode}
    \bigbreak \noindent 
    Compare the recursive implementation with a nonrecursive version of the same function:
    \bigbreak \noindent 
    \begin{cppcode}
        void simpleIterativeReverse() {
            char stack[80];
            int top = 0;
            cin.getline(stack,80);
            for (top = strlen(stack) - 1; top >= 0; cout.put(stack[top--]));
        }
    \end{cppcode}
    \bigbreak \noindent 
    functions like strlen() and
    getline() from the standard C++ library can be used. If we are not supplied with
    such functions, then our iterative function has to be implemented differently:
    \bigbreak \noindent 
    \begin{cppcode}
        void iterativeReverse() {
            char stack[80];

            register int top = 0;
            cin.get(stack[top]);

            while(stack[top]!='\n') {
                cin.get(stack[++top]);
            }
            for (top -= 2; top >= 0; cout.put(stack[top--]));
        }
    \end{cppcode}

    \pagebreak 
    \subsection{Indirect Recursion}
    \bigbreak \noindent 
    The preceding sections discussed only direct recursion, where a function $f()$ called itself. However, $f()$ can call itself indirectly via a chain of other calls. For example, $f()$ can call $g()$, and $g()$ can call $f()$. This is the simplest case of indirect recursion. The chain of intermediate calls can be of an arbitrary length, as in:
    \begin{align*}
        f() \to f_{1}() \to f_{2}() \to ... \to f_{n}() \to f()
    .\end{align*}
    \bigbreak \noindent 
    There is also the situation when $f()$ can call itself indirectly through different chains. Thus, in addition to the chain just given, another chain might also be possible. For instance
    \begin{align*}
        f() \to g_{1}() \to g_{2}() \to ... \to g_{m}() \to f()
    .\end{align*}
    \bigbreak \noindent 
    This situation can be exemplified by three functions used for decoding information. receive() stores the incoming information in a buffer, decode() converts it into legible form, and store() stores it in a file. receive() fills the buffer and calls decode(), which in turn, after finishing its job, submits the buffer with decoded information to store(). After store() accomplishes its tasks, it calls receive() to intercept more encoded information using the same buffer. Therefore, we have the chain of calls
    \begin{align*}
        \text{recieve}() \to \text{decode}() \to \text{store}() \to \text{recieve}() \to \text{decode}() \to ...
    .\end{align*}
    \bigbreak \noindent 
    \pagebreak 
    \subsection{Nested Recursion}
    \bigbreak \noindent 
    A more complicated case of recursion is found in definitions in which a function is
    not only defined in terms of itself, but also is used as one of the parameters. The following definition is an example of such a nesting
    \begin{align*}
        h(n) = 
        \begin{cases}
            0 & \text{if } n = 0 \\
            n & \text{if } n > 4 \\
            h(2 + h(n)) & \text{if } n \leq 4
        \end{cases}
    .\end{align*}
    \bigbreak \noindent 
    \subsection{Excessive Recursion}
    \bigbreak \noindent 
    Logical simplicity and readability are used as an argument supporting the use of recursion. The price for using recursion is slowing down execution time and storing on
    the run-time stack more things than required in a nonrecursive approach. If recursion is too deep (for example, computing $5.6^{100,000}$), then we can run out of space on
the stack and our program crashes. But usually, the number of recursive calls is much
smaller than 100,000, so the danger of overflowing the stack may not be imminent
\bigbreak \noindent 
However, if some recursive function repeats the computations for some parameters, the run time can be prohibitively long even for very simple cases
\bigbreak \noindent 
Consider Fibonacci numbers. A sequence of Fibonacci numbers is defined as follows:
\begin{align*}
    \text{Fib}(n) =
    \begin{cases}
        n & \text{if } n < 2  \\
        \text{Fib}(n-2) + \text{Fib}(n-1) & \text{otherwise}
    \end{cases}
.\end{align*}
\bigbreak \noindent 
The definition states that if the first two numbers are 0 and 1, then any number in the
sequence is the sum of its two predecessors. But these predecessors are in turn sums
of their predecessors, and so on, to the beginning of the sequence. 
\bigbreak \noindent 
How can this definition be implemented in C++? It takes almost term-by-term
translation to have a recursive version, which is
\bigbreak \noindent 
\begin{cppcode}
constexpr unsigned long fib(int n) {
    if (n < 2) return n;
    return fib(n-2) + fib(n-1);
}
\end{cppcode}
\bigbreak \noindent 
The function is simple and easy to understand but extremely inefficient. To see
it, compute Fib(6), the seventh number of the sequence, which is 8. Based on the
definition, the computation runs as follows:
\begin{align*}
    Fib(6)	&= Fib(4) + Fib(5) \\
            &= Fib(2) 	+ 	Fib(3) 	+ Fib(5) \\
            &= Fib(0)+Fib(1) 	+ 	Fib(3) 	+ Fib(5) \\
            &= 0 + 1 	+ 	Fib(3) 	+ Fib(5) \\
            &= 1 	+ Fib(1)+ Fib(2) 	+ Fib(5) \\
            &= 1 	+ Fib(1)+Fib(0)+Fib(1)	+ Fib(5)   
.\end{align*}
Etc... The source of
this inefficiency is the repetition of the same calculations because the system forgets
what has already been calculated. For example, Fib() is called eight times with parameter n = 1 to decide that 1 can be returned. For each number of the sequence, the
function computes all its predecessors without taking into account that it suffices to do
this only once.
\bigbreak \noindent 
It takes almost a quarter of a million calls to find the twenty-sixth Fibonacci
number, and nearly 3 million calls to determine the thirty-first! This is too heavy a
price for the simplicity of the recursive algorithm. As the number of calls and the run
time grow exponentially with n, the algorithm has to be abandoned except for very
small numbers
\bigbreak \noindent 
An iterative algorithm may be produced rather easily as follows:
\bigbreak \noindent 
\begin{cppcode}
    unsigned long iterativeFib(unsigned long n) {
        if (n < 2)
        return n;
        else {
            register long i = 2, tmp, current = 1, last = 0;
            for ( ; i <= n; ++i) {
                tmp = current;
                current += last;
                last = tmp;
            }
            return current;
        }
    }
\end{cppcode}
\bigbreak \noindent 
However, there is another, numerical method for computing Fib(n), using a formula discovered by Abraham de Moivre:
\begin{align*}
    \text{Fib}(n) =  \frac{\phi^{n} - \hat{\phi}^{n}}{\sqrt{5}}
.\end{align*}
Where $\phi = \frac{1}{2}(1+\sqrt{5})$, and $\hat{\phi} = 1-\phi = \frac{1}{2}(1-\sqrt{5}) $. $\hat{\phi}$ becomes very small when $n$ grows, thus it can be omitted. 
\begin{align*}
    \text{Fib}(n) =\frac{\phi^{n}}{\sqrt{5}}
.\end{align*}
Approximated to the nearest integer
\bigbreak \noindent 
\begin{cppcode}
    unsigned long deMoivreFib(unsigned long n) {
        return ceil(exp(n*log(1.6180339897) - log(2.2360679775)) - .5);
    }
\end{cppcode}
\pagebreak 
\subsection{Backtracking}
\bigbreak \noindent 
In solving some problems, a situation arises where there are different ways leading
from a given position, none of them known to lead to a solution. After trying one path
unsuccessfully, we return to this crossroads and try to find a solution using another
path. However, we must ensure that such a return is possible and that all paths can
be tried. This technique is called backtracking, and it allows us to systematically try
all available avenues from a certain point after some of them lead to nowhere. Using
backtracking, we can always return to a position that offers other possibilities for successfully solving the problem. This technique is used in artificial intelligence, and one
of the problems in which backtracking is very useful is the eight queens problem.
\bigbreak \noindent 
The eight queens problem attempts to place eight queens on a chessboard in
such a way that no queen is attacking any other To solve this problem, we try to put the first
queen on the board, then the second so that it cannot take the first, then the third so
that it is not in conflict with the two already placed, and so on, until all of the queens
are placed. What happens if, for instance, the sixth queen cannot be placed in a nonconflicting position? We choose another position for the fifth queen and try again
with the sixth. If this does not work, the fifth queen is moved again. If all the possible
positions for the fifth queen have been tried, the fourth queen is moved and then the process restarts. This process requires a great deal of effort, most of which is spent
backtracking to the first crossroads offering some untried avenues. In terms of code,
however, the process is rather simple due to the power of recursion, which is a natural implementation of backtracking
\bigbreak \noindent 
\begin{cppcode}
    putQueen(row)
        for every position col on the same row
            if position col is available
                place the next queen in position col;
                if (row < 8)
                    putQueen(row+1);
                else success;
                remove the queen from position col;
\end{cppcode}
\bigbreak \noindent 
This algorithm finds all possible solutions without regard to the fact that some of
them are symmetrical.

\pagebreak 
\subsection{Recursion in singly linked lists}
\bigbreak \noindent 
\subsubsection{Traversing}
\bigbreak \noindent 
To traverse a linked list using recursion, you need to define a recursive function that processes the current node and then calls itself with the next node until the list is fully traversed (i.e., until the current node is nullptr).
\bigbreak \noindent 
\begin{cppcode}
    void TraverseList(node* head) {
        if (!head) {
            return;
        }
        TraverseList(head->next);

        // ...
    }
\end{cppcode}
\bigbreak \noindent 
\subsubsection{Printing}
\bigbreak \noindent \bigbreak \noindent 
We can use this, for example, to print each nodes data member
\bigbreak \noindent 
\begin{cppcode}
    void PrintList(node* head) {
        if (!head) return;

        cout << head->data << " ";
        PrintList(head->next);
    }
\end{cppcode}

\bigbreak \noindent 
\subsubsection{Printing in reverse}
\bigbreak \noindent \bigbreak \noindent 
We a slight alter in the print example, we can reverse print the list. 
\bigbreak \noindent 
\begin{cppcode}
    void PrintListReverse(node* head) {
        if (!head) return;

        PrintListReverse(head->next);
        cout << head->data << " ";
    }
\end{cppcode}

\bigbreak \noindent 
\subsubsection{Getting the length}
\bigbreak \noindent 


\bigbreak \noindent 
\subsubsection{Clearing}
\bigbreak \noindent \bigbreak \noindent 
We can also use this to clear the list
\bigbreak \noindent 
\begin{cppcode}
    void clear() {
        std::function<void(node*)> r_clear = [&] (node* p) = {
            if (!head) return;

            r_clear(head->next);
            delete head;
        } 
        r_clear(head);
        head=nullptr;
        size=0;
    }
\end{cppcode}


\pagebreak 
\subsubsection{Reversing}
\bigbreak \noindent 
Let's first take a look at the reverse code
\bigbreak \noindent 
\begin{cppcode}
    void reverse() {
        std::function<void(node*)> r_reverse = [&] (node* p) -> void {
            if (!p->next) {
                head = p;
                return;
            }

            r_reverse(p->next);
            node* q = p->next;
            q->next = p;
            p->next = nullptr;

        };
        r_reverse(head);
    }
\end{cppcode}
\bigbreak \noindent 
The base case is that we are at the end, in this case we set head to this position. Head is now at the end of the list.
\bigbreak \noindent 
Once the base case is triggered and the head is set to the last node in the list, we will be sent back to the n-1 node call.
\bigbreak \noindent 
To get the intuition for linked list logic, we must examine a diagram of the list.
\bigbreak \noindent 
\begin{figure}[ht]
    \centering
    \incfig{diag}
    \label{fig:diag}
\end{figure}
\bigbreak \noindent 
This figure shows the three operations done after each recursive call. In the figure above, we are at the node after the call that set the end to head. We
\begin{enumerate}
    \item Get a pointer to the node ahead of the current $(q)$. 
    \item This allows us to severe its old next pointer and reverse its direction.
    \item Then, set $p$ next to nullptr (set up for next return).
\end{enumerate}
When the callstack returns to the first call, and does its operations, the list will be reversed

\bigbreak \noindent 
It is also a good idea to examine the iterative method.
\bigbreak \noindent 
\begin{cppcode}
    void Itreverse() {
        node* prev=nullptr, *curr=head, *next=nullptr;

        while (curr) {
            next=curr->next; // Move next to the next node
            curr->next=prev; // Change the direction of current nodes next pointer

            prev=curr; // Advance prev
            curr=next; // Advance curr
        }
        head=prev; // Prev is last node, set head to end
    }
\end{cppcode}

\pagebreak 
\subsubsection{Pushing}
\bigbreak \noindent 
\begin{cppcode}
    void push(int data) {
        if (!head) {
            head = new node(nullptr, data);
            return;
        }
        std::function<void(node*, int)> r_push = [&] (node* curr, int data) -> void {

            if (!curr->next) {
                curr->next = new node(nullptr, data);
                ++size;
                return;
            }
            r_push(curr->next, data);
        };
        r_push(head, data);
    }
\end{cppcode}
\bigbreak \noindent 
Base case:
\begin{enumerate}
    \item \textbf{Empty list}: No recursion, make head the new node
\end{enumerate}
\bigbreak \noindent 
Otherwise, recurse from the head until we get to the last node, simply set last nodes next pointer to new node and return

\pagebreak 
\subsubsection{Inserting}
\bigbreak \noindent 
\begin{cppcode}
    void insert(unsigned pos, int element) {
        std::function<void(node*&, unsigned)> r_insert = [&] (node*& p, unsigned curr_pos) {
            if (curr_pos == 0) {
                node* new_node = new node(nullptr, element);
                new_node->next = p;
                p = new_node;
                return;
            }
            r_insert(p->next,  curr_pos-1);
        };
        r_insert(head, pos);
    }
\end{cppcode}
\bigbreak \noindent 
Base case
\begin{enumerate}
    \item \textbf{Recursed the same number of times as the \texttt{pos} arg}: In this case, make a new node, set next to current node in the recursive traversal, set current node to new node.
\end{enumerate}
\bigbreak \noindent 
Otherwise, keep recursing, subtracting one from the curr\_pos.

\pagebreak 
\subsubsection{Popping}
\bigbreak \noindent 
\begin{cppcode}
    void pop() {
        if (!head) return;

        if (!head->next) {
            delete head;
            head=nullptr;
            return;
        }

        std::function<void(node*)> r_pop = [&] (node* p) -> void {
            if (!p->next->next) {
                delete p->next;
                p->next = nullptr;
                --size;
                return;
            }
            r_pop(p->next);
        };
        r_pop(head);
    }
\end{cppcode}
\bigbreak \noindent 
Base cases:
\begin{enumerate}
    \item \textbf{Empty list}: Noop
    \item \textbf{One node (head)}: Delete then reset head
\end{enumerate}
Otherwise, recurse until we are at the second to last node. Then, delete the second to last nodes next node, which is the last node. Set second to last nodes next pointer to nullptr.

\pagebreak 
\subsubsection{Erasing}
\bigbreak \noindent 
\begin{cppcode}
    void erase(int element) {
        std::function<void(node*&)> r_erase = [&] (node*& p) -> void {
            if (p == nullptr) {
                return;
            }

            r_erase(p->next);

            if (p->data == element) {
                node* tmp = p;
                p = p->next;
                delete tmp;
            }
        };
        r_erase(head);
    }
\end{cppcode}
\bigbreak \noindent 
Base case:
\begin{enumerate}
    \item \textbf{Reached the end}: Return, start unwinding
\end{enumerate}
\bigbreak \noindent 
We traverse to the end of the list recursively, once we reach the end the recursion stops and we start unwinding the call stack, going backwards in the list.
\bigbreak \noindent 
For each node, we check if its data is equal to the element, if it is we set this node equal to its next node, then delete.

\pagebreak 
\subsubsection{Searching}
\bigbreak \noindent 
\begin{cppcode}
    node* search(int element) {
        std::function<node*(node*)> r_search = [&] (node* p) -> node* {
            if (p == nullptr)  {
                return nullptr;
            }
            if (p->data == element) {
                return p;
            }
            return r_search(p->next);
        };
        return r_search(head);
    }
\end{cppcode}
\bigbreak \noindent 
Base cases:
\begin{enumerate}
    \item \textbf{Reached the end of the list}: Element is not in list, return nullptr
    \item \textbf{Found the first node with the element}: Return the node
\end{enumerate}
\bigbreak \noindent 
Otherwise, recurse through the nodes until we hit one of the base cases.


\pagebreak 
\unsect{Binary trees}
\bigbreak \noindent 
\subsection{Terminology}
\bigbreak \noindent 
\begin{itemize}
    \item \textbf{Node:} The basic unit of a binary tree, containing data and references to left and right children.
    \item \textbf{Root:} The topmost node in a tree.
    \item \textbf{Child:} A node directly connected to another node when moving away from the root.
    \item \textbf{Descendants}: The descendants of a node are all nodes that come after a given node.
    \item \textbf{Parent:} The node directly above a child node.
    \item \textbf{Grandparents}: The grandparents of a node is all nodes above the parent up to the root.
    \item \textbf{Ancestors}: The ancestors of a node are all the nodes above a node up to the root
    \item \textbf{Leaf:} A node with no children.
    \item \textbf{Branch node}: A non-leaf node is called a branch node
    \item \textbf{Internal Node:} A branch node, a node with at least one child.
    \item \textbf{Subtree:} A tree consisting of a node and its descendants.
    \item \textbf{Height of a node:} The number of edges on the longest path from a node to a leaf.
    \item \textbf{Height of a tree:} The height of the tree is the height of the root
    \item \textbf{Depth:} The number of edges from the root to a node.
    \item \textbf{Depth of a tree}: The depth of a tree is the depth of the deepest node
    \item \textbf{Degree of a node}: The number of subtrees of a node is called the degree of the node. In a binary tree, all nodes have degree 0, 1, or 2.
    \item \textbf{Degree of a binary tree}:   The degree of a tree is the maximum degree of a node in the tree. A binary tree is degree 2.
\end{itemize}

\pagebreak 
\subsection{Type of binary trees}
\begin{itemize}
    \item \textbf{Full Binary Tree:} Every internal node has two children, all leaf nodes have zero children. Thus, all nodes are either zero or two, never one. 
    \item \textbf{Complete Binary Tree:} All levels, except possibly the last, are fully filled, and all nodes are as far left as possible.
    \item \textbf{Perfect Binary Tree:} A binary tree where all internal nodes have exactly 2 children, and all leaf nodes are at the same level.
    \item \textbf{Balanced Binary Tree:} A binary tree where the height of the left and right subtrees of every node differs by at most one.
    \item \textbf{Degenerate (or pathological) Tree:} A tree where each parent node has only one child, essentially forming a linked list.
    \item \textbf{Skewed Tree:} A special case of a degenerate tree, where all nodes are skewed to the left or right, forming a linear structure.
\end{itemize}

\pagebreak 
\subsection{Maximum height of a binary tree}
\bigbreak \noindent 
The maximum height of a binary tree with $n$ nodes can be as large as $n−1$ (in the case of a degenerate or skewed tree where each node has only one child). This is true for any binary tree:
\begin{align*}
    h_{\text{max}} = n - 1
.\end{align*}
\bigbreak \noindent 
Which occurs for degenerate trees.

\bigbreak \noindent 
\subsubsection{Minimum height of a binary tree}
\bigbreak \noindent 
The minimum height (best case) for a binary tree with $n$ nodes is achieved when the tree is perfectly balanced:
\begin{align*}
    h_{\text{min}} = \lfloor\log_{2}(n)\rfloor
.\end{align*}
\bigbreak \noindent 
This is because the tree would need to spread nodes evenly across levels

\bigbreak \noindent 
\subsubsection{Number of Leaves in a Binary Tree}
\bigbreak \noindent 
For any binary tree with $n$ nodes, the number of leaves $l$ satisfies the following relationship:
\begin{align*}
    l \leq \frac{n+1}{2}
.\end{align*}
\bigbreak \noindent 
This formula gives the maximum number of leaves, assuming that the tree is full (every internal node has 2 children).

\bigbreak \noindent 
\subsubsection{Relationship Between Internal Nodes and Leaves:}
\bigbreak \noindent 
In any binary tree, the number of internal nodes $i$ (nodes with at least one child) and the number of leaves $l$ are related as follows:
\begin{align*}
    i \leq l-1
.\end{align*}

\bigbreak \noindent 
\subsubsection{Maximum Number of Nodes at Height h}
\bigbreak \noindent 
The maximum number of nodes possible at a given height $h$ (where the height is counted from the root as level 0) in a binary tree is:
\begin{align*}
    \text{Max nodes at height $h$} = 2^{h}
.\end{align*}

\pagebreak 
\subsubsection{Number of Edges in a Binary Tree:}
\bigbreak \noindent 
For any binary tree with $n$ nodes, the number of edges $e$ is always
\begin{align*}
    e = n-1
.\end{align*}
\bigbreak \noindent 
This holds because every node (except the root) is connected to exactly one parent, so there are $n−1$ edges in the tree.

\pagebreak 
\subsection{Full trees}
\bigbreak \noindent 
A full tree is a tree where all internal nodes are degree two, and all leaf nodes are degree zero. Observe
\bigbreak \noindent 
\begin{figure}[ht]
    \centering
    \incfig{fulltree1}
    \label{fig:fulltree1}
\end{figure}
\bigbreak \noindent 
The next three subsections refer to the \textit{full binary tree theorem}, which states for a nonempty, full tree $T$ 
\bigbreak \noindent 
\subsubsection{Number of leaves}
\bigbreak \noindent 
If $T$ has $I$ internal nodes, the number of leaves is given by
\begin{align*}
   L = I + 1 
.\end{align*}
\bigbreak \noindent 
If $T$ has a total of $N$ nodes, the number of leaves is 
\begin{align*}
    L = \frac{N+1}{2}
.\end{align*}
\pagebreak 
\subsubsection{Number of nodes}
\bigbreak \noindent 
If $T$ has $I$ internal nodes, the total number of nodes is 
\begin{align*}
    N = 2I + 1
.\end{align*}
\bigbreak \noindent 
If $T$ has $L$ leaves, the total number of nodes is 
\begin{align*}
    N = 2L - 1
.\end{align*}
\bigbreak \noindent 
\subsubsection{Number of internal nodes}
\bigbreak \noindent 
If $T$ has a total of $N$ nodes, the number of internal nodes is 
\begin{align*}
    I = \frac{N-1}{2}
.\end{align*}
\bigbreak \noindent 
If $T$ has $L$ leaves, the number of internal nodes is 
\begin{align*}
    I = L - 1
.\end{align*}

\pagebreak 
\subsection{Complete Binary Tree}
\bigbreak \noindent 
A complete binary tree has a specific structure defined by how the nodes are filled level by level.
\begin{enumerate}
    \item \textbf{All levels, except possibly the last, are fully filled:}
        \begin{itemize}
            In a complete binary tree, every level up to the second-to-last (penultimate) level must be completely filled with nodes.
            This means that if the tree has height $h$, levels $0$ through $h - 1$ (from the root to the second-to-last level) will have the maximum possible number of nodes for that level.
        \end{itemize}
        \item \textbf{All nodes are as far left as possible:}
            \begin{itemize}
                \item On the last level, the nodes don't need to completely fill the level, but the nodes must be positioned as far to the left as possible.
                \item For example, if some nodes are missing from the last level, they will always be missing from the right side, not from the left.
            \end{itemize}
\end{enumerate}
\bigbreak \noindent 
\textbf{Notes:} The tree is balanced in terms of node distribution, with all the levels except possibly the last fully filled.
\bigbreak \noindent 
Nodes on the last level are always added from the leftmost position first.

\bigbreak \noindent 
\subsubsection{Number of nodes}
\bigbreak \noindent 
The height $h$ of a complete binary tree is defined as the number of edges on the longest path from the root to a leaf node.
\bigbreak \noindent 
The total number of nodes in a complete binary tree is given by
\begin{align*}
    n = 2^{h+1} - 1
.\end{align*}
\bigbreak \noindent 
\subsubsection{Height}
\bigbreak \noindent 
The height $h$ of a complete binary tree with $n$ nodes can be derived as:
\begin{align*}
   h = \lfloor \log_{2}(n) \rfloor
.\end{align*}

\bigbreak \noindent 
\subsubsection{Number of Leaf Nodes (L) in a Complete Binary Tree}
\bigbreak \noindent 
The number of leaf nodes in a complete binary tree can be calculated based on the number of internal nodes or the height of the tree
\begin{align*}
    L = \lceil \frac{n}{2} \rceil
.\end{align*}
\bigbreak \noindent 
\subsubsection{Number of internal nodes}
\bigbreak \noindent 
The number of internal nodes (non-leaf nodes) in a complete binary tree can be calculated as:
\begin{align*}
    I &= N - L \\
    I &= \lfloor \frac{n}{2} \rfloor
.\end{align*}

\bigbreak \noindent 
\subsubsection{Parent and Child Relationships in a Complete Binary Tree}
\bigbreak \noindent 
Parent of node at index $i$ (1-based index):
\[
\text{Parent}(i) = \left\lfloor \frac{i}{2} \right\rfloor
\]
Left child of node at index $i$:
\[
\text{Left child}(i) = 2i
\]
Right child of node at index $i$:
\[
\text{Right child}(i) = 2i + 1
\]
These relationships assume a 1-based indexing system for the nodes in the tree (common in heaps or array-based representations).

\pagebreak 
\subsection{Perfect binary tree}
\bigbreak \noindent 
\subsubsection{Number of Nodes}
\bigbreak \noindent 
\begin{align*}
    N = 2^{h+1} - 1
.\end{align*}

\bigbreak \noindent 
\subsubsection{Number of Leaf Nodes}
\bigbreak \noindent 
\begin{align*}
 L = 2^h
.\end{align*}

\bigbreak \noindent 
\subsubsection{Height of the Tree}
\bigbreak \noindent 
\begin{align*}
    h = \log_2(N+1) - 1
.\end{align*}


\bigbreak \noindent 
\subsubsection{Number of Internal Nodes}
\bigbreak \noindent 
\begin{align*}
    I = N - L = 2^h - 1
.\end{align*}

\bigbreak \noindent 
\subsubsection{Depth}
\bigbreak \noindent 
\begin{align*}
    d = h
.\end{align*}

\pagebreak 
\unsect{Applications of binary trees}
\bigbreak \noindent 
\subsection{Binary search trees}
\bigbreak \noindent 
A binary search tree (BST) is a binary tree in which each node has at most two children and follows these properties:
\begin{itemize}
    \item \textbf{Left Subtree Property:} The value of each node in the left subtree is less than the value of the node itself.
    \item \textbf{Right Subtree Property:} The value of each node in the right subtree is greater than the value of the node itself.
    \item Both left and right subtrees must also be binary search trees.
\end{itemize}
\bigbreak \noindent 
\subsubsection{Interface}
\bigbreak \noindent 
The interface of a Binary Search Tree (BST) typically includes a set of operations for managing and accessing the tree’s nodes.
\begin{itemize}
    \item \textbf{Insert(value):} Inserts a new value into the BST while maintaining its properties.
    \item \textbf{Remove(value):} Removes a value from the BST, adjusting the structure to maintain its properties.
    \item \textbf{Predecessor(node)}: Finds the predecessor of a node
    \item \textbf{Succesor(node)}: Finds the successor of a node
    \item \textbf{Find(value):} Searches for a value in the BST and returns the node containing it or null if not found.
    \item \textbf{FindMin():} Returns the node with the smallest value in the BST.
    \item \textbf{FindMax():} Returns the node with the largest value in the BST.
    \item \textbf{IsEmpty():} Checks if the BST is empty.
    \item \textbf{Traverse(order):} Traverses the tree in a specific order (e.g., in-order, pre-order, post-order).
    \item \textbf{Height():} Returns the height of the BST.
    \item \textbf{Clear():} Removes all nodes from the tree, making it empty
\end{itemize}

\pagebreak 
\subsubsection{Traversals}
\bigbreak \noindent 
We can traverse BST's in one of four ways
\begin{itemize}
    \item Level order
    \item Preorder
    \item Inorder
    \item Postorder
\end{itemize}
\bigbreak \noindent 
\paragraph{Level order}
\bigbreak \noindent \bigbreak \noindent 
Level-order traversal is a way of visiting all the nodes in a binary tree by levels, from top to bottom. It starts at the root and visits nodes level by level, left to right, for each level.
\begin{itemize}
    \item Start with the root node (the topmost node).
    \item Visit all the nodes on the next level (children of the root) from left to right.
    \item Then, visit all nodes on the level below that (grandchildren of the root) from left to right, and so on.
\end{itemize}
A queue is often used to implement level-order traversal, as it helps keep track of nodes to visit in the correct order.
\bigbreak \noindent 
\begin{cppcode}
    void levelorderPrint() {
        if (!root) return; // noop for empty tree

        queue<node*> q;
        q.push(root);

        while (!q.empty()) {
            node* curr = q.front();
            q.pop();

            cout << curr->data << endl;
            if (curr->left) {
                q.push(curr->left);
            }
            if (curr->right) {
                q.push(curr->right);
            }
        }
    }
\end{cppcode}
\begin{enumerate}
    \item If the list is nonempty, construct a queue and push the root node.
    \item While the queue is nonempty, grab the front, process the front, pop the front.
    \item Push left and right nodes to queue, if they exist.
\end{enumerate}

\bigbreak \noindent 
\pagebreak \bigbreak \noindent 
\paragraph{Preorder}
\bigbreak \noindent \bigbreak \noindent 
Pre-order traversal is a way of visiting nodes in a binary tree where you:
\begin{enumerate}
    \item Visit the root node first.
    \item Recursively visit the left subtree.
    \item Recursively visit the right subtree.
\end{enumerate}
To explain simply:
\begin{enumerate}
    \item Start with the root node.
    \item Go as far left as possible, visiting each node along the way.
    \item Once you've reached the end of the left subtree, backtrack and visit the right subtree.
\end{enumerate}
\bigbreak \noindent 
\begin{cppcode}
    void preorderPrint() {
        std::function<void(node*)> r_preorderPrint = [&] (node* p) {
            if (p == nullptr) return;

            cout << p->data << endl;
            r_preorderPrint(p->left);
            r_preorderPrint(p->right);
        };
        r_preorderPrint(root);
    }
\end{cppcode}

\bigbreak \noindent 
\paragraph{Inorder}
\bigbreak \noindent\bigbreak \noindent  
in-order traversal is a way of visiting nodes in a binary tree where you:
\begin{enumerate}
    \item Recursively visit the left subtree first.
    \item Visit the root node.
    \item Recursively visit the right subtree.
\end{enumerate}
To explain simply:
\begin{enumerate}
    \item Start by going all the way to the left, visiting nodes along the way.
    \item Once you reach the leftmost node, visit it, then move up to its parent (the root).
    \item After visiting the root, visit the right subtree.
\end{enumerate}
\bigbreak \noindent 
For a BST, printing the tree with an inorder traversal yields a sorted sequence.
\bigbreak \noindent 
\begin{cppcode}
    void inorderPrint() {
        std::function<void(node*)> r_inorderPrint = [&] (node* p) -> void {
            if (!p) return;

            r_inorderPrint(p->left);
            cout << p->data << endl;
            r_inorderPrint(p->right);
        };
        r_inorderPrint(root);
    }
\end{cppcode}

\pagebreak 
\paragraph{Postorder}
\bigbreak \noindent \bigbreak \noindent 
Post-order traversal is a way of visiting nodes in a binary tree where you:
\begin{enumerate}
    \item Recursively visit the left subtree first.
    \item Recursively visit the right subtree.
    \item Finally, visit the root node.
\end{enumerate}
To explain simply:
\begin{enumerate}
    \item Start by going to the leftmost node, but don't visit it yet.
    \item Then, go to the right subtree and process it.
    \item After both subtrees have been visited, visit the root.
\end{enumerate}
\bigbreak \noindent 
\begin{cppcode}
    void postorderPrint() {
        std::function<void(node*)> r_postorderPrint = [&] (node* p) -> void {
            if (!p) return;

            r_postorderPrint(p->left);
            r_postorderPrint(p->right);
            cout << p->data << endl;
        };
        r_postorderPrint(root);
    }
\end{cppcode}

\bigbreak \noindent 
\subsubsection{Successor of a node}
\bigbreak \noindent 
The successor of a node is defined mathematically as
\begin{align*}
    \text{succ}(X) = \text{min}\{A:\ A > X\}
.\end{align*}
\bigbreak \noindent 
thus, we find the set of all nodes that have values greater than that of $X$, then find the minimum in that set.
\bigbreak \noindent 
By properties of binary search trees we find the successor of a node $X$ by 
\begin{enumerate}
    \item \textbf{If $X$ has a right child:} The successor is the leftmost node in the right subtree of $X$ (the smallest node in the right subtree).
    \item \textbf{If $X$ has no right child}:
        \begin{itemize}
            \item If the node is the left child of its parent, then the parent is its successor.
            \item If the node is the right child of its parent, you move upward until you find a node that is the left child of its parent, and that parent is the successor.
        \end{itemize}
\end{enumerate}

\pagebreak 
\subsubsection{Predecessor}
\bigbreak \noindent 
The predecessor of a node $X$ is defined as
\begin{align*}
    \text{pred}(X) = \text{max}\{A:\ A < X\}
.\end{align*}
\bigbreak \noindent 
In other words it is the largest node that is less than $X$. To find the predecessor:
\begin{enumerate}
    \item \textbf{If $X$ has a left child}: The predecssor is the rightmost node in the left subtree
    \item \textbf{If $X$ has no left child}: The predecessor is the nearest ancestor for which the node is in the right subtree.
\end{enumerate}

\bigbreak \noindent 
\subsubsection{The node}
\bigbreak \noindent 
The node is similar to a linked list node, but instead of a single next pointer, it has two. A left pointer and a right pointer.
\bigbreak \noindent 
\begin{cppcode}
    struct node{
        node* left = nullptr;
        node* right = nullptr;
        int data = 0;

        node() = default;
        node(int data) : data(data) {}
        node(node* left, node* right, int data) : left(left), right(right), data(data) {}
    };
\end{cppcode}

\bigbreak \noindent 
\subsubsection{The class}
\bigbreak \noindent 
For simplicity, we often define the Binary Search Tree (BST) as a class. This allows each instance of the class to hold its own root node, along with other data members such as the size of the tree, that we may need. \bigbreak \noindent If it were not a class, then each function would need to take the root node as an argument and return the (potentially modified) root node to maintain the structure."
\bigbreak \noindent 
If it were not a class, than each function would have to take as an argument a root node, and return the root node to maintain the structure
\bigbreak \noindent 
\begin{cppcode}
    class BST {
    private:
        node* root;
        ...
    public:
        ...
    };
\end{cppcode}

\pagebreak 
\subsubsection{Recursive Insertion}
\bigbreak \noindent 
Because of the nature of  BSt's, we often use recursion to define the needed operations.
\bigbreak \noindent 
\begin{cppcode}
    void insert(int element)  {
        // If the tree is empty, insert new element as root
        if (!root) {
            root = new node(element);
            return;
        }

        std::function<void(node*)> r_insert = [&](node* p) -> void {

            // If the element is less than current node, and p->left exists, go left
            if (element < p->data && p->left) {
                r_insert(p->left);

                // If the element is greater than current node, and p->right exists, go right
            } else if (element > p->data && p->right) {
                r_insert(p->right);
            }

            // If the element is less than current node, and p->left doesn't exist, insert node as current nodes left child
            if (element < p->data && !p->left) {
                p->left = new node(element);
                return;

                // If the element is greater than current node, and p->right doesn't exist, insert node as current nodes right child
            } else if (element > p->data && !p->right) {
                p->right = new node(element);
                return;
            }
        };
        // Start recursion from the root
        r_insert(root);
    }
\end{cppcode}
\bigbreak \noindent 
If the tree is empty, it creates a new root node with the given element.
\bigbreak \noindent 
Otherwise, it uses a recursive lambda function (r\_insert) to:
\begin{itemize}
    \item Traverse the tree: going left if the element is smaller, or right if the element is larger.
    \item Once it finds an appropriate spot (where a left or right child doesn't exist), it inserts the new node as a left or right child accordingly.
\end{itemize}
The process starts from the root and recursively finds the right place to insert the new element.

\pagebreak 
\subsubsection{A better recursive insert}
\bigbreak \noindent 
\begin{cppcode}
    node* r_insertC(node* p, int element) {
        if (!p) return new node(element);

        if (element < p->data) {
            p->left =  r_insertC(p->left,element);
        } else if (element > p->data) {
            p->right =  r_insertC(p->right, element);
        }
        return p;
    }

    void insertC(int element) {
        if (!root) root = new node(element);
        r_insertC(root,element);
    }
\end{cppcode}
\bigbreak \noindent 
The main insertion function, insertC, initiates the process. If the tree’s root is null, meaning the tree is empty, it creates a new root node with the given element. Otherwise, it calls the recursive helper function r\_insertC on the root to handle the insertion process.
\bigbreak \noindent 
The r\_insertC function operates recursively to find the correct location for the new element within the tree. Starting from the given node p, it checks whether p is null; if so, it creates and returns a new node with the specified element, making this node the new leaf of the tree at this position. If p is not null, the function compares the element to p->data. If the element is smaller, the function recursively calls r\_insertC on p->left to continue searching in the left subtree, and if the element is larger, it calls r\_insertC on p->right to search in the right subtree. After setting the appropriate child link, it returns the current node p, maintaining the correct structure of the tree at each level of recursion. This ensures the BST properties are preserved, with each node’s left subtree containing values less than the node’s data and the right subtree containing values greater.
\bigbreak \noindent 
Left as an exercise to the reader to see why we must return $p$ to maintain the tree pointer chain.

\pagebreak 
\subsubsection{Iterative insert}
\bigbreak \noindent 
\begin{cppcode}
    void insertB(int element) {
        if (!root) {
            root = new node(element);
            return;
        }

        node* p = root, *trail = nullptr;
        bool left;

        while (p) {
            trail = p;
            if (element < p->data) {
                p=p->left;
                left=true;
            } else if (element > p->data) {
                p=p->right;
                left=false;
            } else {
                return; // noop if already exists
            }
        }
        if (left) {
            trail->left = new node(element);
        } else {
            trail->right = new node(element);
        }
    }
\end{cppcode}
\bigbreak \noindent 
If the tree is empty, it creates a new root node with the element.
\bigbreak \noindent 
It then iteratively traverses the tree starting from the root:
\begin{itemize}
    \item Moves left if the element is smaller than the current node's data.
    \item Moves right if the element is larger.
    \item If the element already exists, it does nothing and returns.
\end{itemize}
Once it finds an empty spot (either left or right child is nullptr), it inserts the new node as the left or right child of the parent node (trail), depending on the comparison.

\pagebreak 
\subsubsection{Recursive removing}
\bigbreak \noindent 
To remove a node with a given value from a BST, there are three cases
\begin{enumerate}
    \item Node has no children
    \item Node has one child
    \item Noe has two children
\end{enumerate}
For case I, we can simply set the nodes parent to nullptr, and then delete the node.
\bigbreak \noindent 
For case II, we must divert the connection from the nodes parent to the nodes child, and then free the node.
\bigbreak \noindent 
Case III is more involved, we first must find the successor of the node. Once we find the successor, we replace the nodes data value with its successor. Then, instead of deleting the node, we delete its successor. Since to be in this case the node must have exactly two children, the successor is found in the simple way.
\begin{enumerate}
    \item Go right once
    \item Go as far left as possible.
\end{enumerate}
Once we have the successor node, it will either have no children, or exactly one child (a right child), if it were to have a left child, it would not be the true successor because we would have not gone as far left as possible.
\pagebreak \bigbreak \noindent 
\begin{cppcode}
    void remove(int element) {
        if (!root) return; // Noop for empty tree

        std::function<void(node*&, node*&)> r_remove = [&] (node*& p, node*& last) -> void {
            if (!p) return; // Not found in tree

            if (element < p->data) {
                r_remove(p->left, p);
            } else if (element > p->data) {
                r_remove(p->right, p);
            } else { // Found
                // Case I: Node has zero children 
                if (!p->left && !p->right) {
                    node* tmp = p;
                    p=nullptr;
                    delete tmp;
                    // Case II: Node has one child
                } else if (!p->left || !p->right) {
                    node* tmp = p;
                    p = (p->left ? p->left : p->right);
                    delete tmp;
                    // Case III: Two children
                } else {    
                    node* successor = p->right;
                    node* successorParent = p;

                    // Find the in-order successor 
                    while (successor->left) {
                        successorParent = successor;
                        successor = successor->left;
                    }

                    // Replace nodes value with successor value
                    p->data = successor->data;

                    // Now we need to delete the successor node
                    // The successor is a leaf or has a right child
                    if (successorParent->left == successor) {
                        successorParent->left = successor->right; 
                    } else {
                        successorParent->right = successor->right; 
                    }
                    delete successor;
                }
            }
        };
        r_remove(root,root);
    }
    \end{cppcode}
    \pagebreak 
    \subsubsection{Clearing}
    \bigbreak \noindent 
    \begin{cppcode}
        void clear() {
            if (!root) return;

            std::function<void(node*)> r_clear = [&](node* p) -> void {
                if (!p) return;

                r_clear(p->left);
                r_clear(p->right);

                delete p;
            };
            r_clear(root);
            root = nullptr;
        }
    \end{cppcode}
    \bigbreak \noindent 
    This function deletes all nodes in a binary search tree. It recursively traverses the tree, deleting each node after its children have been deleted, and finally sets the root to nullptr, effectively clearing the entire tree



    \pagebreak 
    \subsubsection{Counting the height of the tree (root)}
    \bigbreak \noindent 
    \begin{cppcode}
        size_t height() {
            std::function<size_t(node*)> r_height = [&](node* p) -> size_t {
                // Base case height of a nullptr is zero
                if (!p) return 0;
                return 1+std::max(r_height(p->left), r_height(p->right));
            };
            // Height is counting edges, so its number nodes in longest path from root to leaf - 1
            return r_height(root) -1; 
        }
    \end{cppcode}
    \bigbreak \noindent 
    This code defines a height() function that calculates the height of a binary tree by counting the edges. It uses a recursive lambda function r\_height to traverse the tree. For each node, it returns 1 + max(left subtree height, right subtree height) to find the longest path from the root to any leaf. Since r\_height counts nodes, the function subtracts 1 at the end to convert the node count to edge count, which is the definition of height.

    \bigbreak \noindent 
    \subsubsection{Counting the height of a node}
    \bigbreak \noindent 
    \begin{cppcode}
    int getHeight(node* p) {
        if (!p) return -1;
        return 1 + std::max(getHeight(p->left), getHeight(p->right));
    }
    \end{cppcode}
    \bigbreak \noindent 
    If height counts edges, then a nullptr nodes must return -1. If height counts vertices, then nullptr nodes must return 0.


    \pagebreak 
    \subsubsection{Getting the depth of the node}
    \bigbreak \noindent 
    \begin{cppcode}
        int nodeDepth(node* p) {
            if (p == root) return 1;
            return r_nodeDepth(root, p, 1);
        }

        int r_nodeDepth(node* curr, node* p, int depth) {
            if (curr == nullptr) {
                return -1;  // Node not found
            }
            if (p == curr) {
                return depth;  // Node found, return the depth
            }

            // Recursively search in the left subtree if p's data is smaller
            if (p->data < curr->data) {
                return r_nodeDepth(curr->left, p, depth + 1);
            }
            // Recursively search in the right subtree if p's data is greater
            if (p->data > curr->data) {
                return r_nodeDepth(curr->right, p, depth + 1);
            }

            return -1;  // This should never be reached if the tree is valid
        }
    \end{cppcode}
    \bigbreak \noindent 
    This code defines two functions that work together to calculate the depth of a given node p in a binary search tree (BST):
    \bigbreak \noindent 
    \textbf{nodeDepth:}
    This function is the entry point to calculate the depth of the node $p$.
    \bigbreak \noindent 
    It first checks if $p$ is the root node. If so, it returns 1 since the root node is considered to have a depth of 1.
    \bigbreak \noindent 
    If $p$ is not the root, it calls the helper function r\_nodeDepth to recursively search for the node, starting from the root with an initial depth of 1.
    \bigbreak \noindent 
    \textbf{r\_nodeDepth:}
    This is a recursive helper function that searches for the node $p$ in the tree, while tracking the current depth.
    \bigbreak \noindent 
    It first checks if curr (the current node in the search) is nullptr, which indicates that the node $p$ is not in the tree. In that case, it returns -1.
    \bigbreak \noindent 
    If curr matches $p$, it returns the current depth.
    \bigbreak \noindent 
    Otherwise, it recursively searches in the left or right subtree, depending on whether $p$'s data is less than or greater than the current node's data, and increments the depth by 1 at each recursive step.


    \pagebreak 
    \subsubsection{Degenerate Binary Search trees}
    \bigbreak \noindent 
    A degenerate binary search tree is a tree where each parent node has only one child, causing the tree to resemble a linked list
    \bigbreak \noindent 
    Consider building a binary search tree, taking values from a sorted array from left to right. Since all subsequent entries will be greater than the previous, the insertions will only go in one direction, right. Thus, the final tree will resemble a linked list and thus we will not be able to use the $\text{lg}(n)$ property of binary trees.

    \pagebreak 
    \subsubsection{Complexities}
    \bigbreak \noindent 
    Because BST have no guarantee of being well formed, (ie degenerate trees), the complexity of many operations in the worst case is $O(n)$.
    \bigbreak \noindent 
    \begin{center}
        \begin{tabular}{|c|c|c|c|}
            \hline
            \textbf{Operation} & \textbf{Best Case} & \textbf{Average Case} & \textbf{Worst Case} \\ \hline
            Insertion & \( O(\log n) \) & \( O(\log n) \) & \( O(n) \) \\ \hline
            Search & \( O(1) \) & \( O(\log n) \) & \( O(n) \) \\ \hline
            Removal & \( O(\log n) \) & \( O(\log n) \) & \( O(n) \) \\ \hline
            Height & \( O(\log n) \) & --- & \( O(n) \) \\ \hline
            Traversal & \( O(n) \) & \( O(n) \) & \( O(n) \) \\ \hline
        \end{tabular}
    \end{center}
    \bigbreak \noindent 
    \textbf{Note:} $\Omega(\text{lg}(n)$ for BST operations like search, insert, and delete (best case).
    \bigbreak \noindent 
    $\Omega(\text{lg}(n))$ for operations that require visiting all nodes, like traversals.








    \pagebreak 
    \subsection{Adelson-Velsky and Landis Trees (AVL trees)}
    \bigbreak \noindent 
    When dealing with binary search trees, insertions and removals occur continually, with no predictable order. In some of these applications, it is important to optimize search times by keeping the tree very nearly balanced at all times. The method in this section for achieving this goal was described in 1962 by two Russian mathematicians, G. M. ADEL’SON-VEL’SKI˘I and E. M. LANDIS, and the resulting binary search trees are called AVL trees in their honor.
    \bigbreak \noindent 
    AVL trees achieve the goal that searches, insertions, and removals in a tree
with n nodes can all be achieved in time that is $O(\log n)$, even in the worst case.
The height of an AVL tree with n nodes, as we shall establish, can never exceed
lg $n$, and thus even in the worst case, the behavior of an AVL tree could not
be much below that of a random binary search tree. In almost all cases, however,
the actual length of a search is very nearly lg $n$, and thus the behavior of AVL trees
closely approximates that of the ideal, completely balanced binary search tree.
\bigbreak \noindent 
\subsubsection{Definition}
\bigbreak \noindent 
An AVL tree is a binary search tree in which the heights of the left and right
subtrees of the root differ by at most 1 and in which the left and right subtrees
are again AVL trees.
\bigbreak \noindent 
With each node of an AVL tree is associated a \textit{balance factor} that is \texttt{lefthigher}, \texttt{equal-height}, or \texttt{right-higher} according, respectively, as the left subtree
has height greater than, equal to, or less than that of the right subtree.
\bigbreak \noindent 
\fig{.5}{./figures/1.png}
\bigbreak \noindent 
In drawing diagrams, we shall show a left-higher node by ‘/,’ a node whose balance 358
\bigbreak \noindent 
factor is equal by ‘−,’ and a right-higher node by ‘\.’. The figure above shows several
small AVL trees, as well as some binary trees that fail to satisfy the definition.
Note that the definition does not require that all leaves be on the same or
adjacent levels. 
\bigbreak \noindent 
\fig{.5}{./figures/2.png}
\bigbreak \noindent 
The figure above shows several AVL trees that are quite skewed, with right subtrees having greater height than left subtrees.


\bigbreak \noindent 
\subsubsection{AVL Nodes}
\bigbreak \noindent 
A typical node for an AVL tree is as follows
\bigbreak \noindent 
\begin{cppcode}
    struct node {
        node* left{nullptr}, *right{nullptr};
        int data{0};
        int height{0};
        balance b{0};

        // Constructors
            ... 
    };
\end{cppcode}
\bigbreak \noindent 
In AVL trees, it is common for the node structure to include a height member. This height field is essential for efficiently maintaining the balance of the tree, as the balance factor of a node (the difference in height between its left and right subtrees) is used to determine whether the tree needs rebalancing after insertions or deletions.
\bigbreak \noindent 
Without the height member, recalculating the height of each node during every operation would require traversing the entire subtree, significantly increasing the time complexity. By storing the height in each node, you can retrieve it in constant time, allowing rotations and rebalancing operations to remain efficient.

\bigbreak \noindent 
\subsubsection{Storing the height}
\bigbreak \noindent 
In AVL trees, we should store the height of a node as a data member, updating on insertions or removals into the tree. 

\pagebreak 
\subsubsection{Defining balance factors in C++ with enums}
\bigbreak \noindent 
We employ an enumerated data type to record balance factors
\bigbreak \noindent 
\begin{cppcode}
    enum Balance_factor { left_higher, equal_height, right_higher };
\end{cppcode}
\bigbreak \noindent 
Balance factors must be included in all the nodes of an AVL tree, and we must adapt our former node specification accordingly.

\bigbreak \noindent 
\subsubsection{Defining balance factors with a height calculation}
\bigbreak \noindent 
Instead, we can define the balance factor of a node as $\text{height}(left) - \text{height}(right)$. A negative balance implies a subtree is heavy on the right, a positive balance implies a subtree is heavy on the right. A balance of zero implies equal height subtrees. A balance factor ranges from -2 to 2. If $\abs{\text{balance}} = 2$, we must rotate the tree to restore balance.
\bigbreak \noindent 
\begin{cppcode}
    int getBalance(node* p) {
        if (!p) return -1;
        return (p->left ? p->left->height : -1) - (p->right ? p->right->height : -1);
    }
\end{cppcode}


\pagebreak 
\subsubsection{Interface}
\bigbreak \noindent 
The interface of a Binary Search Tree (BST) and an AVL Tree is generally the same. Both are types of binary trees, and they share similar operations such as:
\begin{itemize}
    \item \textbf{Insert:} Insert a new element into the tree.
    \item \textbf{Remove/Delete:} Remove an element from the tree.
    \item \textbf{Search:} Find whether a particular element exists in the tree.
    \item \textbf{Traversal:} In-order, pre-order, post-order, and level-order traversals.
\end{itemize}
However, behind the scenes, the AVL tree performs additional work to maintain its balance property, but this doesn't typically change the public interface

\bigbreak \noindent 
\subsubsection{Balancing an AVL tree}
\bigbreak \noindent 
As we insert or remove nodes from the tree, it may happen that the resulting tree fails to satisfy the conditions imposed by AVL trees. To \textit{rebalance} the tree, we have a set of operations, called rotations. We have
\bigbreak \noindent 
\begin{enumerate}
    \item \textbf{Right Rotation (RR):} Applied when a left subtree is too deep. The subtree is rotated to the right, reducing the height of the left side.
    \item \textbf{Left Rotation (LL):} Applied when a right subtree is too deep. The subtree is rotated to the left, reducing the height of the right side.
    \item \textbf{Left-Right Rotation (LR):} Occurs when a left subtree has a deep right subtree. First, a left rotation is performed on the left subtree, followed by a right rotation.
    \item \textbf{Right-Left Rotation (RL):} Occurs when a right subtree has a deep left subtree. First, a right rotation is performed on the right subtree, followed by a left rotation.
\end{enumerate}
These rotations are applied based on the balance factor, ensuring the tree remains balanced with a height difference of at most 1 between subtrees.
\bigbreak \noindent 
\textbf{Note:} Left-right and Right-left rotations are also called double right and double left respectively.
\bigbreak \noindent 
When writing our rotation algorithms, we only need to define two. A left rotation algorithm and a right rotation algorithm
\bigbreak \noindent 
\textbf{Note}: Performing a rotation when the height difference is only 1 would be unnecessary and could actually disrupt the balancing of the tree. The tree is still balanced in this case because a height difference of 1 between subtrees is allowed in AVL trees.

\pagebreak 
\subsubsection{Rotations: Right tree}
\bigbreak \noindent 
Let us now consider the case when a new node has been inserted into the taller subtree of a root node and its height has increased, so that now one subtree has height 2 more than the other, and the tree no longer satisfies the AVL requirements. We must now rebuild part of the tree to restore its balance. To be definite, let us assume that we have inserted the new node into the right subtree, its height has increased, and the original tree was right higher. That is, we wish to consider the case covered by the function right\_balance. Let root denote the root of the tree and right\_tree the root of its right subtree.
\bigbreak \noindent 
There are three cases to consider, depending on the balance factor of right\_tree.
\bigbreak \noindent 
\paragraph{Case 1: Right higher}
\bigbreak \noindent 
The first case, when right\_tree is right higher. The action needed in this case is called a left rotation.
We have rotated the node right\_tree
upward to the root, dropping root down into the left subtree of right\_tree; the subtree T2 of nodes with keys between those of root and right\_tree now becomes the
right subtree of root rather than the left subtree of right\_tree. A left rotation is
succinctly described in the following C++ function. Note especially that, when
done in the appropriate order, the steps constitute a rotation of the values in three
pointer variables. Note also that, after the rotation, the height of the rotated tree
has decreased by 1; it had previously increased because of the insertion; hence the
height finishes where it began.
\bigbreak \noindent 
\fig{.6}{./figures/3.png}
\bigbreak \noindent 
Thus, we come to the following implementation of a left rotation.
\bigbreak \noindent 
\begin{cppcode}
    void left_rotate(node* root) {
        if (!root || !root->right) return;
        
        right_subtree = root->right;
        root->right = right_subtree->left;
        right_subtree->left = root;
        right_subtree=root;
    }
\end{cppcode}
\pagebreak \bigbreak \noindent 
\begin{figure}[ht]
    \centering
    \incfig{tr2}
    \label{fig:tr2}
\end{figure}
\bigbreak \noindent 
\begin{enumerate}
    \item \textbf{Step 1 (green)}: Attach right\_subtrees left subtree to the right of root
    \item \textbf{Step 2 (blue)}: Attach root to the left of right\_subtree
    \item \textbf{Step 3}: Make r\_subtree the new root.
\end{enumerate}
Yields the rotated tree
\bigbreak \noindent 
\begin{figure}[ht]
    \centering
    \incfig{tr4}
    \label{fig:tr4}
\end{figure}
\bigbreak \noindent 
Note that we perform a left rotation when the roots right subtree has an insertion into its right subtree. Notice that the balance symbols point in the same direction. The only balance factors that change are the balance factors of root and right\_subtree, they become even (balanced).

\pagebreak 
\paragraph{Case 2: Left higher}
\bigbreak \noindent \bigbreak \noindent 
The second case, when the balance factor of right\_tree is left higher, is slightly more complicated. It is necessary to move two levels, to the node sub\_tree that roots the left subtree of right\_tree, to find the new root. This process is shown in Figure 10.20 and is called a double rotation, because the transformation can be obtained in two steps by first rotating the subtree with root right\_tree to the right (so that sub\_tree becomes its root), and then rotating the tree pointed to by root to the left (moving sub\_tree up to become the new root).
\bigbreak \noindent 
\fig{.5}{./figures/4.png}
\bigbreak \noindent 
We see from the figure that we first perform a right rotation on right\_tree, and right\_trees left subtree, this shifts the subtree unbalance in right\_subtree such that it becomes unbalanced on the right instead of on the left. This enables us to perform a left rotation on root and right\_tree. Note that after the right rotation, the balance symbols point in the same direction. A right rotation is of the form
\bigbreak \noindent 
\begin{figure}[ht]
    \centering
    \incfig{t4}
    \label{fig:t4}
\end{figure}
\pagebreak \bigbreak \noindent 
Which becomes
\begin{figure}[ht]
    \centering
    \incfig{t5}
    \label{fig:t5}
\end{figure}
\bigbreak \noindent 
Thus, we see that like the left rotation, the only subtree that moves is $s_{2}$. In the left rotation, the left subtree of $B$ becomes the right subtree of $A$. In the right rotation, the right subtree of $B$ becomes the left subtree of $A$. A right rotation occurs when a tree becomes unbalanced on the left. Whereas a left rotation occurs when a tree becomes unbalanced on the right.
\bigbreak \noindent 
Thus, we come to the following implementation of a left rotation.
\bigbreak \noindent 
\begin{cppcode}
    void right_rotate(node* root) {
        if (!root || !root->right) return;
        
        left_subtree = root->left;
        root->left = left_subtree->right;
        left_subtree->right = root;
        left_subtree=root;
    }
\end{cppcode}

% \bigbreak \noindent 
% \begin{figure}[ht]
%     \centering
%     \incfig{t1}
%     \label{fig:t1}
% \end{figure}
% \bigbreak \noindent 
% During single rotations, there are always two nodes and three subtrees to consider. In this case we name them $A, B$, and subtrees $s_{1}, s_{2}$, and $s_{3}$. $s_{1}$ is the left subtree of the $A$ (the root of the subtree we are rotating), and $s_{2}, s_{3}$ are the children of $B$. After the single left rotation is performed, the only subtree that gets moved is $s_{2}$. Since this case was a right subtree, right higher, left rotation, $s_{2}$ gets attached as the right child of $A$.
% \bigbreak \noindent 
% After the rotation, the height of the rotated subtree will decrease by one. We must then adjust the balance factors. 

\pagebreak 
\subsubsection{C++ Rotations}
\bigbreak \noindent 
\begin{cppcode}
    node* leftRotate(node* p) {
        node* subp = p->right;
        p->right = subp->left;
        subp->left = p;

        p->height = 1 + 
            std::max(p->left ? p->left->height : -1, 
                    p->right ? p->right->height : -1);

        subp->height = 1 + 
            std::max(subp->left ? subp->left->height : -1, 
                    subp->right ? subp->right->height : -1);

        p->balance = getBalance(p);
        subp->balance = getBalance(subp);
        return subp;
    }

    node* rightRotate(node* p) {
        node* subp = p->left;
        p->left = subp->right;
        subp->right = p;

        p->height = 1 + 
            std::max(p->left ? p->left->height : -1, 
                    p->right ? p->right->height : -1);

        subp->height = 1 + 
            std::max(subp->left ? subp->left->height : -1, 
                    subp->right ? subp->right->height : -1);

        p->balance = getBalance(p);
        subp->balance = getBalance(subp);
        return subp;
    }
\end{cppcode}
\bigbreak \noindent 
In leftRotate, the function takes a node pointer p (the root of a subtree that is right-heavy) and performs a left rotation to reduce the height of its right subtree. First, it saves the right child of p (denoted as subp). Then, it adjusts the pointers so that the left child of subp becomes the new right child of p, and subp becomes the new root of this subtree with p as its left child. After adjusting the structure, it recalculates the height and balance of both p and subp, where the height of each node is determined by adding 1 to the maximum height of its left and right children. The function then returns subp as the new root of the subtree.
\bigbreak \noindent 
Similarly, rightRotate takes a node p (the root of a left-heavy subtree) and performs a right rotation to rebalance it. It stores the left child of p as subp, then updates pointers so that the right child of subp becomes the new left child of p, and subp becomes the new root with p as its right child. Heights and balances for p and subp are recalculated using the heights of their children, and subp is returned as the new root of this subtree. These rotations are essential for keeping the AVL tree balanced after insertions and deletions.
\bigbreak \noindent 
\textbf{Note:} To understand the return values of the rotateMethods, it is necessary to understand the insertion method below.


\pagebreak 
\subsubsection{Balancing}
\bigbreak \noindent 
\begin{cppcode}
    node* balance(node* p) {
        if (p->balance > 1) {  // Left heavy
            if (p->left && p->left->balance < 0) {
                leftRotate(p->left);  // Left-Right case
            }
            return rightRotate(p);  // Left-Left case
        }
        if (p->balance < -1) {  // Right heavy
            if (p->right && p->right->balance > 0) {
                rightRotate(p->right);  // Right-Left case
            }
            return leftRotate(p);  // Right-Right case
        }
        return p;
    }
\end{cppcode}
\bigbreak \noindent 
This function takes a pointer p to a node and checks its balance factor (the difference in height between its left and right subtrees) to determine if the node is unbalanced. If the balance factor is greater than 1, the tree is left-heavy, meaning the left subtree is taller than the right subtree. In this case, the function first checks if a "Left-Right" rotation is needed—this would happen if the left child of p is itself right-heavy, indicated by a negative balance factor. If so, a left rotation is applied to the left child of p to balance it before proceeding. Then, a right rotation is applied to p to correct the "Left-Left" imbalance.
\bigbreak \noindent 
Conversely, if the balance factor is less than -1, the node is right-heavy, indicating the right subtree is taller. For a "Right-Left" case, where the right child of p is left-heavy, a right rotation is applied to the right child of p first. This corrects the imbalance locally and prepares it for the final step: a left rotation applied to p to balance the "Right-Right" case. If p is already balanced, the function simply returns p without modification.

\pagebreak 
\subsubsection{Insertions}
\bigbreak \noindent 
\begin{cppcode}
    node* r_insert(node* p, int data) {
        if (!p) return new node(data);

        if (data < p->data) {
            p->left = r_insert(p->left, data);
        } else if (data > p->data) {
            p->right = r_insert(p->right, data);
        }

        p->height = 1 + std::max(p->left ? p->left->height : -1, p->right ? p->right->height : -1);
        p->balance = getBalance(p);

        return balance(p);
    }

    void insert(int data) {
        root = r_insert(root, data);
    }
\end{cppcode}
\bigbreak \noindent 
The r\_insert function takes two parameters: p, a pointer to the current node in the tree, and data, the integer value to be inserted. If p is nullptr (indicating an empty spot in the tree), a new node with the given data is created and returned, establishing the base case for recursion.
\bigbreak \noindent 
If the node already exists, r\_insert decides where to place the new value by comparing data with p->data. If data is less than p->data, it recursively calls r\_insert on the left child (p->left). Otherwise, if data is greater, it calls r\_insert on the right child (p->right). This ensures that the AVL tree retains its binary search property.
\bigbreak \noindent 
After the recursive insertion, r\_insert updates the height of the current node p by setting it to 1 plus the maximum height of its left and right children. It then calculates the node’s balance factor using getBalance(p), which is essential for determining if the subtree rooted at p has become unbalanced. Finally, balance(p) is called to apply any necessary rotations to maintain the AVL tree's balance, and the function returns the (possibly new) root of the subtree.
\bigbreak \noindent 
The insert function is a wrapper that starts the insertion process at the root node of the tree, calling r\_insert with root and the specified data value. This encapsulates the recursive insertion logic and initiates it from the top of the tree.
\bigbreak \noindent 
\textbf{Note:} Worry about ancestory heights (and therefore balance factors) after rotations is not necessary, height and balance calculations occurs only when the insert method returns to that node in the recursion, and thus only after a rotation will an ancestory height and balance calculation be made.

\pagebreak 
\subsubsection{Removing nodes}
\bigbreak \noindent 
To implement removal from an AVL tree, you need to perform a few key steps: find and remove the node, balance the tree afterward, and adjust heights and balance factors as necessary
\bigbreak \noindent 
Start by locating the node with the target value. If found, delete it using standard BST deletion rules
\bigbreak \noindent 
After removing a node, retrace your steps back to the root, updating the height and balance factor of each ancestor. If a node is unbalanced, apply rotations to restore balance.
\bigbreak \noindent 
As with insertion, use recursive balancing and height adjustments after deletion.
\bigbreak \noindent 
\begin{cppcode}
    node* succ(node* p) {
        while (p->left) p = p->left;
        return p;
    }
    node* r_remove(node* p, int data) {
        if (!p) return nullptr;  // Node not found
        // Traverse the tree to find the node to delete
        if (data < p->data) {
            p->left = r_remove(p->left, data);
        } else if (data > p->data) {
            p->right = r_remove(p->right, data);
        } else {  // Node to delete is found
            if (!p->left) {
                node* temp = p->right;
                delete p;
                return temp;
            } else if (!p->right) {
                node* temp = p->left;
                delete p;
                return temp;
            } else {  // Node has two children
                node* temp = succ(p->right);
                p->data = temp->data;  // Replace data
                p->right = r_remove(p->right, temp->data); 
            }
        }
        // Update height and balance
        p->height = 1 + std::max(p->left ? p->left->height : -1, p->right ? p->right->height : -1);
        p->balance = getBalance(p);

        // Balance the node if necessary
        return balance(p);
    }
    void remove(int data) {
        root = r_remove(root, data);
    }
\end{cppcode}

\pagebreak \bigbreak \noindent 
r\_remove, performs a recursive search for the node to delete based on the value data. If data is less than p->data, it recurses into the left subtree; if data is greater, it recurses into the right subtree. When the node to delete is found, three cases are handled based on the structure of p:
\begin{enumerate}
    \item If p has no left child, p is replaced by its right child, effectively bypassing it in the tree.
    \item If p has no right child, p is replaced by its left child.
    \item If p has two children, the in-order successor (smallest node in p->right) is found using succ. The data in p is replaced with this successor’s data, and the successor node is then removed from p->right to avoid duplication.
\end{enumerate}
After deletion, the function updates the height and balance of p, recalculating the height as one plus the maximum height of its children. The balance factor, which measures the difference in height between the left and right subtrees, is also recalculated. Finally, the function calls balance(p) to ensure that any imbalance introduced by the deletion is corrected. The remove function wraps this entire process, starting the deletion at the root. This ensures the AVL tree remains balanced after a node is removed.


\pagebreak 
\unsect{Heaps and Priority Queues (Zero based)}
\bigbreak \noindent 
The \textit{(binary) heap} data structure is an array object that we can view as a nearly complete binary tree (see Section B.5.3), as shown in Figure 6.1. Each node of the tree corresponds to an element of the array. The tree is completely filled on all levels except possibly the lowest, which is filled from the left up to a point. An array $A[0 : n-1]$ that represents a heap is an object with an attribute $A.\text{heap-size}$, which represents how many elements in the heap are stored within array $A$. That is, although $A[0 : n-1]$ may contain numbers, only the elements in $A[0 : A.\text{heap-size}-1]$, where $0 \leq A.\text{heap-size} \leq n$, are valid elements of the heap. If $A.\text{heap-size} = 0$, then the heap is empty. The root of the tree is $A[0]$, and given the index $i$ of a node,
\bigbreak \noindent 
The \textit{(binary) heap} data structure is an array object that we can view as a nearly complete binary tree (see Section B.5.3), as shown in Figure 6.1. Each node of the tree corresponds to an element of the array. The tree is completely filled on all levels except possibly the lowest, which is filled from the left up to a point. An array $A[1 : n]$ that represents a heap is an object with an attribute $A.\text{heap-size}$, which represents how many elements in the heap are stored within array $A$. That is, although $A[1 : n]$ may contain numbers, only the elements in $A[1 : A.\text{heap-size}]$, where $0 \leq A.\text{heap-size} \leq n$, are valid elements of the heap. If $A.\text{heap-size} = 0$, then the heap is empty. The root of the tree is $A[1]$, and given the index $i$ of a node,
\bigbreak \noindent 
\fig{.6}{./figures/5.png}
\bigbreak \noindent 
there’s a simple way to compute the indices of its parent, left child, and right child with the one-line procedures PARENT, LEFT, and RIGHT.
\bigbreak \noindent 
\begin{cppcode}
    int parent(i) {
        return (i+1)/2 // Floor division
    }

    int left(i) {
        return 2i + 1
    }

    int right(i) {
        return 2i + 2:
    }
\end{cppcode}
\bigbreak \noindent 
For one based indexing, we would have $\frac{i}{2}$, $2i$, and $2i+1$
\bigbreak \noindent 
On most computers, the \texttt{LEFT} procedure can compute $2i$ in one instruction by simply shifting the binary representation of $i$ left by one bit position. Similarly, the \texttt{RIGHT} procedure can quickly compute $2i + 1$ by shifting the binary representation of $i$ left by one bit position and then adding $1$. The \texttt{PARENT} procedure can compute $\left\lfloor \frac{i}{2} \right\rfloor$ by shifting $i$ right one bit position. Good implementations of heapsort often implement these procedures as macros or inline procedures.
\pagebreak 
\subsection{Max and Min heaps}
\bigbreak \noindent 
There are two kinds of binary heaps: max-heaps and min-heaps. In both kinds, the values in the nodes satisfy a \textcolor{cyan}{heap property}, the specifics of which depend on the kind of heap. In a \textcolor{cyan}{max-heap}, the \textcolor{cyan}{max-heap property} is that for every node $i$ other than the root,
\bigbreak \noindent 
\begin{align*}
    A[\text{parent}(i)] \geq A[i]
.\end{align*}
\bigbreak \noindent 
That is, the value of a node is at most the value of its parent. Thus, the largest
element in a max-heap is stored at the root, and the subtree rooted at a node contains
values no larger than that contained at the node itself. A \textcolor{min-heap} is organized in
the opposite way: the \textcolor{cyan}{min-heap property} is that for every node $i$ other than the
root,
\bigbreak \noindent 
\begin{align*}
    A[\text{parent}(i)] \leq A[i]
.\end{align*}
\bigbreak \noindent 
The smallest element in a min-heap is at the root.
\bigbreak \noindent 
\subsection{Heapify an array}
\bigbreak \noindent 
Heapify is a process used to maintain the heap property in a binary heap, either a max-heap or a min-heap. The heap property ensures that for every node in the heap:
\begin{itemize}
    \item In a max-heap, the value of each node is greater than or equal to the values of its children.
    \item In a min-heap, the value of each node is less than or equal to the values of its children.
\end{itemize}
\bigbreak \noindent 
Heapify is typically used when you have an unsorted array and you want to turn it into a valid heap, or after inserting or deleting an element in a heap to restore the heap property.
\bigbreak \noindent 
The bottom-up approach to turning an array into a heap is also called the heapify process. This method is efficient for building a heap from an unordered array and runs in $O(n)$ time, which is faster than building the heap using successive insertions ( $O(nlg \ n)$)
\bigbreak \noindent 
We start from the first non-leaf node, which is at position $\frac{n-1}{2}$, where $n$ is the size of the array.
\bigbreak \noindent 
Traverse all non-leaf nodes from the last one to the root (from right to left in the array
\bigbreak \noindent 
For each non-leaf node, apply the sift down operation (also called percolate down), which ensures that the subtree rooted at this node satisfies the heap property.
\bigbreak \noindent 
\textbf{Percolate Down Operation:}
\begin{enumerate}
    \item Compare the node with its children.
    \item If the heap property is violated (for example, in a max-heap, the node is smaller than one of its children), swap the node with the largest child (for max-heap) or smallest child (for min-heap).
    \item Repeat this process down the subtree until the heap property is restored or the node becomes a leaf.
\end{enumerate}

\pagebreak 
\subsection{Min-heap in c++}
\bigbreak \noindent 
\begin{cppcode}
void min_heapify(int arr[], int n, int i) {
    int smallest = i;    // Initialize smallest as root
    int left = 2 * i + 1;   // Left child index
    int right = 2 * i + 2;  // Right child index

    // If left child is smaller than the root
    if (left < n && arr[left] < arr[smallest])
        smallest = left;

    // If right child is smaller than the smallest so far
    if (right < n && arr[right] < arr[smallest])
        smallest = right;

    // If the smallest is not the root
    if (smallest != i) {
        std::swap(arr[i], arr[smallest]);

        // Recursively heapify the affected subtree
        min_heapify(arr, n, smallest);
    }
}

void build_heap(int arr[], int n) {
    // Start from the last non-leaf node and heapify each node
    for (int i = (n - 1) / 2; i >= 0; --i) {
        min_heapify(arr, n, i);
    }
}
\end{cppcode}
\bigbreak \noindent 
The min\_heapify function is designed to maintain the min-heap property for a subtree rooted at a given index $i$. The process begins by identifying the left and right children of the node at index $i$. The function then compares the current node with its children to find the smallest value. If one of the children is smaller than the current node, a swap occurs between the node and the smallest child. After the swap, the function is recursively called on the affected child to ensure that the min-heap property is maintained further down the subtree.

\pagebreak 
\subsection{Max-heap in c++}
\bigbreak \noindent 
\begin{cppcode}
    void heapify(int arr[], int n, int i) {
        int largest = i;
        int left = 2 * i + 1;
        int right = 2 * i + 2;

        if (left < n && arr[left] > arr[largest]) {
            largest = left;
        }

        if (right < n && arr[right] > arr[largest]) {
            largest = right;
        }

        if (largest != i) {
            std::swap(arr[i], arr[largest]);
            heapify(arr, n, largest);
        }
    }


    void build_heap(int arr[], int n) {
        for (int i=n-1/2; i>=0; --i) {
            heapify(arr, n, i);
        }
    }
\end{cppcode}

\pagebreak 
\subsection{Percolating}
\bigbreak \noindent 
Percolating in heaps refers to adjusting a node’s position to maintain the heap property, which can occur in two directions: percolate up or percolate down.
\bigbreak \noindent 
Note that the percolate direction refers to the direction in which we move a node to get its correct locating in the heap.
\bigbreak \noindent 
If we are comparing a given node to its children, its percolate down. If we compare with its parent, its percolate up.

\bigbreak \noindent 
\subsubsection{Percolate up}
\bigbreak \noindent 
Used when adding a new element to the heap (usually at the end). The new element is moved up the heap by comparing it with its parent node. If it violates the heap property (e.g., it’s smaller than its parent in a min-heap), it swaps with the parent. This continues until the node is correctly positioned.
\bigbreak \noindent 
\begin{cppcode}
void percUp(vector<int>& v, int i) {
    while (i>0) {
        int parent = (i-1)/2;
        if (v[i] > v[parent]) {
            swap(v[i], v[parent]);
            i = parent;
        } else break; // In the correct place
    }
}
\end{cppcode}


\pagebreak \bigbreak \noindent 
\subsubsection{Percolate down}
\bigbreak \noindent 
Used when removing the root element (like in a heap deletion). The last element is moved to the root and then "percolates down" by swapping with the smaller child (in a min-heap) if it violates the heap property. This process repeats until the node is correctly placed.
\bigbreak \noindent 
\begin{cppcode}
    void percDown(vector<int>& v, int n, int i) {
        // Assume the largest is the current node
        int largest = i;
        // Get index of both children
        int left = 2*i+1;
        int right = 2*i+2;

        // If the left child exists, and is larger, update largest
        if (left < n && v[left] > v[largest]) {
            largest = left;
        }
        // If the right child exists, and is larger, update largest
        if (right < n && v[right] > v[largest]) {
            largest = right;
        }

        // If the largest was changed, swap. Then call percUp on the node that had the largest value.
        if (largest != i) {
            swap(v[i], v[largest]);
            percUp(v,n,largest);
        }
    }
\end{cppcode}
\bigbreak \noindent 
Because this function takes a node and compares it with its children, the value in the node moves down. Hence, percolate down.


\pagebreak 
\subsection{Inserting into a heap}
\bigbreak \noindent 
In a top-down approach to inserting into a min-heap, you maintain the heap property while inserting a new element. Specifically, you start by placing the new element at the bottom of the heap (in the next available position), and then you "bubble up" (also known as "heapify up" or "percolate up") to restore the heap property.
\begin{itemize}
    \item \textbf{Insert the new element at the bottom:} Add the new element in the first available position (the next available spot in the array representation, which maintains a complete binary tree structure)
    \item \textbf{Bubble up (heapify up):} Compare the newly inserted element with its parent.
        \bigbreak \noindent 
        If the new element is smaller than its parent (which violates the min-heap property), swap the two.
        \bigbreak \noindent 
        Continue this process (i.e., compare with the next parent) until:
        \begin{itemize}
            \item The new element is larger than or equal to its parent, or
            \item The element reaches the root of the heap.
        \end{itemize}
    \item The process terminates when the new element is in a position where it is larger than its parent or becomes the root.
\end{itemize}
\bigbreak \noindent 
The code for a min-heap insert is as follows
\bigbreak \noindent 
\begin{cppcode}
    void percUp(vector<int>& v, int i) {
        int parent = (i-1)/2;

        while (i>0 && v[i] < v[parent]) {
            std::swap(v[i], v[parent]);

            i = parent;
        }
    }

    void heapInsert(vector<int>& v, int element) {
        v.push_back(element);
        int index = v.size() - 1;
        percUp(v, index);
    }
\end{cppcode}

\pagebreak 
\subsection{Removing the root}
\bigbreak \noindent 
Removing an element from a min-heap (typically the root, i.e., the minimum element) involves maintaining the min-heap property after removal. The most common removal operation is to delete the root (the smallest element in a min-heap). The process of removal involves the following steps:
\bigbreak \noindent 
\begin{enumerate}
    \item \textbf{Replace the root with the last element:} The root (at index 0) is the element to be removed, so we replace it with the last element in the heap (this ensures the complete binary tree property is maintained). 
    \item \textbf{Remove the last element:} After swapping, the last element is removed from the heap (usually by reducing the heap's size).
    \item \textbf{Heapify down (percolate down):} Starting from the root, compare the new root element with its children.
        \bigbreak \noindent 
        If the root is larger than any of its children, swap it with the smaller child (to maintain the min-heap property).
        \bigbreak \noindent 
        Continue this process until the heap property is restored (i.e., the element is smaller than both children or it reaches a leaf node).
\end{enumerate}

\bigbreak \noindent 
\begin{cppcode}
    void removeRoot(vector<int>& v) {
        std::swap(v[0], v[v.size()-1]);
        v.pop_back();
        min_heapify(v,v.size(), 0);
    }
\end{cppcode}

\pagebreak 
\subsection{Removing an arbitary node}
\bigbreak \noindent 
To remove an arbitrary node from a min-heap (not just the root), the process is slightly more complex than simply removing the root
\bigbreak \noindent 
\begin{enumerate}
    \item \textbf{Replace the node to be deleted with the last element:} Swap the node to be deleted with the last element in the heap. This ensures that the complete binary tree property is maintained.
    \item \textbf{Remove the last element:} After the swap, the last element (now at the position of the deleted node) is removed from the heap.
    \item \textbf{Restore the heap property:} After the swap, the heap property might be violated both upwards and downwards. So, depending on the value of the swapped element, either heapify up or heapify down from the position where the node was deleted.
        \begin{itemize}
            \item Heapify up if the swapped element is smaller than its parent.
            \item Heapify down if the swapped element is larger than its children
        \end{itemize}
\end{enumerate}
\bigbreak \noindent 
We continue this process until there are no more nodes that match the passed element. The c++ code for a min-heap erase function is as follows.
\bigbreak \noindent 
\begin{cppcode}
    void erase(vector<int>& v, int element) {
        bool found = false;
        while (!found) {
            found = false;
            int i=0;
            for (;i<(int)v.size(); ++i) {
                if (v[i] == element) {
                    found = true;
                    std::swap(v[i], v[v.size()-1]);
                    v.pop_back();

                    if (v[i] > v[(i-1)/2]) {
                        min_heapify(v, v.size(), i);
                    } else {
                        percUp(v, i);
                    }
                }
            }
            if (i == v.size()) break;
        }
    }
\end{cppcode}








\pagebreak 
\subsection{Priority queues}
\bigbreak \noindent 
Heaps are used to implement priority queues because they efficiently maintain the highest (or lowest) priority element at the root. In a max-heap, the largest element is always at the top, while in a min-heap, the smallest element is. This allows for quick access to the highest priority element in $O(1)$ time. Insertion and removal (reordering) of elements take $O(lg\ n)$ time, making heaps an optimal choice for priority queues where these operations need to be fast and frequent.
\bigbreak \noindent 
\subsubsection{Interface}
\bigbreak \noindent 
\begin{itemize}
    \item \textbf{Insert (or Enqueue):} Adds an element to the priority queue based on its priority.
    \item \textbf{Pop}: Retrieve and remove the item with the highest priority (root of the heap)
    \item \textbf{Top}: Retrieve the item with the highest priority
    \item \textbf{Size:} Get the number of items in the queue
    \item \textbf{Empty:} Checks if the priority queue has any elements.
\end{itemize}
\bigbreak \noindent 
\begin{cppcode}
    class priority_queue {
        vector<int> heap; // or any random access container

        void percDown(int n, int index);
        void PercUp(int index)

    public:
        void insert(int element);
        int pop();
        int top();
        size_t size();
        bool empty();
    };
\end{cppcode}
\bigbreak \noindent 
Percolate down, up, insert, and pop can be found in the heap examples above.

\pagebreak 
\subsubsection{Insert, pop, and top}
\bigbreak \noindent 
These are simple methods to implement.
\bigbreak \noindent 
\begin{cppcode}
    // Insert into the heap
    void insert(int element) {
        heap.push_back(element);
        int index = heap.size() - 1;
        percUp(index);
    }

    // Retrieve and remove the root
    int pop() {
        if (heap.empty()) return -1;

        int ret = heap[0];
        std::swap(heap[0], heap[heap.size()-1]);
        heap.pop_back();
        percDown((int)heap.size(), 0);
        return ret;
    }

    // Retrieve the root
    int top() {
        return heap[0];
    }
\end{cppcode}

\pagebreak 
\subsubsection{Size and Empty}
\bigbreak \noindent 
\begin{cppcode}
    size_t size() {
        return (size_t)heap.size();
    }

    bool empty() {
        return heap.empty();
    }
\end{cppcode}

\pagebreak 
\unsect{Sorting}
\bigbreak \noindent 
Sorting is a fundamental concept in data structures and algorithms where elements in a collection are arranged in a specific order, typically ascending or descending. Sorting enables efficient data access, searching, and data organization in applications. There are various sorting algorithms, each with different characteristics and efficiency, such as time and space complexity.
\bigbreak \noindent 
\subsection{Bubble, selection, insertion}
\bigbreak \noindent 
The three simplest sorting algorithms are bubble sort, selection sort, and insertion sort. They are all $O(n^{2})$ comparison based sorting algorithms.
\bigbreak \noindent 
\subsubsection{Bubble sort}
\bigbreak \noindent 
Bubble Sort repeatedly compares adjacent elements and swaps them if they are in the wrong order, "bubbling" the largest unsorted element to the end of the list on each pass.
\bigbreak \noindent 
\begin{cppcode}
    void bubbleSort(vector<int>& v, int n) {
        bool swapped = true;
        while (swapped) {
            swapped = false;
            for (int i=0; i<n-1; ++i) {
                if (v[i+1] < v[i]) {
                    swap(v[i], v[i+1]);
                    swapped=true;
                }
            }
            --n;
        }
    }
\end{cppcode}
\bigbreak \noindent 
By decrementing \( n \) after each complete pass, you avoid unnecessary comparisons in already sorted parts of the vector, making the algorithm slightly more efficient without affecting the \( O(n^2) \) time complexity.
\begin{itemize}
    \item \textbf{Outer Loop}: The \texttt{while} loop continues as long as swaps are happening, so it terminates early if the list becomes sorted before completing all \( n \) passes.
    \item \textbf{Inner Loop}: The \texttt{for} loop compares adjacent elements and swaps them if they’re out of order. After each pass, the largest unsorted element "bubbles up" to the correct position, so the next pass can ignore the last element.
\end{itemize}
\pagebreak \bigbreak \noindent 
\paragraph{Complexity}
\bigbreak \noindent \bigbreak \noindent 
\textbf{Worst Case}: \( O(n^2) \) \\
\bigbreak \noindent 
In the worst case (a reverse-sorted list), Bubble Sort requires \( n \) passes, and each pass involves up to \( n - 1 \) comparisons.

\bigbreak \noindent 
\textbf{Average Case}: \( O(n^2) \) \\
\bigbreak \noindent 
On average, Bubble Sort still performs \( O(n^2) \) comparisons due to repeated passes through the entire array.

\bigbreak \noindent 
\textbf{Best Case}: \( O(n) \) \\
\bigbreak \noindent 
If the list is already sorted, Bubble Sort can stop early if no swaps occur during a pass (often implemented with a flag to detect this). In this case, it only takes one pass to confirm that the list is sorted.

\pagebreak 
\subsubsection{Selection sort}
\bigbreak \noindent 
Selection Sort sorts an array by repeatedly finding the minimum element from the unsorted part and moving it to the beginning.
\begin{enumerate}
    \item Start at the beginning of the array.
    \item For each position $i$, find the smallest element in the unsorted portion (from $i$ to the end).
    \item Swap this minimum element with the element at position $i$
    \item Move to the next position and repeat until the array is sorted.
\end{enumerate}
\bigbreak \noindent 
\begin{cppcode}
    void selectionSort(vector<int>& v, int n) {
        for (int i=0; i<n-1; ++i) {
            int min = i;
            for (int j=i+1; j<n; ++j) {
                if (v[j] < v[min]) {
                    min = j;
                }
            }
            std::swap(v[min], v[i]);
        }
    }
\end{cppcode}
\bigbreak \noindent 
\paragraph{Complexity}
\bigbreak \noindent \bigbreak \noindent 
\textbf{Worst Case}: \( O(n^2) \) \\
\bigbreak \noindent 
Selection Sort always performs \( n \) passes, each requiring a search through the unsorted portion to find the minimum element, resulting in \( O(n^2) \) comparisons.

\bigbreak \noindent 
\textbf{Average Case}: \( O(n^2) \) \\
\bigbreak \noindent 
Selection Sort performs the same number of comparisons regardless of the initial order of elements, as it always looks for the minimum in the unsorted portion.

\bigbreak \noindent 
\textbf{Best Case}: \( O(n^2) \) \\
\bigbreak \noindent 
Even if the array is already sorted, Selection Sort will still go through \( n \) passes and \( O(n^2) \) comparisons, making it inefficient for nearly sorted data.


\pagebreak 
\subsubsection{Insertion sort}
\bigbreak \noindent 
Insertion Sort sorts an array by building a sorted portion one element at a time
\bigbreak \noindent 
\begin{enumerate}
    \item Start with the second element, assuming the first element is trivially sorted.
    \item For each element, compare it with the elements in the sorted portion to its left.
    \item Shift larger elements one position right until you find the correct spot for the current element.
    \item Insert the current element in its correct position.
    \item Repeat until all elements are sorted.
\end{enumerate}
\bigbreak \noindent 
\begin{cppcode}
    void insertionSort(vector<int>& v, int n) {
        for (int i=1; i<n; ++i) {
            int key=v[i];
            int j=i-1;

        while (j>=0 && v[j] > key) {
            v[j+1] = v[j];
            --j;
        }
        v[j+1] = key;
    }
}
\end{cppcode}
\bigbreak \noindent 
Imagine sorting a hand of playing cards:
\begin{itemize}
    \item You pick up cards one by one.
    \item For each new card, you compare it with the cards in your hand from right to left, sliding them over if they’re larger than the new card, until you find the correct spot to insert it.
    \item Each card you add to your hand stays sorted with the previous ones, just like key gets inserted in the sorted sub-array.
\end{itemize}
This method is efficient when the list is nearly sorted since fewer shifts are needed, but it becomes less efficient for randomly ordered arrays due to repeated comparisons and shifts.

\pagebreak 
\paragraph{Complexity}
\bigbreak \noindent \bigbreak \noindent 
\textbf{Worst Case}: \( O(n^2) \) \\
\bigbreak \noindent 
In the worst case (reverse-sorted list), each element is compared with all previous elements, resulting in \( O(n^2) \) comparisons and shifts.

\bigbreak \noindent 
\textbf{Average Case}: \( O(n^2) \) \\
\bigbreak \noindent 
Insertion Sort generally performs \( O(n^2) \) operations in the average case due to repeated comparisons and shifts.

\bigbreak \noindent 
\textbf{Best Case}: \( O(n) \) \\
\bigbreak \noindent 
If the array is already sorted, Insertion Sort only needs to confirm each element is in place, requiring just \( O(n) \) comparisons and no shifts. This makes it very efficient for nearly sorted arrays.







\pagebreak \bigbreak \noindent 
\subsection{Heap sort}
\bigbreak \noindent 
Heap Sort is a comparison-based sorting algorithm that uses a binary heap data structure, specifically a max-heap, to sort elements in ascending order (or a min-heap for descending order).
\bigbreak \noindent 
\begin{enumerate}
    \item \textbf{Build a Max-Heap:} Convert the array into a max-heap, where each parent node is greater than its children. This ensures the largest element is at the root.
    \item \textbf{Extract Maximum and Swap:} Swap the root (largest element) with the last element in the heap. This moves the largest element to its correct position in the sorted array.
    \item \textbf{Heapify:} After the swap, the heap may no longer satisfy the heap property. Restore the max-heap by performing a "heapify" operation on the root to maintain the max-heap structure.
    \item \textbf{Repeat:} Continue extracting the maximum element, swapping, and heapifying, reducing the heap size each time. This process sorts the array in-place.
\end{enumerate}
\bigbreak \noindent 
\begin{cppcode}
    void heapSort(vector<int>& v, int n) {
        heapify(v, n);
        int last = n-1;
        while (last > 0) {
            swap(v[0], v[last--]);
            percDown(v, last, 0);
        }
    }
\end{cppcode}
\bigbreak \noindent 
The functions \textit{heapify} and \textit{percDown} can be found in the \textit{max-heap in c++} section above, where they are called \textit{build-heap} and \textit{heapify} respectively.
\bigbreak \noindent 
\subsubsection{Complexity}
\bigbreak \noindent 
Heap Sort has a time complexity of $O(n\text{lg}n)$ in the best, worst, and average cases, making it more efficient than  $O(n^{2})$ sorting algorithms for larger datasets
\bigbreak \noindent 
Bulding the max-heap from an arbitary container takes $O(n)$ time. Re-heapify the remaining elements (restore the max-heap property) by "sifting down" the new root takes $O(\text{lg} n)$ time. Since we do this sift operation $n$ times, the  complexity of \textit{heapSort} is therefore $O(n\text{lg}n)$, which makes the entire algorithm $O(n) + O(n\text{lg}n) = O(n \text{lg}n$.

\pagebreak 
\subsection{BST Sort}
\bigbreak \noindent 
BST Sort is a sorting algorithm that leverages a Binary Search Tree (BST) to sort elements. It works by inserting all elements into a BST and then performing an in-order traversal of the tree to retrieve the elements in sorted order.
\bigbreak \noindent 
\subsubsection{Insert}
\bigbreak \noindent 
The insert for bst follows the standard bst insert method described in a previous section.
\bigbreak \noindent 
\begin{cppcode}
    node* insert(node* p, int data) {
        if (!p) return new node(data);

        if (data < p->data) {
            p->left = insert(p->left, data);
        } else {
            p->right = insert(p->right, data);
        }

        return p;
    }
\end{cppcode}

\bigbreak \noindent 
\subsubsection{The inorder traversal}
\bigbreak \noindent 
The inorder follows the same logic as before, but when processing each node, we insert the data member into a passed vector.
\bigbreak \noindent 
\begin{cppcode}
    void inorder(node* p, vector<int>& v) {
        if (!p) return;
        inorder(p->left, v);
        v.push_back(p->data);
        inorder(p->right, v);
    }
\end{cppcode}

\pagebreak \bigbreak \noindent 
\subsubsection{The BST sort function}
\bigbreak \noindent 
\begin{cppcode}
    void bstsort(vector<int>& v) {
        vector<int> sorted;
        node* root = nullptr;

        for (const auto& item : v) {
            root = insert(root, item);
        }
        inorder(root, sorted);
        clear(root);

        v = sorted;
    }
\end{cppcode}
\bigbreak \noindent 
The bstsort function sorts a vector of integers using a Binary Search Tree (BST) approach. It starts by creating an empty vector, sorted, which will eventually hold the sorted elements, and initializes root as nullptr, representing an initially empty BST.
\bigbreak \noindent 
For each item in the input vector v, the function calls insert to add the item to the BST, progressively building the tree in a way that respects the BST property (left child nodes are less than the parent node, and right child nodes are greater). After constructing the BST, the function performs an in-order traversal using inorder, which appends each node’s data to the sorted vector in ascending order.
\bigbreak \noindent 
Finally, clear is called to delete all nodes from the BST, freeing up dynamically allocated memory. The function then assigns sorted back to the input vector v, so v now contains the elements in sorted order.
\bigbreak \noindent 
BST Sort is more of a conceptual exercise in data structures than a practical sorting method because balanced tree structures (e.g., AVL trees, red-black trees) are required to ensure $O(nlg\ n)$ performance consistently.

\pagebreak 
\subsubsection{Inplace sorting}
\bigbreak \noindent 
To perform the sort "in-place" in bstsort, we can eliminate the extra sorted vector by writing the sorted values directly back into the input vector v during the in-order traversal. This approach leverages an index to keep track of the position in v where each next sorted value should be placed
\bigbreak \noindent 
\begin{cppcode}
    void inorder2(node* p, vector<int>& v, int& index) {
        if (!p) return;
        inorder2(p->left, v, index);
        v[index++] = p->data;
        inorder2(p->right, v, index);
    }

    void bstsort(vector<int>& v) {
        node* root = nullptr;

        for (const auto& item : v) {
            root = insert(root, item);
        }
        int index = 0;
        inorder2(root, v, index);
        clear(root);
    }
\end{cppcode}

\bigbreak \noindent 
\subsubsection{Complexity}
\bigbreak \noindent 
\begin{itemize}
    \item \textbf{Average Case:} $O(n\text{lg}\ n)$, where $n$ is the number of elements. This is because inserting each element in a balanced BST takes $O(\text{lg}\ n)$ time
    \item \textbf{Worst Case:} $O(n^{2})$, if the tree becomes unbalanced, like when inserting elements in sorted order into a simple BST, resulting in a degenerate tree (like a linked list).
    \item \textbf{Space Complexity:} $O(n)$, for storing the BST nodes.
\end{itemize}
















\pagebreak 
\unsect{Math algorithms}
\bigbreak \noindent 
\subsubsection{Euclidean GCD Algorithm}
\bigbreak \noindent 
The GCD of two integers $a$ and $b$ (with $a \leq b$) is the largest integer that divides both $a$ and $b$. The Euclidean algorithm is based on the principle that
\begin{align*}
    \text{gcd}(a,b) = \text{gcd}(b, a \text{mod } b)
.\end{align*}
\bigbreak \noindent 
This means that the GCD of two numbers doesn't change if the larger number is replaced by its remainder when divided by the smaller number. You keep repeating this until the remainder is 0, and the GCD will be the last non-zero remainder
\bigbreak \noindent 
\begin{cppcode}
    int gcd(int a, int b)  {
        if (!b) return a;

        return gcd(b, a%b);
    }
\end{cppcode}












\end{document}
