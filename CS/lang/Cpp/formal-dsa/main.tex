\documentclass{report}

\input{~/dev/latex/template/preamble.tex}
\input{~/dev/latex/template/macros.tex}

\title{\Huge{}}
\author{\huge{Nathan Warner}}
\date{\huge{}}
\fancyhf{}
\rhead{}
\fancyhead[R]{\itshape Warner} % Left header: Section name
\fancyhead[L]{\itshape\leftmark}  % Right header: Page number
\cfoot{\thepage}
\renewcommand{\headrulewidth}{0pt} % Optional: Removes the header line
%\pagestyle{fancy}
%\fancyhf{}
%\lhead{Warner \thepage}
%\rhead{}
% \lhead{\leftmark}
%\cfoot{\thepage}
%\setborder
% \usepackage[default]{sourcecodepro}
% \usepackage[T1]{fontenc}

% Change the title
\hypersetup{
    pdftitle={Formal DSA in C++}
}

\begin{document}
    % \maketitle
        \begin{titlepage}
       \begin{center}
           \vspace*{1cm}
    
           \textbf{Formal DSA in C++}
    
           \vspace{0.5cm}
            
                
           \vspace{1.5cm}
    
           \textbf{Nathan Warner}
    
           \vfill
                
                
           \vspace{0.8cm}
         
           \includegraphics[width=0.4\textwidth]{~/niu/seal.png}
                
           Computer Science \\
           Northern Illinois University\\
           United States\\
           
                
       \end{center}
    \end{titlepage}
    \tableofcontents
    \pagebreak 
    \unsect{Preface}
    \bigbreak \noindent 
    \subsection{Universe}
    \bigbreak \noindent 
    In the context of data structures and algorithms, the term universe refers to the complete set of all possible elements or values that could be involved in a particular problem or data structure. It defines the range or domain from which data can be selected. We denote the universe with a capital $U$

    \bigbreak \noindent 
    \subsection{Dynamic and static sets}
    \bigbreak \noindent 
    A dynamic set is a data structure that supports not only membership queries but also allows the insertion, deletion, and sometimes modification of elements over time. Unlike static sets, which are fixed once defined, dynamic sets can change as elements are added or removed




    \pagebreak 
    \unsect{Elementary complexity theory}
    \bigbreak \noindent 
   \begin{itemize}
       \item \textbf{Idea}: The same problem can frequently be solved with algorithms that differ in efficiency. The differences between the algorithms may be immaterial for processing a small number of data items, but these differences grow with the amount of data. To compare the efficiency of algorithms, a measure of the degree of difficulty of an algorithm called computational complexity was developed by Juris Hartmanis and Richard E. Stearns
           \bigbreak \noindent 
           Computational complexity indicates how much effort is needed to apply an algorithm or how costly it is. This cost can be measured in a variety of ways, and the particular context determines its meaning. This book concerns itself with the two efficiency criteria: time and space. The factor of time is usually more important than that of space, so efficiency considerations usually focus on the amount of time elapsed when processing data. However, the most inefficient algorithm run on a Cray computer can execute much faster than the most efficient algorithm run on a PC, so run time is always system-dependent. For example, to compare 100 algorithms, all of them would have to be run on the same machine. Furthermore, the results of run-time tests depend on the language in which a given algorithm is written, even if the tests are performed on the same machine. If programs are compiled, they execute much faster than when they are interpreted. A program written in C or Ada may be 20 times faster than the same program encoded in BASIC or LISP.
        \item \textbf{Units}: To evaluate an algorithm’s efficiency, real-time units such as microseconds and nanoseconds should not be used. Rather, logical units that express a relationship between the size $n$ of a file or an array and the amount of time $t$ required to process the data should be used
            \bigbreak \noindent 
            If there is a linear relationship between the size $n$ and time $t$, that is, $t_{1} = cn_{1}$, then an increase of data by a factor of 5 results in the increase of the execution time by the same factor. If $n_{2} = 5n_{1}$, then $t_{2}= 5t_{1} $
            \bigbreak \noindent 
            Similarly, if $t_1 = \log_2 n$, then doubling $n$ increases $t$ by only one unit of time. Therefore, if $t_2 = \log_2(2n)$, then $t_2 = t_1 + 1$.
        \item \textbf{Eliminating insignificant terms}: A function expressing the relationship between $n$ and $t$ is usually much more complex, and calculating such a function is important only in regard to large bodies of data; any terms that do not substantially change the function’s magnitude should  be eliminated from the function. The resulting function gives only an approximate measure of efficiency of the original function. However, this approximation is sufficiently close to the original, especially for a function that processes large quantities of data.
        \item \textbf{Asymptotic complexity}:  This measure of efficiency is called asymptotic complexity and is used when disregarding certain terms of a function to express the efficiency of an algorithm or when calculating a function is difficult or impossible and only approximations can be found
        \item \textbf{Big-O Notation}: The most commonly used notation for specifying asymptotic complexity—that is, for estimating the rate of function growth—is the big-O notation introduced in 1894 by Paul Bachmann.
            \bigbreak \noindent 
             Given two positive-valued functions $f$ and $g$, consider the following definition:
             \bigbreak \noindent 
             $f(n)$ is $O(g(n))$ if there exist positive numbers $c$ and $N$ such that $f(n) \leq c \cdot g(n)$ for all $n \geq N$.
             \begin{align*}
                 f(n) \text{ is } O(g(n)) \iff \exists\ c,N \in \mathbb{Z}^{+} \mid f(n) \le cg(n)\ \forall\ n \ge N
             .\end{align*}
             \bigbreak \noindent 
             Big-O notation says that for large enough $n$, the function $f(n)$ does not grow faster than a constant multiple of $g(n)$. So, $g(n)$ provides an upper bound on how fast $f(n)$ can grow as $n$ increases.
             \bigbreak \noindent 
             In other words, $f$ is big-O of $g$ if there is a positive number $c$ such that $f$ is not larger than $c \cdot g$ for sufficiently large $n$s; that is, for all $n$s larger than some number $N$. The relationship between $f$ and $g$ can be expressed by stating either that $g(n)$ is an upper bound on the value of $f(n)$ or that, in the long run, $f$ grows at most as fast as $g$.
             \bigbreak \noindent 
             The problem with this definition is that, first, it states only that there must exist
             certain $c$ and $N$, but it does not give any hint of how to calculate these constants. Second, it does not put any restrictions on these values and gives little guidance in situations when there are many candidates. In fact, there are usually infinitely many pairs
             of $c$'s and $N$'s that can be given for the same pair of functions $f$ and $g$.
             \bigbreak \noindent 
             For example, suppose 
             \begin{align*}
                 f(n) = 2n^{2} + 3n + 1 = O(n^{2})
             .\end{align*}
             Where $g(n) = n^{2}$. Candidate values for $c$ and $N$ are

             \[
                 \begin{array}{c|cccccccc}
                     c & \geq 6 & \geq 3^{\frac{3}{4}} & \geq 3^{\frac{1}{9}} & \geq 2^{\frac{13}{16}} & \geq 2^{\frac{16}{25}} & \cdots & \rightarrow & 2 \\
                     \hline
                     N & 1 & 2 & 3 & 4 & 5 & \cdots & \rightarrow & \infty \\
                 \end{array}
             \]
             \bigbreak \noindent 
             We obtain these values by solving the inequality:
             \begin{align*}
                 2n^{2}  + 3n  + 1 \leq cn^{2}
             .\end{align*}
             Or equivalently
             \begin{align*}
                 2 + \frac{3}{n}  + \frac{1}{n^{2}} \leq c
             .\end{align*}
             For different $n$'s
             \bigbreak \noindent 
             For large $n$, the terms $\frac{3}{n}$ and $\frac{1}{n^2}$ get smaller. Let's find $N$ such that for all $n \geq N$, the right-hand side stays bounded.
             \bigbreak \noindent 
             As $n$ gets larger, $\frac{3}{n}$ and $\frac{1}{n^2}$ approach zero. To simplify the analysis, choose $N=1$ initially and check how small $\frac{3}{n}$ and $\frac{1}{n^2}$ are:
             \[
                 2 + \frac{3}{1} + \frac{1}{1^2} = 2 + 3 + 1 = 6.
             \]
             From the inequality, at $N=1$, we have $6 \leq c$. Therefore, we can choose $c=6$. This ensures that for all $n \geq 1$, the inequality holds:
             \[
                 2 + \frac{3}{n} + \frac{1}{n^2} \leq 6.
             \]
             Thus, you can choose $c=6$ and $N=1$.
             \bigbreak \noindent 
             different pairs of constants $c$ and $N$ for the same function $g(= n^{2})$ can be determined.
         \item \textbf{Choosing the best $c$, $N$}: To choose the best $c$ and $N$, it should be determined for which N a certain term in $f$ becomes the largest and stays the largest.
             \bigbreak \noindent 
             In the example above, The only candidates for the largest term are \(2n^2\) and \(3n\);
             these terms can be compared using the inequality \(2n^2 > 3n\) that holds for \(n > 1.5\).
             Thus, \(N = 2\) and \(c \geq \frac{15}{4} = 3.75\).
         \item \textbf{Significance}: What is the practical significance of the pairs of constants just listed? All of them
             are related to the same function \(g(n) = n^2\) and to the same \(f(n)\). For a fixed \(g\), an infinite
             number of pairs of \(c\)'s and \(N\)'s can be identified. The point is that \(f\) and \(g\) grow at the same
             rate. The definition states, however, that \(g\) is almost always greater than or equal to \(f\) if it
             is multiplied by a constant \(c\). "Almost always" means for all \(n\)'s not less than a constant \(N\).
             The crux of the matter is that the value of \(c\) depends on which \(N\) is chosen, and vice
             versa.
         \item \textbf{Inherent imprecision: Choosing best $g(n)$}: The inherent imprecision of the big-O notation goes even further, because there 
             can be infinitely many functions \(g\) for a given function \(f\). For example, the \(f\) from 
             Equation 2.2 is big-O not only of \(n^2\), but also of \(n^3\), \(n^4\), \dots, \(n^k\), \dots for any \(k \geq 2\). 
             To avoid this embarrassment of riches, the smallest function \(g\) is chosen, \(n^2\) in this case.
        \item \textbf{Big-o as approximating terms}:  The approximation of function f can be refined using big-O notation only for
            the part of the equation suppressing irrelevant information. For example, in the equation below, the contribution of the third and last terms to the value of the function can
            be omitted
            \bigbreak \noindent 
            \begin{align*}
                f(n) &=n^{2} + 100n + \log(n) + 1000 \\
                \implies  f(n) &= n^{2} + 100n + O(\log(n))
            .\end{align*}
            \bigbreak \noindent 
            Similarly, 
            \begin{align*}
                f(n) &= 2n^{2} + 3n + 1 \\
                \implies f(n) &= 2n^{2} + O(n)
            .\end{align*}
            \bigbreak \noindent 
            This equation says that for large values of \(n\), the expression \(2n^2 + 3n + 1\) behaves like \(2n^2\) plus some terms that grow linearly or slower (captured by \(O(n)\)). The exact contributions of \(3n\) and \(1\) are not important for asymptotic analysis; what matters is that their growth is slower compared to \(2n^2\).
        




    \item \textbf{Algorithm analysis: Most common time complexities}: Ranked slowest to fastest growth
        \begin{itemize}
            \item \textbf{$O(1)$:} Constant time 
            \item \textbf{$O(\log(\log(n)))$}: Logarithmic time
            \item \textbf{$O(\log(n))$}: Logarthmic time
            \item \textbf{$O(n)$}: Linear time
            \item \textbf{$O(n\log(n))$}: Log-linear time
            \item \textbf{$O(n^{k}),\ k>1$}: Polynomial time
            \item \textbf{$O(a^{n}),\ a>1$}: Exponential time
            \item \textbf{$O(n!)$}: Factorial time
        \end{itemize}
    \item \textbf{Ranking complexities from slowest to fastest: Process}: Given 
        \begin{enumerate}[label=(\alph*)]
            \item $O(25) $
            \item $O(n^{\frac{1}{2}} + \log^{2}(n)) $
            \item $O(\log^{200}(n)) $
            \item $O(n^{3}\log^{4}(n)) $
            \item $O(n^{200} + 3^{n}) $
            \item $O(n\log^{40}(n)) $
            \item $O(4^{n}\log(n)) $
            \item $O(n^{3}\log(\log(n))) $
        \end{enumerate}
        How can we go about sorting these slowest to fastest. Well, to start, in the expressions with plus or minus, we can throw out the slower terms. Thus,
        \begin{enumerate}[label=(\alph*)]
            \item $O(n^{\frac{1}{2}}) $
            \item $O(25) $
            \item $O(\log^{200}(n)) $
            \item $O(n^{3}\log^{4}(n)) $
            \item $O(3^{n}) $
            \item $O(n\log^{40}(n)) $
            \item $O(4^{n}\log(n)) $
            \item $O(n^{3}\log(\log(n))) $
        \end{enumerate}
        In product terms, we disregard the slower term unless there are complexites with the same dominant term. For example, $O(n^{3}\log(\log(n)))$ grows slower than $O(n^{3}\log^{4}(n))$ because although they have the same dominant term $n^{3}$, $\log(\log(n))$ grows slower than $\log^{4}(n)$. Thus, the correct sequence is 
        \begin{enumerate}[label=(\alph*)]
            \item [(b)] $O(25)$
            \item [(c)] $O(\log^{200}(n)) $
            \item [(a)] $O(n^{\frac{1}{2}} + \log^{2}(n)) $
            \item [(f)] $O(n\log^{40}(n)) $
            \item [(h)] $O(n^{3}\log(\log(n))) $
            \item [(d)] $O(n^{3}\log^{4}(n)) $
            \item [(e)] $O(n^{200} + 3^{n})$
            \item [(g)] $O(4^{n}\log(n))$
        \end{enumerate}
    \item \textbf{Properties of Big-O notation}
        \begin{enumerate}
            \item \textbf{Transitivity}: $\text{ If } f(n) \text{ is } O(g(n)) \text{ and } g(n) \text{ is } O(h(n)), \text{ then } f(n) \text{ is } O(h(n)).$ \\
            \bigbreak \noindent 
            \textbf{Proof:} According to the definition, $f(n)$ is $O(g(n))$ if there exist positive numbers $c_1$ and $N_1$ such that $f(n) \leq c_1 g(n)$ for all $n \geq N_1$, and $g(n)$ is $O(h(n))$ if there exist positive numbers $c_2$ and $N_2$ such that $g(n) \leq c_2 h(n)$ for all $n \geq N_2$. Hence, $c_1 g(n) \leq c_1 c_2 h(n)$ for $n \geq N$ where $N$ is the larger of $N_1$ and $N_2$. If we take $c = c_1 c_2$, then $f(n) \leq ch(n)$ for $n \geq N$, which means that $f$ is $O(h(n))$.
        \item \textbf{Addition}: If $f(n)$ is $O(h(n))$ and $g(n)$ is $O(h(n))$, then $f(n) + g(n)$ is $O(h(n))$.
            \bigbreak \noindent 
            \textbf{Proof}: If $f(n) \leq c_{1}h(n)$, and $g(n) \leq c_{2}h(n)$, then $f(n) + g(n) \leq c_{1}h(n) + c_{2}h(n) \leq (c_1 + c_2) h(n)$. $\ell$et $c = c_1 + c_2$, then $f(n) + g(n) \leq ch(n)$ and $f(n) + g(n)$ is $O(h(n)) $
        \item \textbf{Polynomial bounds}: The function $an^{k}$ is $O(n^{k})$
            \bigbreak \noindent 
            \textbf{Proof}: $an^{k} \leq cn^{k}$ for $c \geq a$. Since we can always find some constant $c \geq a$, $an^{k}$ is $O(n^{k})$
            \bigbreak \noindent 
            \textbf{Observation}: For $an^{k} \leq cn^{k}$ to hold, $c \geq a$ is necessary
        \item \textbf{Domination of higher-degree polynomials}: $n^{k}$ is $O(n^{k+j}) \ \forall \ j > 0 $
            \bigbreak \noindent 
            This statement holds if $c = N = 1$
            \bigbreak \noindent 
            It follows from all these facts that every polynomial is big-O of $n$ raised to the largest power, or
            \begin{align*}
                f(n) = a_k n^k + a_{k-1} n^{k-1} + \cdots + a_1 n + a_0 \text{ is } O(n^k)
            .\end{align*}
        \item \textbf{Logs}: The function $\log_a n$ is $O(\log_b n)$ for any positive numbers $a$ and $b \neq 1$.
            \bigbreak \noindent 
            This correspondence holds between logarithmic functions. The fact above states that regardless of their bases, logarithmic functions are big-O of each other; that is, all these functions have the same rate of growth.
            \bigbreak \noindent 
            \textbf{Proof}: Let $\log_{a}(n) = x$, and $\log_{b}(n) = y $, then $a^{x} = n,\ b^{y} =n$. Take the natural log of both sides
            \begin{align*}
                \ln{(a^{x})} = \ln{(n)} &\quad \ln{(b^{y})} = \ln{(n)} \\
                \implies x\ln{(a)} = \ln{(n)} &\quad y\ln{(b)} = \ln{(n)} \\
                \implies x\ln{(a)} &= y\ln{(b)}
            .\end{align*}
            Since $x = \log_{a}{(n)}$, and $y=\log_{b}{(n)}$, then we have
            \begin{align*}
                \ln{(a)}\log_{a}{(n)} &= \ln{(b)}\log_{b}{n} \\
                \log_{a}{(n)} &= \frac{\ln{(b)}}{\ln{(a)}} \log_{b}{(n)}
            .\end{align*}
            $\ell$et $c=\frac{\ln{(b)}}{\ln{(a)}}$, then $\log_{a}{(n)} = c\log_{b}{(n)}$, which proves that $\log_{a}(n)$ and $\log_{b}{(n)}$ are multiples of each other. Thus, $\log_{a}{(n)}$ is $O(\log_{b}{n})$
            \bigbreak \noindent 
            \textbf{Note:} Because the base of the logarithm is irrelevant in the context of big-O notation, we can always use just one base.
            \bigbreak \noindent 
            \begin{align*}
               \therefore \log_{a}{(n)} \text{ is } O(\text{lg}n)
            .\end{align*}
            For any positive $a\ne 1$, where $\text{lg}(n)$ is $\log_{2}{(n)}$
    \end{enumerate}
\item \textbf{Big-$\Omega$}. The function $f(n)$ is $\Omega(g(n))$ iff $\exists\ c,N \in \mathbb{R}^{+}\ \mid \ f(n) \geq cg(n) \ \forall \ n \geq N$.
    \bigbreak \noindent 
    In other words, $cg(n)$ is a lower bound on the size of $f(n)$, or, in the long run, $f$ grows at least at the rate of $g$
    \bigbreak \noindent 
    There is an interconnection between these two notations expressed by the equivalence
    \begin{align*}
        f(n) \text{ is } \Omega(g(n)) \text{ iff } g(n) is O(f(n))
    .\end{align*}
    \bigbreak \noindent 
    There are an infinite number of possible lower bounds for the function $f$; that is, there is an infinite set of $g$s such that $f(n)$ is $\Omega(g(n))$ as well as an unbounded number of possible upper bounds of $f$. This may be somewhat disquieting, so we restrict our attention to the smallest upper bounds and the largest lower bounds. Note that there is a common ground for big-O and $\Omega$ notations indicated by the equalities in the definitions of these notations: Big-O is defined in terms of ``$\leq$'' and $\Omega$ in terms of ``$\geq$''; ``$=$'' is included in both inequalities. This suggests a way of restricting the sets of possible lower and upper bounds. 
\item \textbf{Big-$\Theta$}: $f(n)$ is $\Theta(g(n))$ iff $\exists\ c_{1}, c_{2}, N \in \mathbb{R}^{+} \ \mid \ c_{1}g(n) \leq f(n) \leq c_{2}g(n) \ \forall \ n \geq N$
    \bigbreak \noindent 
    We see that $f(n)$ is $\Theta(g(n))$ if $f(n)$ is $O(g(n))$ and $f(n)$ is $\Omega(g(n))$.
    \bigbreak \noindent 
    When applying any of these notations, do not forget that they are approximations that hide some detail that in many cases may be considered important.
\item \textbf{Double $O$ notation}: $f$ is $OO(g(n))$ if it is $O(g(n))$ and the constant $c$ is too large to have practical significance. Thus, $10^{8}n$ is $OO(n)$. However, the definition of “too large” depends on the particular application.
\item \textbf{Using asymptotic complexity to estimate time}:
    If an algorithm is $O(n^2)$, the time to process $n$ elements is proportional to $n^2$. 
    \bigbreak \noindent 
    Let $T(n)$ represent the time, so $T(n) = k \cdot n^2$ where $k$ is a constant.
    \bigbreak \noindent 
    To find the time for 1 million elements ($n = 10^6$):
    \[
        T(10^6) = k \cdot (10^6)^2 = k \cdot 10^{12}
    \]
    For example, if processing $1000$ elements takes $1$ second, then:
    \bigbreak \noindent 
    \[
        T(1000) = k \cdot 1000^2 = k \cdot 10^6 \implies k = \frac{1}{10^6}
    \]
    Now, for $n = 10^6$:
    \[
        T(10^6) = \frac{1}{10^6} \cdot (10^6)^2 = 10^6 \text{ seconds} = 1,000,000 \text{ seconds} \approx 11.57 \text{ days}.
    \]
\item \textbf{Finding asymptotic complexites}: Asymptotic bounds are used to estimate the efficiency of algorithms by assessing the
amount of time and memory needed to accomplish the task for which the algorithms
were designed. This section illustrates how this complexity can be determined.
In most cases, we are interested in time complexity, which usually measures the
number of assignments and comparisons performed during the execution of a program. For now let's focus on assignments
\bigbreak \noindent 
Consider a simple loop to calculate the sum of numbers in an array
\bigbreak \noindent 
\begin{cppcode}
    for (i = sum = 0; i < n; i++)
        sum += a[i];
\end{cppcode}
\bigbreak \noindent 
First, two variables are initialized, then the for loop iterates $n$ times, and during each iteration, it executes two assignments, one of which updates sum and the other of which updates $i$. Thus, there are $2 + 2n$ assignments for the complete run of this for loop; its asymptotic complexity is $O(n)$.
\bigbreak \noindent 
Complexity usually grows if nested loops are used, as in the following code, which outputs the sums of all the subarrays that begin with position 0:
\bigbreak \noindent 
\begin{cppcode}
    for (i = 0; i < n; i++) {
        for (j = 1, sum = a[0]; j <= i; j++)
            sum += a[j];
        cout<<"sum for subarray 0 through "<< i <<" is "<<sum<<endl;
    }
\end{cppcode}
\bigbreak \noindent 
Before the loops start, $i$ is initialized. The outer loop is performed $n$ times, executing in each iteration an inner \texttt{for} loop, print statement, and assignment statements for $i$, $j$, and \texttt{sum}. The inner loop is executed $i$ times for each $i \in \{1, \ldots, n-1\}$ with two assignments in each iteration: one for \texttt{sum} and one for $j$. Therefore, there are
\[
1 + 3n + \sum_{i=1}^{n-1} 2i = 1 + 3n + 2(1 + 2 + \cdots + n - 1) = 1 + 3n + n(n - 1)
\]
$= O(n) + O(n^2) = O(n^2)$ assignments executed before the program is completed.
\item \textbf{Amortized complexity}: amortized analysis can be used
to find the average complexity of a worst case sequence of operations

















   \end{itemize}

   \pagebreak 
   \unsect{Binary search}
   \bigbreak \noindent 
   \subsection{Avoiding overflow with suitable mid calculations}
   \bigbreak \noindent 
   In general, we have two options
   \bigbreak \noindent 
   \begin{cppcode}
   int mid = (left + right) / 2;
   int mid = left + (right - left) / 2;
   \end{cppcode}
   \bigbreak \noindent 
   The first one is easier to write, but may cause overflow if left + right is large. Thus, we would have to do 
   \bigbreak \noindent 
   \begin{cppcode}
    int mid = ((long long) left + right) / 2;   
   \end{cppcode}
   \bigbreak \noindent 
   Whereas the second option,
   \bigbreak \noindent 
   \begin{cppcode}
   int mid = left + (right - left) / 2;
   \end{cppcode}
   \bigbreak \noindent 
   Would be perfectly fine.



   \bigbreak \noindent 
   \subsection{The while(left < right), left = mid+1, right = mid pattern}
   \bigbreak \noindent 
   Suppose we have a search space. It could be an array, a range, etc. Usually it's sorted in ascending order. For most tasks, we can transform the requirement into the following generalized form:
   \begin{center}
       \textbf{Minimize $k$ s.t. condition($k$) is True}
   \end{center}
   \bigbreak \noindent 
   The following code is the most generalized binary search template:
   \bigbreak \noindent 
   \begin{cppcode}
       int binarysearch(vector<int>& v) {
           int left = 0, right = v.size()-1;

           while (left < right) {
               int mid = (left + right) / 2;

               if (condition(v[mid])) {
                   right = mid;
               } else {
                   left =  mid+1;
               }
           }
           return left;
       }
   \end{cppcode}
   \begin{itemize}
       \item        \textbf{Correctly initialize the boundary variables}: left and right to specify search space. Only one rule: set up the boundary to include all possible elements;
       \item \textbf{Decide return value}:. Is it return left or return left - 1? Remember this: after exiting the while loop, left is the minimal k​ satisfying the condition function;
       \item \textbf{Design the condition function}: This is the most difficult and most beautiful part. Needs lots of practice.
   \end{itemize}
   \bigbreak \noindent 
   more often are the situations where the search space and search target are not so readily available. Sometimes we won't even realize that the problem should be solved with binary search -- we might just turn to dynamic programming or DFS and get stuck for a very long time.
   \bigbreak \noindent 
   As for the question "When can we use binary search?", my answer is that, If we can discover some kind of monotonicity, for example, if condition(k) is True then condition(k + 1) is True, then we can consider binary search.
   \bigbreak \noindent 
   In this form, the case where there is no such $k$, left and right will be both at the last element of the array. If all elements satisfy $k$, the left and right will both be at the first element of the array.
   \bigbreak \noindent 
   Observe that in any case, left and right will always remain within the starting bounds.

   \bigbreak \noindent 
   \subsubsection{First bad version}
   \bigbreak \noindent 
   You are a product manager and currently leading a team to develop a new product. Since each version is developed based on the previous version, all the versions after a bad version are also bad. Suppose you have n versions [1, 2, ..., n] and you want to find out the first bad one, which causes all the following ones to be bad. You are given an API bool isBadVersion(version) which will return whether version is bad.
   \bigbreak \noindent 
   \begin{cppcode}
       int firstBadVersion(int n) {
           int left = 1, right = n;
           bool found = false;

           while (left < right) {
               int mid = left + (right - left) / 2;

               if (isBadVersion(mid)) {
                   right = mid;
                   found = true;
               } else {
                   left = mid+1;
               }
           }
           return found ? left : -1;
       }
   \end{cppcode}

   \bigbreak \noindent 
   \subsubsection{The almost equivalent form}
   \bigbreak \noindent 
   \begin{cppcode}
       int binarysearch(vector<int>& v) {
           int left = 0, right = v.size()-1;
           while (left <= right) {
               int mid = left + (right - left) / 2;

               if (condition(v[mid])){
                   right = mid-1;
               } else {
                   left = mid+1;
               }
           }
           return left;
       }
   \end{cppcode}
   \bigbreak \noindent 
   In this case, left would point to the minimal $k$, with right as $left-1$. In this form, if for some reason we instead need to return left-1 in form above, we could simply return right;
   \bigbreak \noindent 
   In the case that the search does not find any such $k$ that meets the condition, right will be at the end of the array, and left will step out of bounds (on the right). If all elements satisfy the condition, left will be at the start of the array, and right will step out of bounds on the left.
   \bigbreak \noindent 
   In the case that all elements satisfy, left will be at the correct position.

   \bigbreak \noindent 
   \begin{cppcode}
       class Solution {
           public:
           int firstBadVersion(int n) {
               int left = 1, right = n;

               while (left <= right) {
                   int mid = left + (right - left) / 2;

                   if (isBadVersion(mid)) {
                       right = mid-1;
                   } else {
                       left = mid+1;
                   }
               }
               return left <= n ? left : -1;
           }
       };
   \end{cppcode}

   \bigbreak \noindent 
   \subsection{Search Insert Position}
   \bigbreak \noindent 
   Given a sorted array of distinct integers and a target value, return the index if the target is found. If not, return the index where it would be if it were inserted in order.
   \bigbreak \noindent 
   Thus, we have the condition \texttt{nums[k] $\geq$ target}. The minimal $k$ that satisfies this condition will be either the target value itself, or the correct position for target.
   \bigbreak \noindent 
   \begin{cppcode}
       int searchInsert(vector<int>& nums, int target) {
           int left = 0, right = nums.size()-1;

           while (left <= right) {
               int mid = left + (right - left) / 2;

               if (nums[mid] >= target) {
                   right = mid-1;
               } else {
                   left = mid+1;
               }
           }
           return left;
       }
   \end{cppcode}
   \bigbreak \noindent 
   Or with the first pattern.
   \bigbreak \noindent 
   \begin{cppcode}
       int searchInsert(vector<int>& nums, int target) {
           int left = 0, right = nums.size();

           while (left < right) {
               int mid = left + (right - left) / 2;

               if (nums[mid] >= target) {
                   right = mid;
               } else {
                   left = mid+1;
               }
           }
           return left;
       }
   \end{cppcode}
   \bigbreak \noindent 
   Notice one crucial detail... using the first pattern (second solution), we must set right to start at nums.size() instead of nums.size()-1.
   \bigbreak \noindent 
   In the first solution, when target is greater than all elements of nums, there is no $k$ that satisfies the condition, and right will be at the end of the array, and left will step out of bounds of the array (which is the correct position of target).
   \bigbreak \noindent 
   In the second solution, when target is greater than all elements of nums, then both left and right will end at the last element of the array (if right=nums.size()-1 instead of nums.size()). Returning left in this case would be incorrect. By setting right=nums.size(), we allow left to step outside the bounds of the array if no $k$ satisfies the condition. This mimics the behavior of the first solution.
   \bigbreak \noindent 
   In both cases, when all elements of nums satisfy the condition, left ends at the correct position (zero).

   \bigbreak \noindent 
   \subsection{Binary search on two arrays}
   \bigbreak \noindent 
   \subsubsection{Minimum common value}
   \bigbreak \noindent 
   Given two integer arrays nums1 and nums2, sorted in non-decreasing order, return the minimum integer common to both arrays. If there is no common integer amongst nums1 and nums2, return -1.
   \bigbreak \noindent 
   Note that an integer is said to be common to nums1 and nums2 if both arrays have at least one occurrence of that integer.
   \bigbreak \noindent 
   The binary search approach is to iterate over the smaller array left to right, and binary search the larger array, the first value found in the second larger array is the answer.
   \bigbreak \noindent 
   \begin{cppcode}
       class Solution {
       public:
           int getCommon(vector<int>& nums1, vector<int>& nums2) {
               if (nums1.size() > nums2.size()) {
                   getCommon(nums2, nums1);
               }
               for (const auto& e : nums1) {
                   if (binarysearch(nums2, e)) return e;
               }
               return -1;
           }
       private:
           bool binarysearch(vector<int>& v, int target) {
               int left = 0, right = v.size()-1;
               while (left <= right) {
                   int mid = left + (right - left) / 2;

                   if (v[mid] == target) return true;
                   else if (v[mid] > target) {
                       right = mid-1;
                   } else {
                       left = mid+1;
                   }
               }
               return false;
           }
       };
   \end{cppcode}
   \bigbreak \noindent 
   Note that since both arrays are sorted, there is a simple $O(n+m)$ two pointer solution, which works in a similar fashion to the merge step in merge sort.
   \bigbreak \noindent 
   \begin{cppcode}
       class Solution {
           public:
           int getCommon(vector<int>& nums1, vector<int>& nums2) {
               int l1 = 0, l2=0;

               while (l1 < nums1.size() && l2 < nums2.size()) {
                   if (nums1[l1] == nums2[l2]) return nums1[l1];
                   else if(nums1[l1] < nums2[l2]) ++l1;
                   else ++l2;
               }

               return -1;
           }
       };
   \end{cppcode}

   \bigbreak \noindent 
   \subsection{Advanced application}
   \bigbreak \noindent 
   The above problems are quite easy to solve, because they already give us the array to be searched. We'd know that we should use binary search to solve them at first glance. However, more often are the situations where the search space and search target are not so readily available. Sometimes we won't even realize that the problem should be solved with binary search -- we might just turn to dynamic programming or DFS and get stuck for a very long time.
   \bigbreak \noindent 
   As for the question "When can we use binary search?", my answer is that, If we can discover some kind of monotonicity, for example, if condition($k$) is True then condition($k + 1$) is True, then we can consider binary search.


   








   \pagebreak 
   \unsect{Sliding window}
   \bigbreak \noindent 
   The sliding window technique is an efficient algorithmic approach used primarily to solve problems involving contiguous segments of a data structure (like an array or a string). It significantly reduces the time complexity by eliminating the need for nested loops when processing every possible subarray or substring.
   \bigbreak \noindent 
   The algorithm typically uses two pointers (or indices) that represent the boundaries of a "window." One pointer marks the beginning (left) and the other marks the end (right) of the window.
   \bigbreak \noindent 
   Move the right pointer to include new elements into the window, updating the necessary computation (such as a running sum or count)
   \bigbreak \noindent 
   Once a condition is met (for example, a target sum or unique character count), move the left pointer to shrink the window. This step helps in finding an optimal solution like the smallest or largest window that satisfies the condition.
   \bigbreak \noindent 
   \subsection{Types of Sliding Window Problems}
   \bigbreak \noindent 
   \begin{itemize}
       \item \textbf{Fixed-Size Window:} The size of the window remains constant.
           \bigbreak \noindent 
           Example: Finding the maximum sum of any subarray of length k. Here, you slide the window of size k across the array, update the sum by subtracting the element that leaves and adding the new element entering, and keep track of the maximum sum.
        \item \textbf{Variable-Size Window:} The window size can expand or contract based on the conditions of the problem.
            \bigbreak \noindent 
            Example: Determining the smallest subarray with a sum greater than or equal to a target value. Once the sum meets or exceeds the target, the algorithm contracts the window from the left to try and minimize its size while still meeting the condition.
   \end{itemize}
   \bigbreak \noindent 
   It typically reduces the time complexity from $O(n^{2})$ to $O(n)$ because each element is processed at most twice (once when it enters the window and once when it leaves).

   \bigbreak \noindent 
   \subsection{Minimum Recolors to Get K Consecutive Black Blocks}
   \bigbreak \noindent 
   You are given a 0-indexed string blocks of length $n$, where $blocks[i]$ is either '$W$' or '$B$', representing the color of the $i$th block. The characters '$W$' and '$B$' denote the colors white and black, respectively.
   \bigbreak \noindent 
   You are also given an integer $k$, which is the desired number of consecutive black blocks.
   \bigbreak \noindent 
   In one operation, you can recolor a white block such that it becomes a black block.
   \bigbreak \noindent 
   Return the minimum number of operations needed such that there is at least one occurrence of k consecutive black blocks.
   \bigbreak \noindent 
   \begin{cppcode}
       class Solution {
           public:
           int minimumRecolors(string blocks, int k) {
               int window{};
               int min = INT_MAX;
               for (int i=0; i<k; ++i) {
                   if (blocks[i] == 'W') ++window;
               }
               min = std::min(min, window);
               for (int i=1; i<=(int)blocks.size()-k; ++i) {
                   if (blocks[i-1] == 'W') --window; 
                   if (blocks[i+k-1] == 'W') ++window;
                   min=std::min(min, window);
               }
               return min;
           }
       };
   \end{cppcode}
   \bigbreak \noindent 
   The code first initializes a window by counting the number of white blocks ('$W$') in the first $k$ characters of the string. This is done in the first for loop. The count is stored in the variable window.
   \bigbreak \noindent 
   The algorithm then "slides" the window one position at a time over the string.
   \bigbreak \noindent 
   It subtracts the count of the element that is leaving the window (the character at blocks[i-1]).
   \bigbreak \noindent 
   It adds the count for the new element entering the window (the character at blocks[i+k-1]).
   \bigbreak \noindent 
   The variable min is updated to keep track of the smallest number of white blocks found in any window of length $k$
   \bigbreak \noindent 
   Instead of recalculating the number of white blocks for each window from scratch, the algorithm efficiently updates the count in constant time for each slide. This is the hallmark of the sliding window technique.
   \bigbreak \noindent 
   Each iteration does a constant amount of work (checking two characters and updating the count). Thus, the total time complexity is $O(n)$.





    \pagebreak 
    \unsect{Linked lists}
    \bigbreak \noindent 
    \subsection{Singly-linked lists}
    \bigbreak \noindent 
    If a node contains a data member that is a pointer to another node, then many nodes
    can be strung together using only one variable to access the entire sequence of nodes.
    Such a sequence of nodes is the most frequently used implementation of a linked list,
    which is a data structure composed of nodes, each node holding some information
    and a pointer to another node in the list. If a node has a link only to its successor in
    this sequence, the list is called a singly linked list
    \bigbreak \noindent 
    Each node resides on the heap
    \bigbreak \noindent 
    Linked lists can easily grow and shrink in size without reallocating memory or moving elements. Adding or removing nodes (especially at the beginning or middle) is more efficient compared to arrays, as no shifting of elements is required. Memory is allocated as needed, avoiding wasted space typical in arrays with fixed sizes.
    \bigbreak \noindent 
    However, each node requires extra memory for the pointer to the next node. Accessing elements requires traversal from the head, making lookups slower (O(n)) compared to arrays, which offer O(1) access via indexing.  Nodes are scattered in memory, leading to poor cache performance compared to arrays, which have contiguous memory locations.



    \bigbreak \noindent 
    \subsubsection{Structure of the node}
    \bigbreak \noindent 
    The node structure is typically implemented in the following way
    \bigbreak \noindent 
    \begin{cppcode}
        struct node {
            node* next = nullptr;
            T data = 0; 

            node() = default;
            node(data) : data(data) {}
            node(next, data) : next(next), data(data) {}
        }
    \end{cppcode}
    \bigbreak \noindent 
    A node includes two data members: info and next. The info member is used
    to store information, and this member is important to the user. The next member is
    used to link nodes to form a linked list. It is an auxiliary data member used to maintain the linked list. It is indispensable for implementation of the linked list, but less
    important (if at all) from the user’s perspective. Note that node is defined
    in terms of itself because one data member, next, is a pointer to a node of the same
    type that is just being defined. Objects that include such a data member are called
    self-referential objects.

    \bigbreak \noindent 
    \subsubsection{The list class/struct}
    \bigbreak \noindent 
    We also implement the list structure as a class or struct.
    \bigbreak \noindent 
    \begin{cppcode}
        class single_list {
            node* head = nullptr;
        public:
            ...
        };
    \end{cppcode}


    \bigbreak \noindent 
    \subsubsection{Interface of a singly linked list stack}
    \bigbreak \noindent 
    The interface typically includes the following operations:
    \begin{enumerate}
        \item \textbf{Insert:} Add a node at the beginning, end, or a specific position in the list.
        \item \textbf{Delete:} Remove a node from the beginning, end, or a specific position.
        \item \textbf{Search:} Find a node with a given value.
        \item \textbf{Traverse:} Iterate through the list to access or print each node's data.
        \item \textbf{IsEmpty:} Check if the list is empty.
        \item \textbf{Size:} Return the number of nodes in the list. The first node is called the head, and the last node points to nullptr (indicating the end of the list).
    \end{enumerate}

    \pagebreak 
    \subsubsection{Traversing}
    \bigbreak \noindent 
    Traversing a list is simple.
    \bigbreak \noindent 
    \begin{cppcode}
    node* curr = head;

    while (curr) {
        curr = curr->next;
        ...
    }
    \end{cppcode}

    \pagebreak 
    \subsubsection{Printing}
    \bigbreak \noindent 
    Now that we can traverse, we can print each node
    \bigbreak \noindent 
    \begin{cppcode}
    node* curr = head;
    while (curr) {
        cout << curr->data;
        curr=curr->next;
    }
    \end{cppcode}

    \pagebreak 
    \subsubsection{Printing in reverse}
    \bigbreak \noindent 
    Printing in reverse requires creating a stack.
    \bigbreak \noindent 
    \begin{cppcode}
    if (!head) return; // noop, dont even bother creating a vector.

    vector<node*> stack;
    node* curr = head;

    while (curr) {
        stack.push_back(curr);
        curr=curr->next;
    }

    for (int i=(int)stack.size()-1; i>=0; --i) {
        cout << stack[i]->data << " ";
    }
    cout << endl;
    \end{cppcode}

    \pagebreak 
    \subsubsection{Getting the length}
    \bigbreak \noindent 
    While we traverse, just increment a counter.
    \bigbreak \noindent 
    \begin{cppcode}
        size_t len() {
            size_t len = 0;
            for (node* curr = head; curr; curr=curr->next, ++len);
            return len;
        }
    \end{cppcode}

    \pagebreak 
    \subsubsection{Clearing}
    \bigbreak \noindent 
    \begin{cppcode}
        void clear() {
            node* curr=head, *prev=nullptr;

            while (curr) {
                prev=curr;
                curr=curr->next;
                delete prev;
            }
            head = nullptr;
        }
    \end{cppcode}

    \pagebreak 
    \subsubsection{Reversing}
    \bigbreak \noindent 
    Reversing is pretty straight forward
    \bigbreak \noindent 
    \begin{cppcode}
        void reverse() {
            node* prev=nullptr, *curr=head, *next=nullptr;

            while(curr) {
                next=curr->next;
                curr->next = prev;
                prev = curr;
                curr=next;
            }

            head = prev;
        }
    \end{cppcode}
    \bigbreak \noindent 
    In each iteration, next temporarily holds the next node so you don’t lose track of it when reversing the link.
    \bigbreak \noindent 
    The curr->next pointer is set to prev, effectively reversing the link.
    \bigbreak \noindent 
    Prev is then updated to curr, and curr is updated to next to continue the process.


    \pagebreak 
    \subsubsection{Pushing}
    \bigbreak \noindent 
    \begin{cppcode}
        void push(int element) {
            if (!head) {
                head = new node(element);
                return;
            }

            node* curr = head;
            while (curr->next) {
                curr=curr->next;
            }
            curr->next = new node(element);
        }
    \end{cppcode}

    \pagebreak 
    \subsubsection{Inserting}
    \bigbreak \noindent 
    \begin{cppcode}
        void insert(int pos, int element) {
            if (!head || pos == 0) {
                node* new_node = new node(element);
                new_node->next = head;
                head = new_node;
                return;
            }
            node* curr = head;

            int count=0;
            while (count != pos-1 && curr->next) {
                curr=curr->next;
                ++count;
            }
            node* new_node = new node(element);

            new_node->next = curr->next;
            curr->next = new_node;
        }
    \end{cppcode}
    \bigbreak \noindent 
    \begin{enumerate}
        \item \textbf{Check if the list is empty or inserting at the head (position 0):}
            \begin{itemize}
                \item If head is nullptr (meaning the list is empty) or pos == 0 (you want to insert at the beginning), a new node is created with the given element.
                \item The new node's next pointer is set to the current head (which could be nullptr if the list is empty), and then head is updated to point to this new node.
                \item This handles the case where the new node becomes the first node in the list.
            \end{itemize}
        \item \textbf{Traverse to the correct position:}
            \begin{itemize}
                \item If you are inserting somewhere other than the head, the function uses a loop to find the node just before the desired position (pos - 1).
                \item It starts at the head and moves along the list until it reaches the node right before where the new node will be inserted.
            \end{itemize}
        \item \textbf{Insert the new node:}
            \begin{itemize}
                \item Once the loop finds the right place (curr points to the node before the insertion position), a new node is created.
                \item The new node’s next pointer is set to curr->next (the node currently in the target position).
                \item Then, curr->next is updated to point to the new node, effectively inserting the new node into the list.
            \end{itemize}

    \end{enumerate}


    \pagebreak 
    \subsubsection{Popping}
    \bigbreak \noindent 
    \begin{cppcode}
        void pop() {
            if (!head) return;
            if (!head->next) {
                delete head;
                head=nullptr;
                return;
            }

            node* prev=nullptr, *curr = head;
            while (curr->next) {
                prev=curr;
                curr=curr->next;
            }
            delete curr;
            prev->next=nullptr;
        }
    \end{cppcode}
    \bigbreak \noindent 
    \begin{enumerate}
        \item \textbf{Empty List Check:} If the list is empty (head == nullptr), it does nothing.
        \item \textbf{Single Node Case:} If the list has only one node, it deletes the head and sets head to nullptr.
        \item \textbf{Multiple Nodes:} It traverses to the last node using two pointers (prev and curr), deletes the last node (curr), and sets the second-to-last node's next pointer (prev->next) to nullptr to mark the new end of the list.
    \end{enumerate}

    \pagebreak 
    \subsubsection{Erasing}
    \bigbreak \noindent 
    \begin{cppcode}
        void erase(int element) {
            if (!head) return;

            while (head->data == element) {
                if (head->next && head->data == element) {
                    node* tmp = head;
                    head = head->next;
                    delete tmp;
                }
            }

            node* prev=nullptr, *curr=head;

            while (curr) {
                if (curr->data == element) {
                    node* tmp = curr;
                    prev->next = curr->next;
                    curr=curr->next;
                    delete tmp;
                } else {
                    prev=curr;
                    curr=curr->next;
                }
            }
        }
    \end{cppcode}
    \bigbreak \noindent 
    This erase function removes all nodes with a specific value (element) from the list:
    \begin{itemize}
        \item \textbf{Empty List Check:} If the list is empty (head == nullptr), it returns immediately.
        \item \textbf{Head Node Deletion:} If the head contains the target value, it deletes the head and updates it to the next node. We keep doing this until the head node no longer contains the data we want to remove
        \item \textbf{Traverse and Delete:} It iterates through the list, and for each node with the target value, it removes the node by adjusting the next pointer of the previous node and deleting the current node.
    \end{itemize}


    \pagebreak 
    \subsubsection{Searching}
    \bigbreak \noindent 
    \begin{cppcode}
        node* search(int element) {
            node* curr = head;
            while (curr) {
                if (curr->data == element) {
                    return curr;
                }
            }
            return nullptr;
        }
    \end{cppcode}

    \pagebreak 
    \subsection{Doubly-linked list}
    \bigbreak \noindent 
    A doubly linked list is a data structure consisting of nodes where each node contains three components:
    \begin{enumerate}
        \item \textbf{Data:} The value or information the node holds.
        \item \textbf{Next pointer:} A reference to the next node in the list.
        \item \textbf{Previous pointer:} A reference to the previous node in the list.
    \end{enumerate}
    \bigbreak \noindent 
    Each node links to both its predecessor and successor. Traversal is possible in both forward and backward directions. It requires more memory than a singly linked list due to the extra pointer.
    \bigbreak \noindent 
    \subsubsection{Node structure}
    \bigbreak \noindent 
    Instead of just a next pointer, a doubly linked list also has a prev pointer
    \bigbreak \noindent 
    \begin{cppcode}
    struct node {
        int data = 0;
        node* next = nullptr, *prev = nullptr;

        node(int data) : data(data) {}
    };
    \end{cppcode}

    \bigbreak \noindent 
    \subsubsection{The list class}
    \bigbreak \noindent 
    The list class implements the interface
    \bigbreak \noindent 
    \begin{cppcode}
class list {
    node* head=nullptr, *tail=nullptr;
    size_t n = 0;

public:
    void push_back(int element);
    void push_front(int element);
    void pop_back();
    void pop_front();
    bool find();
    size_t size();
    bool empty();
    void print();
    void clear();
};
    \end{cppcode}
    \bigbreak \noindent 
    Size, empty, print and clear are trivial, so their implementation will not be discussed
    \pagebreak 
    \subsubsection{push\_back and push\_front}
    \bigbreak \noindent 
    \begin{cppcode}
        void push_back(int element) {
            if (!head) {
                head = new node(element);
                tail = head;
                ++n;
                return;
            }
            tail->next = new node(element);
            tail->next->prev = tail;
            tail = tail->next;
            ++n;
        }

        void push_front(int element) {
            if (!head) {
                head = new node(element);
                tail = head;
                ++n;
                return;
            }
            head->prev = new node(element);
            head->prev->next = head;
            head=head->prev;
            ++n;
        }
    \end{cppcode}
    \bigbreak \noindent 
    The push\_back function adds a new element to the end of the doubly linked list. If the list is empty (indicated by a null head), it creates a new node as both the head and tail of the list and increments the size counter n. Otherwise, it creates a new node, links it as the next node of the current tail, sets the previous pointer of the new node to the old tail, updates the tail to the new node, and increments n.
    \bigbreak \noindent 
    The push\_front function inserts a new element at the beginning of the doubly linked list. If the list is empty, it operates similarly to push\_back by creating a new node as both the head and tail. For non-empty lists, it creates a new node, links it as the previous node of the current head, sets the next pointer of the new node to the old head, updates the head to the new node, and increments n. Both functions maintain the integrity of the doubly linked list by appropriately updating the pointers.

    \pagebreak 
    \subsubsection{pop\_back and pop\_frot}
    \bigbreak \noindent 
    \begin{cppcode}
        void pop_back() {
            node* save = tail->prev;
            save->next = nullptr;
            delete tail;
            tail = save;
            --n;
        }

        void pop_front() {
            node* save = head->next;
            save->prev = nullptr;
            delete head;
            head = save;
            --n;
        }
    \end{cppcode}
    \bigbreak \noindent 
    The pop\_back function removes the last element from the doubly linked list. It starts by saving the previous node of the current tail into a temporary pointer (save). Then, it disconnects the tail by setting the next pointer of the saved node to nullptr. Afterward, it deletes the current tail node, updates the tail pointer to the saved node, and decrements the size counter n.
    \bigbreak \noindent 
    The pop\_front function removes the first element from the doubly linked list. It saves the next node of the current head into a temporary pointer (save). It then disconnects the head by setting the prev pointer of the saved node to nullptr. The current head is deleted, the head pointer is updated to the saved node, and n is decremented. Both functions ensure proper memory management and maintain the structural integrity of the list.

    \pagebreak 
    \subsubsection{Searching}
    \bigbreak \noindent 
    \begin{itemize}
        \item \textbf{Start from the head:} Traverse the list from the beginning since it is more straightforward and avoids redundant checks.
        \item \textbf{Compare each node's data with the target value:} If a match is found, return a pointer to the node.
        \item \textbf{Continue until the end:} If no match is found by the time you reach the end of the list (when current becomes nullptr), return nullptr to indicate the value is not in the list.
    \end{itemize}
    \bigbreak \noindent 
    \begin{cppcode}
        bool find(int element) {
            node* curr = head;
            while (curr) {
                if (curr->data == element) return true;
                curr=curr->next;
            }
            return false;
        }
    \end{cppcode}

    \pagebreak 
    \unsect{Monotonic stack}
    \bigbreak \noindent 
    A monotonic stack is a specialized stack data structure that maintains its elements in a sorted order—either strictly increasing or strictly decreasing. This ordering property makes it particularly effective for solving problems where you need to quickly find the next or previous element that meets a certain condition (like being greater or smaller).
    \begin{itemize}
        \item \textbf{Monotonic increasing stack}: Elements are arranged so that every new element is greater than or equal to the one before it. This is useful for finding the next smaller element.
        \item \textbf{Monotonic decreasing stack}: Elements are arranged so that every new element is less than or equal to the one before it. This helps when you need to find the next larger element.
    \end{itemize}
    \bigbreak \noindent 
    \subsection{Next greater element}
    \bigbreak \noindent 
    \begin{cppcode}
        vector<int> nextGreaterElement(vector<int>& nums) {
            vector<int> res(nums.size(), -1);
            stack<int> s;

            for (int i=0; i<(int)nums.size(); ++i) {
            while (s.size() && (nums[i] > nums[s.top()])) {
                int index = s.top();
                s.pop();
                res[index] = nums[i];
            }
            s.push(i);
        }

        return res;
    }
    int main() {
        vector<int> v{2,1,5,4,6};
        vector<int> res = nextGreaterElement(v);
        for (const auto& item : res) cout << item << " ";
        // 5 5 6 6 -1
    }
    \end{cppcode}
    \bigbreak \noindent 
    The stack (monoStack) stores indices of the array elements. This allows you to keep track of positions for which the next greater element hasn't been found yet.
    \bigbreak \noindent 
    As you iterate over the array, if the current element (nums[i]) is greater than the element at the index on top of the stack, it means the current element is the next greater element for that index. The stack pops until the current element is not greater than the element at the new top of the stack.
    \bigbreak \noindent 
    Every element is pushed and popped at most once, ensuring an overall time complexity of $O(n)$.

    \pagebreak 
    \subsection{Next smaller element}
    \bigbreak \noindent 
    We simply flip the greater than condition in the while loop above to less than.



    \pagebreak 
    \unsect{Recursion}
    \bigbreak \noindent 
    \subsection{Recursion vs iteration}
    \bigbreak \noindent 
    In theory, any problem that can be solved recursively can be solved iteratively. This also means that any problem that can be solved iteratively can also be solved recursively.
    \bigbreak \noindent 
    The question is, for any problem that can be solved, which method can be used such that the problem is easier to solve.

    \pagebreak \bigbreak \noindent 
    \subsection{Elementary recursion}
    \bigbreak \noindent 
    A recursive definition consists of two parts. In the first part, called the anchor or
    the ground case, the basic elements that are the building blocks of all other elements
    of the set are listed. In the second part, rules are given that allow for the construction
    of new objects out of basic elements or objects that have already been constructed.
    These rules are applied again and again to generate new objects. For example, to construct the set of natural numbers, one basic element, 0, is singled out, and the operation of incrementing by 1 is given as:
    \begin{enumerate}
        \item $0 \in \mathbb{N}$
        \item If $n\in \mathbb{N},\ then (n+1) \in \mathbb{N}$
        \item There are no other objects in the set $\mathbb{N}$
    \end{enumerate}
    It is more convenient to use the following definition, which encompasses the whole range of Arabic numeric heritage:
    \begin{enumerate}
        \item $0, 1,2,3,4,5,6,7,8,9 \in \mathbb{N} $
        \item If $n\in \mathbb{N}$, then $n0, n1,n2,n3,n4,n5,n6,n7,n8,n9 \in \mathbb{N}$
        \item These are the only natural numbers
    \end{enumerate}
    \bigbreak \noindent 
    Recursive definitions serve two purposes: generating new elements, as already
    indicated, and testing whether an element belongs to a set. In the case of testing, the
    problem is solved by reducing it to a simpler problem, and if the simpler problem is
    still too complex it is reduced to an even simpler problem, and so on, until it is reduced to a problem indicated in the anchor

    \bigbreak \noindent 
    \subsection{Base cases}
    \bigbreak \noindent 
    In recursion, a base case is a condition that stops further recursive calls and provides a direct answer without further recursion
    \bigbreak \noindent 
    If there were no base case, there would be nothing to stop the recursion. Thus, it would go on until the program crashes. For this reason, all recursive functions must have at least one base case.
    \bigbreak \noindent 
    If a base case in a recursive function returns a value, then every recursive call leading up to that base case should also return a value. This is necessary to ensure that the result of the recursion is propagated back up the call stack.
    \bigbreak \noindent 
    In a recursive function, the base case stops the recursion, and if the base case returns something (e.g., a node pointer, integer, etc.), the recursive calls that occur before reaching the base case need to return that result so it can propagate back to the original caller.

    \bigbreak \noindent 
    \subsubsection{Factorials}
    \bigbreak \noindent 
    \begin{align*}
        n! = 
        \begin{cases}
            1 & \text{if } n = 0 \\
            n(n-1)! & \text{if } n \ne 0
        \end{cases}
    .\end{align*}
    \bigbreak \noindent 
    \begin{cppcode}
        int factorial(int n) {
            if (n == 0) return 1;
            return n * factorial(n-1);

            // Expands to 
            // n * n-1 * n-2 * ... * 1
        }
    \end{cppcode}

    \bigbreak \noindent 
    \subsubsection{Powers}
    \bigbreak \noindent 
    Consider the recursive definition for a power of $x$
    \bigbreak \noindent 
    \begin{align*}
        x^{n} = 
        \begin{cases}
            1 & \text{if } n=0 \\\
            x\cdot x^{n-1} &\text{if } n>0
        \end{cases}
    .\end{align*}
    \bigbreak \noindent 
    \begin{cppcode}
        constexpr int power(int x, int n) {
            if (n == 0) return 1;
            return x * power(x,n-1);
        }
    \end{cppcode}
    \bigbreak \noindent 
    \bigbreak \noindent 
    The function power() can be implemented differently, without using any recursion, as in the following loop:
    \bigbreak \noindent 
    \begin{cppcode}
        int power2(int x, int n) {
            int res = 1;

            for (res = x; n > 1; --n) {
                res*=x;
            }
            return res;
        }
    \end{cppcode}
    \bigbreak \noindent 
    Do we gain anything by using recursion instead of a loop? The recursive version
seems to be more intuitive because it is similar to the original definition of the power
function. The definition is simply expressed in C++ without losing the original structure of the definition. The recursive version increases program readability, improves
self-documentation, and simplifies coding. In our example, the code of the nonrecursive
version is not substantially larger than in the recursive version, but for most recursive
implementations, the code is shorter than it is in the nonrecursive implementations

    \pagebreak 
    \subsection{Tail recursion}
    \bigbreak \noindent 
    Tail recursion is a type of recursion where the recursive call is the last thing the function does before returning a result. This means there are no more computations or operations to perform after the recursive call.
    \bigbreak \noindent 
    Because of this, tail recursion can be optimized by some compilers or interpreters to avoid adding new frames to the call stack, making it more memory-efficient than regular recursion.
    \bigbreak \noindent 
    In simple terms, if a recursive function calls itself, and after that call there’s nothing left to do, it's tail recursion. This allows the function to reuse the same memory space, preventing stack overflow in cases with deep recursion.
    \bigbreak \noindent 
    the recursive call is not only the last statement but there are no earlier recursive calls, direct or indirect. For example, the function tail() defined as
    \bigbreak \noindent 
    \begin{cppcode}
        void tail(int i) {
            if (i > 0) {
                cout << i << '';
                tail(i-1);
            }
        }
    \end{cppcode}
    \bigbreak \noindent 
    Is an example of a function with tail recursion, whereas the function nonTail() defined as
    \bigbreak \noindent 
    \begin{cppcode}
        void nonTail(int i) {
            if (i > 0) {
                nonTail(i-1);
                cout << i << '';
                nonTail(i-1);
            }
        }
    \end{cppcode}
    \bigbreak \noindent 
    Is not. Tail recursion is simply a glorified loop and can be easily replaced by one. In this example, it is replaced by substituting a loop for the if statement and decrementing the variable i in accordance with the level of recursive call. In this way, tail() can be expressed by an iterative function:
    \bigbreak \noindent 
    \begin{cppcode}
        void iterativeEquivalentOfTail(int i) {
            for ( ; i > 0; i--)
            cout << i << '';
        }
    \end{cppcode}
    \bigbreak \noindent 
    Is there any advantage in using tail recursion over iteration? For languages such as C++, there may be no compelling advantage, but in a language such as Prolog, which has no explicit loop construct (loops are simulated by recursion), tail recursion acquires a much greater weight. In languages endowed with a loop or its equivalents, such as an if statement combined with a goto statement, tail recursion should not be used.
    \bigbreak \noindent 
    Another problem that can be implemented in recursion is printing an input line in reverse order. Here is a simple recursive implementation:
    \bigbreak \noindent 
    \begin{cppcode}
        void reverse() {
            char ch;
            cin.get(ch);
            if (ch != '\n') {
                reverse();
                cout.put(ch); 
            }
        }
    \end{cppcode}
    \bigbreak \noindent 
    Compare the recursive implementation with a nonrecursive version of the same function:
    \bigbreak \noindent 
    \begin{cppcode}
        void simpleIterativeReverse() {
            char stack[80];
            int top = 0;
            cin.getline(stack,80);
            for (top = strlen(stack) - 1; top >= 0; cout.put(stack[top--]));
        }
    \end{cppcode}
    \bigbreak \noindent 
    functions like strlen() and
    getline() from the standard C++ library can be used. If we are not supplied with
    such functions, then our iterative function has to be implemented differently:
    \bigbreak \noindent 
    \begin{cppcode}
        void iterativeReverse() {
            char stack[80];

            register int top = 0;
            cin.get(stack[top]);

            while(stack[top]!='\n') {
                cin.get(stack[++top]);
            }
            for (top -= 2; top >= 0; cout.put(stack[top--]));
        }
    \end{cppcode}

    \pagebreak 
    \subsection{Indirect Recursion}
    \bigbreak \noindent 
    The preceding sections discussed only direct recursion, where a function $f()$ called itself. However, $f()$ can call itself indirectly via a chain of other calls. For example, $f()$ can call $g()$, and $g()$ can call $f()$. This is the simplest case of indirect recursion. The chain of intermediate calls can be of an arbitrary length, as in:
    \begin{align*}
        f() \to f_{1}() \to f_{2}() \to ... \to f_{n}() \to f()
    .\end{align*}
    \bigbreak \noindent 
    There is also the situation when $f()$ can call itself indirectly through different chains. Thus, in addition to the chain just given, another chain might also be possible. For instance
    \begin{align*}
        f() \to g_{1}() \to g_{2}() \to ... \to g_{m}() \to f()
    .\end{align*}
    \bigbreak \noindent 
    This situation can be exemplified by three functions used for decoding information. receive() stores the incoming information in a buffer, decode() converts it into legible form, and store() stores it in a file. receive() fills the buffer and calls decode(), which in turn, after finishing its job, submits the buffer with decoded information to store(). After store() accomplishes its tasks, it calls receive() to intercept more encoded information using the same buffer. Therefore, we have the chain of calls
    \begin{align*}
        \text{recieve}() \to \text{decode}() \to \text{store}() \to \text{recieve}() \to \text{decode}() \to ...
    .\end{align*}
    \bigbreak \noindent 
    \pagebreak 
    \subsection{Nested Recursion}
    \bigbreak \noindent 
    A more complicated case of recursion is found in definitions in which a function is
    not only defined in terms of itself, but also is used as one of the parameters. The following definition is an example of such a nesting
    \begin{align*}
        h(n) = 
        \begin{cases}
            0 & \text{if } n = 0 \\
            n & \text{if } n > 4 \\
            h(2 + h(n)) & \text{if } n \leq 4
        \end{cases}
    .\end{align*}
    \bigbreak \noindent 
    \subsection{Excessive Recursion}
    \bigbreak \noindent 
    Logical simplicity and readability are used as an argument supporting the use of recursion. The price for using recursion is slowing down execution time and storing on
    the run-time stack more things than required in a nonrecursive approach. If recursion is too deep (for example, computing $5.6^{100,000}$), then we can run out of space on
the stack and our program crashes. But usually, the number of recursive calls is much
smaller than 100,000, so the danger of overflowing the stack may not be imminent
\bigbreak \noindent 
However, if some recursive function repeats the computations for some parameters, the run time can be prohibitively long even for very simple cases
\bigbreak \noindent 
Consider Fibonacci numbers. A sequence of Fibonacci numbers is defined as follows:
\begin{align*}
    \text{Fib}(n) =
    \begin{cases}
        n & \text{if } n < 2  \\
        \text{Fib}(n-2) + \text{Fib}(n-1) & \text{otherwise}
    \end{cases}
.\end{align*}
\bigbreak \noindent 
The definition states that if the first two numbers are 0 and 1, then any number in the
sequence is the sum of its two predecessors. But these predecessors are in turn sums
of their predecessors, and so on, to the beginning of the sequence. 
\bigbreak \noindent 
How can this definition be implemented in C++? It takes almost term-by-term
translation to have a recursive version, which is
\bigbreak \noindent 
\begin{cppcode}
constexpr unsigned long fib(int n) {
    if (n < 2) return n;
    return fib(n-2) + fib(n-1);
}
\end{cppcode}
\bigbreak \noindent 
The function is simple and easy to understand but extremely inefficient. To see
it, compute Fib(6), the seventh number of the sequence, which is 8. Based on the
definition, the computation runs as follows:
\begin{align*}
    Fib(6)	&= Fib(4) + Fib(5) \\
            &= Fib(2) 	+ 	Fib(3) 	+ Fib(5) \\
            &= Fib(0)+Fib(1) 	+ 	Fib(3) 	+ Fib(5) \\
            &= 0 + 1 	+ 	Fib(3) 	+ Fib(5) \\
            &= 1 	+ Fib(1)+ Fib(2) 	+ Fib(5) \\
            &= 1 	+ Fib(1)+Fib(0)+Fib(1)	+ Fib(5)   
.\end{align*}
Etc... The source of
this inefficiency is the repetition of the same calculations because the system forgets
what has already been calculated. For example, Fib() is called eight times with parameter n = 1 to decide that 1 can be returned. For each number of the sequence, the
function computes all its predecessors without taking into account that it suffices to do
this only once.
\bigbreak \noindent 
It takes almost a quarter of a million calls to find the twenty-sixth Fibonacci
number, and nearly 3 million calls to determine the thirty-first! This is too heavy a
price for the simplicity of the recursive algorithm. As the number of calls and the run
time grow exponentially with n, the algorithm has to be abandoned except for very
small numbers
\bigbreak \noindent 
An iterative algorithm may be produced rather easily as follows:
\bigbreak \noindent 
\begin{cppcode}
    unsigned long iterativeFib(unsigned long n) {
        if (n < 2)
        return n;
        else {
            register long i = 2, tmp, current = 1, last = 0;
            for ( ; i <= n; ++i) {
                tmp = current;
                current += last;
                last = tmp;
            }
            return current;
        }
    }
\end{cppcode}
\bigbreak \noindent 
However, there is another, numerical method for computing Fib(n), using a formula discovered by Abraham de Moivre:
\begin{align*}
    \text{Fib}(n) =  \frac{\phi^{n} - \hat{\phi}^{n}}{\sqrt{5}}
.\end{align*}
Where $\phi = \frac{1}{2}(1+\sqrt{5})$, and $\hat{\phi} = 1-\phi = \frac{1}{2}(1-\sqrt{5}) $. $\hat{\phi}$ becomes very small when $n$ grows, thus it can be omitted. 
\begin{align*}
    \text{Fib}(n) =\frac{\phi^{n}}{\sqrt{5}}
.\end{align*}
Approximated to the nearest integer
\bigbreak \noindent 
\begin{cppcode}
    unsigned long deMoivreFib(unsigned long n) {
        return ceil(exp(n*log(1.6180339897) - log(2.2360679775)) - .5);
    }
\end{cppcode}
\pagebreak 
\subsection{Backtracking}
\bigbreak \noindent 
In solving some problems, a situation arises where there are different ways leading
from a given position, none of them known to lead to a solution. After trying one path
unsuccessfully, we return to this crossroads and try to find a solution using another
path. However, we must ensure that such a return is possible and that all paths can
be tried. This technique is called backtracking, and it allows us to systematically try
all available avenues from a certain point after some of them lead to nowhere. Using
backtracking, we can always return to a position that offers other possibilities for successfully solving the problem. This technique is used in artificial intelligence, and one
of the problems in which backtracking is very useful is the eight queens problem.
\bigbreak \noindent 
The eight queens problem attempts to place eight queens on a chessboard in
such a way that no queen is attacking any other To solve this problem, we try to put the first
queen on the board, then the second so that it cannot take the first, then the third so
that it is not in conflict with the two already placed, and so on, until all of the queens
are placed. What happens if, for instance, the sixth queen cannot be placed in a nonconflicting position? We choose another position for the fifth queen and try again
with the sixth. If this does not work, the fifth queen is moved again. If all the possible
positions for the fifth queen have been tried, the fourth queen is moved and then the process restarts. This process requires a great deal of effort, most of which is spent
backtracking to the first crossroads offering some untried avenues. In terms of code,
however, the process is rather simple due to the power of recursion, which is a natural implementation of backtracking
\bigbreak \noindent 
\begin{cppcode}
    putQueen(row)
        for every position col on the same row
            if position col is available
                place the next queen in position col;
                if (row < 8)
                    putQueen(row+1);
                else success;
                remove the queen from position col;
\end{cppcode}
\bigbreak \noindent 
This algorithm finds all possible solutions without regard to the fact that some of
them are symmetrical.

\pagebreak 
Backtracking is a systematic approach to exploring all possible solutions to a problem using recursion. It involves:
\begin{itemize}
    \item \textbf{Recursive Exploration:} A recursive function is used to explore all potential solutions.
    \item \textbf{Pruning:} When a partial solution cannot lead to a valid answer, the algorithm "backtracks" by undoing the last decision and tries a different path.
    \item \textbf{State Restoration:} The state of the problem is restored before exploring a new path, ensuring no side effects from previous choices affect future decisions.
\end{itemize}
\bigbreak \noindent 
Backtracking often involves a loop because it systematically explores multiple options at each recursive step. The loop allows the algorithm to iterate over all possible choices available at a particular point. Without the loop, you would only be able to handle a single choice at each step, which would limit the ability to explore multiple branches of the problem space.
\bigbreak \noindent 
The loop ensures that each choice is explored one by one before moving to the next recursive call. After each recursive call, the algorithm backtracks and tries the next option.
\bigbreak \noindent 
Backtracking is essentially a tree traversal algorithm. The loop represents branching into different nodes of the tree. Without a loop, you'd only traverse a linear path, which isn't sufficient for exploring all solutions.
\bigbreak \noindent 
\subsubsection{N-Queens}
\bigbreak \noindent 
Assume we have already have an $8\times 8$ array to represent the chess board. At each spot in the board, a zero will indicate no queen, and a one will indicate a queen. We also assume we have a function to check if a position is safe. The only function left to implement is a \textit{place\_queen} function.
\bigbreak \noindent 
\begin{cppcode}
    bool place_queens(bool arr[][COLS], int row_n) { 
        // Base case: Placed a queen on all rows
        if (row_n >= ROWS)
        return true;

        // For each column in a row
        for (int j=0; j<=7; ++j) {
            // If the spot is safe
            if (safe(arr, row_n, j)) {
                // Place the queen
                arr[row_n][j] = true;
                // Check the row beneath
                if (place_queens(arr, row_n+1)) {
                    return true;
                }
                // If the row beneath cannot place a queen at any column, backtrack one row, place that queen in a different column
                arr[row_n][j] = false;
            }
        }
        // Queen could not be placed at any column
        return false;
    }
\end{cppcode}

\bigbreak \noindent 
\subsubsection{Greedy backtracking: Traveling salesman problem (TSP)}
\bigbreak \noindent 
Greedy backtracking is a variation of backtracking that incorporates greedy principles, attempting to make the "best" or "most promising" choice at each step rather than systematically exploring all possible choices. It combines the greedy approach (making locally optimal choices) with backtracking to handle situations where the greedy choice might fail.
\begin{itemize}
    \item \textbf{Greedy Choice:} At each step, a heuristic \footnote{proceeding to a solution by trial and error or by rules that are only loosely defined.} is used to choose the most promising option that seems likely to lead to a solution.
    \item \textbf{Backtracking for Correction:} If the greedy choice leads to a dead end, the algorithm backtracks and tries the next best option.
    \item \textbf{Efficiency:} By attempting the most promising options first, it often reduces the search space compared to regular backtracking.
\end{itemize}
\bigbreak \noindent 
\textbf{TSP}: The goal is to find the shortest path that visits all cities exactly once and returns to the starting city.
\begin{itemize}
    \item \textbf{Greedy Step:} At each step, visit the nearest unvisited city.
    \item \textbf{Backtracking:} If the current path does not lead to a valid solution, backtrack and try the next nearest city.
\end{itemize}
\bigbreak \noindent 
Assume we have a distance matrix
\bigbreak \noindent 
\begin{cppcode}
    vector<vector<double>> distances = {
        {0, 10, 15, 20},
        {10, 0, 35, 25},
        {15, 35, 0, 30},
        {20, 25, 30, 0}
    };
\end{cppcode}
\bigbreak \noindent 
The vector<vector<double>> distances represents a distance matrix for a graph where the nodes represent cities (or points of interest), and the entries in the matrix represent the distances between pairs of cities.
\bigbreak \noindent 
In the context of the Traveling Salesman Problem (TSP), this matrix provides the distances between any two cities in the graph.
\begin{itemize}
    \item \textbf{distances[i][j]:} Represents the distance between city $i$ and city $j$
    \item \textbf{Diagonal entries (distances[i][i]):} These are typically 0, because the distance from a city to itself is 0
\end{itemize}
\bigbreak \noindent 
Assume we also have a visited array, to mark the cities visited.
\bigbreak \noindent 
\begin{cppcode}
    vector<bool> visited(distances.size(), false);
\end{cppcode}
\bigbreak \noindent 
The solution function signature will be 
\bigbreak \noindent 
\begin{cppcode}
double tspGreedyBacktracking(vector<vector<double>>& distances, vector<bool>& visited, int currentCity, int citiesVisited, double currentCost, double& bestCost);
\end{cppcode}
\bigbreak \noindent 
\begin{itemize}
    \item \textbf{vector<vector<double>>\& distances:} Represents the distance matrix for the graph. Each entry distances[i][j] gives the distance from city $i$ to city $j$.
        \bigbreak \noindent 
        Passed by reference to avoid copying the matrix for each function call, which would be computationally expensive.
\item \textbf{vector<bool>\& visited:} A boolean vector where visited[i] indicates whether city $i$ has been visited. Ensures that each city is visited exactly once. Passed by reference to track visited cities across recursive calls.
\item \textbf{int currentCity:} The city the algorithm is currently processing. Determines the row in the distance matrix to explore potential next cities. Helps calculate the distance for traveling to other unvisited cities.
\item \textbf{int citiesVisited:} Tracks the number of cities visited so far in the current path. Used as a stopping condition: when citiesVisited == distances.size(), all cities have been visited.
\item \textbf{double currentCost:} The cumulative cost (distance) of the path taken so far. This is updated at each recursive step to include the distance of the last move.
\item \textbf{double\& bestCost:} Tracks the minimum cost (shortest path) found so far across all recursive paths. Passed by reference so that all recursive calls update the same variable. Used to prune paths: If the currentCost exceeds bestCost, the algorithm skips further exploration of that path.
\end{itemize}

\pagebreak 
\begin{cppcode}
    double tspGreedyBacktracking(vector<vector<double>>& distances, vector<bool>& visited, int currentCity, int citiesVisited, double currentCost, double& bestCost) {
        if (citiesVisited == visited.size()) {
            // Add the cost to return to the starting city
            return currentCost + distances[currentCity][0];
        }

        for (int nextCity = 0; nextCity < distances.size(); ++nextCity) {
            if (!visited[nextCity]) {
                visited[nextCity] = true;

                // Calculate the cost for this path
                double nextCost = currentCost + distances[currentCity][nextCity];
                if (nextCost < bestCost) { // Only proceed if this path is promising
                    bestCost = min(bestCost, tspGreedyBacktracking(distances, visited, nextCity, citiesVisited + 1, nextCost, bestCost));
                }

                visited[nextCity] = false; // Backtrack
            }
        }
        return bestCost;
    }
    int main() {
        vector<vector<double>> distances = {
            {0, 10, 15, 20},
            {10, 0, 35, 25},
            {15, 35, 0, 30},
            {20, 25, 30, 0}
        };
        vector<bool> visited(distances.size(), false);
        visited[0] = true;

        double bestCost = DBL_MAX;
        tspGreedyBacktracking(distances, visited, 0, 1, 0, bestCost);

        cout << "Minimum cost: " << bestCost << endl;
        return 0;
    }
\end{cppcode}
\bigbreak \noindent 
This function implements a backtracking approach to solve the Traveling Salesman Problem (TSP) with an optimization that leverages a greedy principle to prune unnecessary paths. The algorithm starts from a given city (currentCity) and recursively explores all possible paths to visit every other city exactly once. It maintains a visited array to track which cities have already been visited, ensuring that no city is revisited during the current path.
\bigbreak \noindent 
The function uses a loop to evaluate each unvisited city as the next possible destination. For each potential move, it calculates the cumulative cost of the current path (currentCost) and compares it with the best cost (bestCost) found so far. If the current path exceeds bestCost, it abandons further exploration of that branch. Otherwise, it marks the city as visited, proceeds recursively to explore the next cities, and updates the bestCost if a better solution is found. After processing, the function backtracks by unmarking the city, restoring the state for the next iteration.
\bigbreak \noindent 
When all cities have been visited (citiesVisited == visited.size()), the function completes the path by adding the cost to return to the starting city. This final cost is returned and compared with bestCost, ensuring that the shortest valid path is recorded. The combination of backtracking for completeness and greedy pruning for efficiency helps the algorithm systematically explore the search space while avoiding unnecessary computations.
\bigbreak \noindent 
It will begin by starting from the first city, then going $1 \to 2,\ 2 \to 3,\ 3 \to 4$. At this point, it will have a best cost for path one (65 in this case). It will then begin backtracking back to city one, enabling us to explore other paths. For example, $1 \to 3,\ 3 \to 2, ...$, only taking the path if it is promising (currentcost less than best cost). Once we have explored all options starting from city one, we return the best cost.

\pagebreak 
\subsubsection{Combinations}
\bigbreak \noindent 
To perform combinations using backtracking in C++, you can recursively explore subsets of elements. Here’s an example of how to generate all combinations of size $k$ from a given array or vector
\bigbreak \noindent 
\begin{cppcode}
    void generateCombinations(const std::vector<int>& nums, int k, int start, std::vector<int>& current, std::vector<std::vector<int>>& result) {
        if (current.size() == k) {
            result.push_back(current);
            return;
        }

        for (int i=start; i<nums.size(); ++i) {
            current.push_back(nums[i]);
            generateCombinations(nums, k, i+1, current, result);
            current.pop_back();
        }
    }
\end{cppcode}
\bigbreak \noindent 
The backtracking logic in this function generates combinations by systematically exploring subsets of the input array. The key idea is to build each combination step-by-step, adding elements to the current subset (current) until its size reaches the desired length $k$. At that point, the subset is complete and added to the result.
\bigbreak \noindent 
The function iterates through the array starting from the given index start. For each element, it adds the element to the current combination and recursively calls itself with the next index (i+1) to explore subsequent elements. This ensures that no element is reused and combinations are formed in a non-repeating manner.
\bigbreak \noindent 
Once the recursive call completes, the function removes the last element added (backtracking) to explore other possible combinations. This backtracking step is crucial, as it resets the state of current to allow the exploration of different subsets without interference from previous paths. This approach ensures all unique combinations of size $k$ are generated.

\pagebreak 
\bigbreak \noindent 
\subsubsection{Combinations that sum to $m$}
\bigbreak \noindent 
Expanding on the code above, we can add a simple check before we push the combination to the result vector, the check to see if the sum of the combination fits a requested sum.
\bigbreak \noindent 
\begin{cppcode}
    void generateCombinations(const std::vector<int>& nums, int k, int start, std::vector<int>& current, std::vector<std::vector<int>>& result, const int& sum, int& currentSum) {
        if (current.size() == k) {
            if (currentSum == sum)  {
                result.push_back(current);
            }
            return;
        }

        for (int i=start; i<nums.size(); ++i) {
            current.push_back(nums[i]);
            // Update the current sum
            currentSum += nums[i];

            generateCombinations(nums, k, i+1, current, result, sum, currentSum);

            // Decrease the current sum
            currentSum-=nums[i];
            current.pop_back();
        }
    }
\end{cppcode}

\pagebreak 
\subsubsection{Permutations}
\bigbreak \noindent 
To generate permutations, the key idea is to explore all possible arrangements of elements where the order matters. The typical approach involves swapping elements to produce different orders and backtracking to restore the original state before exploring further.
\bigbreak \noindent 
\begin{cppcode}
    void generatePermutations(std::vector<int>& nums, int start, std::vector<std::vector<int>>& result) {
        // Base case: if the starting index is at the end, we've formed a permutation
        if (start == nums.size()) {
            result.push_back(nums);
            return;
        }

        // Loop through the array to swap the current index with all subsequent indices
        for (int i = start; i < nums.size(); ++i) {
            // Swap the current element with the one at index `i`
            std::swap(nums[start], nums[i]);

            // Recur to generate permutations for the next index
            generatePermutations(nums, start + 1, result);

            // Backtrack: restore the original order
            std::swap(nums[start], nums[i]);
        }
    }
\end{cppcode}
\bigbreak \noindent 
\begin{itemize}
    \item The function starts with a start index, which determines the position being fixed for the current permutation.
    \item For every index from start to the end of the array, it swaps the element at start with the element at the current index (i) to create a new arrangement.
    \item It recursively generates permutations for the remaining portion of the array (start + 1 onward).
    \item After the recursive call, it swaps back to restore the original array (backtracking), ensuring other permutations can be explored without interference.
\end{itemize}

\pagebreak 
\subsection{Recursion in singly linked lists}
\bigbreak \noindent 
\subsubsection{Traversing}
\bigbreak \noindent 
To traverse a linked list using recursion, you need to define a recursive function that processes the current node and then calls itself with the next node until the list is fully traversed (i.e., until the current node is nullptr).
\bigbreak \noindent 
\begin{cppcode}
    void TraverseList(node* head) {
        if (!head) {
            return;
        }
        TraverseList(head->next);

        // ...
    }
\end{cppcode}
\bigbreak \noindent 
\subsubsection{Printing}
\bigbreak \noindent \bigbreak \noindent 
We can use this, for example, to print each nodes data member
\bigbreak \noindent 
\begin{cppcode}
    void PrintList(node* head) {
        if (!head) return;

        cout << head->data << " ";
        PrintList(head->next);
    }
\end{cppcode}

\bigbreak \noindent 
\subsubsection{Printing in reverse}
\bigbreak \noindent \bigbreak \noindent 
We a slight alter in the print example, we can reverse print the list. 
\bigbreak \noindent 
\begin{cppcode}
    void PrintListReverse(node* head) {
        if (!head) return;

        PrintListReverse(head->next);
        cout << head->data << " ";
    }
\end{cppcode}

\bigbreak \noindent 
\subsubsection{Getting the length}
\bigbreak \noindent 


\bigbreak \noindent 
\subsubsection{Clearing}
\bigbreak \noindent \bigbreak \noindent 
We can also use this to clear the list
\bigbreak \noindent 
\begin{cppcode}
    void clear() {
        std::function<void(node*)> r_clear = [&] (node* p) = {
            if (!head) return;

            r_clear(head->next);
            delete head;
        } 
        r_clear(head);
        head=nullptr;
        size=0;
    }
\end{cppcode}


\pagebreak 
\subsubsection{Reversing}
\bigbreak \noindent 
Let's first take a look at the reverse code
\bigbreak \noindent 
\begin{cppcode}
    void reverse() {
        std::function<void(node*)> r_reverse = [&] (node* p) -> void {
            if (!p->next) {
                head = p;
                return;
            }

            r_reverse(p->next);
            node* q = p->next;
            q->next = p;
            p->next = nullptr;

        };
        r_reverse(head);
    }
\end{cppcode}
\bigbreak \noindent 
The base case is that we are at the end, in this case we set head to this position. Head is now at the end of the list.
\bigbreak \noindent 
Once the base case is triggered and the head is set to the last node in the list, we will be sent back to the n-1 node call.
\bigbreak \noindent 
To get the intuition for linked list logic, we must examine a diagram of the list.
\bigbreak \noindent 
\begin{figure}[ht]
    \centering
    \incfig{diag}
    \label{fig:diag}
\end{figure}
\bigbreak \noindent 
This figure shows the three operations done after each recursive call. In the figure above, we are at the node after the call that set the end to head. We
\begin{enumerate}
    \item Get a pointer to the node ahead of the current $(q)$. 
    \item This allows us to severe its old next pointer and reverse its direction.
    \item Then, set $p$ next to nullptr (set up for next return).
\end{enumerate}
When the callstack returns to the first call, and does its operations, the list will be reversed

\bigbreak \noindent 
It is also a good idea to examine the iterative method.
\bigbreak \noindent 
\begin{cppcode}
    void Itreverse() {
        node* prev=nullptr, *curr=head, *next=nullptr;

        while (curr) {
            next=curr->next; // Move next to the next node
            curr->next=prev; // Change the direction of current nodes next pointer

            prev=curr; // Advance prev
            curr=next; // Advance curr
        }
        head=prev; // Prev is last node, set head to end
    }
\end{cppcode}

\pagebreak 
\subsubsection{Pushing}
\bigbreak \noindent 
\begin{cppcode}
    void push(int data) {
        if (!head) {
            head = new node(nullptr, data);
            return;
        }
        std::function<void(node*, int)> r_push = [&] (node* curr, int data) -> void {

            if (!curr->next) {
                curr->next = new node(nullptr, data);
                ++size;
                return;
            }
            r_push(curr->next, data);
        };
        r_push(head, data);
    }
\end{cppcode}
\bigbreak \noindent 
Base case:
\begin{enumerate}
    \item \textbf{Empty list}: No recursion, make head the new node
\end{enumerate}
\bigbreak \noindent 
Otherwise, recurse from the head until we get to the last node, simply set last nodes next pointer to new node and return

\pagebreak 
\subsubsection{Inserting}
\bigbreak \noindent 
\begin{cppcode}
    void insert(unsigned pos, int element) {
        std::function<void(node*&, unsigned)> r_insert = [&] (node*& p, unsigned curr_pos) {
            if (curr_pos == 0) {
                node* new_node = new node(nullptr, element);
                new_node->next = p;
                p = new_node;
                return;
            }
            r_insert(p->next,  curr_pos-1);
        };
        r_insert(head, pos);
    }
\end{cppcode}
\bigbreak \noindent 
Base case
\begin{enumerate}
    \item \textbf{Recursed the same number of times as the \texttt{pos} arg}: In this case, make a new node, set next to current node in the recursive traversal, set current node to new node.
\end{enumerate}
\bigbreak \noindent 
Otherwise, keep recursing, subtracting one from the curr\_pos.

\pagebreak 
\subsubsection{Popping}
\bigbreak \noindent 
\begin{cppcode}
    void pop() {
        if (!head) return;

        if (!head->next) {
            delete head;
            head=nullptr;
            return;
        }

        std::function<void(node*)> r_pop = [&] (node* p) -> void {
            if (!p->next->next) {
                delete p->next;
                p->next = nullptr;
                --size;
                return;
            }
            r_pop(p->next);
        };
        r_pop(head);
    }
\end{cppcode}
\bigbreak \noindent 
Base cases:
\begin{enumerate}
    \item \textbf{Empty list}: Noop
    \item \textbf{One node (head)}: Delete then reset head
\end{enumerate}
Otherwise, recurse until we are at the second to last node. Then, delete the second to last nodes next node, which is the last node. Set second to last nodes next pointer to nullptr.

\pagebreak 
\subsubsection{Erasing}
\bigbreak \noindent 
\begin{cppcode}
    ListNode* removeElements(ListNode*& head, const int& val) {
        if (!head) return nullptr;
        head->next = removeElements(head->next, val);

        if (head->val == val) {
            ListNode* tmp = head;
            head = head->next;
            delete tmp;
        }

        return head;
    }
\end{cppcode}
\bigbreak \noindent 
Base case:
\begin{enumerate}
    \item \textbf{Reached the end}: Return, start unwinding
\end{enumerate}
\bigbreak \noindent 
We traverse to the end of the list recursively, once we reach the end the recursion stops and we start unwinding the call stack, going backwards in the list.
\bigbreak \noindent 
For each node, we check if its data is equal to the element, if it is we set this node equal to its next node, then delete.

\pagebreak 
\subsubsection{Searching}
\bigbreak \noindent 
\begin{cppcode}
    node* search(int element) {
        std::function<node*(node*)> r_search = [&] (node* p) -> node* {
            if (p == nullptr)  {
                return nullptr;
            }
            if (p->data == element) {
                return p;
            }
            return r_search(p->next);
        };
        return r_search(head);
    }
\end{cppcode}
\bigbreak \noindent 
Base cases:
\begin{enumerate}
    \item \textbf{Reached the end of the list}: Element is not in list, return nullptr
    \item \textbf{Found the first node with the element}: Return the node
\end{enumerate}
\bigbreak \noindent 
Otherwise, recurse through the nodes until we hit one of the base cases.

\pagebreak 
\unsect{More on backtracking}
\bigbreak \noindent 
\subsection{(Power set) Subsets}
\bigbreak \noindent 
\begin{cppcode}
    void r_subsets(vector<int>& nums, vector<vector<int>>& res, vector<int>& subset, const int& i) {
        // Number of choices is the number of available elements
        if (i>=(int)nums.size()) {
            res.push_back(subset);
            return;
        }
        // Two descisions: Either place or don't place

        // Decision 1: Place
        subset.push_back(nums[i]);
        r_subsets(nums,res,subset,i+1);

        // Decision 2: Don't place
        subset.pop_back();
        r_subsets(nums,res,subset, i+1);
    }       
    vector<vector<int>> subsets(vector<int>& nums) {
        vector<vector<int>> res;
        vector<int> sub;
        r_subsets(nums,res,sub,0);
        return res;
    }
\end{cppcode}
\bigbreak \noindent 
Consider nums being the set $\{1,2,3\}$, the decision tree for the above algorithm would be
\bigbreak \noindent 
\fig{.5}{./figures/statetree1.png}
\bigbreak \noindent 
The leaves make up the solution set. Because there are $2^{n}$ subsets, and worse case the subset requires placing $n$ elements, the time complexity is $\Theta(n \cdot 2^{n})$.
\bigbreak \noindent 
\pagebreak 
\subsection{Power set with duplicates}
\bigbreak \noindent 
\begin{cppcode}
    class Solution {
        public:
        void r_subsets(vector<int>& nums, vector<vector<int>>& res, vector<int>& subset, int i) {
            if (i>=nums.size()) {
                res.push_back(subset);
                return;
            }

            subset.push_back(nums[i]);
            r_subsets(nums,res,subset, i+1);
            subset.pop_back();

            while ( ((i+1) < (int)nums.size()) && (nums[i] == nums[i+1])) {
                ++i;
            }
            r_subsets(nums,res,subset, i+1);
        }
        vector<vector<int>> subsetsWithDup(vector<int>& nums) {
            std::sort(nums.begin(), nums.end());
            vector<vector<int>> res;
            vector<int> subset;
            r_subsets(nums,res,subset,0);
            return res;
        }
    };
\end{cppcode}

\pagebreak \bigbreak \noindent 
\subsection{Generate permutations (Without swaps)}
\bigbreak \noindent 
\begin{cppcode}
    void r_generate_perms(vector<char>& v, vector<vector<char>>& res, vector<char>& perm, vector<bool>& used, const int& k) {
        // If the permutation size is k, push the result and return
        if ((int) perm.size() == k) {
            res.push_back(perm);
            return;
        }

        // Loop through the available items in v, start recursion at each item
        for (int i=0; i<(int)v.size(); ++i) {
            // If the item is not used
            if (!used[i]) {
                // Use it
                used[i] = true;
                perm.push_back(v[i]);
                // Recurse with updated current perm and used array
                r_generate_perms(v, res, perm, used, k);
                // Done generating the perm, reset used array and current permutation
                used[i] = false;
                perm.pop_back();
            }
        }
    }

    // Helper function to allow the use to only have to call with the input vector and k
    vector<vector<char>> generate_perms(vector<char>& v, const int& k) {
        vector<vector<char>> res; 
        vector<char> perm;
        vector<bool> used(v.size(), 0);
        r_generate_perms(v, res, perm, used, k);
        return res;
    }
\end{cppcode}

\pagebreak 
\bigbreak \noindent 
\subsection{Permutations with repetition}
\bigbreak \noindent 
To accomplish this, we can just drop the used vector
\bigbreak \noindent 
\begin{cppcode}
void _pwr(vector<vector<char>>& res, vector<char>& v, vector<char>& curr, int k) {
    if (curr.size() == k) {
        res.push_back(curr);
        return;
    }

    for (int i=0; i<(int)v.size(); ++i) {
        curr.push_back(v[i]);
        _pwr(res, v, curr, k);
        curr.pop_back();
    }
}

vector<vector<char>> pwr(vector<char> v, int k) {
    vector<vector<char>> res;
    vector<char> cur;
    _pwr(res, v, cur, k);
    return res;
}
\end{cppcode}
\bigbreak \noindent 
The function generates all possible sequences of length \( k \) from a set of \( n \) characters (where \( n \) is the size of the vector \texttt{v}), which amounts to \( n^k \) total sequences. At each leaf (base case), copying a sequence of length \( k \) takes \( O(k) \) time. Additionally, the recursive calls themselves contribute a constant amount of work at each node.
\bigbreak \noindent 
Overall, the time complexity is:
\[
O(n^k \times k)
\]
This is because you generate \( n^k \) sequences, and for each sequence, the work done (including copying) is proportional to \( k \).
\bigbreak \noindent 
The overall space complexity is dominated by the space needed to store all the generated permutations. If \( n \) is the size of the input vector \texttt{v}, then there are \( n^k \) sequences each of length \( k \), resulting in an output space of 
\[
O(n^k \times k).
\]
If we only consider the auxiliary space (i.e., the recursion stack and temporary storage), the maximum depth of the recursion is \( k \), so that part uses 
\[
O(k)
\]
space.
\bigbreak \noindent 
Thus, including the output, the space complexity is 
\[
O(n^k \times k),
\]
and excluding the output, it is 
\[
O(k).
\]

\pagebreak 
\subsection{Combinations with repetitions}
\bigbreak \noindent 
\begin{cppcode}
    void _cwr(vector<vector<char>>& res, vector<char>& v, vector<char>& curr, int k, int start) {
        if (curr.size() == k) {
            res.push_back(curr);
            return;
        }

        for (int i=start; i<(int)v.size(); ++i) {
            curr.push_back(v[i]);
            _cwr(res, v, curr, k, i);
            curr.pop_back();
        }
    }

    vector<vector<char>> cwr(vector<char> v, int k) {
        vector<vector<char>> res;
        vector<char> cur;
        _cwr(res, v, cur, k, 0);
        return res;
    }
\end{cppcode}
\bigbreak \noindent 
The time and space complexity is $O\left(\binom{n+k-1}{k} \cdot k \right)$, with auxiliary space $O(k)$

\pagebreak 
\subsection{Backtracking with pruning}
\bigbreak \noindent 
Pruning enhances backtracking by eliminating branches of the search tree that are guaranteed not to yield a valid solution, which makes the algorithm more efficient.
\bigbreak \noindent 
After each addition, the algorithm checks whether the candidate solution satisfies the necessary constraints. If it does, the algorithm proceeds further; if not, it “backtracks” by removing the last element and trying a different option.
\bigbreak \noindent 
Pruning can drastically reduce the number of candidates the algorithm needs to consider, which is especially beneficial in problems with a vast search space (such as the N-Queens problem, Sudoku, or permutation puzzles).










\pagebreak 
\unsect{The basics of Dynamic Programming (DP)}
\bigbreak \noindent 
Dynamic Programming is a problem-solving paradigm used to solve optimization and combinatorial problems efficiently by breaking them into overlapping subproblems, solving each subproblem once, and storing its result for reuse. This avoids redundant calculations, making DP particularly effective for problems with overlapping subproblems and optimal substructure
\subsection{Key Concepts of DP}
\bigbreak \noindent 
\begin{itemize}
    \item \textbf{Overlapping Subproblems:} Problems that can be broken down into smaller, overlapping subproblems. For example, the Fibonacci sequence involves repeatedly calculating the same terms, like $F(2)$, $F(3)$, etc., in a naive recursive approach.
    \item \textbf{Optimal Substructure:} A problem exhibits optimal substructure if the optimal solution of the problem can be composed from the optimal solutions of its subproblems. For example, in the shortest path problem, the shortest path from $A$ to $C$ passing through $B$ is the sum of the shortest path from $A$ to $B$ and $B$ to $C$.
\end{itemize}
\bigbreak \noindent 
\subsection{Bottom-up DP}
\bigbreak \noindent 
Bottom-up dynamic programming is a technique used to solve problems efficiently by breaking them into smaller subproblems, solving those subproblems iteratively (starting with the simplest cases), and building up solutions to the larger problems. It avoids the overhead of recursion by explicitly organizing computation in a tabular form, thereby reducing redundant calculations.
\bigbreak \noindent 
\subsubsection{Key Characteristics}
\bigbreak \noindent 
\begin{itemize}
    \item \textbf{Iterative Approach:} Unlike top-down dynamic programming (which uses recursion with memoization), bottom-up DP explicitly fills a table iteratively.
    \item \textbf{Tabulation:} A table (usually an array or a matrix) is used to store solutions to subproblems, so they can be reused when solving larger problems.
    \item \textbf{Base Cases First:} The base cases are explicitly calculated and stored in the table before computing other values.
    \item \textbf{No Recursion:} Since it avoids recursion, it typically has better space efficiency because it does not use a call stack.
\end{itemize}
\bigbreak \noindent 
\subsubsection{Steps for Bottom-Up DP}
\bigbreak \noindent 
\begin{itemize}
    \item \textbf{Define the State:} Determine what your subproblem represents. For example, $dp[i]$ might represent the solution to the problem for the first $i$ elements.
    \item \textbf{Base Case Initialization:} Set the initial values in your DP table based on the simplest cases of the problem.
    \item \textbf{Transition Relation:} Determine how to compute the value of a subproblem (e.g., $dp[i]$) based on previously computed subproblems (e.g., $dp[j]$ where $j<i$).
    \item \textbf{Fill the Table:} Use loops to iteratively compute all the entries in the DP table up to the desired solution.
    \item \textbf{Extract the Solution:} The final entry in the table often represents the solution to the overall problem.
\end{itemize}

\bigbreak \noindent 
\subsection{Bottom-up DP: Fibonacci}
\bigbreak \noindent 
The Fibonacci sequence is a classic example where DP can be applied. The sequence is defined as
\begin{align*}
    F(n) = F(n-1) + F(n-2), \quad F(0) = 0,\ F(1) = 1
.\end{align*}
\bigbreak \noindent 
\begin{cppcode}
    int fib(int n) {
        if (n == 0) return 0;
        if (n == 1) return 1;

        int dp[n+1];
        dp[0] = 0;
        dp[1] = 1;

        for (int i=2; i<=n; ++i) {
            dp[i] = dp[i-1] + dp[i-2];
        }
        return dp[n];
    }
\end{cppcode}

\bigbreak \noindent 
\subsection{Memoization (top down DP)}
\bigbreak \noindent 
Top-Down Dynamic Programming, also known as memoization, is a technique where a problem is solved recursively, but results of subproblems are stored (memoized) to avoid redundant calculations. This is particularly effective for problems with overlapping subproblems.
\bigbreak \noindent 
\begin{itemize}
    \item \textbf{Recursive Approach:} The problem is solved recursively by breaking it down into smaller subproblems.
        \bigbreak \noindent 
        The recursive function calls itself for these subproblems.
    \item \textbf{Memoization:} Subproblem solutions are stored in a data structure (e.g., an array, map, or dictionary).
        \bigbreak \noindent 
        If a subproblem has already been solved, its stored solution is reused, skipping further computation.
    \item \textbf{Base Cases:} Base cases are defined to stop recursion and provide known values.
    \item \textbf{Optimal Substructure:} The solution to a problem is built from solutions to its subproblems.
\end{itemize}

\bigbreak \noindent 
\subsection{Top-down DP: Fibonacci}
\bigbreak \noindent 
The Fibonacci sequence is defined as:
\begin{align*}
    F(n) = \begin{cases}
        0 & \text{if } n = 0 \\    
        1 & \text{if } n = 1    \\ 
        F(n-1) + F(n-2) &\text{if } n > 1
    \end{cases}
.\end{align*}
\bigbreak \noindent 
A simple recursive solution calculates $F(n)$ but does so inefficiently due to repeated calculations:
\bigbreak \noindent 
With memoization, we store already-computed Fibonacci values to avoid redundant work
\bigbreak \noindent 
\begin{cppcode}
    int fibonacci(int n, int memo[]) {
        // Base cases
        if (n <= 1) return n;

        // Check if the result is already computed
        if (memo[n] != -1) return memo[n];

        // Compute and store the result
        memo[n] = fibonacci(n - 1, memo) + fibonacci(n - 2, memo);
        return memo[n];
    }
\end{cppcode}

\pagebreak 
\unsect{Dynamic programming}
\bigbreak \noindent 
\subsection{LCS (longest common subsequence)}
\bigbreak \noindent 
Given two strings text1 and text2, return the length of their longest common subsequence. If there is no common subsequence, return 0.
\bigbreak \noindent 
A subsequence of a string is a new string generated from the original string with some characters (can be none) deleted without changing the relative order of the remaining characters.
\bigbreak \noindent 
For example, "ace" is a subsequence of "abcde".




\pagebreak 
\unsect{Binary trees}
\bigbreak \noindent 
\subsection{Terminology}
\bigbreak \noindent 
\begin{itemize}
    \item \textbf{Node:} The basic unit of a binary tree, containing data and references to left and right children.
    \item \textbf{Root:} The topmost node in a tree.
    \item \textbf{Child:} A node directly connected to another node when moving away from the root.
    \item \textbf{Descendants}: The descendants of a node are all nodes that come after a given node.
    \item \textbf{Parent:} The node directly above a child node.
    \item \textbf{Grandparents}: The grandparents of a node is all nodes above the parent up to the root.
    \item \textbf{Ancestors}: The ancestors of a node are all the nodes above a node up to the root
    \item \textbf{Leaf:} A node with no children.
    \item \textbf{Branch node}: A non-leaf node is called a branch node
    \item \textbf{Internal Node:} A branch node, a node with at least one child.
    \item \textbf{Subtree:} A tree consisting of a node and its descendants.
    \item \textbf{Height of a node:} The number of edges on the longest path from a node to a leaf.
    \item \textbf{Height of a tree:} The height of the tree is the height of the root
    \item \textbf{Depth:} The number of edges from the root to a node.
    \item \textbf{Depth of a tree}: The depth of a tree is the depth of the deepest node
    \item \textbf{Degree of a node}: The number of subtrees of a node is called the degree of the node. In a binary tree, all nodes have degree 0, 1, or 2.
    \item \textbf{Degree of a binary tree}:   The degree of a tree is the maximum degree of a node in the tree. A binary tree is degree 2.
\end{itemize}

\pagebreak 
\subsection{Type of binary trees}
\begin{itemize}
    \item \textbf{Full Binary Tree:} Every internal node has two children, all leaf nodes have zero children. Thus, all nodes are either zero or two, never one. 
    \item \textbf{Complete Binary Tree:} All levels, except possibly the last, are fully filled, and all nodes are as far left as possible.
    \item \textbf{Perfect Binary Tree:} A binary tree where all internal nodes have exactly 2 children, and all leaf nodes are at the same level.
    \item \textbf{Balanced Binary Tree:} A binary tree where the height of the left and right subtrees of every node differs by at most one.
    \item \textbf{Degenerate (or pathological) Tree:} A tree where each parent node has only one child, essentially forming a linked list.
    \item \textbf{Skewed Tree:} A special case of a degenerate tree, where all nodes are skewed to the left or right, forming a linear structure.
\end{itemize}

\pagebreak 
\subsection{Maximum height of a binary tree}
\bigbreak \noindent 
The maximum height of a binary tree with $n$ nodes can be as large as $n−1$ (in the case of a degenerate or skewed tree where each node has only one child). This is true for any binary tree:
\begin{align*}
    h_{\text{max}} = n - 1
.\end{align*}
\bigbreak \noindent 
Which occurs for degenerate trees.

\bigbreak \noindent 
\subsubsection{Minimum height of a binary tree}
\bigbreak \noindent 
The minimum height (best case) for a binary tree with $n$ nodes is achieved when the tree is perfectly balanced:
\begin{align*}
    h_{\text{min}} = \lfloor\log_{2}(n)\rfloor
.\end{align*}
\bigbreak \noindent 
This is because the tree would need to spread nodes evenly across levels

\bigbreak \noindent 
\subsubsection{Number of Leaves in a Binary Tree}
\bigbreak \noindent 
For any binary tree with $n$ nodes, the number of leaves $l$ satisfies the following relationship:
\begin{align*}
    l \leq \frac{n+1}{2}
.\end{align*}
\bigbreak \noindent 
This formula gives the maximum number of leaves, assuming that the tree is full (every internal node has 2 children).

\bigbreak \noindent 
\subsubsection{Relationship Between Internal Nodes and Leaves:}
\bigbreak \noindent 
In any binary tree, the number of internal nodes $i$ (nodes with at least one child) and the number of leaves $l$ are related as follows:
\begin{align*}
    i \leq l-1
.\end{align*}

\bigbreak \noindent 
\subsubsection{Maximum Number of Nodes at Height h}
\bigbreak \noindent 
The maximum number of nodes possible at a given height $h$ (where the height is counted from the root as level 0) in a binary tree is:
\begin{align*}
    \text{Max nodes at height $h$} = 2^{h}
.\end{align*}

\pagebreak 
\subsubsection{Number of Edges in a Binary Tree:}
\bigbreak \noindent 
For any binary tree with $n$ nodes, the number of edges $e$ is always
\begin{align*}
    e = n-1
.\end{align*}
\bigbreak \noindent 
This holds because every node (except the root) is connected to exactly one parent, so there are $n−1$ edges in the tree.

\pagebreak 
\subsection{Full trees}
\bigbreak \noindent 
A full tree is a tree where all internal nodes are degree two, and all leaf nodes are degree zero. Observe
\bigbreak \noindent 
\begin{figure}[ht]
    \centering
    \incfig{fulltree1}
    \label{fig:fulltree1}
\end{figure}
\bigbreak \noindent 
The next three subsections refer to the \textit{full binary tree theorem}, which states for a nonempty, full tree $T$ 
\bigbreak \noindent 
\subsubsection{Number of leaves}
\bigbreak \noindent 
If $T$ has $I$ internal nodes, the number of leaves is given by
\begin{align*}
   L = I + 1 
.\end{align*}
\bigbreak \noindent 
If $T$ has a total of $N$ nodes, the number of leaves is 
\begin{align*}
    L = \frac{N+1}{2}
.\end{align*}
\pagebreak 
\subsubsection{Number of nodes}
\bigbreak \noindent 
If $T$ has $I$ internal nodes, the total number of nodes is 
\begin{align*}
    N = 2I + 1
.\end{align*}
\bigbreak \noindent 
If $T$ has $L$ leaves, the total number of nodes is 
\begin{align*}
    N = 2L - 1
.\end{align*}
\bigbreak \noindent 
\subsubsection{Number of internal nodes}
\bigbreak \noindent 
If $T$ has a total of $N$ nodes, the number of internal nodes is 
\begin{align*}
    I = \frac{N-1}{2}
.\end{align*}
\bigbreak \noindent 
If $T$ has $L$ leaves, the number of internal nodes is 
\begin{align*}
    I = L - 1
.\end{align*}

\pagebreak 
\subsection{Complete Binary Tree}
\bigbreak \noindent 
A complete binary tree has a specific structure defined by how the nodes are filled level by level.
\begin{enumerate}
    \item \textbf{All levels, except possibly the last, are fully filled:}
        \begin{itemize}
            \item In a complete binary tree, every level up to the second-to-last (penultimate) level must be completely filled with nodes.
            \item This means that if the tree has height $h$, levels $0$ through $h - 1$ (from the root to the second-to-last level) will have the maximum possible number of nodes for that level.
        \end{itemize}
        \item \textbf{All nodes are as far left as possible:}
            \begin{itemize}
                \item On the last level, the nodes don't need to completely fill the level, but the nodes must be positioned as far to the left as possible.
                \item For example, if some nodes are missing from the last level, they will always be missing from the right side, not from the left.
            \end{itemize}
\end{enumerate}
\bigbreak \noindent 
\textbf{Notes:} The tree is balanced in terms of node distribution, with all the levels except possibly the last fully filled.
\bigbreak \noindent 
Nodes on the last level are always added from the leftmost position first.

\bigbreak \noindent 
\subsubsection{Number of nodes}
\bigbreak \noindent 
The height $h$ of a complete binary tree is defined as the number of edges on the longest path from the root to a leaf node.
\bigbreak \noindent 
The total number of nodes in a complete binary tree is given by
\begin{align*}
    n = 2^{h+1} - 1
.\end{align*}
\bigbreak \noindent 
\subsubsection{Height}
\bigbreak \noindent 
The height $h$ of a complete binary tree with $n$ nodes can be derived as:
\begin{align*}
   h = \lfloor \log_{2}(n) \rfloor
.\end{align*}

\bigbreak \noindent 
\subsubsection{Number of Leaf Nodes (L) in a Complete Binary Tree}
\bigbreak \noindent 
The number of leaf nodes in a complete binary tree can be calculated based on the number of internal nodes or the height of the tree
\begin{align*}
    L = \lceil \frac{n}{2} \rceil
.\end{align*}
\bigbreak \noindent 
\subsubsection{Number of internal nodes}
\bigbreak \noindent 
The number of internal nodes (non-leaf nodes) in a complete binary tree can be calculated as:
\begin{align*}
    I &= N - L \\
    I &= \lfloor \frac{n}{2} \rfloor
.\end{align*}

\bigbreak \noindent 
\subsubsection{Parent and Child Relationships in a Complete Binary Tree}
\bigbreak \noindent 
Parent of node at index $i$ (1-based index):
\[
\text{Parent}(i) = \left\lfloor \frac{i}{2} \right\rfloor
\]
Left child of node at index $i$:
\[
\text{Left child}(i) = 2i
\]
Right child of node at index $i$:
\[
\text{Right child}(i) = 2i + 1
\]
These relationships assume a 1-based indexing system for the nodes in the tree (common in heaps or array-based representations).

\pagebreak 
\subsection{Perfect binary tree}
\bigbreak \noindent 
\subsubsection{Number of Nodes}
\bigbreak \noindent 
\begin{align*}
    N = 2^{h+1} - 1
.\end{align*}

\bigbreak \noindent 
\subsubsection{Number of Leaf Nodes}
\bigbreak \noindent 
\begin{align*}
 L = 2^h
.\end{align*}

\bigbreak \noindent 
\subsubsection{Height of the Tree}
\bigbreak \noindent 
\begin{align*}
    h = \log_2(N+1) - 1
.\end{align*}


\bigbreak \noindent 
\subsubsection{Number of Internal Nodes}
\bigbreak \noindent 
\begin{align*}
    I = N - L = 2^h - 1
.\end{align*}

\bigbreak \noindent 
\subsubsection{Depth}
\bigbreak \noindent 
\begin{align*}
    d = h
.\end{align*}

\pagebreak 
\subsection{Invert a binary tree}
\bigbreak \noindent 
To invert a binary tree about a root is to flip the binary tree about the $y$-axis (with the given root as the pivot)
\bigbreak \noindent 
\fig{.5}{./figures/1.jpg}
\fc{Binary tree inverted about the root 4}
\bigbreak \noindent 
\subsubsection{Swap node helper method}
\bigbreak \noindent 
\begin{cppcode}
    static void swapNode(TreeNode*& u, TreeNode*& v) {
        v = std::exchange(u,v);
    }
\end{cppcode}
\bigbreak \noindent 
\subsubsection{Level order (BFS) solution}
\bigbreak \noindent 
\begin{cppcode}
    TreeNode* invertTree(TreeNode* root) {
        if (!root) return nullptr;
        queue<TreeNode*> q;
        q.push(root);

        while (!q.empty()) {
            TreeNode* curr = q.front(); q.pop();
            if (!curr) continue;

            TreeNode*& left = curr->left, *&right = curr->right;

            swapNode(left, right);

            if (left) q.push(left);
            if (right) q.push(right);
        }
        return root;
    }
\end{cppcode}

\bigbreak \noindent 
\subsubsection{Recursive postorder (DFS)}
\bigbreak \noindent 
\begin{cppcode}
    TreeNode* invertTree(TreeNode* root) {
        if (!root) return nullptr;

        invertTree(root->left);
        invertTree(root->right);

        swapNode(root->left, root->right);
        return root;
    }
\end{cppcode}


\pagebreak 
\unsect{Applications of binary trees}
\bigbreak \noindent 
\subsection{Binary search trees}
\bigbreak \noindent 
A binary search tree (BST) is a binary tree in which each node has at most two children and follows these properties:
\begin{itemize}
    \item \textbf{Left Subtree Property:} The value of each node in the left subtree is less than the value of the node itself.
    \item \textbf{Right Subtree Property:} The value of each node in the right subtree is greater than the value of the node itself.
    \item Both left and right subtrees must also be binary search trees.
\end{itemize}
\bigbreak \noindent 
\subsubsection{Interface}
\bigbreak \noindent 
The interface of a Binary Search Tree (BST) typically includes a set of operations for managing and accessing the tree’s nodes.
\begin{itemize}
    \item \textbf{Insert(value):} Inserts a new value into the BST while maintaining its properties.
    \item \textbf{Remove(value):} Removes a value from the BST, adjusting the structure to maintain its properties.
    \item \textbf{Predecessor(node)}: Finds the predecessor of a node
    \item \textbf{Succesor(node)}: Finds the successor of a node
    \item \textbf{Find(value):} Searches for a value in the BST and returns the node containing it or null if not found.
    \item \textbf{FindMin():} Returns the node with the smallest value in the BST.
    \item \textbf{FindMax():} Returns the node with the largest value in the BST.
    \item \textbf{IsEmpty():} Checks if the BST is empty.
    \item \textbf{Traverse(order):} Traverses the tree in a specific order (e.g., in-order, pre-order, post-order).
    \item \textbf{Height():} Returns the height of the BST.
    \item \textbf{Clear():} Removes all nodes from the tree, making it empty
\end{itemize}

\pagebreak 
\subsubsection{Traversals}
\bigbreak \noindent 
We can traverse BST's in one of four ways
\begin{itemize}
    \item Level order
    \item Preorder
    \item Inorder
    \item Postorder
\end{itemize}
\bigbreak \noindent 
\paragraph{Level order}
\bigbreak \noindent \bigbreak \noindent 
Level-order traversal is a way of visiting all the nodes in a binary tree by levels, from top to bottom. It starts at the root and visits nodes level by level, left to right, for each level.
\begin{itemize}
    \item Start with the root node (the topmost node).
    \item Visit all the nodes on the next level (children of the root) from left to right.
    \item Then, visit all nodes on the level below that (grandchildren of the root) from left to right, and so on.
\end{itemize}
A queue is often used to implement level-order traversal, as it helps keep track of nodes to visit in the correct order.
\bigbreak \noindent 
\begin{cppcode}
    void levelorderPrint() {
        if (!root) return; // noop for empty tree

        queue<node*> q;
        q.push(root);

        while (!q.empty()) {
            node* curr = q.front();
            q.pop();

            cout << curr->data << endl;
            if (curr->left) {
                q.push(curr->left);
            }
            if (curr->right) {
                q.push(curr->right);
            }
        }
    }
\end{cppcode}
\begin{enumerate}
    \item If the list is nonempty, construct a queue and push the root node.
    \item While the queue is nonempty, grab the front, process the front, pop the front.
    \item Push left and right nodes to queue, if they exist.
\end{enumerate}

\bigbreak \noindent 
\pagebreak \bigbreak \noindent 
\paragraph{Preorder}
\bigbreak \noindent \bigbreak \noindent 
Pre-order traversal is a way of visiting nodes in a binary tree where you:
\begin{enumerate}
    \item Visit the root node first.
    \item Recursively visit the left subtree.
    \item Recursively visit the right subtree.
\end{enumerate}
To explain simply:
\begin{enumerate}
    \item Start with the root node.
    \item Go as far left as possible, visiting each node along the way.
    \item Once you've reached the end of the left subtree, backtrack and visit the right subtree.
\end{enumerate}
\bigbreak \noindent 
\begin{cppcode}
    void preorderPrint() {
        std::function<void(node*)> r_preorderPrint = [&] (node* p) {
            if (p == nullptr) return;

            cout << p->data << endl;
            r_preorderPrint(p->left);
            r_preorderPrint(p->right);
        };
        r_preorderPrint(root);
    }
\end{cppcode}

\bigbreak \noindent 
\paragraph{Inorder}
\bigbreak \noindent\bigbreak \noindent  
in-order traversal is a way of visiting nodes in a binary tree where you:
\begin{enumerate}
    \item Recursively visit the left subtree first.
    \item Visit the root node.
    \item Recursively visit the right subtree.
\end{enumerate}
To explain simply:
\begin{enumerate}
    \item Start by going all the way to the left, visiting nodes along the way.
    \item Once you reach the leftmost node, visit it, then move up to its parent (the root).
    \item After visiting the root, visit the right subtree.
\end{enumerate}
\bigbreak \noindent 
For a BST, printing the tree with an inorder traversal yields a sorted sequence.
\bigbreak \noindent 
\begin{cppcode}
    void inorderPrint() {
        std::function<void(node*)> r_inorderPrint = [&] (node* p) -> void {
            if (!p) return;

            r_inorderPrint(p->left);
            cout << p->data << endl;
            r_inorderPrint(p->right);
        };
        r_inorderPrint(root);
    }
\end{cppcode}

\pagebreak 
\paragraph{Postorder}
\bigbreak \noindent \bigbreak \noindent 
Post-order traversal is a way of visiting nodes in a binary tree where you:
\begin{enumerate}
    \item Recursively visit the left subtree first.
    \item Recursively visit the right subtree.
    \item Finally, visit the root node.
\end{enumerate}
To explain simply:
\begin{enumerate}
    \item Start by going to the leftmost node, but don't visit it yet.
    \item Then, go to the right subtree and process it.
    \item After both subtrees have been visited, visit the root.
\end{enumerate}
\bigbreak \noindent 
\begin{cppcode}
    void postorderPrint() {
        std::function<void(node*)> r_postorderPrint = [&] (node* p) -> void {
            if (!p) return;

            r_postorderPrint(p->left);
            r_postorderPrint(p->right);
            cout << p->data << endl;
        };
        r_postorderPrint(root);
    }
\end{cppcode}

\bigbreak \noindent 
\subsubsection{Successor of a node}
\bigbreak \noindent 
The successor of a node is defined mathematically as
\begin{align*}
    \text{succ}(X) = \text{min}\{A:\ A > X\}
.\end{align*}
\bigbreak \noindent 
thus, we find the set of all nodes that have values greater than that of $X$, then find the minimum in that set.
\bigbreak \noindent 
By properties of binary search trees we find the successor of a node $X$ by 
\begin{enumerate}
    \item \textbf{If $X$ has a right child:} The successor is the leftmost node in the right subtree of $X$ (the smallest node in the right subtree).
    \item \textbf{If $X$ has no right child}:
        \begin{itemize}
            \item If the node is the left child of its parent, then the parent is its successor.
            \item If the node is the right child of its parent, you move upward until you find a node that is the left child of its parent, and that parent is the successor.
        \end{itemize}
\end{enumerate}

\pagebreak 
\subsubsection{Predecessor}
\bigbreak \noindent 
The predecessor of a node $X$ is defined as
\begin{align*}
    \text{pred}(X) = \text{max}\{A:\ A < X\}
.\end{align*}
\bigbreak \noindent 
In other words it is the largest node that is less than $X$. To find the predecessor:
\begin{enumerate}
    \item \textbf{If $X$ has a left child}: The predecssor is the rightmost node in the left subtree
    \item \textbf{If $X$ has no left child}: The predecessor is the nearest ancestor for which the node is in the right subtree.
\end{enumerate}

\bigbreak \noindent 
\subsubsection{The node}
\bigbreak \noindent 
The node is similar to a linked list node, but instead of a single next pointer, it has two. A left pointer and a right pointer.
\bigbreak \noindent 
\begin{cppcode}
    struct node{
        node* left = nullptr;
        node* right = nullptr;
        int data = 0;

        node() = default;
        node(int data) : data(data) {}
        node(node* left, node* right, int data) : left(left), right(right), data(data) {}
    };
\end{cppcode}

\bigbreak \noindent 
\subsubsection{The class}
\bigbreak \noindent 
For simplicity, we often define the Binary Search Tree (BST) as a class. This allows each instance of the class to hold its own root node, along with other data members such as the size of the tree, that we may need. \bigbreak \noindent If it were not a class, then each function would need to take the root node as an argument and return the (potentially modified) root node to maintain the structure."
\bigbreak \noindent 
If it were not a class, than each function would have to take as an argument a root node, and return the root node to maintain the structure
\bigbreak \noindent 
\begin{cppcode}
    class BST {
    private:
        node* root;
        ...
    public:
        ...
    };
\end{cppcode}

\pagebreak 
\subsubsection{Recursive Insertion}
\bigbreak \noindent 
Because of the nature of  BSt's, we often use recursion to define the needed operations.
\bigbreak \noindent 
\begin{cppcode}
    void insert(int element)  {
        // If the tree is empty, insert new element as root
        if (!root) {
            root = new node(element);
            return;
        }

        std::function<void(node*)> r_insert = [&](node* p) -> void {

            // If the element is less than current node, and p->left exists, go left
            if (element < p->data && p->left) {
                r_insert(p->left);

                // If the element is greater than current node, and p->right exists, go right
            } else if (element > p->data && p->right) {
                r_insert(p->right);
            }

            // If the element is less than current node, and p->left doesn't exist, insert node as current nodes left child
            if (element < p->data && !p->left) {
                p->left = new node(element);
                return;

                // If the element is greater than current node, and p->right doesn't exist, insert node as current nodes right child
            } else if (element > p->data && !p->right) {
                p->right = new node(element);
                return;
            }
        };
        // Start recursion from the root
        r_insert(root);
    }
\end{cppcode}
\bigbreak \noindent 
If the tree is empty, it creates a new root node with the given element.
\bigbreak \noindent 
Otherwise, it uses a recursive lambda function (r\_insert) to:
\begin{itemize}
    \item Traverse the tree: going left if the element is smaller, or right if the element is larger.
    \item Once it finds an appropriate spot (where a left or right child doesn't exist), it inserts the new node as a left or right child accordingly.
\end{itemize}
The process starts from the root and recursively finds the right place to insert the new element.

\pagebreak 
\subsubsection{A better recursive insert}
\bigbreak \noindent 
\begin{cppcode}
    node* r_insertC(node* p, int element) {
        if (!p) return new node(element);

        if (element < p->data) {
            p->left =  r_insertC(p->left,element);
        } else if (element > p->data) {
            p->right =  r_insertC(p->right, element);
        }
        return p;
    }

    void insertC(int element) {
        if (!root) root = new node(element);
        r_insertC(root,element);
    }
\end{cppcode}
\bigbreak \noindent 
The main insertion function, insertC, initiates the process. If the tree’s root is null, meaning the tree is empty, it creates a new root node with the given element. Otherwise, it calls the recursive helper function r\_insertC on the root to handle the insertion process.
\bigbreak \noindent 
The r\_insertC function operates recursively to find the correct location for the new element within the tree. Starting from the given node p, it checks whether p is null; if so, it creates and returns a new node with the specified element, making this node the new leaf of the tree at this position. If p is not null, the function compares the element to p->data. If the element is smaller, the function recursively calls r\_insertC on p->left to continue searching in the left subtree, and if the element is larger, it calls r\_insertC on p->right to search in the right subtree. After setting the appropriate child link, it returns the current node p, maintaining the correct structure of the tree at each level of recursion. This ensures the BST properties are preserved, with each node’s left subtree containing values less than the node’s data and the right subtree containing values greater.
\bigbreak \noindent 
Left as an exercise to the reader to see why we must return $p$ to maintain the tree pointer chain.

\pagebreak 
\subsubsection{Iterative insert}
\bigbreak \noindent 
\begin{cppcode}
    void insertB(int element) {
        if (!root) {
            root = new node(element);
            return;
        }

        node* p = root, *trail = nullptr;
        bool left;

        while (p) {
            trail = p;
            if (element < p->data) {
                p=p->left;
                left=true;
            } else if (element > p->data) {
                p=p->right;
                left=false;
            } else {
                return; // noop if already exists
            }
        }
        if (left) {
            trail->left = new node(element);
        } else {
            trail->right = new node(element);
        }
    }
\end{cppcode}
\bigbreak \noindent 
If the tree is empty, it creates a new root node with the element.
\bigbreak \noindent 
It then iteratively traverses the tree starting from the root:
\begin{itemize}
    \item Moves left if the element is smaller than the current node's data.
    \item Moves right if the element is larger.
    \item If the element already exists, it does nothing and returns.
\end{itemize}
Once it finds an empty spot (either left or right child is nullptr), it inserts the new node as the left or right child of the parent node (trail), depending on the comparison.

\pagebreak 
\subsubsection{Recursive removing}
\bigbreak \noindent 
To remove a node with a given value from a BST, there are three cases
\begin{enumerate}
    \item Node has no children
    \item Node has one child
    \item Noe has two children
\end{enumerate}
For case I, we can simply set the nodes parent to nullptr, and then delete the node.
\bigbreak \noindent 
For case II, we must divert the connection from the nodes parent to the nodes child, and then free the node.
\bigbreak \noindent 
Case III is more involved, we first must find the successor of the node. Once we find the successor, we replace the nodes data value with its successor. Then, instead of deleting the node, we delete its successor. Since to be in this case the node must have exactly two children, the successor is found in the simple way.
\begin{enumerate}
    \item Go right once
    \item Go as far left as possible.
\end{enumerate}
Once we have the successor node, it will either have no children, or exactly one child (a right child), if it were to have a left child, it would not be the true successor because we would have not gone as far left as possible.
\pagebreak \bigbreak \noindent 
\begin{cppcode}
    void remove(int element) {
        if (!root) return; // Noop for empty tree

        std::function<void(node*&, node*&)> r_remove = [&] (node*& p, node*& last) -> void {
            if (!p) return; // Not found in tree

            if (element < p->data) {
                r_remove(p->left, p);
            } else if (element > p->data) {
                r_remove(p->right, p);
            } else { // Found
                // Case I: Node has zero children 
                if (!p->left && !p->right) {
                    node* tmp = p;
                    p=nullptr;
                    delete tmp;
                    // Case II: Node has one child (note the use of xor)
                } else if (!p->left ^ !p->right) {
                    node* tmp = p;
                    p = (p->left ? p->left : p->right);
                    delete tmp;
                    // Case III: Two children
                } else {    
                    node* successor = p->right;
                    node* successorParent = p;

                    // Find the in-order successor 
                    while (successor->left) {
                        successorParent = successor;
                        successor = successor->left;
                    }

                    // Replace nodes value with successor value
                    p->data = successor->data;

                    // Now we need to delete the successor node
                    // The successor is a leaf or has a right child
                    if (successorParent->left == successor) {
                        successorParent->left = successor->right; 
                    } else {
                        successorParent->right = successor->right; 
                    }
                    delete successor;
                }
            }
        };
        r_remove(root,root);
    }
    \end{cppcode}
    \pagebreak 
    \subsubsection{Clearing}
    \bigbreak \noindent 
    \begin{cppcode}
        void clear() {
            if (!root) return;

            std::function<void(node*)> r_clear = [&](node* p) -> void {
                if (!p) return;

                r_clear(p->left);
                r_clear(p->right);

                delete p;
            };
            r_clear(root);
            root = nullptr;
        }
    \end{cppcode}
    \bigbreak \noindent 
    This function deletes all nodes in a binary search tree. It recursively traverses the tree, deleting each node after its children have been deleted, and finally sets the root to nullptr, effectively clearing the entire tree



    \pagebreak 
    \subsubsection{Counting the height of the tree (root)}
    \bigbreak \noindent 
    \begin{cppcode}
        size_t height() {
            std::function<size_t(node*)> r_height = [&](node* p) -> size_t {
                // Base case height of a nullptr is zero
                if (!p) return 0;
                return 1+std::max(r_height(p->left), r_height(p->right));
            };
            // Height is counting edges, so its number nodes in longest path from root to leaf - 1
            return r_height(root) -1; 
        }
    \end{cppcode}
    \bigbreak \noindent 
    This code defines a height() function that calculates the height of a binary tree by counting the edges. It uses a recursive lambda function r\_height to traverse the tree. For each node, it returns 1 + max(left subtree height, right subtree height) to find the longest path from the root to any leaf. Since r\_height counts nodes, the function subtracts 1 at the end to convert the node count to edge count, which is the definition of height.

    \bigbreak \noindent 
    \subsubsection{Counting the height of a node}
    \bigbreak \noindent 
    \begin{cppcode}
    int getHeight(node* p) {
        if (!p) return -1;
        return 1 + std::max(getHeight(p->left), getHeight(p->right));
    }
    \end{cppcode}
    \bigbreak \noindent 
    If height counts edges, then a nullptr nodes must return -1. If height counts vertices, then nullptr nodes must return 0.


    \pagebreak 
    \subsubsection{Getting the depth of the node}
    \bigbreak \noindent 
    \begin{cppcode}
        int nodeDepth(node* p) {
            if (p == root) return 1;
            return r_nodeDepth(root, p, 1);
        }

        int r_nodeDepth(node* curr, node* p, int depth) {
            if (curr == nullptr) {
                return -1;  // Node not found
            }
            if (p == curr) {
                return depth;  // Node found, return the depth
            }

            // Recursively search in the left subtree if p's data is smaller
            if (p->data < curr->data) {
                return r_nodeDepth(curr->left, p, depth + 1);
            }
            // Recursively search in the right subtree if p's data is greater
            if (p->data > curr->data) {
                return r_nodeDepth(curr->right, p, depth + 1);
            }

            return -1;  // This should never be reached if the tree is valid
        }
    \end{cppcode}
    \bigbreak \noindent 
    This code defines two functions that work together to calculate the depth of a given node p in a binary search tree (BST):
    \bigbreak \noindent 
    \textbf{nodeDepth:}
    This function is the entry point to calculate the depth of the node $p$.
    \bigbreak \noindent 
    It first checks if $p$ is the root node. If so, it returns 1 since the root node is considered to have a depth of 1.
    \bigbreak \noindent 
    If $p$ is not the root, it calls the helper function r\_nodeDepth to recursively search for the node, starting from the root with an initial depth of 1.
    \bigbreak \noindent 
    \textbf{r\_nodeDepth:}
    This is a recursive helper function that searches for the node $p$ in the tree, while tracking the current depth.
    \bigbreak \noindent 
    It first checks if curr (the current node in the search) is nullptr, which indicates that the node $p$ is not in the tree. In that case, it returns -1.
    \bigbreak \noindent 
    If curr matches $p$, it returns the current depth.
    \bigbreak \noindent 
    Otherwise, it recursively searches in the left or right subtree, depending on whether $p$'s data is less than or greater than the current node's data, and increments the depth by 1 at each recursive step.

    \pagebreak 
    \subsubsection{Counting the number of nodes}
    \bigbreak \noindent 
    \begin{cppcode}
        template <typename NODE>
        int count(NODE * root) {
            if (!root) return 0;

            return 1 + count(root->left) + count(root->right);
        }

    \end{cppcode}

    \bigbreak \noindent 
    \subsubsection{Comparison traversals}
    \bigbreak \noindent 
    In this section we show a typical recursive algorithm to compare two binary search trees via an inorder traversal.
    \bigbreak \noindent 
    \begin{cppcode}
        // Node compare
        bool nc(node* p, node* q) {
            return p->data == q->data;
        }

        bool r_inorderComp(node* p, node* q) {
            if (!p && !q) return true;
            if (!p || !q) return false;

            if (!r_inorderComp(p->left, q->left)) return false;
            if (!nc(p,q)) return false;
            if (!r_inorderComp(p->right, q->right)) return false;

            return true;

        }

        bool inorderComp(tree& t1, tree& t2) {

            if (!t1.root && !t2.root) return true;
            if (!t1.root || !t2.root) return false;

            return r_inorderComp(t1.root, t2.root);
        }
    \end{cppcode}
    \bigbreak \noindent 
    The core of the comparison is performed by two functions, nc and r\_inorderComp, which are used to check if each corresponding node in the two trees holds the same data value and is structured identically.
    \bigbreak \noindent 
    The nc function is a helper that takes two nodes as parameters and simply compares their data values, returning true if the values are equal and false otherwise.
    \bigbreak \noindent 
    The function r\_inorderComp performs a recursive, in-order traversal of two nodes, p and q. If both nodes are nullptr, it means that this position in both trees is empty, so they match and the function returns true. If one node is nullptr and the other is not, it indicates a structural mismatch, so the function returns false. The recursion first compares the left child nodes, then the current nodes themselves using nc, and finally the right child nodes. If any comparison fails, the function returns false; otherwise, it completes successfully, confirming that the structures and values match in this part of the trees.
    \bigbreak \noindent 
    The inorderComp function is the main entry point for the comparison. It takes two tree objects, t1 and t2, and checks if both trees are empty, returning true if they are. If one tree is empty while the other is not, it returns false due to a structural difference. If both trees have a root node, inorderComp then calls r\_inorderComp on the root nodes of t1 and t2, performing the recursive in-order comparison on the entire structures.

    \pagebreak 
    \subsubsection{Finding the smallest and largest values}
    \bigbreak \noindent 
    By properties of BST's, to find the smallest value in a tree, we start from the root and go as far left as possible. To find the largest, we go as far right as possible
    \bigbreak \noindent 
    \begin{cppcode}
        node* r_min(node* p) {
            if (!p->left) return p;
            return r_min(p->left);
        }
        node* min() {
            if (!root) return nullptr;
            return r_min(root);
        }

        node* r_max(node* p) {
            if (!p->right) return p;
            return r_max(p->right);
        }
        node* max() {
            if (!root) return nullptr;
            return r_max(root);
        }
    \end{cppcode}

    \pagebreak 
    \subsubsection{Getting the widths of a bst}
    \bigbreak \noindent 
    \begin{cppcode}
        std::vector<int> getWidths() {
            if (!root) return;

            std::vector<int> w(height());
            std::queue<node*> q;
            w[0] =1;
            q.push(root);
            q.push(nullptr);

            int level = 1, width=0;
            while (!q.empty()) {
                node* curr = q.front();
                q.pop();

                if (curr == nullptr) {
                    w[level++] = width;
                    width = 0;
                    q.push(nullptr);
                    continue;
                }
                if (level == height()) {
                    break;
                }

                if (curr->left) {
                    q.push(curr->left);
                    ++width;
                }
                if (curr->right) {
                    q.push(curr->right);
                    ++width;
                }
            }
            return w;
        }
    \end{cppcode}
    \bigbreak \noindent 
    This C++ code defines a getWidths function that calculates and returns a vector of integers representing the width of each level in a binary tree. The width of a level is defined as the number of nodes at that level.
    \bigbreak \noindent 
    The function first creates a vector w with a size equal to the height of the tree (obtained from a hypothetical height() function) and initializes the first element, w[0], to 1, assuming the root level has one node. It also sets up a queue q to facilitate level-order traversal (breadth-first traversal) of the tree, starting by pushing the root node and a nullptr marker, which denotes the end of each level.
    \bigbreak \noindent 
    The main loop continues as long as the queue is not empty. It dequeues the front node in each iteration, checking if it’s nullptr. When a nullptr is encountered, it means the current level has ended, so the function records the width for that level in the w vector, resets the width counter, and pushes another nullptr if there are more nodes to process at deeper levels. The loop then proceeds to the next level.
    \bigbreak \noindent 
    If the dequeued node is not nullptr, it examines its left and right children. If they exist, they are added to the queue, and the width counter is incremented for each child node. This process continues until all levels are processed or the traversal reaches the specified tree height, returning the vector w with each level’s width at the end.


    \pagebreak 
    \subsubsection{Degenerate Binary Search trees}
    \bigbreak \noindent 
    A degenerate binary search tree is a tree where each parent node has only one child, causing the tree to resemble a linked list
    \bigbreak \noindent 
    Consider building a binary search tree, taking values from a sorted array from left to right. Since all subsequent entries will be greater than the previous, the insertions will only go in one direction, right. Thus, the final tree will resemble a linked list and thus we will not be able to use the $\text{lg}(n)$ property of binary trees.

    \bigbreak \noindent 
    \subsubsection{Verifying a binary search tree}
    \bigbreak \noindent 
    To verify a binary search tree, we need to check that for a given node, all nodes to the left have values less than the node, and all nodes to the right have values greater than the node. his can be achieved using recursion and passing down minimum and maximum value constraints for each subtree.
    \bigbreak \noindent 
    \begin{cppcode}
        bool r_is_bst(node* p, int min, int max) {
            if (!p) return true;

            if (p->data < min || p->data > max) return false;

            return r_is_bst(p->left, min, p->data) && r_is_bst(p->right, p->data, max);
        }

        bool is_bst() {
            if (!root) return true;
            return r_is_bst(root, std::numeric_limits<int>::min(), std::numeric_limits<int>::max());
        }
    \end{cppcode}
    \bigbreak \noindent 
    \begin{itemize}
        \item We start with the root node, which has the range of INT\_MIN to INT\_MAX.
        \item For the left child, we update the maximum allowed value to the parent’s value.
        \item For the right child, we update the minimum allowed value to the parent’s value.
    \end{itemize}


    \pagebreak 
    \subsubsection{Complexities}
    \bigbreak \noindent 
    Because BST have no guarantee of being well formed, (ie degenerate trees), the complexity of many operations in the worst case is $O(n)$.
    \bigbreak \noindent 
    \begin{center}
        \begin{tabular}{|c|c|c|c|}
            \hline
            \textbf{Operation} & \textbf{Best Case} & \textbf{Average Case} & \textbf{Worst Case} \\ \hline
            Insertion & \( O(\log n) \) & \( O(\log n) \) & \( O(n) \) \\ \hline
            Search & \( O(1) \) & \( O(\log n) \) & \( O(n) \) \\ \hline
            Removal & \( O(\log n) \) & \( O(\log n) \) & \( O(n) \) \\ \hline
            Height & \( O(\log n) \) & --- & \( O(n) \) \\ \hline
            Traversal & \( O(n) \) & \( O(n) \) & \( O(n) \) \\ \hline
        \end{tabular}
    \end{center}
    \bigbreak \noindent 
    \textbf{Note:} $\Omega(\text{lg}(n)$ for BST operations like search, insert, and delete (best case).
    \bigbreak \noindent 
    $\Omega(\text{lg}(n))$ for operations that require visiting all nodes, like traversals.








    \pagebreak 
    \subsection{Adelson-Velsky and Landis Trees (AVL trees)}
    \bigbreak \noindent 
    When dealing with binary search trees, insertions and removals occur continually, with no predictable order. In some of these applications, it is important to optimize search times by keeping the tree very nearly balanced at all times. The method in this section for achieving this goal was described in 1962 by two Russian mathematicians, G. M. ADEL’SON-VEL’SKI˘I and E. M. LANDIS, and the resulting binary search trees are called AVL trees in their honor.
    \bigbreak \noindent 
    AVL trees achieve the goal that searches, insertions, and removals in a tree
with n nodes can all be achieved in time that is $O(\log n)$, even in the worst case.
The height of an AVL tree with n nodes, as we shall establish, can never exceed
lg $n$, and thus even in the worst case, the behavior of an AVL tree could not
be much below that of a random binary search tree. In almost all cases, however,
the actual length of a search is very nearly lg $n$, and thus the behavior of AVL trees
closely approximates that of the ideal, completely balanced binary search tree.
\bigbreak \noindent 
\subsubsection{Definition}
\bigbreak \noindent 
An AVL tree is a binary search tree in which the heights of the left and right
subtrees of the root differ by at most 1 and in which the left and right subtrees
are again AVL trees.
\bigbreak \noindent 
With each node of an AVL tree is associated a \textit{balance factor} that is \texttt{lefthigher}, \texttt{equal-height}, or \texttt{right-higher} according, respectively, as the left subtree
has height greater than, equal to, or less than that of the right subtree.
\bigbreak \noindent 
\fig{.5}{./figures/1.png}
\bigbreak \noindent 
In drawing diagrams, we shall show a left-higher node by ‘/,’ a node whose balance 358
\bigbreak \noindent 
factor is equal by ‘−,’ and a right-higher node by ‘\.’. The figure above shows several
small AVL trees, as well as some binary trees that fail to satisfy the definition.
Note that the definition does not require that all leaves be on the same or
adjacent levels. 
\bigbreak \noindent 
\fig{.5}{./figures/2.png}
\bigbreak \noindent 
The figure above shows several AVL trees that are quite skewed, with right subtrees having greater height than left subtrees.


\bigbreak \noindent 
\subsubsection{AVL Nodes}
\bigbreak \noindent 
A typical node for an AVL tree is as follows
\bigbreak \noindent 
\begin{cppcode}
    struct node {
        node* left{nullptr}, *right{nullptr};
        int data{0};
        int height{0};
        balance b{0};

        // Constructors
            ... 
    };
\end{cppcode}
\bigbreak \noindent 
In AVL trees, it is common for the node structure to include a height member. This height field is essential for efficiently maintaining the balance of the tree, as the balance factor of a node (the difference in height between its left and right subtrees) is used to determine whether the tree needs rebalancing after insertions or deletions.
\bigbreak \noindent 
Without the height member, recalculating the height of each node during every operation would require traversing the entire subtree, significantly increasing the time complexity. By storing the height in each node, you can retrieve it in constant time, allowing rotations and rebalancing operations to remain efficient.

\bigbreak \noindent 
\subsubsection{Storing the height}
\bigbreak \noindent 
In AVL trees, we should store the height of a node as a data member, updating on insertions or removals into the tree. 

\pagebreak 
\subsubsection{Defining balance factors in C++ with enums}
\bigbreak \noindent 
We employ an enumerated data type to record balance factors
\bigbreak \noindent 
\begin{cppcode}
    enum Balance_factor { left_higher, equal_height, right_higher };
\end{cppcode}
\bigbreak \noindent 
Balance factors must be included in all the nodes of an AVL tree, and we must adapt our former node specification accordingly.

\bigbreak \noindent 
\subsubsection{Defining balance factors with a height calculation}
\bigbreak \noindent 
Instead, we can define the balance factor of a node as $\text{height}(left) - \text{height}(right)$. A negative balance implies a subtree is heavy on the right, a positive balance implies a subtree is heavy on the right. A balance of zero implies equal height subtrees. A balance factor ranges from -2 to 2. If $\abs{\text{balance}} = 2$, we must rotate the tree to restore balance.
\bigbreak \noindent 
\begin{cppcode}
    int getBalance(node* p) {
        if (!p) return -1;
        return (p->left ? p->left->height : -1) - (p->right ? p->right->height : -1);
    }
\end{cppcode}


\pagebreak 
\subsubsection{Interface}
\bigbreak \noindent 
The interface of a Binary Search Tree (BST) and an AVL Tree is generally the same. Both are types of binary trees, and they share similar operations such as:
\begin{itemize}
    \item \textbf{Insert:} Insert a new element into the tree.
    \item \textbf{Remove/Delete:} Remove an element from the tree.
    \item \textbf{Search:} Find whether a particular element exists in the tree.
    \item \textbf{Traversal:} In-order, pre-order, post-order, and level-order traversals.
\end{itemize}
However, behind the scenes, the AVL tree performs additional work to maintain its balance property, but this doesn't typically change the public interface

\bigbreak \noindent 
\subsubsection{Balancing an AVL tree}
\bigbreak \noindent 
As we insert or remove nodes from the tree, it may happen that the resulting tree fails to satisfy the conditions imposed by AVL trees. To \textit{rebalance} the tree, we have a set of operations, called rotations. We have
\bigbreak \noindent 
\begin{enumerate}
    \item \textbf{Right Rotation (RR):} Applied when a left subtree is too deep. The subtree is rotated to the right, reducing the height of the left side.
    \item \textbf{Left Rotation (LL):} Applied when a right subtree is too deep. The subtree is rotated to the left, reducing the height of the right side.
    \item \textbf{Left-Right Rotation (LR):} Occurs when a left subtree has a deep right subtree. First, a left rotation is performed on the left subtree, followed by a right rotation.
    \item \textbf{Right-Left Rotation (RL):} Occurs when a right subtree has a deep left subtree. First, a right rotation is performed on the right subtree, followed by a left rotation.
\end{enumerate}
These rotations are applied based on the balance factor, ensuring the tree remains balanced with a height difference of at most 1 between subtrees.
\bigbreak \noindent 
\textbf{Note:} Left-right and Right-left rotations are also called double right and double left respectively.
\bigbreak \noindent 
When writing our rotation algorithms, we only need to define two. A left rotation algorithm and a right rotation algorithm
\bigbreak \noindent 
\textbf{Note}: Performing a rotation when the height difference is only 1 would be unnecessary and could actually disrupt the balancing of the tree. The tree is still balanced in this case because a height difference of 1 between subtrees is allowed in AVL trees.

\pagebreak 
\subsubsection{Rotations: Right tree}
\bigbreak \noindent 
Let us now consider the case when a new node has been inserted into the taller subtree of a root node and its height has increased, so that now one subtree has height 2 more than the other, and the tree no longer satisfies the AVL requirements. We must now rebuild part of the tree to restore its balance. To be definite, let us assume that we have inserted the new node into the right subtree, its height has increased, and the original tree was right higher. That is, we wish to consider the case covered by the function right\_balance. Let root denote the root of the tree and right\_tree the root of its right subtree.
\bigbreak \noindent 
There are three cases to consider, depending on the balance factor of right\_tree.
\bigbreak \noindent 
\paragraph{Case 1: Right higher}
\bigbreak \noindent 
The first case, when right\_tree is right higher. The action needed in this case is called a left rotation.
We have rotated the node right\_tree
upward to the root, dropping root down into the left subtree of right\_tree; the subtree T2 of nodes with keys between those of root and right\_tree now becomes the
right subtree of root rather than the left subtree of right\_tree. A left rotation is
succinctly described in the following C++ function. Note especially that, when
done in the appropriate order, the steps constitute a rotation of the values in three
pointer variables. Note also that, after the rotation, the height of the rotated tree
has decreased by 1; it had previously increased because of the insertion; hence the
height finishes where it began.
\bigbreak \noindent 
\fig{.6}{./figures/3.png}
\bigbreak \noindent 
Thus, we come to the following implementation of a left rotation.
\bigbreak \noindent 
\begin{cppcode}
    void left_rotate(node* root) {
        if (!root || !root->right) return;
        
        right_subtree = root->right;
        root->right = right_subtree->left;
        right_subtree->left = root;
        right_subtree=root;
    }
\end{cppcode}
\pagebreak \bigbreak \noindent 
\begin{figure}[ht]
    \centering
    \incfig{tr2}
    \label{fig:tr2}
\end{figure}
\bigbreak \noindent 
\begin{enumerate}
    \item \textbf{Step 1 (green)}: Attach right\_subtrees left subtree to the right of root
    \item \textbf{Step 2 (blue)}: Attach root to the left of right\_subtree
    \item \textbf{Step 3}: Make r\_subtree the new root.
\end{enumerate}
Yields the rotated tree
\bigbreak \noindent 
\begin{figure}[ht]
    \centering
    \incfig{tr4}
    \label{fig:tr4}
\end{figure}
\bigbreak \noindent 
Note that we perform a left rotation when the roots right subtree has an insertion into its right subtree. Notice that the balance symbols point in the same direction. The only balance factors that change are the balance factors of root and right\_subtree, they become even (balanced).

\pagebreak 
\paragraph{Case 2: Left higher}
\bigbreak \noindent \bigbreak \noindent 
The second case, when the balance factor of right\_tree is left higher, is slightly more complicated. It is necessary to move two levels, to the node sub\_tree that roots the left subtree of right\_tree, to find the new root. This process is shown in Figure 10.20 and is called a double rotation, because the transformation can be obtained in two steps by first rotating the subtree with root right\_tree to the right (so that sub\_tree becomes its root), and then rotating the tree pointed to by root to the left (moving sub\_tree up to become the new root).
\bigbreak \noindent 
\fig{.5}{./figures/4.png}
\bigbreak \noindent 
We see from the figure that we first perform a right rotation on right\_tree, and right\_trees left subtree, this shifts the subtree unbalance in right\_subtree such that it becomes unbalanced on the right instead of on the left. This enables us to perform a left rotation on root and right\_tree. Note that after the right rotation, the balance symbols point in the same direction. A right rotation is of the form
\bigbreak \noindent 
\begin{figure}[ht]
    \centering
    \incfig{t4}
    \label{fig:t4}
\end{figure}
\pagebreak \bigbreak \noindent 
Which becomes
\begin{figure}[ht]
    \centering
    \incfig{t5}
    \label{fig:t5}
\end{figure}
\bigbreak \noindent 
Thus, we see that like the left rotation, the only subtree that moves is $s_{2}$. In the left rotation, the left subtree of $B$ becomes the right subtree of $A$. In the right rotation, the right subtree of $B$ becomes the left subtree of $A$. A right rotation occurs when a tree becomes unbalanced on the left. Whereas a left rotation occurs when a tree becomes unbalanced on the right.
\bigbreak \noindent 
Thus, we come to the following implementation of a left rotation.
\bigbreak \noindent 
\begin{cppcode}
    void right_rotate(node* root) {
        if (!root || !root->right) return;
        
        left_subtree = root->left;
        root->left = left_subtree->right;
        left_subtree->right = root;
        left_subtree=root;
    }
\end{cppcode}

% \bigbreak \noindent 
% \begin{figure}[ht]
%     \centering
%     \incfig{t1}
%     \label{fig:t1}
% \end{figure}
% \bigbreak \noindent 
% During single rotations, there are always two nodes and three subtrees to consider. In this case we name them $A, B$, and subtrees $s_{1}, s_{2}$, and $s_{3}$. $s_{1}$ is the left subtree of the $A$ (the root of the subtree we are rotating), and $s_{2}, s_{3}$ are the children of $B$. After the single left rotation is performed, the only subtree that gets moved is $s_{2}$. Since this case was a right subtree, right higher, left rotation, $s_{2}$ gets attached as the right child of $A$.
% \bigbreak \noindent 
% After the rotation, the height of the rotated subtree will decrease by one. We must then adjust the balance factors. 

\pagebreak 
\subsubsection{C++ Rotations}
\bigbreak \noindent 
\begin{cppcode}
    node* leftRotate(node* p) {
        node* subp = p->right;
        p->right = subp->left;
        subp->left = p;

        p->height = 1 + 
            std::max(p->left ? p->left->height : -1, 
                    p->right ? p->right->height : -1);

        subp->height = 1 + 
            std::max(subp->left ? subp->left->height : -1, 
                    subp->right ? subp->right->height : -1);

        p->balance = getBalance(p);
        subp->balance = getBalance(subp);
        return subp;
    }

    node* rightRotate(node* p) {
        node* subp = p->left;
        p->left = subp->right;
        subp->right = p;

        p->height = 1 + 
            std::max(p->left ? p->left->height : -1, 
                    p->right ? p->right->height : -1);

        subp->height = 1 + 
            std::max(subp->left ? subp->left->height : -1, 
                    subp->right ? subp->right->height : -1);

        p->balance = getBalance(p);
        subp->balance = getBalance(subp);
        return subp;
    }
\end{cppcode}
\bigbreak \noindent 
In leftRotate, the function takes a node pointer p (the root of a subtree that is right-heavy) and performs a left rotation to reduce the height of its right subtree. First, it saves the right child of p (denoted as subp). Then, it adjusts the pointers so that the left child of subp becomes the new right child of p, and subp becomes the new root of this subtree with p as its left child. After adjusting the structure, it recalculates the height and balance of both p and subp, where the height of each node is determined by adding 1 to the maximum height of its left and right children. The function then returns subp as the new root of the subtree.
\bigbreak \noindent 
Similarly, rightRotate takes a node p (the root of a left-heavy subtree) and performs a right rotation to rebalance it. It stores the left child of p as subp, then updates pointers so that the right child of subp becomes the new left child of p, and subp becomes the new root with p as its right child. Heights and balances for p and subp are recalculated using the heights of their children, and subp is returned as the new root of this subtree. These rotations are essential for keeping the AVL tree balanced after insertions and deletions.
\bigbreak \noindent 
\textbf{Note:} To understand the return values of the rotateMethods, it is necessary to understand the insertion method below.


\pagebreak 
\subsubsection{Balancing}
\bigbreak \noindent 
\begin{cppcode}
    node* balance(node* p) {
        if (p->balance > 1) {  // Left heavy
            if (p->left && p->left->balance < 0) {
                leftRotate(p->left);  // Left-Right case
            }
            return rightRotate(p);  // Left-Left case
        }
        if (p->balance < -1) {  // Right heavy
            if (p->right && p->right->balance > 0) {
                rightRotate(p->right);  // Right-Left case
            }
            return leftRotate(p);  // Right-Right case
        }
        return p;
    }
\end{cppcode}
\bigbreak \noindent 
This function takes a pointer p to a node and checks its balance factor (the difference in height between its left and right subtrees) to determine if the node is unbalanced. If the balance factor is greater than 1, the tree is left-heavy, meaning the left subtree is taller than the right subtree. In this case, the function first checks if a "Left-Right" rotation is needed—this would happen if the left child of p is itself right-heavy, indicated by a negative balance factor. If so, a left rotation is applied to the left child of p to balance it before proceeding. Then, a right rotation is applied to p to correct the "Left-Left" imbalance.
\bigbreak \noindent 
Conversely, if the balance factor is less than -1, the node is right-heavy, indicating the right subtree is taller. For a "Right-Left" case, where the right child of p is left-heavy, a right rotation is applied to the right child of p first. This corrects the imbalance locally and prepares it for the final step: a left rotation applied to p to balance the "Right-Right" case. If p is already balanced, the function simply returns p without modification.

\pagebreak 
\subsubsection{Insertions}
\bigbreak \noindent 
\begin{cppcode}
    node* r_insert(node* p, int data) {
        if (!p) return new node(data);

        if (data < p->data) {
            p->left = r_insert(p->left, data);
        } else if (data > p->data) {
            p->right = r_insert(p->right, data);
        }

        p->height = 1 + std::max(p->left ? p->left->height : -1, p->right ? p->right->height : -1);
        p->balance = getBalance(p);

        return balance(p);
    }

    void insert(int data) {
        root = r_insert(root, data);
    }
\end{cppcode}
\bigbreak \noindent 
The r\_insert function takes two parameters: p, a pointer to the current node in the tree, and data, the integer value to be inserted. If p is nullptr (indicating an empty spot in the tree), a new node with the given data is created and returned, establishing the base case for recursion.
\bigbreak \noindent 
If the node already exists, r\_insert decides where to place the new value by comparing data with p->data. If data is less than p->data, it recursively calls r\_insert on the left child (p->left). Otherwise, if data is greater, it calls r\_insert on the right child (p->right). This ensures that the AVL tree retains its binary search property.
\bigbreak \noindent 
After the recursive insertion, r\_insert updates the height of the current node p by setting it to 1 plus the maximum height of its left and right children. It then calculates the node’s balance factor using getBalance(p), which is essential for determining if the subtree rooted at p has become unbalanced. Finally, balance(p) is called to apply any necessary rotations to maintain the AVL tree's balance, and the function returns the (possibly new) root of the subtree.
\bigbreak \noindent 
The insert function is a wrapper that starts the insertion process at the root node of the tree, calling r\_insert with root and the specified data value. This encapsulates the recursive insertion logic and initiates it from the top of the tree.
\bigbreak \noindent 
\textbf{Note:} Worry about ancestory heights (and therefore balance factors) after rotations is not necessary, height and balance calculations occurs only when the insert method returns to that node in the recursion, and thus only after a rotation will an ancestory height and balance calculation be made.

\pagebreak 
\subsubsection{Removing nodes}
\bigbreak \noindent 
To implement removal from an AVL tree, you need to perform a few key steps: find and remove the node, balance the tree afterward, and adjust heights and balance factors as necessary
\bigbreak \noindent 
Start by locating the node with the target value. If found, delete it using standard BST deletion rules
\bigbreak \noindent 
After removing a node, retrace your steps back to the root, updating the height and balance factor of each ancestor. If a node is unbalanced, apply rotations to restore balance.
\bigbreak \noindent 
As with insertion, use recursive balancing and height adjustments after deletion.
\bigbreak \noindent 
\begin{cppcode}
    node* succ(node* p) {
        while (p->left) p = p->left;
        return p;
    }
    node* r_remove(node* p, int data) {
        if (!p) return nullptr;  // Node not found
        // Traverse the tree to find the node to delete
        if (data < p->data) {
            p->left = r_remove(p->left, data);
        } else if (data > p->data) {
            p->right = r_remove(p->right, data);
        } else {  // Node to delete is found
            if (!p->left) {
                node* temp = p->right;
                delete p;
                return temp;
            } else if (!p->right) {
                node* temp = p->left;
                delete p;
                return temp;
            } else {  // Node has two children
                node* temp = succ(p->right);
                p->data = temp->data;  // Replace data
                p->right = r_remove(p->right, temp->data); 
            }
        }
        // Update height and balance
        p->height = 1 + std::max(p->left ? p->left->height : -1, p->right ? p->right->height : -1);
        p->balance = getBalance(p);

        // Balance the node if necessary
        return balance(p);
    }
    void remove(int data) {
        root = r_remove(root, data);
    }
\end{cppcode}

\pagebreak \bigbreak \noindent 
r\_remove, performs a recursive search for the node to delete based on the value data. If data is less than p->data, it recurses into the left subtree; if data is greater, it recurses into the right subtree. When the node to delete is found, three cases are handled based on the structure of p:
\begin{enumerate}
    \item If p has no left child, p is replaced by its right child, effectively bypassing it in the tree.
    \item If p has no right child, p is replaced by its left child.
    \item If p has two children, the in-order successor (smallest node in p->right) is found using succ. The data in p is replaced with this successor’s data, and the successor node is then removed from p->right to avoid duplication.
\end{enumerate}
After deletion, the function updates the height and balance of p, recalculating the height as one plus the maximum height of its children. The balance factor, which measures the difference in height between the left and right subtrees, is also recalculated. Finally, the function calls balance(p) to ensure that any imbalance introduced by the deletion is corrected. The remove function wraps this entire process, starting the deletion at the root. This ensures the AVL tree remains balanced after a node is removed.

\pagebreak 
\subsection{Red-black trees}
\bigbreak \noindent 
A red-black tree is a binary search tree with one extra bit of storage per node: its
color, which can be either \textcolor{red}{RED} or \textbf{BLACK}. By constraining the node colors on
any simple path from the root to a leaf, red-black trees ensure that no such path is
more than twice as long as any other, so that the tree is approximately balanced.
\bigbreak \noindent 
as we’re about to see, the height of a red-black tree with $n$ keys is at most $2 \text{lg}(n+1) $, which is $O(\text{lg}\ n ) $
\bigbreak \noindent 
Each node of the tree now contains the attributes color, key, left, right, and $p$. If
a child or the parent of a node does not exist, the corresponding pointer attribute of
the node contains the value NIL. Think of these NILs as pointers to leaves (external
nodes) of the binary search tree and the normal, key-bearing nodes as internal nodes
of the tree. 
\bigbreak \noindent 
A red-black tree is a binary search tree that satisfies the following red-black properties:
\begin{enumerate}
    \item Every node is either red or black.
    \item The root is black.
    \item Every leaf (NIL) is black.
    \item If a node is red, then both its children are black. (no double red)
    \item For each node, all simple paths from the node to descendant leaves contain the same number of black nodes.
\end{enumerate}
\bigbreak \noindent 
As a matter of convenience in dealing with boundary conditions in red-black
tree code, we use a single sentinel to represent NIL
For a red-black
tree $T$, the sentinel $T$.nil is an object with the same attributes as an ordinary node
in the tree. Its color attribute is BLACK, and its other attributes $p$, left , right ,
and key can take on arbitrary values
\bigbreak \noindent 
Why use the sentinel? The sentinel makes it possible to treat a NIL child of a
node $x$ as an ordinary node whose parent is $x$. An alternative design would use a
distinct sentinel node for each NIL in the tree, so that the parent of each NIL is well
deûned. That approach needlessly wastes space, however. Instead, just the one
sentinel $T$.nil represents all the NILs, all leaves and the root’s parent. The values
of the attributes $p$, left, right , and key of the sentinel are immaterial. The red-black
tree procedures can place whatever values in the sentinel that yield simpler code
\bigbreak \noindent 
We call the number of black nodes on any simple path from, but not including, a node $x$ down to a leaf the black-height of the node, denoted $bh(x)$.
\bigbreak \noindent 
By property 5, the notion of black-height is well defined, since all descending simple paths from the node have the same number of black nodes. The black-height of a red-black tree is the black-height of its root.
\bigbreak \noindent 
\fig{.5}{./figures/18.png}
\bigbreak \noindent 
We generally confine our interest to the internal nodes of a red-black tree, since they hold the key values. The remainder of this chapter omits the leaves in drawings of red-black trees, as shown in Figure 13.1(c).
\bigbreak \noindent 
\textbf{Lemma}. A red-black tree with $n$ internal nodes has height at most $2\text{lg}(n+1) $
\bigbreak \noindent 
\textbf{Proof}. 
We start by showing that the subtree rooted at any node \(x\) contains at least \(2^{bh(x)} - 1\) internal nodes. We prove this claim by induction on the height of \(x\). 
\bigbreak \noindent 
If the height of \(x\) is 0, then \(x\) must be a leaf (\(T.\text{nil}\)), and the subtree rooted at \(x\) indeed contains at least 
\[
2^{bh(x)} - 1 = 2^0 - 1 = 0
\] 
internal nodes.
\bigbreak \noindent 
For the inductive step, consider a node \(x\) that has positive height and is an internal node. Then node \(x\) has two children, either or both of which may be a leaf. If a child is black, then it contributes 1 to \(x\)'s black-height but not to its own. If a child is red, then it contributes to neither \(x\)'s black-height nor its own. 
\bigbreak \noindent 
Therefore, each child has a black-height of either \(bh(x) - 1\) (if it’s black) or \(bh(x)\) (if it’s red). Since the height of a child of \(x\) is less than the height of \(x\) itself, we can apply the inductive 
hypothesis to conclude that each child has at least \(2^{bh(x)-1} - 1\) internal nodes. Thus, the subtree rooted at \(x\) contains at least
\[
    (2^{bh(x)-1} - 1) + (2^{bh(x)-1} - 1) + 1 = 2^{bh(x)} - 1
\]
internal nodes, which proves the claim.
\bigbreak \noindent 
To complete the proof of the lemma, let \(h\) be the height of the tree. According to property 4, at least half the nodes on any simple path from the root to a leaf, not including the root, must be black. Consequently, the black-height of the root must be at least \(h/2\), and thus,
\[
    n \geq 2^{h/2} - 1.
\]
Moving the \(1\) to the left-hand side and taking logarithms on both sides yields
\[
    \lg(n + 1) \geq h/2, \text{ or } h \leq 2\lg(n + 1).
\]
\qed
\bigbreak \noindent 
As an immediate consequence of this lemma, each of the dynamic-set operations SEARCH, MINIMUM, MAXIMUM, SUCCESSOR, and PREDECESSOR runs in $O(\text{lg}\ n) $ time on a red-black tree, since each can run in $O(h)$ time on a binary search tree of height $h$ 
 and any red-black tree on $n$ nodes is a binary search tree with height $O(\text{lg}\ n)$.
 \bigbreak \noindent 
 \fig{.5}{./figures/1}

 \bigbreak \noindent 
 \subsubsection{Rotations}
 \bigbreak \noindent 
 red-black trees (RB trees) and AVL trees use the same basic rotation logic to maintain balance: left rotation and right rotation. However, they differ significantly in why and when they apply these rotations
 \bigbreak \noindent 
 The search-tree operations TREE-INSERT and TREE-DELETE, when run on a redblack tree with n keys, take O.lg n/ time. Because they modify the tree, the result may violate the red-black properties. To restore these properties, colors and pointers within nodes need to change.
 \bigbreak \noindent 
 The pointer structure changes through \textit{rotation}, which is a local operation in a search tree that preserves the binary-search-tree property. Figure 13.2 shows the two kinds of rotations: left rotations and right rotations. Let’s look at a left rotation on a node $x$, which transforms the structure on the right side of the figure to the structure on the left. Node $x$ has a right child $y$, which must not be $T.nil$. The left rotation changes the subtree originally rooted at $x$ by “twisting” the link between $x$ and $y$ to the left. The new root of the subtree is node $y$, with $x$ as $y$’s left child and $y$’s original left child (the subtree represented by $\beta$ in the figure) as $x$’s right child.
 \bigbreak \noindent 
 Both LEFT-ROTATE and RIGHT-ROTATE run in $O(1)$ time. Only pointers are changed by a rotation, and all other attributes in a node remain the same.
 \bigbreak \noindent 
 \begin{cppcode}
    left-rotate(T, x) {
        y = x.right
        // Start by attaching y's left to x's right
        x.right = y.left

        // If y's left existed, adjust its parent
        if (y.left != nil)
            y.left.p = x

        y.p = x.p

        // If x was the root, set y to the new root
        if (x.p == nil) 
            root = y
        // If x was not the root, check what x was to its parent (left or right), change to y
        else if (x.p.left == x) 
            x.p.left = y
        else 
            x.p.right = y

        // Finish up
        y.left = x
        x.p = y
    }
 \end{cppcode}
 \bigbreak \noindent 
 A right rotate is symmetrical to left rotate
 \bigbreak \noindent 
 \begin{cppcode}
     right-rotate(T, x) {
        y = x.left
        x.left = y.right

        if (y.right != nil) 
            y.right.p = x

        y.p = x.p
        if (x.p == nil)
            root = y
        else if (x.p.left == x)
            x.p.left = y
        else 
            x.p.right = y

        y.right = x
        x.p = y
     }
 \end{cppcode}

 \pagebreak 
 \subsubsection{Inserting}
 \bigbreak \noindent 
 The procedure RB-INSERT starts by inserting node $z$ into the tree $T$ as if it were an ordinary binary search tree, and then it colors $z$ red.
 \bigbreak \noindent 
 Coloring $z$ red ensures that:
 \begin{enumerate}
     \item The black-height of the tree remains unchanged.
     \item No violations of the Red-Black Tree properties occur.
 \end{enumerate}
 \bigbreak \noindent 
To guarantee that the red-black properties are preserved, an auxiliary procedure RB-INSERT-FIXUP on the facing page recolors nodes and performs rotations.
\bigbreak \noindent 
The call insert($T,z$) inserts node $z$, whose key is assumed to have
already been filled in, into the red-black tree $T$.
\bigbreak \noindent 
\begin{cppcode}
    insert(T,z) {
        x=T.root
        y=T.nil

        while (x != T.nil) {
            y = x;

            if (z.key < x.key) {
                x = x.left
            } else x = x.right
        }
        z.p = y

        // Tree was empty
        if (y == T.nil) {
            T.root = z
        } else if (z.key < y.key) {
            y.left = z
        } else y.right = z
        
        z.left = T.nil
        z.right = T.nil
        z.color = RED
        insert-fixup(T,z)
            
    }
\end{cppcode}

\pagebreak 
\bigbreak \noindent 
\begin{cppcode}
    insert-fixup(T, z) {
        while (z.p.color == red) {
            if  (z.p == z.p.p.left) { // Is z's parent a left child? 
                y = z.p.p.right // y is z's uncle
                if (y.color == red) { // are z's parent and uncle both red?
                    /* Case 1 */
                    z.p.color = black
                    y.color = black
                    z.p.p.color = red
                    z = z.p.p
                    /* ------ */
                    
                } else {
                    if (z== z.p.right) {
                        /* case 2 */
                        z = z.p
                        left-rotate(T,z)
                        /* ------ */
                    }
                    /* case 3 */
                    z.p.color = black
                    z.p.p.color = red
                    right-rotate(T,z.p.p)
                    /* ------ */
                }
            // Right and left exchanged
            } else {
                y = z.p.p.left 
                if (y.color == red) { 
                    z.p.color = black
                    y.color = black
                    z.p.p.color = red
                    z = z.p.p
                } else {
                    if (z== z.p.left) {
                        z = z.p
                        right-rotate(T,z)
                    }
                    z.p.color = black
                    z.p.p.color = red
                    left-rotate(T,z.p.p)
            }
        }
    }
    T.root.color = black
\end{cppcode}
\bigbreak \noindent 
To understand how RB-INSERT-FIXUP works, let’s examine the code in three
major steps. First, we’ll determine which violations of the red-black properties
might arise in RB-INSERT upon inserting node $z$ and coloring it red. Second, we’ll
consider the overall goal of the while loop in lines 1329. Finally, we’ll explore each
of the three cases within the while loop’s body (case 2 falls through into case 3, so
these two cases are not mutually exclusive) and see how they accomplish the goal.
\bigbreak \noindent 
In describing the structure of a red-black tree, we’ll often need to refer to the sibling of a node’s parent. We use the term uncle for such a node.
\bigbreak \noindent 
\fig{.5}{./figures/21.png}
\bigbreak \noindent 
What violations of the red-black properties might occur upon the call to \texttt{RB-INSERT-FIXUP}? Property 1 certainly continues to hold (every node is either red or black), as does property 3 (every leaf is black), since both children of the newly inserted red node are the sentinel \(T.\text{nil}\). Property 5, which says that the number of black nodes is the same on every simple path from a given node, is satisfied as well, because node \(z\) replaces the (black) sentinel, and node \(z\) is red with sentinel children. 
\bigbreak \noindent 
Thus, the only properties that might be violated are property 2, which requires the root to be black, and property 4, which says that a red node cannot have a red child. Both possible violations may arise because \(z\) is colored red. Property 2 is violated if \(z\) is the root, and property 4 is violated if \(z\)'s parent is red.
\bigbreak \noindent 
The \texttt{while} loop of lines 13–29 has two symmetric possibilities: lines 33–15 deal with the situation in which node \(z\)'s parent \(z.p\) is a left child of \(z\)'s grandparent \(z.p.p\), and lines 17–29 apply when \(z\)'s parent is a right child. Our proof will focus only on lines 33–15, relying on the symmetry in lines 17–29.

\bigbreak \noindent 
We’ll show that the while loop maintains the following three-part invariant at the start of each iteration of the loop
\begin{enumerate}
    \item Node \(z\) is red.
    \item If \(z.p\) is the root, then \(z.p\) is black.
    \item If the tree violates any of the red-black properties, then it violates at most one of them, and the violation is of either property 2 or property 4, but not both. If the tree violates property 2, it is because \(z\) is the root and is red. If the tree violates property 4, it is because both \(z\) and \(z.p\) are red.
\end{enumerate}
\bigbreak \noindent 
Part (c), which deals with violations of red-black properties, is more central to showing that \texttt{RB-INSERT-FIXUP} restores the red-black properties than parts (a) and (b), which we’ll use along the way to understand situations in the code. 
\bigbreak \noindent 
Because we’ll be focusing on node \(z\) and nodes near it in the tree, it helps to know from part (a) that \(z\) is red. Part (b) will help show that \(z\)’s grandparent \(z.p.p\) exists when it’s referenced in lines 2, 3, 7, 8, 14, and 15 (recall that we’re focusing only on lines 3–15).
\bigbreak \noindent 
To use a loop invariant, we need to show that the invariant is true upon entering the first iteration of the loop, that each iteration maintains it, that the loop terminates, and that the loop invariant gives us a useful property at loop termination. We’ll see that each iteration of the loop has two possible outcomes: either the pointer $z$ moves up the tree, or some rotations occur and then the loop terminates.
\bigbreak \noindent 
Before RB-INSERT is called, the red-black tree has no violations. RB-INSERT adds a red node $z$ and calls RB-INSERT-FIXUP. We’ll show that each part of the invariant holds at the time RB-INSERT-FIXUP is called:
\begin{enumerate}
    \item[a.] When \texttt{RB-INSERT-FIXUP} is called, $z$ is the red node that was added.
    \item[b.] If $z.p$ is the root, then $z.p$ started out black and did not change before the call of \texttt{RB-INSERT-FIXUP}.
    \item[c.] We have already seen that properties 1, 3, and 5 hold when \texttt{RB-INSERT-FIXUP} is called.
        \bigbreak \noindent 
        If the tree violates property 2 (the root must be black), then the red root must be the newly added node $z$, which is the only internal node in the tree. Because the parent and both children of $z$ are the sentinel, which is black, the tree does not also violate property 4 (both children of a red node are black). Thus this violation of property 2 is the only violation of red-black properties in the entire tree.
        \bigbreak \noindent 
        If the tree violates property 4, then, because the children of node $z$ are black sentinels and the tree had no other violations prior to $z$ being added, the violation must be because both $z$ and $z.p$ are red. Moreover, the tree violates no other red-black properties.
\end{enumerate}
\bigbreak \noindent 
\textbf{Maintenance:} There are six cases within the \texttt{while} loop, but we’ll examine only the three cases in lines 3–15, when node $z$’s parent $z.p$ is a left child of $z$’s grandparent $z.p.p$. The proof for lines 17–29 is symmetric. The node $z.p.p$ exists, since by part (b) of the loop invariant, if $z.p$ is the root, then $z.p$ is black. Since \texttt{RB-INSERT-FIXUP} enters a loop iteration only if $z.p$ is red, we know that $z.p$ cannot be the root. Hence, $z.p.p$ exists.
\bigbreak \noindent 
Case 1 differs from cases 2 and 3 by the color of $z$’s uncle $y$. Line 3 makes $y$ point to $z$’s uncle $z.p.p.\texttt{right}$, and line 4 tests $y$’s color. If $y$ is red, then case 1 executes. Otherwise, control passes to cases 2 and 3. In all three cases, $z$’s grandparent $z.p.p$ is black, since its parent $z.p$ is red, and property 4 is violated only between $z$ and $z.p$.
\bigbreak \noindent 
\textbf{Case 1: $z$'s uncle $y$ is red}
\bigbreak \noindent 
The figure below shows the situation for case 1 (lines 5–8), which occurs when both $z.p$ and $y$ are red. Because $z$’s grandparent $z.p.p$ is black, its blackness can transfer down one level to both $z.p$ and $y$, thereby fixing the problem of $z$ and $z.p$ both being red. Having had its blackness transferred down one level, $z$’s grandparent becomes red, thereby maintaining property 5. The \texttt{while} loop repeats with $z.p.p$ as the new node $z$, so that the pointer $z$ moves up two levels in the tree.
\bigbreak \noindent 
\fig{.5}{./figures/23.png}

\bigbreak \noindent 
\textbf{Case 2: $z$'s uncle $y$ is black and $z$ is a right child}
\bigbreak \noindent 
\textbf{Case 3: $z$'s uncle $y$ is black and $z$ is a left child}
\bigbreak \noindent 
In cases 2 and 3, the color of $z$’s uncle $y$ is black. We distinguish the two cases, which assume that $z$’s parent $z.p$ is red and a left child, according to whether $z$ is a right or left child of $z.p$. Lines 11–12 constitute case 2, which is shown in Figure 13.6 together with case 3. In case 2, node $z$ is a right child of its parent. A left rotation immediately transforms the situation into case 3 (lines 13–15), in which node $z$ is a left child. Because both $z$ and $z.p$ are red, the rotation affects neither the black-heights of nodes nor property 5. Whether case 3 executes directly or through case 2, $z$’s uncle $y$ is black, since otherwise case 1 would have run. Additionally, the node $z.p.p$ exists, since we have argued that this Node existed at the time that lines 2 and 3 were executed, and after moving $z$ up one level in line 11 and then down one level in line 12, the identity of $z.p.p$ remains unchanged. Case 3 performs some color changes and a right rotation, which preserve property 5. At this point, there are no longer two red nodes in a row. The \texttt{while} loop terminates upon the next test in line 1, since $z.p$ is now black.
\bigbreak \noindent 
\fig{.5}{./figures/24.png}

\pagebreak 
\subsubsection{Deletion}
\bigbreak \noindent 
Like the other basic operations on an $n$-node red-black tree, deletion of a node takes $O(\text{lg}\ n)$ time. Deleting a node from a red-black tree is more complicated than inserting a node.
\bigbreak \noindent 
First, we discuss the \textit{transplant} procedure. Red-Black Trees need a transplant function as part of their removal operation because it simplifies the process of replacing one subtree with another. This is particularly important for maintaining the binary search tree (BST) property and handling parent-child relationships efficiently during node deletion.
\bigbreak \noindent 
In a Red-Black Tree (and in a general BST), node removal often requires replacing the node being removed (u) with one of its subtrees (v). For example:
\begin{itemize}
    \item If the node being removed has one child, that child replaces the node.
    \item If the node being removed has two children, its successor (or predecessor) replaces the node.
\end{itemize}
The transplant function encapsulates this replacement logic, making the process cleaner and more modular.
\bigbreak \noindent 
When replacing a node, the parent pointers of the replacement node and the replaced node's parent need to be updated. Manually handling these pointers can become error-prone, especially in a tree with many cases like a Red-Black Tree. The transplant function handles this pointer management consistently.
\bigbreak \noindent 
\begin{cppcode}
    transplant(T,u,v) {
        if (u.p == T.nil)
            T.root = v
        else if (u == u.p.left)
            u.p.left = v
        else u.p.right = v

        v.p = u.p
    }
\end{cppcode}
\bigbreak \noindent 
The procedure \textbf{RB-DELETE} on the next page is like the \textbf{TREE-DELETE} procedure, but with additional lines of pseudocode. The additional lines deal with nodes \(x\) and \(y\) that may be involved in violations of the red-black properties. When the node \(z\) being deleted has at most one child, then \(y\) will be \(z\). When \(z\) has two children, then, as in \textbf{TREE-DELETE}, \(y\) will be \(z\)'s successor, which has no left child and moves into \(z\)'s position in the tree. Additionally, \(y\) takes on \(z\)'s color. In either case, node \(y\) has at most one child: node \(x\), which takes \(y\)'s place in the tree. (Node \(x\) will be the sentinel \(T.nil\) if \(y\) has no children.) Since node \(y\) will be either removed from the tree or moved within the tree, the procedure needs to keep track of \(y\)'s original color. If the red-black properties might be violated after deleting node \(z\), \textbf{RB-DELETE} calls the auxiliary procedure \textbf{RB-DELETE-FIXUP}, which changes colors and performs rotations to restore the red-black properties.

\pagebreak 
\bigbreak \noindent 
\begin{cppcode}
    RB-DELETE(T,z) {
        y = z 
        y-original-color = y.color
        if (z.left == T.nil)
            x = z.right
            RB-TRANSPLANT(T,z,z.right) // replace z by its right child
        else if (z.right == T.nil)
            x = z.left
            RB-TRANSPLANT(T,z,z.left) // replace z by its left child 
        else 
            y = MINIMUM(z.right) // y is z's successor
            y-original-color = y.color
            x = y.right
            if (y != z.right) // is y farther down the tree?
                RB-TRANSPLANT(T,y,y.right)  // replace y by its right child 
                y.right = z.right // z's right child becomes
                y.right.p = y // y’s right child
            else x.p = y // in case x is T.nil
            RB-TRANSPLANT(T,z,y) // replace z by its successor y
            y.left = z.left // and give z's left child to y, 
            y.left.p = y // which had no left child
            y.color = z.color
        if (y-original-color == BLACK) // if any red-black violations occurred,
            RB-DELETE-FIXUP(T,x) // Correct them
    }
\end{cppcode}
\bigbreak \noindent 
Because node \(y\)'s color might change, the variable \(y\text{-}original\text{-}color\) stores \(y\)'s color before any changes occur. Lines 2 and 10 set this variable immediately after assignments to \(y\). When node \(z\) has two children, then nodes \(y\) and \(z\) are
Distinct. In this case, line 17 moves \(y\) into \(z\)'s original position in the tree (that is, \(z\)'s location in the tree at the time \textbf{RB-DELETE} was called), and line 20 gives \(y\) the same color as \(z\). When node \(y\) was originally black, removing or moving it could cause violations of the red-black properties, which are corrected by the call of \textbf{RB-DELETE-FIXUP} in line 22.
\bigbreak \noindent 
As discussed, the procedure keeps track of the node \(x\) that moves into node \(y\)'s original position at the time of call. The assignments in lines 4, 7, and 11 set \(x\) to point to either \(y\)'s only child or, if \(y\) has no children, the sentinel \(T.nil\).
\bigbreak \noindent 
Since node \(x\) moves into node \(y\)'s original position, the attribute \(x.p\) must be set correctly. If node \(z\) has two children and \(y\) is \(z\)'s right child, then \(y\) just moves into \(z\)'s position, with \(x\) remaining a child of \(y\). Line 12 checks for this case. Although you might think that setting \(x.p\) to \(y\) in line 16 is unnecessary since \(x\) is a child of \(y\), the call of \textbf{RB-DELETE-FIXUP} relies on \(x.p\) being \(y\) even if \(x\) is \(T.nil\). Thus, when \(z\) has two children and \(y\) is \(z\)'s right child, executing line 16 is necessary if $y$’s right child is T.nil, and otherwise it does not change anything.
\bigbreak \noindent 
Finally, if node $y$ was black, one or more violations of the red-black properties
might arise. The call of RB-DELETE-FIXUP in line 22 restores the red-black
properties. If $y$ was red, the red-black properties still hold when $y$ is removed
or moved, for the following reasons:
\begin{enumerate}
    \item No black-heights in the tree have changed
    \item No red nodes have been made adjacent. If \(z\) has at most one child, then \(y\) and \(z\) are the same node. That node is removed, with a child taking its place. If the removed node was red, then neither its parent nor its children can also be red, so moving a child to take its place cannot cause two red nodes to become adjacent. If, on the other hand, \(z\) has two children, then \(y\) takes \(z\)'s place in the tree, along with \(z\)'s color, so there cannot be two adjacent red nodes at \(y\)'s new position in the tree. In addition, if \(y\) was not \(z\)'s right child, then \(y\)'s original right child \(x\) replaces \(y\) in the tree. Since \(y\) is red, \(x\) must be black, and so replacing \(y\) by \(x\) cannot cause two red nodes to become adjacent.
    \item Because \(y\) could not have been the root if it was red, the root remains black.
\end{enumerate}
\bigbreak \noindent 
If node \(y\) was black, three problems may arise, which the call of \textbf{RB-DELETE-FIXUP} will remedy. First, if \(y\) was the root and a red child of \(y\) became the new root, property 2 is violated. Second, if both \(x\) and its new parent are red, then a violation of property 4 occurs. Third, moving \(y\) within the tree causes any simple path that previously contained \(y\) to have one less black node. Thus, property 5 is now violated by any ancestor of \(y\) in the tree. We can correct the violation of property 5 by saying that when the black node \(y\) is removed or moved, its blackness transfers to the node \(x\) that moves into \(y\)'s original position, giving \(x\) an “extra” black. That is, if we add 1 to the count of black nodes on any simple path that contains \(x\), then under this interpretation, property 5 holds. But now another problem emerges: node \(x\) is neither red nor black, thereby violating property 1. Instead, node \(x\) is either “doubly black” or “red-and-black,” and it contributes either 2 or 1, respectively, to the count of black nodes on simple paths containing \(x\). The color attribute of \(x\) will still be either \textbf{RED} (if \(x\) is red-and-black) or \textbf{BLACK} (if \(x\) is doubly black). In other words, the extra black on a node is reflected in \(x\)'s pointing to the node rather than in the color attribute.
\bigbreak \noindent 
The procedure RB-DELETE-FIXUP on the next page restores properties 1, 2, and 4
\bigbreak \noindent 
The goal of the while loop in lines 1-43 is to move the extra black up the tree until
\bigbreak \noindent 
\begin{enumerate}
    \item $x$ points to a red-and-black node, in which case line 44 colors $x$ (singly) black;
    \item $x$ points to the root, in which case the extra black simply vanishes; or
    \item having performed suitable rotations and recolorings, the loop exits. 
\end{enumerate}
\bigbreak \noindent 
Like RB-INSERT-FIXUP, the RB-DELETE-FIXUP procedure handles two symmetric situations: lines 3-22 for when node $x$ is a left child, and lines 24-43 for
when $x$ is a right child. Our proof focuses on the four cases shown in lines 3-22
\pagebreak \bigbreak \noindent 
\begin{cppcode}
RB-DELETE-FIXUP(T, x)
\end{cppcode}







\pagebreak
\subsubsection{C++ implementation}
\bigbreak \noindent 
\paragraph{The node structure}
\bigbreak \noindent \bigbreak \noindent 
\begin{cppcode}
    struct node {
        enum color {BLACK, RED};

        int data=0;
        bool color;
        node* left, *right, *parent;

        node() = default;
        node(int data) : data(data) {}
    };
\end{cppcode}
\bigbreak \noindent 
A red-black bst node in this example has three pointers, left, right, and parent. It also has a color  and a data field. The color enum defines the colors. Black is zero, red is one.
\bigbreak \noindent 
\paragraph{The tree class and defining nil}
\bigbreak \noindent \bigbreak \noindent 
\begin{cppcode}
    class tree {
        node* root;
        node* nil;

        tree() {
            nil = new node();
            nil->color = node::BLACK;
            nil->left = nil;
            nil->right = nil;
            nil->parent = nil;
            root = nil;
        }
    };
\end{cppcode}
\bigbreak \noindent 
We create two nodes, a root node and the nil node. In the constructor, we create the nil node, and set its attributes. An empty tree is one whos root is nil. Note that all nodes that would be nullptrs in a standard binary search tree now become the nil node in the red-black tree

\pagebreak 
\paragraph{Rotation methods}
\bigbreak \noindent \bigbreak \noindent 
\begin{cppcode}
    void left_rotate(node* x) {
        node* y = x->right;
        x->right = y->left;

        if (y->left != nil) {
            y->left->parent = x;
        }

        y->parent = x->parent;
        if (x->parent == nil) {
            root = y;
        } else if (x == x->parent->left) {
            x->parent->left = y;
        } else {
            x->parent->right = y;
        }
        y->left = x;
        x->parent = y;
    }
\end{cppcode}
\bigbreak \noindent 
\texttt{right\_rotate} is achieved by replacing left with right, and right with left in \texttt{left\_rotate}.

\pagebreak 
\paragraph{Insert}
\bigbreak \noindent \bigbreak \noindent 
\begin{cppcode}
    void insert(int element) {
        node* z = new node(element);
        z->left = nil;
        z->right = nil;
        z->color = node::RED;

        node* x = root;
        node* y = nil;

        while (x != nil) {
            y = x;
            if (z->data < x->data) {
                x = x->left;
            } else {
                x = x->right;
            }
        }

        z->parent = y;
        if (y == nil) {
            root = z;
        } else if (z->data < y->data) {
            y->left = z;
        } else {
            y->right = z;
        }

        insert_fixup(z);
    }
\end{cppcode}


\pagebreak 
\paragraph{Insert fixup}
\bigbreak \noindent 
\begin{cppcode}
    void insert_fixup(node* z) {
        while (z->parent->color == node::RED) {
            if (z->parent == z->parent->parent->left) {
                node* y = z->parent->parent->right;
                if (y->color == node::RED) {
                    z->parent->color = node::BLACK;
                    y->color = node::BLACK;
                    z->parent->parent->color = node::RED;
                    z = z->parent->parent;
                } else {
                    if (z == z->parent->right) {
                        z = z->parent;
                        left_rotate(z);
                    }
                    z->parent->color = node::BLACK;
                    z->parent->parent->color = node::RED;
                    right_rotate(z->parent->parent);
                }
            } else {
                node* y = z->parent->parent->left;
                if (y->color == node::RED) {
                    z->parent->color = node::BLACK;
                    y->color = node::BLACK;
                    z->parent->parent->color = node::RED;
                    z = z->parent->parent;
                } else {
                    if (z == z->parent->left) {
                        z = z->parent;
                        right_rotate(z);
                    }
                    z->parent->color = node::BLACK;
                    z->parent->parent->color = node::RED;
                    left_rotate(z->parent->parent);
                }
            }
        }
        root->color = node::BLACK;
    }
\end{cppcode}

\pagebreak \bigbreak \noindent 
\subsection{State space tree}
\bigbreak \noindent 
A state space tree is a tree structure that shows all possible states of a problem.
\begin{itemize}
    \item The root node is the initial state of the problem.
    \item Each child node is a new state that results from a possible action.
    \item The branches represent transitions between states.
    \item The leaves represent either goal states or states with no further possible moves.
\end{itemize}
\bigbreak \noindent 
Consider the state space tree for generating the three element permutations of the set $\{A,B,C\}$. There are a grand total of $3P3 = 3! = 6$ total configurations. Let's draw the state space tree and see how we can us it to find them all.
\bigbreak \noindent 
\begin{figure}[ht]
    \centering
    \incfig{statespace2}
    \label{fig:statespace2}
\end{figure}
\bigbreak \noindent 
Then we see we can start at a leaf, and traverse up the tree until we get to the root. This traversal from each leaf to the root generates our six permutations from $3P3$.









\pagebreak 
\unsect{Heaps and Priority Queues (Zero based)}
\bigbreak \noindent 
The \textit{(binary) heap} data structure is an array object that we can view as a nearly complete binary tree (see Section B.5.3), as shown in Figure 6.1. Each node of the tree corresponds to an element of the array. The tree is completely filled on all levels except possibly the lowest, which is filled from the left up to a point. An array $A[0 : n-1]$ that represents a heap is an object with an attribute $A.\text{heap-size}$, which represents how many elements in the heap are stored within array $A$. That is, although $A[0 : n-1]$ may contain numbers, only the elements in $A[0 : A.\text{heap-size}-1]$, where $0 \leq A.\text{heap-size} \leq n$, are valid elements of the heap. If $A.\text{heap-size} = 0$, then the heap is empty. The root of the tree is $A[0]$, and given the index $i$ of a node,
\bigbreak \noindent 
The \textit{(binary) heap} data structure is an array object that we can view as a nearly complete binary tree (see Section B.5.3), as shown in Figure 6.1. Each node of the tree corresponds to an element of the array. The tree is completely filled on all levels except possibly the lowest, which is filled from the left up to a point. An array $A[1 : n]$ that represents a heap is an object with an attribute $A.\text{heap-size}$, which represents how many elements in the heap are stored within array $A$. That is, although $A[1 : n]$ may contain numbers, only the elements in $A[1 : A.\text{heap-size}]$, where $0 \leq A.\text{heap-size} \leq n$, are valid elements of the heap. If $A.\text{heap-size} = 0$, then the heap is empty. The root of the tree is $A[1]$, and given the index $i$ of a node,
\bigbreak \noindent 
\fig{.6}{./figures/5.png}
\bigbreak \noindent 
there’s a simple way to compute the indices of its parent, left child, and right child with the one-line procedures PARENT, LEFT, and RIGHT.
\bigbreak \noindent 
\begin{cppcode}
    int parent(i) {
        return (i+1)/2 // Floor division
    }

    int left(i) {
        return 2i + 1
    }

    int right(i) {
        return 2i + 2:
    }
\end{cppcode}
\bigbreak \noindent 
For one based indexing, we would have $\frac{i}{2}$, $2i$, and $2i+1$
\bigbreak \noindent 
On most computers, the \texttt{LEFT} procedure can compute $2i$ in one instruction by simply shifting the binary representation of $i$ left by one bit position. Similarly, the \texttt{RIGHT} procedure can quickly compute $2i + 1$ by shifting the binary representation of $i$ left by one bit position and then adding $1$. The \texttt{PARENT} procedure can compute $\left\lfloor \frac{i}{2} \right\rfloor$ by shifting $i$ right one bit position. Good implementations of heapsort often implement these procedures as macros or inline procedures.
\pagebreak 
\subsection{Max and Min heaps}
\bigbreak \noindent 
There are two kinds of binary heaps: max-heaps and min-heaps. In both kinds, the values in the nodes satisfy a \textcolor{cyan}{heap property}, the specifics of which depend on the kind of heap. In a \textcolor{cyan}{max-heap}, the \textcolor{cyan}{max-heap property} is that for every node $i$ other than the root,
\bigbreak \noindent 
\begin{align*}
    A[\text{parent}(i)] \geq A[i]
.\end{align*}
\bigbreak \noindent 
That is, the value of a node is at most the value of its parent. Thus, the largest
element in a max-heap is stored at the root, and the subtree rooted at a node contains
values no larger than that contained at the node itself. A \textcolor{cyan}{min-heap} is organized in
the opposite way: the \textcolor{cyan}{min-heap property} is that for every node $i$ other than the
root,
\bigbreak \noindent 
\begin{align*}
    A[\text{parent}(i)] \leq A[i]
.\end{align*}
\bigbreak \noindent 
The smallest element in a min-heap is at the root.
\bigbreak \noindent 
\subsection{Heapify an array}
\bigbreak \noindent 
Heapify is a process used to maintain the heap property in a binary heap, either a max-heap or a min-heap. The heap property ensures that for every node in the heap:
\begin{itemize}
    \item In a max-heap, the value of each node is greater than or equal to the values of its children.
    \item In a min-heap, the value of each node is less than or equal to the values of its children.
\end{itemize}
\bigbreak \noindent 
Heapify is typically used when you have an unsorted array and you want to turn it into a valid heap, or after inserting or deleting an element in a heap to restore the heap property.
\bigbreak \noindent 
The bottom-up approach to turning an array into a heap is also called the heapify process. This method is efficient for building a heap from an unordered array and runs in $O(n)$ time, which is faster than building the heap using successive insertions ( $O(nlg \ n)$)
\bigbreak \noindent 
We start from the first non-leaf node, which is at position $\frac{n-1}{2}$, where $n$ is the size of the array.
\bigbreak \noindent 
Traverse all non-leaf nodes from the last one to the root (from right to left in the array
\bigbreak \noindent 
For each non-leaf node, apply the sift down operation (also called percolate down), which ensures that the subtree rooted at this node satisfies the heap property.
\bigbreak \noindent 
\textbf{Percolate Down Operation:}
\begin{enumerate}
    \item Compare the node with its children.
    \item If the heap property is violated (for example, in a max-heap, the node is smaller than one of its children), swap the node with the largest child (for max-heap) or smallest child (for min-heap).
    \item Repeat this process down the subtree until the heap property is restored or the node becomes a leaf.
\end{enumerate}

\pagebreak 
\subsection{Min-heap in c++}
\bigbreak \noindent 
\begin{cppcode}
void min_heapify(int arr[], int n, int i) {
    int smallest = i;    // Initialize smallest as root
    int left = 2 * i + 1;   // Left child index
    int right = 2 * i + 2;  // Right child index

    // If left child is smaller than the root
    if (left < n && arr[left] < arr[smallest])
        smallest = left;

    // If right child is smaller than the smallest so far
    if (right < n && arr[right] < arr[smallest])
        smallest = right;

    // If the smallest is not the root
    if (smallest != i) {
        std::swap(arr[i], arr[smallest]);

        // Recursively heapify the affected subtree
        min_heapify(arr, n, smallest);
    }
}

void build_heap(int arr[], int n) {
    // Start from the last non-leaf node and heapify each node
    for (int i = (n - 1) / 2; i >= 0; --i) {
        min_heapify(arr, n, i);
    }
}
\end{cppcode}
\bigbreak \noindent 
The min\_heapify function is designed to maintain the min-heap property for a subtree rooted at a given index $i$. The process begins by identifying the left and right children of the node at index $i$. The function then compares the current node with its children to find the smallest value. If one of the children is smaller than the current node, a swap occurs between the node and the smallest child. After the swap, the function is recursively called on the affected child to ensure that the min-heap property is maintained further down the subtree.

\pagebreak 
\subsection{Max-heap in c++}
\bigbreak \noindent 
\begin{cppcode}
    void heapify(int arr[], int n, int i) {
        int largest = i;
        int left = 2 * i + 1;
        int right = 2 * i + 2;

        if (left < n && arr[left] > arr[largest]) {
            largest = left;
        }

        if (right < n && arr[right] > arr[largest]) {
            largest = right;
        }

        if (largest != i) {
            std::swap(arr[i], arr[largest]);
            heapify(arr, n, largest);
        }
    }


    void build_heap(int arr[], int n) {
        for (int i=n-1/2; i>=0; --i) {
            heapify(arr, n, i);
        }
    }
\end{cppcode}

\pagebreak 
\subsection{Percolating}
\bigbreak \noindent 
Percolating in heaps refers to adjusting a node’s position to maintain the heap property, which can occur in two directions: percolate up or percolate down.
\bigbreak \noindent 
Note that the percolate direction refers to the direction in which we move a node to get its correct locating in the heap.
\bigbreak \noindent 
If we are comparing a given node to its children, its percolate down. If we compare with its parent, its percolate up.

\bigbreak \noindent 
\subsubsection{Percolate up}
\bigbreak \noindent 
Used when adding a new element to the heap (usually at the end). The new element is moved up the heap by comparing it with its parent node. If it violates the heap property (e.g., it’s smaller than its parent in a min-heap), it swaps with the parent. This continues until the node is correctly positioned.
\bigbreak \noindent 
\begin{cppcode}
void percUp(vector<int>& v, int i) {
    while (i>0) {
        int parent = (i-1)/2;
        if (v[i] > v[parent]) {
            swap(v[i], v[parent]);
            i = parent;
        } else break; // In the correct place
    }
}
\end{cppcode}


\pagebreak \bigbreak \noindent 
\subsubsection{Percolate down}
\bigbreak \noindent 
Used when removing the root element (like in a heap deletion). The last element is moved to the root and then "percolates down" by swapping with the smaller child (in a min-heap) if it violates the heap property. This process repeats until the node is correctly placed.
\bigbreak \noindent 
\begin{cppcode}
    void percDown(vector<int>& v, int n, int i) {
        // Assume the largest is the current node
        int largest = i;
        // Get index of both children
        int left = 2*i+1;
        int right = 2*i+2;

        // If the left child exists, and is larger, update largest
        if (left < n && v[left] > v[largest]) {
            largest = left;
        }
        // If the right child exists, and is larger, update largest
        if (right < n && v[right] > v[largest]) {
            largest = right;
        }

        // If the largest was changed, swap. Then call percUp on the node that had the largest value.
        if (largest != i) {
            swap(v[i], v[largest]);
            percDown(v,n,largest);
        }
    }
\end{cppcode}
\bigbreak \noindent 
Because this function takes a node and compares it with its children, the value in the node moves down. Hence, percolate down.


\pagebreak 
\subsection{Inserting into a heap}
\bigbreak \noindent 
In a top-down approach to inserting into a min-heap, you maintain the heap property while inserting a new element. Specifically, you start by placing the new element at the bottom of the heap (in the next available position), and then you "bubble up" (also known as "heapify up" or "percolate up") to restore the heap property.
\begin{itemize}
    \item \textbf{Insert the new element at the bottom:} Add the new element in the first available position (the next available spot in the array representation, which maintains a complete binary tree structure)
    \item \textbf{Bubble up (heapify up):} Compare the newly inserted element with its parent.
        \bigbreak \noindent 
        If the new element is smaller than its parent (which violates the min-heap property), swap the two.
        \bigbreak \noindent 
        Continue this process (i.e., compare with the next parent) until:
        \begin{itemize}
            \item The new element is larger than or equal to its parent, or
            \item The element reaches the root of the heap.
        \end{itemize}
    \item The process terminates when the new element is in a position where it is larger than its parent or becomes the root.
\end{itemize}
\bigbreak \noindent 
The code for a min-heap insert is as follows
\bigbreak \noindent 
\begin{cppcode}
    void percUp(vector<int>& v, int i) {
        int parent = (i-1)/2;

        while (i>0 && v[i] < v[parent]) {
            std::swap(v[i], v[parent]);

            i = parent;
        }
    }

    void heapInsert(vector<int>& v, int element) {
        v.push_back(element);
        int index = v.size() - 1;
        percUp(v, index);
    }
\end{cppcode}

\pagebreak 
\subsection{Removing the root}
\bigbreak \noindent 
Removing an element from a min-heap (typically the root, i.e., the minimum element) involves maintaining the min-heap property after removal. The most common removal operation is to delete the root (the smallest element in a min-heap). The process of removal involves the following steps:
\bigbreak \noindent 
\begin{enumerate}
    \item \textbf{Replace the root with the last element:} The root (at index 0) is the element to be removed, so we replace it with the last element in the heap (this ensures the complete binary tree property is maintained). 
    \item \textbf{Remove the last element:} After swapping, the last element is removed from the heap (usually by reducing the heap's size).
    \item \textbf{Heapify down (percolate down):} Starting from the root, compare the new root element with its children.
        \bigbreak \noindent 
        If the root is larger than any of its children, swap it with the smaller child (to maintain the min-heap property).
        \bigbreak \noindent 
        Continue this process until the heap property is restored (i.e., the element is smaller than both children or it reaches a leaf node).
\end{enumerate}

\bigbreak \noindent 
\begin{cppcode}
    void removeRoot(vector<int>& v) {
        std::swap(v[0], v[v.size()-1]);
        v.pop_back();
        min_heapify(v,v.size(), 0);
    }
\end{cppcode}

\pagebreak 
\subsection{Removing an arbitary node}
\bigbreak \noindent 
To remove an arbitrary node from a min-heap (not just the root), the process is slightly more complex than simply removing the root
\bigbreak \noindent 
\begin{enumerate}
    \item \textbf{Replace the node to be deleted with the last element:} Swap the node to be deleted with the last element in the heap. This ensures that the complete binary tree property is maintained.
    \item \textbf{Remove the last element:} After the swap, the last element (now at the position of the deleted node) is removed from the heap.
    \item \textbf{Restore the heap property:} After the swap, the heap property might be violated both upwards and downwards. So, depending on the value of the swapped element, either heapify up or heapify down from the position where the node was deleted.
        \begin{itemize}
            \item Heapify up if the swapped element is smaller than its parent.
            \item Heapify down if the swapped element is larger than its children
        \end{itemize}
\end{enumerate}
\bigbreak \noindent 
We continue this process until there are no more nodes that match the passed element. The c++ code for a min-heap erase function is as follows.
\bigbreak \noindent 
\begin{cppcode}
    void erase(vector<int>& v, int element) {
        bool found = false;
        while (!found) {
            found = false;
            int i=0;
            for (;i<(int)v.size(); ++i) {
                if (v[i] == element) {
                    found = true;
                    std::swap(v[i], v[v.size()-1]);
                    v.pop_back();

                    if (v[i] > v[(i-1)/2]) {
                        min_heapify(v, v.size(), i);
                    } else {
                        percUp(v, i);
                    }
                }
            }
            if (i == v.size()) break;
        }
    }
\end{cppcode}








\pagebreak 
\subsection{Priority queues}
\bigbreak \noindent 
Heaps are used to implement priority queues because they efficiently maintain the highest (or lowest) priority element at the root. In a max-heap, the largest element is always at the top, while in a min-heap, the smallest element is. This allows for quick access to the highest priority element in $O(1)$ time. Insertion and removal (reordering) of elements take $O(lg\ n)$ time, making heaps an optimal choice for priority queues where these operations need to be fast and frequent.
\bigbreak \noindent 
\subsubsection{Interface}
\bigbreak \noindent 
\begin{itemize}
    \item \textbf{Insert (or Enqueue):} Adds an element to the priority queue based on its priority.
    \item \textbf{Pop}: Retrieve and remove the item with the highest priority (root of the heap)
    \item \textbf{Top}: Retrieve the item with the highest priority
    \item \textbf{Size:} Get the number of items in the queue
    \item \textbf{Empty:} Checks if the priority queue has any elements.
\end{itemize}
\bigbreak \noindent 
\begin{cppcode}
    class priority_queue {
        vector<int> heap; // or any random access container

        void percDown(int n, int index);
        void PercUp(int index)

    public:
        void insert(int element);
        int pop();
        int top();
        size_t size();
        bool empty();
    };
\end{cppcode}
\bigbreak \noindent 
Percolate down, up, insert, and pop can be found in the heap examples above.

\pagebreak 
\subsubsection{Insert, pop, and top}
\bigbreak \noindent 
These are simple methods to implement.
\bigbreak \noindent 
\begin{cppcode}
    // Insert into the heap
    void insert(int element) {
        heap.push_back(element);
        int index = heap.size() - 1;
        percUp(index);
    }

    // Retrieve and remove the root
    int pop() {
        if (heap.empty()) return -1;

        int ret = heap[0];
        std::swap(heap[0], heap[heap.size()-1]);
        heap.pop_back();
        percDown((int)heap.size(), 0);
        return ret;
    }

    // Retrieve the root
    int top() {
        return heap[0];
    }
\end{cppcode}

\pagebreak 
\subsubsection{Size and Empty}
\bigbreak \noindent 
\begin{cppcode}
    size_t size() {
        return (size_t)heap.size();
    }

    bool empty() {
        return heap.empty();
    }
\end{cppcode}

\pagebreak 
\subsection{Median of a Stream}
\bigbreak \noindent 

\pagebreak 
\unsect{Sorting}
\bigbreak \noindent 
Sorting is a fundamental concept in data structures and algorithms where elements in a collection are arranged in a specific order, typically ascending or descending. Sorting enables efficient data access, searching, and data organization in applications. There are various sorting algorithms, each with different characteristics and efficiency, such as time and space complexity.
\bigbreak \noindent 
\subsection{Bubble, selection, insertion}
\bigbreak \noindent 
The three simplest sorting algorithms are bubble sort, selection sort, and insertion sort. They are all $O(n^{2})$ comparison based sorting algorithms.
\bigbreak \noindent 
\subsubsection{Bubble sort}
\bigbreak \noindent 
Bubble Sort repeatedly compares adjacent elements and swaps them if they are in the wrong order, "bubbling" the largest unsorted element to the end of the list on each pass.
\bigbreak \noindent 
\begin{cppcode}
    void bubbleSort(vector<int>& v, int n) {
        bool swapped = true;
        while (swapped) {
            swapped = false;
            for (int i=0; i<n-1; ++i) {
                if (v[i+1] < v[i]) {
                    swap(v[i], v[i+1]);
                    swapped=true;
                }
            }
            --n;
        }
    }
\end{cppcode}
\bigbreak \noindent 
By decrementing \( n \) after each complete pass, you avoid unnecessary comparisons in already sorted parts of the vector, making the algorithm slightly more efficient without affecting the \( O(n^2) \) time complexity.
\begin{itemize}
    \item \textbf{Outer Loop}: The \texttt{while} loop continues as long as swaps are happening, so it terminates early if the list becomes sorted before completing all \( n \) passes.
    \item \textbf{Inner Loop}: The \texttt{for} loop compares adjacent elements and swaps them if they’re out of order. After each pass, the largest unsorted element "bubbles up" to the correct position, so the next pass can ignore the last element.
\end{itemize}
\pagebreak \bigbreak \noindent 
\paragraph{Complexity}
\bigbreak \noindent \bigbreak \noindent 
\textbf{Worst Case}: \( O(n^2) \) \\
\bigbreak \noindent 
In the worst case (a reverse-sorted list), Bubble Sort requires \( n \) passes, and each pass involves up to \( n - 1 \) comparisons.

\bigbreak \noindent 
\textbf{Average Case}: \( O(n^2) \) \\
\bigbreak \noindent 
On average, Bubble Sort still performs \( O(n^2) \) comparisons due to repeated passes through the entire array.

\bigbreak \noindent 
\textbf{Best Case}: \( O(n) \) \\
\bigbreak \noindent 
If the list is already sorted, Bubble Sort can stop early if no swaps occur during a pass (often implemented with a flag to detect this). In this case, it only takes one pass to confirm that the list is sorted.

\pagebreak 
\subsubsection{Selection sort}
\bigbreak \noindent 
Selection Sort sorts an array by repeatedly finding the minimum element from the unsorted part and moving it to the beginning.
\begin{enumerate}
    \item Start at the beginning of the array.
    \item For each position $i$, find the smallest element in the unsorted portion (from $i$ to the end).
    \item Swap this minimum element with the element at position $i$
    \item Move to the next position and repeat until the array is sorted.
\end{enumerate}
\bigbreak \noindent 
\begin{cppcode}
    void selectionSort(vector<int>& v, int n) {
        for (int i=0; i<n-1; ++i) {
            int min = i;
            for (int j=i+1; j<n; ++j) {
                if (v[j] < v[min]) {
                    min = j;
                }
            }
            std::swap(v[min], v[i]);
        }
    }
\end{cppcode}
\bigbreak \noindent 
\paragraph{Complexity}
\bigbreak \noindent \bigbreak \noindent 
\textbf{Worst Case}: \( O(n^2) \) \\
\bigbreak \noindent 
Selection Sort always performs \( n \) passes, each requiring a search through the unsorted portion to find the minimum element, resulting in \( O(n^2) \) comparisons.

\bigbreak \noindent 
\textbf{Average Case}: \( O(n^2) \) \\
\bigbreak \noindent 
Selection Sort performs the same number of comparisons regardless of the initial order of elements, as it always looks for the minimum in the unsorted portion.

\bigbreak \noindent 
\textbf{Best Case}: \( O(n^2) \) \\
\bigbreak \noindent 
Even if the array is already sorted, Selection Sort will still go through \( n \) passes and \( O(n^2) \) comparisons, making it inefficient for nearly sorted data.


\pagebreak 
\subsubsection{Insertion sort}
\bigbreak \noindent 
Insertion Sort sorts an array by building a sorted portion one element at a time
\bigbreak \noindent 
\begin{enumerate}
    \item Start with the second element, assuming the first element is trivially sorted.
    \item For each element, compare it with the elements in the sorted portion to its left.
    \item Shift larger elements one position right until you find the correct spot for the current element.
    \item Insert the current element in its correct position.
    \item Repeat until all elements are sorted.
\end{enumerate}
\bigbreak \noindent 
\begin{cppcode}
    void insertionSort(vector<int>& v, int n) {
        for (int i=1; i<n; ++i) {
            int key=v[i];
            int j=i-1;

        while (j>=0 && v[j] > key) {
            v[j+1] = v[j];
            --j;
        }
        v[j+1] = key;
    }
}
\end{cppcode}
\bigbreak \noindent 
Imagine sorting a hand of playing cards:
\begin{itemize}
    \item You pick up cards one by one.
    \item For each new card, you compare it with the cards in your hand from right to left, sliding them over if they’re larger than the new card, until you find the correct spot to insert it.
    \item Each card you add to your hand stays sorted with the previous ones, just like key gets inserted in the sorted sub-array.
\end{itemize}
This method is efficient when the list is nearly sorted since fewer shifts are needed, but it becomes less efficient for randomly ordered arrays due to repeated comparisons and shifts.

\pagebreak 
\paragraph{Complexity}
\bigbreak \noindent \bigbreak \noindent 
\textbf{Worst Case}: \( O(n^2) \) \\
\bigbreak \noindent 
In the worst case (reverse-sorted list), each element is compared with all previous elements, resulting in \( O(n^2) \) comparisons and shifts.

\bigbreak \noindent 
\textbf{Average Case}: \( O(n^2) \) \\
\bigbreak \noindent 
Insertion Sort generally performs \( O(n^2) \) operations in the average case due to repeated comparisons and shifts.

\bigbreak \noindent 
\textbf{Best Case}: \( O(n) \) \\
\bigbreak \noindent 
If the array is already sorted, Insertion Sort only needs to confirm each element is in place, requiring just \( O(n) \) comparisons and no shifts. This makes it very efficient for nearly sorted arrays.







\pagebreak \bigbreak \noindent 
\subsection{Heap sort}
\bigbreak \noindent 
Heap Sort is a comparison-based sorting algorithm that uses a binary heap data structure, specifically a max-heap, to sort elements in ascending order (or a min-heap for descending order).
\bigbreak \noindent 
\begin{enumerate}
    \item \textbf{Build a Max-Heap:} Convert the array into a max-heap, where each parent node is greater than its children. This ensures the largest element is at the root.
    \item \textbf{Extract Maximum and Swap:} Swap the root (largest element) with the last element in the heap. This moves the largest element to its correct position in the sorted array.
    \item \textbf{Heapify:} After the swap, the heap may no longer satisfy the heap property. Restore the max-heap by performing a "heapify" operation on the root to maintain the max-heap structure.
    \item \textbf{Repeat:} Continue extracting the maximum element, swapping, and heapifying, reducing the heap size each time. This process sorts the array in-place.
\end{enumerate}
\bigbreak \noindent 
\begin{cppcode}
    void heapSort(vector<int>& v, int n) {
        heapify(v, n);
        int last = n-1;
        while (last > 0) {
            swap(v[0], v[last--]);
            percDown(v, last, 0);
        }
    }
\end{cppcode}
\bigbreak \noindent 
The functions \textit{heapify} and \textit{percDown} can be found in the \textit{max-heap in c++} section above, where they are called \textit{build-heap} and \textit{heapify} respectively.
\bigbreak \noindent 
\subsubsection{Complexity}
\bigbreak \noindent 
Heap Sort has a time complexity of $O(n\text{lg}n)$ in the best, worst, and average cases, making it more efficient than  $O(n^{2})$ sorting algorithms for larger datasets
\bigbreak \noindent 
Bulding the max-heap from an arbitary container takes $O(n)$ time. Re-heapify the remaining elements (restore the max-heap property) by "sifting down" the new root takes $O(\text{lg} n)$ time. Since we do this sift operation $n$ times, the  complexity of \textit{heapSort} is therefore $O(n\text{lg}n)$, which makes the entire algorithm $O(n) + O(n\text{lg}n) = O(n \text{lg}n$.

\pagebreak 
\subsection{BST Sort}
\bigbreak \noindent 
BST Sort is a sorting algorithm that leverages a Binary Search Tree (BST) to sort elements. It works by inserting all elements into a BST and then performing an in-order traversal of the tree to retrieve the elements in sorted order.
\bigbreak \noindent 
\subsubsection{Insert}
\bigbreak \noindent 
The insert for bst follows the standard bst insert method described in a previous section.
\bigbreak \noindent 
\begin{cppcode}
    node* insert(node* p, int data) {
        if (!p) return new node(data);

        if (data < p->data) {
            p->left = insert(p->left, data);
        } else {
            p->right = insert(p->right, data);
        }

        return p;
    }
\end{cppcode}

\bigbreak \noindent 
\subsubsection{The inorder traversal}
\bigbreak \noindent 
The inorder follows the same logic as before, but when processing each node, we insert the data member into a passed vector.
\bigbreak \noindent 
\begin{cppcode}
    void inorder(node* p, vector<int>& v) {
        if (!p) return;
        inorder(p->left, v);
        v.push_back(p->data);
        inorder(p->right, v);
    }
\end{cppcode}

\pagebreak \bigbreak \noindent 
\subsubsection{The BST sort function}
\bigbreak \noindent 
\begin{cppcode}
    void bstsort(vector<int>& v) {
        vector<int> sorted;
        node* root = nullptr;

        for (const auto& item : v) {
            root = insert(root, item);
        }
        inorder(root, sorted);
        clear(root);

        v = sorted;
    }
\end{cppcode}
\bigbreak \noindent 
The bstsort function sorts a vector of integers using a Binary Search Tree (BST) approach. It starts by creating an empty vector, sorted, which will eventually hold the sorted elements, and initializes root as nullptr, representing an initially empty BST.
\bigbreak \noindent 
For each item in the input vector v, the function calls insert to add the item to the BST, progressively building the tree in a way that respects the BST property (left child nodes are less than the parent node, and right child nodes are greater). After constructing the BST, the function performs an in-order traversal using inorder, which appends each node’s data to the sorted vector in ascending order.
\bigbreak \noindent 
Finally, clear is called to delete all nodes from the BST, freeing up dynamically allocated memory. The function then assigns sorted back to the input vector v, so v now contains the elements in sorted order.
\bigbreak \noindent 
BST Sort is more of a conceptual exercise in data structures than a practical sorting method because balanced tree structures (e.g., AVL trees, red-black trees) are required to ensure $O(nlg\ n)$ performance consistently.

\pagebreak 
\subsubsection{Inplace sorting}
\bigbreak \noindent 
To perform the sort "in-place" in bstsort, we can eliminate the extra sorted vector by writing the sorted values directly back into the input vector v during the in-order traversal. This approach leverages an index to keep track of the position in v where each next sorted value should be placed
\bigbreak \noindent 
\begin{cppcode}
    void inorder2(node* p, vector<int>& v, int& index) {
        if (!p) return;
        inorder2(p->left, v, index);
        v[index++] = p->data;
        inorder2(p->right, v, index);
    }

    void bstsort(vector<int>& v) {
        node* root = nullptr;

        for (const auto& item : v) {
            root = insert(root, item);
        }
        int index = 0;
        inorder2(root, v, index);
        clear(root);
    }
\end{cppcode}

\bigbreak \noindent 
\subsubsection{Complexity}
\bigbreak \noindent 
\begin{itemize}
    \item \textbf{Average Case:} $O(n\text{lg}\ n)$, where $n$ is the number of elements. This is because inserting each element in a balanced BST takes $O(\text{lg}\ n)$ time
    \item \textbf{Worst Case:} $O(n^{2})$, if the tree becomes unbalanced, like when inserting elements in sorted order into a simple BST, resulting in a degenerate tree (like a linked list).
    \item \textbf{Space Complexity:} $O(n)$, for storing the BST nodes.
\end{itemize}

\pagebreak 
\subsection{Quick sort}
\bigbreak \noindent 
Quicksort, like merge sort, applies the divide-and-conquer method
\bigbreak \noindent 
\begin{itemize}
    \item \textbf{Divide} by partitioning (rearranging) the array \( A[p : r] \) into two (possibly empty) subarrays \( A[p : q - 1] \) (\textit{the low side}) and \( A[q + 1 : r] \) (\textit{the high side}) such that each element in the low side of the partition is less than or equal to the \textit{pivot} \( A[q] \), which is, in turn, less than or equal to each element in the high side. Compute the index \( q \) of the pivot as part of this partitioning procedure.
    \item \textbf{Conquer} by calling quicksort recursively to sort each of the subarrays \( A[p : q - 1] \) and \( A[q + 1 : r] \).
    \item \textbf{Combine} by doing nothing: because the two subarrays are already sorted, no work is needed to combine them. All elements in \( A[p : q - 1] \) are sorted and less than or equal to \( A[q] \), and all elements in \( A[q + 1 : r] \) are sorted and greater than or equal to the pivot \( A[q] \). The entire subarray \( A[p : r] \) cannot help but be sorted!
\end{itemize}
The \textsc{Quicksort} procedure implements quicksort. To sort an entire \( n \)-element array \( A[1 : n] \), the initial call is \textsc{Quicksort}(\( A, 1, n \)).
\bigbreak \noindent 
\subsubsection{Partitioning}
\bigbreak \noindent 
The key to the algorithm is the PARTITION procedure, which rearranges the subarray $A[p:r]$ in place, returning the index of the dividing point between the two sides of the partition
\bigbreak \noindent 
PARTITION always selects the element $A[r]$ (last element) as the pivot.
\bigbreak \noindent 
\begin{cppcode}
PARTITION(A, p,r)
    x = A[r]                // the pivot
    i = p-1                 // highest index into the low side
    for j = p to r - 1      // process each element other than the pivot
        if A[j] <= x        // does this element belong on the low side?
            ++i                 // index of a new slot in the low side 
            swap(A[i], A[j])    // put this element there 
    swap(A[i+1], A[r])      // pivot goes just to the right of the low side 
    return i + 1            // new index of the pivot 
\end{cppcode}
\bigbreak \noindent 
At the beginning of each iteration of the loop of lines 3–6, for any array index \( k \), the following conditions hold:
\begin{enumerate}
    \item if \( p \leq k \leq i \), then \( A[k] \leq x \) (the tan region of Figure 7.2);
    \item if \( i + 1 \leq k \leq j - 1 \), then \( A[k] > x \) (the blue region);
    \item if \( k = r \), then \( A[k] = x \) (the yellow region).
\end{enumerate}
\bigbreak \noindent 
\textbf{Note: } an invariant is a property or condition that remains true throughout the execution of a program, particularly during specific phases or loops. In other words, an invariant is something that holds steady or stays consistent at key points in an algorithm, typically at the beginning or end of each loop iteration
\bigbreak \noindent 
\fig{.8}{./figures/16.png}

\bigbreak \noindent 
\begin{cppcode}
int partition(vector<int>& v, int p, int r) {
    int x = v[r];
    int i=p-1;

    for (int j=p; j<r; ++j) {
        if (v[j] <= x) {
            ++i;
            swap(v[j], v[i]);
        }
    }
    swap(v[i+1], v[r]);
    return i+1;
}
\end{cppcode}

\bigbreak \noindent 
\subsubsection{The quick sort procedure}
\bigbreak \noindent 
\begin{cppcode}
void qsort(vector<int>& v, int start, int end) {
    if (start < end) {
        int pivot = partition(v, start, end);
        qsort(v, start, pivot-1);
        qsort(v, pivot+1, end);
    }
}
\end{cppcode}
\bigbreak \noindent 
In each recursive call to qsort, the function first checks if the start index is less than the end index. This condition serves as the base case for the recursion, ensuring that the function will terminate when the segment of the array being processed is either empty or has only one element (both of which are already sorted by default). If start is not less than end, the function simply returns without making further recursive calls, signaling that this segment is already sorted.
\bigbreak \noindent 
When start is less than end, the function calls partition, passing in the vector and the current segment boundaries (start and end). The partition function typically selects a pivot element, rearranges elements in the segment so that all elements less than or equal to the pivot appear on the left side, and all elements greater than the pivot appear on the right side. partition then returns the index of the pivot element after rearranging. This index is crucial because it marks the point in the array where the pivot element is correctly positioned in the sorted order.
\bigbreak \noindent 
After the array is partitioned around the pivot, the qsort function calls itself recursively on the two subarrays divided by the pivot. The first recursive call processes the left subarray, from start to pivot - 1, containing elements less than or equal to the pivot. The second recursive call processes the right subarray, from pivot + 1 to end, containing elements greater than the pivot. Each of these recursive calls will, in turn, partition their respective subarrays and further divide and sort them, following the same logic.
\bigbreak \noindent 
Through this recursive process, each segment of the array is progressively divided and sorted around pivot elements, until every element is in its correct position. This "divide and conquer" approach ensures that, by the end of the process, the entire array is sorted in ascending order. The function achieves efficiency by reducing the problem size with each recursive call, ultimately sorting the array in an average time complexity of $O(n\text{lg}\ n)$, making QuickSort one of the fastest sorting algorithms in practice.

\pagebreak \bigbreak \noindent 
\subsubsection{A better partition}
\bigbreak \noindent 
In the example above, we always selected the last element as the pivot. We could instead select the middle element.
\bigbreak \noindent 
\begin{cppcode}
    int partition2(std::vector<int>& v, int p, int r) {
        int middle = p + (r - p) / 2; // Calculate the middle index
        std::swap(v[middle], v[r]);   // Move the middle element to the end temporarily

        int x = v[r];                 // Use the last element (original middle) as pivot
        int i = p - 1;

        for (int j = p; j < r; ++j) {
            if (v[j] <= x) {
                ++i;
                std::swap(v[j], v[i]);
            }
        }
        std::swap(v[i + 1], v[r]);    // Place the pivot in its correct position
        return i + 1;                 // Return the pivot index
    }
\end{cppcode}

\bigbreak \noindent 
\subsubsection{Median of three partition}
\bigbreak \noindent 
To use the median-of-three approach for selecting the pivot in QuickSort, we need to compare three elements in the array — typically the first, middle, and last elements of the current subarray — and select the median (the middle value of these three) as the pivot. This approach helps improve the performance of QuickSort by reducing the chances of poor pivot choices in partially sorted or skewed data.
\begin{itemize}
    \item \textbf{Identify the Three Elements:} We find the indices of the first element, the middle element, and the last element of the current subarray.
    \item \textbf{Calculate the Median of Three:} We choose the median value among these three elements and use it as the pivot.
    \item \textbf{Swap the Pivot to the End:} To keep the partition logic the same, we swap the median-of-three pivot element with the last element, so we can proceed with the same partitioning logic as before.
\end{itemize}
\bigbreak \noindent 
\begin{cppcode}
    int partition3(std::vector<int>& v, int p, int r) {
        int middle = p + (r-p) / 2;

        // Determine the median of v[p], v[middle], and v[r]
        if ((v[p] > v[middle]) != (v[p] > v[r])) {
            std::swap(v[p], v[r]); // Median is at v[p]
        } else if ((v[middle] > v[p]) != (v[middle] > v[r])) {
            std::swap(v[middle], v[r]); // Median is at v[middle]
        } 
        // If v[r] is the median, no swap is needed since we want it as the pivot

        int x = v[r]; // The pivot is now the median of the three values
        int i = p - 1;

        // Standard partition logic
        for (int j = p; j < r; ++j) {
            if (v[j] <= x) {
                ++i;
                std::swap(v[j], v[i]);
            }
        }
        std::swap(v[i + 1], v[r]); // Place the pivot in its correct position
        return i + 1; // Return the pivot index
    }
\end{cppcode}

\pagebreak 
\bigbreak \noindent 
\subsubsection{Quicksort with iterators}
\bigbreak \noindent 
\begin{cppcode}
    template<typename ForwardIt>
    ForwardIt partition4(ForwardIt p, ForwardIt r) {
        auto middle = p + std::distance(p, r) / 2; // Calculate the middle iterator
        std::iter_swap(middle, r - 1);   // Move the middle element to the end temporarily

        auto pivot = *(r - 1); // Use the last element (original middle) as pivot
        auto i = p; // Boundary for elements less than or equal to the pivot

        for (auto j = p; j != r - 1; ++j) { // Iterate up to the pivot position
            if (*j <= pivot) {
                std::iter_swap(j, i);
                ++i;
            }
        }
        std::iter_swap(i, r - 1); // Place the pivot in its correct position
        return i; // Return the pivot iterator
    }

    template <typename ForwardIt>
    void qsort2(ForwardIt start, ForwardIt end) {
        if (std::distance(start, end) > 1) { // Correct base case: at least two elements
            auto pivot = partition4(start, end);
            qsort2(start, pivot); // Sort elements before the pivot
            qsort2(pivot + 1, end); // Sort elements after the pivot
        }
    }
\end{cppcode}





\pagebreak 
\unsect{Multi-way (m-way) search trees}
\bigbreak \noindent 
A multiway search tree is a type of search tree where each node can have more than two children, unlike binary search trees, which are limited to two children per node. Multiway search trees generalize binary search trees by allowing nodes to hold multiple keys and have multiple pointers to child nodes, making them well-suited for managing large amounts of data in a balanced structure.
\bigbreak \noindent 
An $m$-Way tree of order $m$, each node contains a maximum of $m - 1$ elements and $m$ children.
\bigbreak \noindent 
The goal of an $m$-Way search tree of height $h$ is to achieve $O(h)$ accesses for an insert, delete, or retrieval operation. Therefore, it ensures that the height $h$ is close to $\log_m(n + 1)$.
\bigbreak \noindent 
Each internal node of a multi-way search tree $T$:
\begin{enumerate}
    \item Has at least two children
    \item stores a collection of items of the form $(k, x)$, where $k$ is a key and $x$ is an element
    \item contains $d - 1$ items, where $d$ is the number of children
\end{enumerate}
\bigbreak \noindent 
Children of each internal node are "between" items. All keys in the subtree rooted at the child fall between keys of those items
\bigbreak \noindent 
\fig{.5}{./figures/6.png}
\bigbreak \noindent 
\subsection{Multi-way Searching}
\bigbreak \noindent 
Similar to binary searching, where if $s < k_{1}$, search the leftmost child. If $s>k_{d-1}$ , search the rightmost child. But what if $d>2$? Simply Find two keys $k_{i-1}$ and $k_{i}$ between which $s$ falls, and search the child $v_{i}$ .

\pagebreak 
\subsection{2-4 (2-3-4) Trees}
\bigbreak \noindent 
A 2-4 tree, also known as a 2-3-4 tree, is a multi-way search tree the following properties
\begin{enumerate}
    \item Nodes may contain 1, 2 or 3 items
    \item A node with $k$ items has $k + 1$ children, except for leaf nodes. Such a node is called $(k+1)$-node
    \item All leaves are on the same level
\end{enumerate}
\bigbreak \noindent 
\fig{.5}{./figures/7.png}
\bigbreak \noindent 
\subsubsection{Insertion}
\bigbreak \noindent 
To insert into a 2-4 tree, first, find the appropriate leaf. If there is room, just add the element to the leaf. If there is no room, move the middle item to parent and split remaining items among two children.
\bigbreak \noindent 
\fig{.5}{./figures/8.png}
\bigbreak \noindent 
\fig{.5}{./figures/9.png}

\bigbreak \noindent 
\subsubsection{Removal}
\bigbreak \noindent 
First, we find the key with a simple multi-way search. There are two cases
\begin{enumerate}
    \item \textbf{Case 1:} It may be on the leave
    \item \textbf{Case 2:} It may be in internal node
\end{enumerate}
\bigbreak \noindent 
If the item to delete is in internal node, we can reduce to case 1 by first finding its immediate predecessor, swapping them, and then removing the item.
\bigbreak \noindent 
\fig{.5}{./figures/10.png}
\bigbreak \noindent 
But what if there are not enough items in the node after removal? this is known as an \textit{underflow}. In this case, pull an item from the parent, replace it with an item from a sibling - transfer 
\bigbreak \noindent 
\fig{.5}{./figures/11.png}
\bigbreak \noindent 
But what happens if the the siblings are 2-nodes (one element), we will not be able to steal from them. In this case, we preform \textit{node merging}.
\bigbreak \noindent 
\fig{.5}{./figures/12.png}
\bigbreak \noindent 
\fig{.5}{./figures/13.png}

\bigbreak \noindent 
\subsubsection{Properties}
\bigbreak \noindent 
\begin{itemize}
    \item 2-4 trees are easy to maintain
    \item Insertion and deletion take O(log n)
    \item Balanced trees
\end{itemize}
%
\pagebreak 
\subsection{B-trees}
\bigbreak \noindent 
Up to now, all data that has been stored in the tree has been in memory. If data gets too big for main memory, what do we do? If we keep a pointer to the tree in main memory, we could bring in just the nodes that we need.
\bigbreak \noindent 
For instance, to do an insert with a BST, if we need the left child, we do a disk access and retrieve the left child. If the left child is NIL, then we can do the insert, and store the child node on the disk
\bigbreak \noindent 
But storing the data requires disk accesses, which is expensive, compared to execution of machine instructions. If we can reduce the number of disk accesses, then the procedures run faster
\bigbreak \noindent 
The only way to reduce the number of disk accesses is to increase the number of keys in a node, wee see the problem in using this technique with binary search trees... The BST allows only one key per leaf/node.
\bigbreak \noindent 
If we increase the number of keys in the nodes, how will we do any tree operations effectively?
\bigbreak \noindent 
A B-tree is a self-balancing search tree data structure that maintains sorted data and allows searches, insertions, deletions, and sequential access in logarithmic time. B-trees are especially useful for managing large blocks of data in systems like databases and file systems, where data is stored on disk or other slow-access storage and needs to be accessed efficiently.
\bigbreak \noindent 
Unlike binary trees, where each node has at most two children, B-trees are multi-way trees where each node can have multiple children. The number of children a node can have is determined by the order of the tree.
\bigbreak \noindent 
 B-trees maintain balance by ensuring that every path from the root to a leaf node has the same length, which guarantees logarithmic height and thus efficient operations.
 \bigbreak \noindent 
 Each node can contain a range of keys (from a minimum to a maximum number). This enables efficient storage and retrieval by storing more keys in fewer nodes, which minimizes the number of disk reads needed to access data.
 \bigbreak \noindent 
 The order $m$ of a B-tree defines the maximum number of children each node can have. An order-$m$ B-tree node can have up to $m-1$ keys and $m$ children.
 \bigbreak \noindent 
 The minimum degree $t$ (often used instead of order) specifies the minimum number of children a non-root node must have, which is $t$ or more.
 \begin{itemize}
     \item \textbf{Insertion:} When adding a key, nodes split if they reach their maximum capacity, ensuring the B-tree remains balanced.
     \item \textbf{Deletion:} When deleting a key, nodes may need to merge with their siblings if they go below their minimum capacity.
 \end{itemize}
 \bigbreak \noindent 
  Because each node contains multiple keys, B-trees have a low height relative to the number of keys they store. This makes B-trees very efficient for disk-based storage, as it minimizes the number of disk accesses needed.
  \bigbreak \noindent 
  B-trees are widely used in database indexing, file systems, and other applications that involve managing large volumes of data that cannot fit into main memory.

  \pagebreak 
  \unsect{Hashing (hash tables)}
  \bigbreak \noindent 
  Many applications require a dynamic set that supports only the dictionary operations INSERT, SEARCH, and DELETE. For example, a compiler that translates a
programming language maintains a symbol table, in which the keys of elements
are arbitrary character strings corresponding to identifers in the language. A hash
table is an effective data structure for implementing dictionaries. Although searching for an element in a hash table can take as long as searching for an element in a
linked list - $\Theta(n)$ time in the worst case. In practice, hashing performs extremely
well. Under reasonable assumptions, the average time to search for an element in
a hash table is $O(1)$
\bigbreak \noindent 
A hash table generalizes the simpler notion of an ordinary array. Directly addressing into an ordinary array takes advantage of the $O(1)$ access time for any array element.
\bigbreak \noindent 
To use direct addressing, you must be able to allocate an array that contains a position for every possible key
\bigbreak \noindent 
When the number of keys actually stored is small relative to the total number of possible keys, hash tables become an effective alternative to directly addressing an array, since a hash table typically uses an array of size proportional to the number of keys actually stored. Instead of using the key as an array index directly, we compute the array index from the key.

\bigbreak \noindent 
\subsection{Direct-address table}
\bigbreak \noindent 
Direct addressing is a simple technique that works well when the universe U of
keys is reasonably small. Suppose that an application needs a dynamic set in which
each element has a distinct key drawn from the universe $U = \{0,1,..,m-1\} $, where $m$ is not too large.
\bigbreak \noindent 
To represent the dynamic set, you can use an array, or \textit{direct-address table}, denoted by \( T[0 : m - 1] \), in which each position, or \textit{slot}, corresponds to a key in the universe \( U \). Figure 11.1 illustrates this approach. Slot \( k \) points to an element in the set with key \( k \). If the set contains no element with key \( k \), then \( T[k] = \text{NIL} \).
\bigbreak \noindent 
The dictionary operations \textsc{Direct-Address-Search}, \textsc{Direct-Address-Insert}, and \textsc{Direct-Address-Delete} on the following page are trivial to implement. Each takes only \( O(1) \) time.
\bigbreak \noindent 
The dictionary operations DIRECT-ADDRESS-SEARCH, DIRECT-ADDRESS INSERT, and DIRECT-ADDRESS-DELETE on the following page are trivial to implement. Each takes only $O(1)$ time
\bigbreak \noindent 
For some applications, the direct-address table itself can hold the elements in the dynamic set. That is, rather than storing an element’s key and satellite data in an object external to the direct-address table, with a pointer from a slot in the table to the object, save space by storing the object directly in the slot. To indicate an empty slot, use a special key. Then again, why store the key of the object at all? The index of the object is its key! Of course, then you’d need some way to tell whether slots are empty
\bigbreak \noindent 
\fig{.5}{./figures/14.png}
\bigbreak \noindent 
\begin{cppcode}
direct-address-search(T,k) 
    return T[k]
direct-address-insert(T,x)
    T[x.key] = x
direct-address-delete(T,x)
    T[x.key] = nil
\end{cppcode}

\bigbreak \noindent 
\subsection{Hash tables}
\bigbreak \noindent 
The downside of direct addressing is apparent: if the universe \( U \) is large or infinite, storing a table \( T \) of size \( |U| \) may be impractical, or even impossible, given the memory available on a typical computer. Furthermore, the set \( K \) of keys actually stored may be so small relative to \( U \) that most of the space allocated for \( T \) would be wasted.
\bigbreak \noindent 
When the set \( K \) of keys stored in a dictionary is much smaller than the universe \( U \) of all possible keys, a hash table requires much less storage than a direct-address table. Specifically, the storage requirement reduces to \( \Theta(|K|) \) while maintaining the benefit that searching for an element in the hash table still requires only \( O(1) \) time. The catch is that this bound is for the \textit{average-case time}\(^1\), whereas for direct addressing it holds for the \textit{worst-case time}.
\bigbreak \noindent 
With direct addressing, an element with key \( k \) is stored in slot \( k \), but with hashing, we use a \textit{hash function} \( h \) to compute the slot number from the key \( k \), so that the element goes into slot \( h(k) \). The hash function \( h \) maps the universe \( U \) of keys into the slots of a \textit{hash table} \( T[0 : m - 1] \):
\bigbreak \noindent 
\begin{align*}
    h : U \rightarrow \{0, 1, \dots, m - 1\}
.\end{align*}
\bigbreak \noindent 
where the size \( m \) of the hash table is typically much less than \( |U| \). We say that an element with key \( k \) \textit{hashes to} slot \( h(k) \), and we also say that \( h(k) \) is the \textit{hash value} of key \( k \). The hash function reduces the range of array indices and hence the size of the array. Instead of a size of $\abs{U}$, the array can have size $m$
\bigbreak \noindent 
An example of a simple, but not particularly good, hash function is $h(k) = k\ \text{mod} \ m $
\bigbreak \noindent 
There is one hitch, namely that two keys may hash to the same slot. We call this situation a collision. Fortunately, there are effective techniques for resolving the conflict created by collisions.
\bigbreak \noindent 
Of course, the ideal solution is to avoid collisions altogether. We might try to
achieve this goal by choosing a suitable hash function h. One idea is to make h appear to be <random,= thus avoiding collisions or at least minimizing their number.

\bigbreak \noindent 
\fig{.5}{./figures/15.png}
\bigbreak \noindent 
although a well-designed, random looking hash function can reduce the number of collisions, we still need a method for resolving the collisions that do occur
\bigbreak \noindent 
\subsection{Independent uniform hashing}
\bigbreak \noindent 
An "ideal" hashing function \( h \) would have, for each possible input \( k \) in the domain \( U \), an output \( h(k) \) that is an element randomly and independently chosen uniformly from the range \( \{0, 1, \dots, m - 1\} \). Once a value \( h(k) \) is randomly chosen, each subsequent call to \( h \) with the same input \( k \) yields the same output \( h(k) \).
\bigbreak \noindent 
We call such an ideal hash function an independent uniform hash function. Such a function is also often called a random oracle
\bigbreak \noindent 
When hash tables are implemented with an independent uniform hash function, we say we are using independent uniform hashing
\bigbreak \noindent 
Independent uniform hashing is an ideal theoretical abstraction, but it is not something that can reasonably be implemented in practice.
\bigbreak \noindent 
\subsection{Collision resolution by chaining}
\bigbreak \noindent 
At a high level, you can think of hashing with chaining as a nonrecursive form
of divide-and-conquer: the input set of $n$ elements is divided randomly into $m$
subsets, each of approximate size $\frac{n}{m}$. A hash function determines which subset
an element belongs to. Each subset is managed independently as a list.
\bigbreak \noindent 
each nonempty slot points to a linked list, and all the elements that hash to the same slot go into that slot’s linked list. Slot $j$ contains a pointer to the head of the list of all stored elements with hash value $j$.
If there are no such elements, then slot $j$ contains NIL
\bigbreak \noindent 
The worst-case running time for insertion is $O(1)$ 
The insertion procedure is fast in part because it assumes that the element $x$ being inserted is not already present in the table. To enforce this assumption, you
can search (at additional cost) for an element whose key is $x$.key before inserting
\bigbreak \noindent 
For searching, the worst-case running time is proportional to the length of the list
\bigbreak \noindent 
Deletion takes $O(1)$ time if the lists are doubly linked. If the hash table supports deletion, then its linked lists should be doubly linked in order to delete an item quickly.

\bigbreak \noindent 
\subsection{Analysis of hashing with chaining}
\bigbreak \noindent 
Given a hash table \( T \) with \( m \) slots that stores \( n \) elements, we define the load factor \( \alpha \) for \( T \) as \( \alpha = \frac{n}{m} \), that is, the average number of elements stored in a chain. Our analysis will be in terms of \( \alpha \), which can be less than, equal to, or greater than 1.
\bigbreak \noindent 
The worst-case behavior of hashing with chaining is terrible: all \( n \) keys hash to the same slot, creating a list of length \( n \). The worst-case time for searching is thus \( \Theta(n) \) plus the time to compute the hash function—no better than using one linked list for all the elements. We clearly don’t use hash tables for their worst-case performance.
\bigbreak \noindent 
The average-case performance of hashing depends on how well the hash function $h$ distributes the set of keys to be stored among the $m$ slots, on the average
\bigbreak \noindent 
for now we assume that any given element is equally likely to hash into any of the $m$ slots. That is, the hash function is uniform. We further assume that where a given element hashes to is independent of where any other elements hash to. In other words, we assume that we are using independent uniform hashing
\bigbreak \noindent 
Because hashes of distinct keys are assumed to be independent, independent uniform hashing is universal: the chance that any two distinct keys $k_{1}$ and $k_{2}$ collide is at most $\frac{1}{m}$.

\bigbreak \noindent 
\subsection{Hash functions}
\bigbreak \noindent 
For hashing to work well, it needs a good hash function. Along with being efficiently computable, what properties does a good hash function have? How do you design good hash functions?
\bigbreak \noindent 
This section first attempts to answer these questions based on two ad hoc approaches for creating hash functions: hashing by division and hashing by multiplication.
\bigbreak \noindent 
Although these methods work well for some sets of input keys, they are limited because they try to provide a single fixed hash function that works well on any data, an approach called static hashing.
\bigbreak \noindent 
We then see that provably good average-case performance for any data can be obtained by designing a suitable family of hash functions and choosing a hash function at random from this family at runtime, independent of the data to be hashed. The approach we examine is called random hashing
\bigbreak \noindent 
A good hash function satisûes (approximately) the assumption of independent uniform hashing: each key is equally likely to hash to any of the m slots, independently of where any other keys have hashed to. What does "equally likely" mean here? If the hash function is fixed, any probabilities would have to be based on the probability distribution of the input keys
\bigbreak \noindent 
Unfortunately, you typically have no way to check this condition, unless you happen to know the probability distribution from which the keys are drawn. Moreover, the keys might not be drawn independently
\bigbreak \noindent 
Occasionally you might know the distribution. For example, if you know that
the keys are random real numbers $k$ independently and uniformly distributed in the
range $0 < k < 1$, then the hash function
\begin{align*}
    h(k) = \lfloor km \rfloor
.\end{align*}
\bigbreak \noindent 
satisfies the condition of independent uniform hashing.
\bigbreak \noindent 
A good static hashing approach derives the hash value in a way that you expect to be independent of any patterns that might exist in the data. For example, the "division method" computes the hash value as the remainder when the key is divided by a speciûed prime number. This method may give good results, if you (somehow) choose a prime number that is unrelated to any patterns in the distribution of keys.
\bigbreak \noindent 
Random hashing picks the hash function to be used at random from a suitable family of hashing functions. This approach removes any need to know anything about the probability distribution of the input keys, as the randomization necessary for good average-case behavior then comes from the (known) random process used to pick the hash function from the family of hash functions, rather than from the (unknown) process used to create the input keys. We recommend that you use random hashing. 
\bigbreak \noindent 
In practice, a hash function is designed to handle keys that are one of the following two types:
\begin{itemize}
    \item A short nonnegative integer that fits in a $w$-bit machine word. Typical values for $w$ would be 32 or 64.
    \item A short vector of nonnegative integers, each of bounded size. For example, each element might be an 8-bit byte, in which case the vector is often called a (byte) string. The vector might be of variable length. 
\end{itemize}
\bigbreak \noindent 
To begin, we assume that keys are short nonnegative integers.
\bigbreak \noindent 
\subsubsection{Static hashing}
\bigbreak \noindent 
Static hashing uses a single, fixed hash function. The only randomization available is through the (usually unknown) distribution of input keys. This section discusses two standard approaches for static hashing: the division method and the multiplication method. Although static hashing is no longer recommended, the multiplication method also provides a good foundation for "nonstatic hashing", better known as random hashing, where the hash function is chosen at random from a suitable family of hash functions.
\bigbreak \noindent 
\subsubsection{The division method}
\bigbreak \noindent 
The division method for creating hash functions maps a key k into one of $m$ slots
by taking the remainder of $k$ divided by $m$. That is, the hash function is
\begin{align*}
    h(k) = k \ \text{mod} \ m
.\end{align*}
\bigbreak \noindent 
For example, if the hash table has size $m=12 $ and the key is $k=100$, then $h(k) = 4$. Since it requires only a single division operation, hashing by division is quite fast
\bigbreak \noindent 
The division method may work well when $m$ is a prime not too close to an exact power of 2. There is no guarantee that this method provides good average-case performance, however, and it may complicate applications since it constrains the size of the hash tables to be prime.
\bigbreak \noindent 
It is best to ensure that the table size is a prime number,  because keys are typically not randomly distributed, and usually have some pattern.




\bigbreak \noindent 
\subsubsection{The multiplication method}
\bigbreak \noindent 
The general multiplication method for creating hash functions operates in two steps. First, multiply the key \( k \) by a constant \( A \) in the range \( 0 < A < 1 \) and extract the fractional part of \( kA \). Then, multiply this value by \( m \) and take the floor of the result. That is, the hash function is
\begin{align*}
    h(k) = \lfloor m (kA \ \text{mod} \ 1) \rfloor
.\end{align*}
\bigbreak \noindent 
where "$kA \ \text{mod} \ 1$" means the fractional part of $kA$, that is, $kA - \lfloor kA \rfloor $
\bigbreak \noindent 
The general multiplication method has the advantage that the value of $m$ is not critical and you can choose it independently of how you choose the multiplicative constant $A$


\bigbreak \noindent 
\subsection{Open addressing}
\bigbreak \noindent 
Open addressing hashing is a collision resolution technique used in hash tables. When a hash collision occurs (i.e., two keys are hashed to the same index), open addressing searches for the next available slot in the array to store the colliding key. This eliminates the need for linked lists or separate chaining.
\bigbreak \noindent 
The two methods of open addressing we will discuss are
\begin{enumerate}
    \item linear probing
    \item quadratic probing
\end{enumerate}




\bigbreak \noindent 
\subsubsection{Linear probing}
\bigbreak \noindent 
Linear probing is a simple and commonly used method for handling collisions in hash tables. Collisions occur when two different keys hash to the same index in the table. Linear probing resolves these collisions by searching sequentially through the table to find the next available slot.
\bigbreak \noindent 
The formula for linear probing is
\[
    h'(k, i) = (h(k) + i) \mod m
\]
\begin{itemize}
    \item \(h(k)\): Original hash function.
    \item \(i\): Probe sequence (0, 1, 2, ...).
    \item \(m\): Table size.
\end{itemize}

\bigbreak \noindent 
\subsubsection{Quadratic probing}
\bigbreak \noindent 
Quadratic probing is a collision resolution technique used in hash tables as part of open addressing. When a collision occurs, instead of probing linearly (as in linear probing), quadratic probing uses a quadratic function to calculate the next index to probe. This helps reduce clustering and improves performanc
\bigbreak \noindent 
The formula for quadratic probing is
\[
h'(k, i) = \left(h(k) + c_1 i + c_2 i^2\right) \mod m
\]
Where:
\begin{itemize}
    \item \(h(k)\): Original hash function.
    \item \(i\): Probe number (starting from 0).
    \item \(c_1, c_2\): Constants to control the probing step size.
    \item \(m\): Size of the hash table (typically a prime number).
\end{itemize}
\bigbreak \noindent 
Simple quadratic probing is when $c_{1} = 0,\ c_{2} = 1$

\bigbreak \noindent 
\subsubsection{Deletion problem in open addressing}
\bigbreak \noindent 
In open addressing, when an element is removed from a hash table, simply marking the corresponding slot as unoccupied (occupied[index] = false) can break the probe sequence. This makes it impossible to find keys that were inserted after the deleted element, even though they still exist in the table.
\begin{itemize}
    \item Insert 10,20,30 into a hash table with linear probing.
    \item Suppose 10 and 20 are hashed to the same index due to collisions.
    \item Remove 10 by marking its slot as unoccupied.
    \item Now, if you search for 20, the probe sequence will terminate prematurely because it will encounter the unoccupied slot where 10 was stored.
\end{itemize}
\bigbreak \noindent 
The next few sections introduce solutions to this problem.


\bigbreak \noindent 
\subsubsection{Tombstoning}
\bigbreak \noindent 
One way to address the deletion problem in open addressing is tombstoning. This technique introduces a new boolean array called \textit{tombstone}. When we remove an item, we mark its slot unoccupied, but we also mark the spot with a tombstone. This way, when we probe for an element, we can continue searching if we encounter a hole, as long as there is a tombstone. We will see this method more in depth in the upcoming c++ example.

\bigbreak \noindent 
\subsubsection{Lazy deletion}
\bigbreak \noindent 
Instead of marking a slot as a tombstone, simply leave the slot as occupied but ignore its value in future insertions.
\bigbreak \noindent 
Mark the slot as "deleted" logically but do not clear the occupied flag.
\bigbreak \noindent 
Searches and insertions treat these "deleted" slots as valid for probing but ignore their values.

\bigbreak \noindent 
\subsubsection{Rehashing}
\bigbreak \noindent 
Instead of handling deletions within the existing table structure, rebuild the entire hash table when a deletion occurs
\bigbreak \noindent 
When a key is deleted, construct a new hash table and reinsert all active keys from the old table into the new one.
\bigbreak \noindent 
This eliminates any empty or deleted slots and ensures clean probe sequences.
\bigbreak \noindent 
This simplifies search, insertion, and deletion logic (no need for tombstones or special flags).
\bigbreak \noindent 
It also avoids performance degradation from tombstones or empty clusters.
\bigbreak \noindent 
However, this method is expensive in terms of time complexity for frequent deletions, as rehashing involves reinserting all active keys.
\bigbreak \noindent 
\begin{cppcode}
    void rehashTable() {
        hashtable<TABLE_SIZE> newTable;
        for (int i=0; i<TABLE_SIZE; ++i) {
            if (occupied[i]) {
                newTable.insert(table[i]);
            }
        }
        *this = newTable;
    }
\end{cppcode}
\bigbreak \noindent 
Then we we remove an element, we mark its spot unoccupied, then rehash the table.
\bigbreak \noindent 
\begin{cppcode}
    while (occupied[index]) {
        if (table[index] == key) {
            // Key found, mark as removed
            occupied[index] = false;
            rehashTable();
            return true;
        }
        ...
\end{cppcode}










\bigbreak \noindent 
\subsection{Load factor}
\bigbreak \noindent 
The load factor of a hash table is a measure of how full the table is, expressed as a ratio of the number of elements in the table to its total capacity. It helps determine when a hash table might need to be resized to maintain efficient performance.
\bigbreak \noindent 
The load factor $\alpha$ is calculated as:
\begin{align*}
\alpha = \frac{\text{number of elements in the table}}{\text{total number of slots in the table}}
.\end{align*}



\bigbreak \noindent 
\subsection{Static hashing}
\bigbreak \noindent 
When a hash table uses a fixed-size array as its storage, the approach is generally called static hashing. In static hashing, the table’s size is determined at the time of its creation and does not change as elements are added or removed. This is in contrast to dynamic hashing, where the table can grow or shrink based on the number of entries.
\bigbreak \noindent 
\subsection{Static linear hashing}
\bigbreak \noindent 
Static linear hashing is a hashing technique that combines aspects of static hashing (fixed-size table) with linear probing for collision resolution. In this approach:
\begin{itemize}
    \item \textbf{Fixed-Size Table:} The hash table is created with a fixed number of slots, meaning its size does not grow or shrink based on the number of elements. This fixed size makes it suitable for scenarios where the maximum number of entries is known in advance or memory is constrained.
    \item \textbf{Hash Function:} A hash function maps each key to a specific index within the table. Since the table is of fixed size, the hash function ensures that the resulting index is within the range of available slots, typically using a modulus operation, such as
        \[
            \text{hash(key)} \mod \text{table size}.
        \]
    \item \textbf{Linear Probing for Collision Resolution:} When two keys hash to the same index (a collision), linear probing is used to find the next available slot. This means that if a collision occurs at index \( i \), the algorithm checks \( i + 1 \), \( i + 2 \), and so on, wrapping around to the start of the table if necessary, until an empty slot is found.
    \item \textbf{Load Factor Constraints:} Because the table size is static, as the number of entries increases, so does the load factor. Higher load factors increase the likelihood of collisions, which can slow down insertion and search operations. Generally, static linear hashing performs best when the load factor is kept below a certain threshold (often around 0.7 to 0.8).
\end{itemize}

\pagebreak 
\subsection{Static hashing with linear probe implementation}
\bigbreak \noindent 
\subsubsection{Interface}
\bigbreak \noindent 
We support three operations
\begin{enumerate}
    \item Inserting
    \item Searching
    \item Removing
\end{enumerate}
\bigbreak \noindent 
\subsubsection{The basics}
\bigbreak \noindent 
\begin{cppcode}
template<size_t TABLE_SIZE>
class hashtable {
    int table[TABLE_SIZE];
    bool occupied[TABLE_SIZE]; 
public:
    hashtable() {
        fill(std::begin(occupied), std::end(occupied), 0);
    }
};
\end{cppcode}
\bigbreak \noindent 
By including template<size\_t TABLE\_SIZE> at the start, the class becomes a template, which means the table size is set by the user when they create an instance of the class. size\_t is used as the type for the TABLE\_SIZE parameter because it's ideal for holding non-negative values and represents sizes and counts. With this approach, users can create hash tables of varying sizes without needing dynamic memory allocation.
\bigbreak \noindent 
Within the class definition, there’s an array named table that has a size of TABLE\_SIZE. Since TABLE\_SIZE is defined as a template parameter, it’s treated as a compile-time constant, allowing the table array to be statically allocated. This fixed-size array avoids the need for dynamic memory management, which is often desirable in systems where memory usage needs to be predictable.
\bigbreak \noindent 
We also create a boolean array of the same size to track the spots in the hash table that have been filled. In the constructor, we fill the occupied array with 0 to mark all spots empty.
\bigbreak \noindent 
\subsubsection{The hash function}
\bigbreak \noindent 
For this example, we will use a simple hashing function
\begin{align*}
    h(x) = x \ \text{mod} \ n
.\end{align*}
Where $n$ is the size of the table.
\bigbreak \noindent 
\begin{cppcode}
    int hash(int element) {
        return element % TABLE_SIZE;
    }
\end{cppcode}

\pagebreak \bigbreak \noindent 
\subsubsection{Inserts}
\bigbreak \noindent 
\begin{cppcode}
    bool insert(int key) {
        int index = hash(key);
        int originalIndex = index;

        // Linear probing to find an open slot
        while (occupied[index]) {
            if (table[index] == key) {
                // Key already exists, insertion fails to avoid duplicates
                return false;
            }
            // Increment index, looping around if necessary
            index = (index + 1) % TABLE_SIZE;
            if (index == originalIndex) {
                // Table is full, insertion fails
                return false;
            }
        }
        
        table[index] = key;
        occupied[index] = true;
        return true;
    }
\end{cppcode}

\pagebreak \bigbreak \noindent 
\subsubsection{Searching}
\bigbreak \noindent 
\begin{cppcode}
    bool search(int key) const {
        int index = hash(key);
        int originalIndex = index;

        // Linear probing to search for the key
        while (occupied[index]) {
            if (table[index] == key) {
                return true;  // Key found
            }
            index = (index + 1) % TABLE_SIZE;
            if (index == originalIndex) {
                // Key not found, returned to original index
                return false;
            }
        }
        return false;  // Key not found
    }
\end{cppcode}
\bigbreak \noindent 
\subsubsection{Removing}
\bigbreak \noindent 
\begin{cppcode}
    bool remove(int key) {
        int index = hash(key);
        int originalIndex = index;

        // Linear probing to find the key
        while (occupied[index]) {
            if (table[index] == key) {
                // Key found, mark as removed
                table[index] = -1;
                occupied[index] = false;
                return true;
            }
            index = (index + 1) % TABLE_SIZE;
            if (index == originalIndex) {
                return false;
            }
        }

        return false;
    }
\end{cppcode}

\bigbreak \noindent 
\bigbreak \noindent 
\subsubsection{Searching and removing with tombstones}
\bigbreak \noindent 
Consider the hash table
\begin{align*}
    0:\ 142 \\ 
    1:\ 27  \\
    2:\ 15  \\
    3:\ \text{empty}  \\
    4:\ \text{empty} \\
    5:\ 83  \\
    6:\ 18 \\ 
    7:\ \text{empty} \quad \text{add 109}\\
    8:\ 47 \\
    9:\ \text{empty} \\ 
    10:\ 10 \\ 
    11:\ \text{empty} \\
    12:\ 90 
.\end{align*}
\bigbreak \noindent 
Suppose now we add key 109, which hashes to slot 5. Since 5 is taken, and we are using open addressing with a linear probe, 109 gets added to slot 7 instead.
\bigbreak \noindent 
Since the while loop depends on not finding any unoccupied slots in the path to the key, it could happen that on the path an element that previously existed was removed, thereby creating an unoccupied slot in the linear path to the requested element. In this case, the search function would incorrectly assume that the requested key does not exist! The while loop detected an empty slot before finding the key, therefore the key must not exist. To solve this problem, we need a way to indicate that slots that are empty have had data previously in them. We introduce a new array to track what we call tombstones, slots that are empty but previously held data.
\bigbreak \noindent 
In the example above, suppose we remove key 18, doing so creates a hole between 83 and 109, which both hash to 5. Searching for 109 in this case would give false, when it is clearly a member of the table.
\bigbreak \noindent 
Since the remove function also relies on searching for the key, it too needs the additional tombstone logic. 
\bigbreak \noindent 
Thus, we simply create a boolean tombstone array, and make slight adjustmennts to the search and remove logic.
\bigbreak \noindent 
\begin{cppcode}
    bool tombstone[TABLE_INDEX]; // Make sure to initially fill with zeros.

    // Adjustments to search
    while (occupied[index] || tombstone[index]) {
        if (occupied[index] && table[index] == key) {
            return true;  // Key found
        }
        ...

    // Adjustments to remove
    while (occupied[index] || tombstone[index]) {
        if (occupied[index] && table[index] == key) {
            // Key found, mark as removed
            occupied[index] = false;
            tombstone[index] = true;
            return true;
        }
        ...
\end{cppcode}

\pagebreak 
\subsection{Static hashing with quadratic probing implementation}
\bigbreak \noindent 
The code for quadratic probing is similar to linear probing, but with a slight adjustment to the probe increment. For this example, we use
\begin{align*}
    h^{\prime}(x) = h(x) + i^{2} \ \text{for } i=1,2,3,...,m
.\end{align*}
Where $m$ is the size of the table.
\bigbreak \noindent 
\subsubsection{Inserting}
\bigbreak \noindent 
\begin{cppcode}
    bool insert(int key) {
        int index = hash(key);
        int originalIndex = index;
        int i=1;

        // quadratic probing to find an open slot
        while (occupied[index]) {
            if (table[index] == key) {
                // Key already exists, insertion fails to avoid duplicates
                return false;
            }
            index = (index + (i*i)) % TABLE_SIZE;
            ++i; if (i>TABLE_SIZE) return false;
        }

        table[index] = key;
        occupied[index] = true;
        return true;
    }
\end{cppcode}

\pagebreak 
\bigbreak \noindent 
\subsubsection{Searching}
\bigbreak \noindent 
\begin{cppcode}
    bool search(int key) const {
        int index = hash(key);
        int originalIndex = index;
        int i=1;

        // Linear probing to search for the key
        while (occupied[index]) {
            if (table[index] == key) {
                return true;  // Key found
            }
            index = (index + (i*i)) % TABLE_SIZE;
            ++i; if (i>TABLE_SIZE) return false;
        }
        return false;  // Key not found
    }
\end{cppcode}

\pagebreak 
\subsubsection{Removing}
\bigbreak \noindent 
\begin{cppcode}
    bool remove(int key) {
        int index = hash(key);
        int originalIndex = index;
        int i=1;

        // Linear probing to find the key
        while (occupied[index]) {
            if (table[index] == key) {
                // Key found, mark as removed
                occupied[index] = false;
                rehash();
                return true;
            }
            index = (index + (i*i)) % TABLE_SIZE;
            ++i; if (i > TABLE_SIZE) return false;
        }

        return false;
    }
\end{cppcode}





\pagebreak 
\subsection{Static hashing with chaining implementation}
\bigbreak \noindent 
An improvement on the collision resolution technique above would be to instead use linked lists as the hash table elements. This way, if we have collisions, we can just add the element that hashes to the same slot to that slots linked list front. The hash function will remain the same
\bigbreak \noindent 
\subsubsection{The basics}
\bigbreak \noindent 
\begin{cppcode}
template<size_t TABLE_SIZE>
class hashtable {
    list<int> table[TABLE_SIZE];
    bool occupied[TABLE_SIZE]; 

public:
    hashtable() {
        fill(std::begin(occupied), std::end(occupied), 0);
    }
};
\end{cppcode}
\bigbreak \noindent 
Instead of a table of integers, we now use a table of doubly linked lists of integers. The occupied array remains the same.
\bigbreak \noindent 
\subsubsection{Inserting}
\bigbreak \noindent 
The insert function becomes much easier, we simply add the key to the slots linked list.
\bigbreak \noindent 
\begin{cppcode}
    void insert(int key) {
        int hx = hash(key);
        table[hx].push_front(key);
        occupied[hx] = 1;
    }
\end{cppcode}
\bigbreak \noindent 
\subsubsection{Searching}
\bigbreak \noindent 
\begin{cppcode}
    bool search(int key) const {
        int hx = hash(key);
        if (!occupied[hx]) 
        return false;

        if (std::find(table[hx].begin(), table[hx].end(), key) != table[hx].end()) 
        return true;
        return false;
    }
\end{cppcode}

\bigbreak \noindent 
\subsubsection{Removing}
\bigbreak \noindent 
\begin{cppcode}
    bool remove(int key) {
        int hx = hash(key);

        if (!occupied[hx])
        return false;

        table[hx].remove(key);    
        if (table[hx].empty()) 
        occupied[hx] = 0;

        return true;
    }
\end{cppcode}

\pagebreak 
\subsection{Injective hashing}
\bigbreak \noindent 
A hash function is injective if it maps every distinct input to a distinct output, i.e., it has no collisions. In other words,
\begin{align*}
    h(k_{1}) = h(k_{2}) \implies k_{1} = k_{2}
.\end{align*}
\bigbreak \noindent 
However, this is often impractical for large or infinite domains because it requires the output space to be at least as large as the input space.

\bigbreak \noindent 
\subsection{Perfect hashing}
\bigbreak \noindent 
 A hash function is perfect if it is collision-free for a specific, finite set of inputs. For a given set $S$, a perfect hash function $h$ maps every element in $S$ to a unique value (injectivity is guaranteed within $S$).
 \bigbreak \noindent 
 If the output range of $h$ matches the size of $S$, it is a minimal perfect hash function
 \bigbreak \noindent
 It applies only to a known, finite set of inputs. Does not guarantee injectivity outside the specific set $S$. Computationally feasible and often used when the set of inputs is fixed, such as in compiler keyword lookups.


 \pagebreak 
 \unsect{Implementing an unordered\_map}
 \bigbreak \noindent 
 Creating a custom data structure in C++ that mimics std::unordered\_map requires implementing a hash table. We use an array of buckets, where each bucket stores a linked list to handle collisions.
 \bigbreak \noindent 
 \textbf{Note:} Use std::hash for simplicity
 \bigbreak \noindent 
 \subsection{Fields}
 \bigbreak \noindent 
 \begin{itemize}
     \item \textbf{default\_buckets}: The default number of buckets
    \item \textbf{load\_factor}:  The load factor, usually 0.75
    \item \textbf{size}: Number of elements
    \item \textbf{buckets}: \texttt{vector<list<pair<key, value>>>}
 \end{itemize}
 Thus, we have
 \bigbreak \noindent 
 \begin{cppcode}
     template<typename key, typename value>
     class umap {
         private:
         static const size_t default_buckets = 16;
         static constexpr float load_factor = 0.75f;

         vector<list<pair<key, value>>> buckets;
         size_t size = 0;
     };  
 \end{cppcode}

 \bigbreak \noindent 
 \subsection{Interface}
 \bigbreak \noindent 
 We will support the following operations
 \begin{itemize}
     \item \textbf{Insert:} 
     \item \textbf{Find:} 
     \item \textbf{Erase:}
     \item \textbf{Empty:}
     \item \textbf{Index ([] operator):}
 \end{itemize}
 \bigbreak \noindent 
 \begin{cppcode}
    void insert(const key& k, const value& v);
    value* find(const key& k) const;
    bool empty();
    void erase(const key& k);
    value& operator[](const key& k);
 \end{cppcode}

 \bigbreak \noindent 
 \subsection{Public methods: Hash and rehash}
 \bigbreak \noindent 
 \begin{cppcode}
     size_t hash(const key& k) const {
         return std::hash<key>{}(k) % buckets.size();
     }
     void rehash() {
         vector<list<pair<key,value>>> new_buckets(buckets.size() * 2);

         for (const auto& bucket : buckets) {
             for (const auto& pair : bucket) {
                 size_t h = std::hash<key>{}(pair.first) % (buckets.size() * 2);
                 new_buckets[h].emplace_back(pair.first, pair.second);
             }
         }
         buckets.swap(new_buckets);
     }
 \end{cppcode}

 \bigbreak \noindent 
 \subsection{Inserting and erasing}
 \bigbreak \noindent 
 \subsubsection{Insertion}
 \bigbreak \noindent 
 Regarding insertion, if the load of the map (the number of elements divided by the size of the buckets container) exceeds the prescribed limit of 0.75, we trigger a rehash, which doubles the size of the buckets container.
 \bigbreak \noindent 
 \begin{cppcode}
     void insert(const key& k, const value& v) {
         if (static_cast<float>(size) / buckets.size() > load_factor) {
             rehash();
         } 

         size_t h = hash(k);        
         auto& bucket = buckets[h];

         for (auto it = bucket.begin(); it!=bucket.end(); ++it) {
             if (it->first == k) {
                 it->second = v;
                 return;
             }
         }
         bucket.emplace_back(k,v);
         ++size;
     }
 \end{cppcode}
 \bigbreak \noindent 
 If we find that the key already exists, we simply replace its value. If the key does not exist, we can add it to the bucket.
 \bigbreak \noindent 
 \subsubsection{Erasing}
 \bigbreak \noindent 
 \begin{cppcode}
     void erase(const key& k) {
         size_t h = hash(k);
         auto& bucket = buckets[h];

         for (auto it = bucket.begin(); it != bucket.end(); ++it) {
             if (it->first == k) {
                 bucket.erase(it);
                 --size;
                 return;
             }
         }
     }
 \end{cppcode}
 \bigbreak \noindent 
 If the key is found, we erase it. Otherwise, noop.

 \bigbreak \noindent 
 \subsection{Finding and checking if the map is empty}
 \bigbreak \noindent 
 \subsubsection{Finding}
 \bigbreak \noindent 
 \begin{cppcode}
     value* find(const key& k) {
        size_t h = hash(k);
        auto& bucket = buckets[h];

        for (auto it = bucket.begin(); it!=bucket.end(); ++it) {
            if (it->first == k) {
                return &(it->second);
            }
        }
        return nullptr;
    }
 \end{cppcode}
 \bigbreak \noindent 
 If the key is found in the map, we return a pointer to its value. Otherwise, return nullptr

 \bigbreak \noindent 
 \subsubsection{Empty}
 \bigbreak \noindent 
 \begin{cppcode}
    bool empty() const {
        return size ==  0;
    }
 \end{cppcode}

 \bigbreak \noindent 
 \subsection{The subscript operator}
 \bigbreak \noindent 
 \begin{cppcode}
     value& operator[](const key& k) {
         size_t h = hash(k);
         auto& bucket = buckets[h];
         for (auto it = bucket.begin(); it!=bucket.end(); ++it) {
             if (it->first == k) return it->second;
         }

         buckets[h].emplace_back(k, value());
         ++size;
         return buckets[h].back().second;
     }
 \end{cppcode}
 \bigbreak \noindent 
 If the key is found, return a reference to its value. Otherwise, add the key to the map with a default value. Once added, return a reference to that default value.



\pagebreak 
\unsect{Table indexing and row-major order}
\bigbreak \noindent 
Table indexing in multidimensional arrays is a method to access elements in an array by calculating their position in memory based on their indices. Since multidimensional arrays are often stored in contiguous memory as a single, linear array, table indexing is used to convert the array's multidimensional coordinates (like $A[i][j][k]$) into a single-dimensional index. This process allows programming languages to efficiently store and retrieve elements without the overhead of creating separate memory locations for each dimension.
\bigbreak \noindent 
\subsection{Row-major order}
\bigbreak \noindent 
In row-major order (used by C, C++, and many other languages), the elements are stored row by row. For a 2D array $A$ with dimensions $[m][n]$, the element $A[i][j]$ is stored in a 1D array at position:
\begin{align*}
    \text{index } = i \times n + j 
.\end{align*}
\bigbreak \noindent 
For a 3D array $[x][y][z]$, the element $A[i][j][k]$ is at
\begin{align*}
    \text{index } = i \times (y \times z ) + j \times z + k
.\end{align*}
\bigbreak \noindent 
This approach generalizes for any $n$-dimensional array by multiplying indices by the product of the sizes of subsequent dimensions.
\bigbreak \noindent 
For an \( n \)-dimensional array \( A[d_1][d_2] \ldots [d_n] \), where each \( d_k \) is the size of the \( k \)-th dimension, the index of element \( A[i_1][i_2] \ldots [i_n] \) can be calculated as:
\bigbreak \noindent 
\[
\text{index} = i_1 \times (d_2 \times d_3 \ldots \times d_n) + i_2 \times (d_3 \ldots \times d_n) + \ldots + i_n
\]
\bigbreak \noindent 
Table indexing is essential for optimizing memory usage and access speed in multidimensional arrays. It enables direct memory access with a single calculation, avoiding nested loops or additional data structures, which is especially useful in low-level programming and performance-critical applications.

\pagebreak 
\subsection{Row-major order in c++}
\bigbreak \noindent 
Implementing a row-major order vector in C++ typically involves creating a 1D vector that represents a 2D array. In row-major order, the elements of the rows of a 2D array are stored contiguously in memory.
\bigbreak \noindent 
\begin{enumerate}
    \item Define the dimensions of your 2D structure (rows and columns).
    \item Flatten the 2D structure into a 1D vector, using a formula to map 2D indices $(i, j)$ to a 1D index in the vector, where the formula is 
        \begin{align*}
            \text{index } = i \times \text{num\_cols } + j
        .\end{align*}
    Where 
    \begin{itemize}
        \item $i$ is the row index (0-based).
        \item $j$ is the column index (0-based).
        \item num\_cols is the number of columns in the 2D array.
    \end{itemize}
\end{enumerate}

\pagebreak 
\unsect{Graphs}
\bigbreak \noindent 
A graph is a collection of nodes and the connections between them. There are many different types of graphs
\bigbreak \noindent 
\begin{itemize}
    \item Simple graph
    \item Directed graph
    \item Multi-graph
    \item Weighted graph
    \item Complete graph
\end{itemize}
etc...
\bigbreak \noindent 
\subsection{Simple Graph}
\bigbreak \noindent 
A simple graph $G=(V,E)$ consists of a nonempty set $V$ of vertices and a possibly empty set $E$ of edges. Each edge connects two vertices from $V$:
\bigbreak \noindent 
A simple graph is a graph that satisfies the following properties:
\begin{enumerate}
    \item \textbf{Undirected:} The edges have no direction.
    \item \textbf{No Loops:} No vertex has an edge to itself.
    \item \textbf{No Multiple Edges:} There is at most one edge between any pair of vertices.
\end{enumerate}
\bigbreak \noindent 
In a undirected graph, $\{v_{i}, v_{j}\} = \{v_{j}, v_{i}\}$. We denote the number of vertices in a graph by $\abs{V}$, and the number of edges in a graph by $\abs{E}$
\bigbreak \noindent 
\subsection{Undirected graph}
\bigbreak \noindent 
An undirected graph is a graph where edges have no direction. If there is an edge between $u$ and $v$, it is the same as the edge between $v$ and $u$
\bigbreak \noindent 
\subsection{Directed graph}
\bigbreak \noindent 
A directed graph (digraph) $G=(V,E)$ consists of a nonempty set $V$ of vertices and a possibly empty set $E$ of edges (or archs). Each edge connects two vertices from $V$
\begin{align*}
    \{v_{i}, v_{j}\} \ne \{v_{j}, v_{i}\}
.\end{align*}

\bigbreak \noindent 
\subsection{Weighted Graph}
\bigbreak \noindent 
A weighted graph is a graph where edges have assigned numbers. The numbers could be distances values, lengths, costs, ..., etc
\bigbreak \noindent 
\subsection{Complete Graph}
\bigbreak \noindent 
For each pair of vertices, there is exactly one edge connecting them.

\bigbreak \noindent 
\subsection{More terms}
\bigbreak \noindent 
\begin{itemize}
    \item \textbf{Subgraph}: $G^{\prime}$ of $G=(V,E)$ is a graph $(V^{\prime},E^{\prime})$ such that $V^{\prime}  \subseteq V$, and $E^{\prime} \subseteq E $
    \item \textbf{Adjacent vertices}: Two vertices $v_{i}$ and $v_{j}$ are adjacent if edge $(v_{i}, v_{j})  \subseteq E$
        \bigbreak \noindent 
        Such edge is called incident with vertices $v_{i}$ and $v_{j}$
    \item \textbf{Degree}: The degree of a vertex $v$, $\text{deg}(v)$, is the number of edges incident with $v$
        \bigbreak \noindent 
        If $\text{deg}(v) = 0$, $v$ is called isolated vertex.
    \item \textbf{Path}: A sequence of edges \((v_1, v_2), (v_2, v_3), \dots, (v_{n-1}, v_n)\), denoted as the path:
        \[
            v_1 v_2 v_3 \dots v_n
        \]
    \item \textbf{Circuit}: There exists a path \(v_1, v_2, \dots, v_n\) where \(v_1 = v_n\), and no edge is repeated.
    \item \textbf{Cycle}: If all vertices in a circuit are different.
\end{itemize}

\bigbreak \noindent 
\subsection{Graph representations}
\bigbreak \noindent 
One way to represent a graph is an \textit{adjacency list}. Vertices are stored as records or objects, and every vertex stores a list of adjacent vertices.
\bigbreak \noindent 
This data structure allows the storage of additional data on the vertices.
\bigbreak \noindent 
\begin{minipage}[]{0.47\textwidth}
    \fig{.5}{./figures/25.png}
\end{minipage}
\begin{minipage}[]{0.47\textwidth}
\begin{align*}
    &a,c,d,f \\
    &b, d, e \\
    &c, a, f \\
    &d, a, b, e, f \\
    &e,b,d \\ 
    &f,a,c,d \\
    &g
.\end{align*}
\end{minipage}
\bigbreak \noindent 
We can also store the same information as a matrix, called an \textit{adjacency matrix}. Rows represent source vertices, columns represent destination vertices. Data on edges and vertices are stored externally
\bigbreak \noindent 
\[
\begin{bmatrix}
0 & 0 & 1 & 1 & 0 & 1 & 0 \\
0 & 0 & 1 & 1 & 1 & 0 & 0 \\
1 & 0 & 0 & 0 & 0 & 1 & 0 \\
1 & 1 & 0 & 0 & 1 & 1 & 0 \\
0 & 1 & 0 & 1 & 0 & 0 & 0 \\
1 & 0 & 1 & 1 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 \\
\end{bmatrix}
\]
\bigbreak \noindent 
Adjacency matrices have rows and columns in order, in the example above, row one column one is a,a. 
\bigbreak \noindent 
Adjacency matrices are symmetric for simple graphs, non-symmetric for digraphs
\bigbreak \noindent 
In case of weighted graphs, values in matrix indicate weights of edges, but only if we don't allow weights to be zero.
\bigbreak \noindent 
\subsection{Interface of adjacency lists and matrices}
\bigbreak \noindent 
\begin{itemize}
    \item Insert node
    \item Delete node
    \item Insert edge
    \item Delete edge
    \item Check existence of edge
    \item Get/set edge weight
    \item Get neighbors of node
\end{itemize}

\bigbreak \noindent 
\subsection{Complexities for adjacency operations}
\bigbreak \noindent 
\begin{center}
    \begin{tabular}{|l|c|c|}
        \hline
        \textbf{Operation}               & \textbf{Adjacency Matrix} & \textbf{Adjacency List} \\ \hline
        Add a node                       & \(O(|V|^2)\)             & \(O(1)\)               \\ \hline
        Remove a node                    & \(O(|V|^2)\)             & \(O(|E|)\)             \\ \hline
        Add an edge                      & \(O(1)\)                 & \(O(1)\)               \\ \hline
        Remove an edge                   & \(O(1)\)                 & \(O(|V|)\)             \\ \hline
        Get neighbors of a node          & \(O(|V|)\)               & \(O(|V|)\)             \\ \hline
        Test an edge                     & \(O(1)\)                 & \(O(|V|)\)             \\ \hline
        Get/set edge                     & \(O(1)\)                 & \(O(|V|)\)             \\ \hline
        Storage                          & \(O(|V|^2)\)             & \(O(|V| + |E|)\)       \\ \hline
    \end{tabular}
\end{center}

\bigbreak \noindent 
\subsection{Graph traversal/search}
\bigbreak \noindent 
We want to visit all vertices once and only once. There may be cycles, which can cause infinite loops. There may be isolated vertices or isolated sub- graphs
\bigbreak \noindent 
\subsubsection{Breadth-first-traversal}
\bigbreak \noindent 
Visit siblings first before visiting children.
\bigbreak \noindent 
\begin{cppcode}
BFS(Graph, startNode):
    1. Create an empty queue `Q` to store nodes to be visited.
    2. Create a set `visited` to track visited nodes.
    3. Enqueue the `startNode` into `Q`.
    4. Add `startNode` to the `visited` set.

    5. While `Q` is not empty:
        a. Dequeue a node `current` from `Q`.
        b. Process `current` (e.g., print it or record it).

        c. For each neighbor `neighbor` of `current`:
            i. If `neighbor` is not in `visited`:
                - Enqueue `neighbor` into `Q`.
                - Add `neighbor` to the `visited` set
\end{cppcode}

\bigbreak \noindent 
\subsubsection{Depth-first traversal}
\bigbreak \noindent 
Visit children before siblings, recursive.
\bigbreak \noindent 
\begin{cppcode}
DFS(Graph, currentNode, visited):
    1. If `currentNode` is not in `visited`:
        a. Add `currentNode` to `visited`.
        b. Process `currentNode` (e.g., print it or record it).

        c. For each neighbor `neighbor` of `currentNode`:
            i. Call DFS(Graph, neighbor, visited).
\end{cppcode}

\bigbreak \noindent 
\subsection{Shortest path problems}
\bigbreak \noindent 
\begin{itemize}
    \item \textbf{Single pair shortest path problem}: Finding a path between two vertices that the sum of weights of its edges is minimal
    \item \textbf{Single source shortest path problem}: Finding shortest paths from one vertices to all others
    \item \textbf{All pair shortest path problem}: Finding shortest paths for all pairs of vertices
\end{itemize}

\pagebreak 
\subsubsection{Dijkstra's shortest path algorithm}
\bigbreak \noindent 
Dijkstra's algorithm is a graph traversal algorithm used to find the shortest path from a source vertex to all other vertices in a graph with non-negative edge weights.
\begin{itemize}
    \item \textbf{Graph Representation:} The graph is represented as a set of vertices $V$ connected by edges $E$, with each edge having a weight (cost).
    \item \textbf{Shortest Path:} The goal is to compute the shortest distance from the source vertex $S$ to every other vertex.
    \item \textbf{Priority Queue:} Often implemented using a min-heap, this is used to efficiently pick the vertex with the smallest tentative distance.
\end{itemize}
\bigbreak \noindent 
Start by assigning a tentative distance of 0 to the source node and $\infty$ (infinity) to all other nodes.
\bigbreak \noindent 
Create a priority queue (or a min-heap) to store nodes based on their tentative distances.
\bigbreak \noindent 
Keep a set of visited nodes (or a boolean array) to avoid revisiting them.
\bigbreak \noindent 
Start with the source node, mark it as visited, and consider all its unvisited neighbors.
\bigbreak \noindent 
For each neighbor, calculate the tentative distance: tentative distance of neighbor = distance to current node + edge weight to neighbor
\bigbreak \noindent 
If the newly calculated tentative distance is smaller than the previously recorded distance, update the distance.
\bigbreak \noindent 
Push or update the neighbors in the priority queue with their new tentative distances.
\bigbreak \noindent 
Remove the source node from the priority queue since it has been visited.
\bigbreak \noindent 
Select the unvisited node with the smallest tentative distance from the priority queue and repeat the process for its neighbors.
\bigbreak \noindent
Continue the process until all nodes have been visited or the priority queue is empty
\bigbreak \noindent 
Once the algorithm finishes, the shortest distances to all nodes from the source node are recorded.
\pagebreak \bigbreak \noindent 
\subsubsection{Dijkstras's algorithm in c++}
\bigbreak \noindent 
First, we need to define infinite. For this, we use numeric limits
\bigbreak \noindent 
\begin{cppcode}
    const int INF = numeric_limits<int>::max(); // Define infinity
\end{cppcode}
\bigbreak \noindent 
The graph in this case will be directed, and represented by an adjacency list
\bigbreak \noindent 
\begin{cppcode}
    // Hardcoded graph
    int n = 5; // Number of nodes
    vector<vector<pair<int, int>>> graph(n); // Adjacency list (node, weight)

    // Add edges (u -> v with weight w)
    graph[0].emplace_back(1, 1); // Edge 0 -> 1 with weight 1
    graph[0].emplace_back(2, 4); // Edge 0 -> 2 with weight 4
    graph[1].emplace_back(2, 2); // Edge 1 -> 2 with weight 2
    graph[1].emplace_back(3, 6); // Edge 1 -> 3 with weight 6
    graph[2].emplace_back(3, 3); // Edge 2 -> 3 with weight 3
    graph[3].emplace_back(4, 1); // Edge 3 -> 4 with weight 1
\end{cppcode}
\bigbreak \noindent 
\pagebreak \bigbreak \noindent 
\begin{cppcode}
    void dijkstra(int source, const vector<vector<pair<int, int>>>& graph) {
        int n = graph.size(); // Number of nodes in the graph
        vector<int> dist(n, INF); // Distance array initialized to infinity
        vector<bool> visited(n, false); // Visited array

        // Min-heap to store (distance, node) pairs
        priority_queue<pair<int, int>, vector<pair<int, int>>, greater<>> pq;

        // Initialize the source node
        dist[source] = 0;
        pq.push({0, source}); // Push the source with distance 0

        while (!pq.empty()) {
            int currentDist = pq.top().first;
            int currentNode = pq.top().second;
            pq.pop();

            if (visited[currentNode]) continue; // Skip already visited nodes
            visited[currentNode] = true;

            // Explore all neighbors
        for (const auto& [neighbor, weight] : graph[currentNode]) {
            int newDist = currentDist + weight;

            if (newDist < dist[neighbor]) {
                dist[neighbor] = newDist; // Update distance
                pq.push({newDist, neighbor}); // Push neighbor into the heap
            }
        }
    }
    // From here we can print the distances through the dist vector
}
\end{cppcode}



% Dijkstra's algorithm solves the single-source shortest-paths problem on a weighted, directed graph \( G = (V, E) \), but it requires nonnegative weights on all edges: \( w(u, v) \geq 0 \) for each edge \( (u, v) \in E \).
% \bigbreak \noindent 
% You can think of Dijkstra's algorithm as generalizing breadth-first search to
% weighted graphs. A wave emanates from the source, and the first time that a wave
% arrives at a vertex, a new wave emanates from that vertex. Whereas breadth-first
% search operates as if each wave takes unit time to traverse an edge, in a weighted
% graph, the time for a wave to traverse an edge is given by the edges weight. Because a shortest path in a weighted graph might not have the fewest edges, a simple, first-in, first-out queue won't suffice for choosing the next vertex from which
% to send out a wave.
% \bigbreak \noindent 
% Instead, Dijkstra's algorithm maintains a set \( S \) of vertices whose final shortest-path weights from the source \( s \) have already been determined. The algorithm repeatedly selects the vertex \( u \in V - S \) with the minimum shortest-path estimate, adds \( u \) into \( S \), and relaxes all edges leaving \( u \). The procedure \textsc{Dijkstra} replaces the first-in, first-out queue of breadth-first search by a min-priority queue \( Q \) of vertices, keyed by their \( d \) values.
% \bigbreak \noindent 
% \begin{cppcode}
% dijkstra(G, w, s)
%     INITIALIZE-SINGLE-SOURCE(G,s)
%     S = emptyset
%     Q = emptyset
%
%     for each vertex u in G.V
%         insert(Q,u)
%     while Q != empty
%         u = extract-min(Q)
%         S = S union {u}
%         for each vertex v in G.adj[u]
%             relax(u,v,w)
%             if the call of relax decreased v.d
%                 decrease-key(q,v,v.d)
% \end{cppcode}

\pagebreak 
\subsection{Undirected graph in c++ with an adjacency list}
\bigbreak \noindent 
\subsubsection{Internal structure}
\bigbreak \noindent 
To represent the adjacency list, we use a map to map nodes to a vector of adjacent nodes. In this example we use an orderded map
\bigbreak \noindent 
\begin{cppcode}
    struct graph {
        unorderd_map<int, vector<int>> adjlist;
    };
\end{cppcode}
\bigbreak \noindent 
\subsubsection{Interface}
\bigbreak \noindent 
\begin{cppcode}
   void addEdge(int v1, int v2) {}
   void addNode(int v) {}
   vector<int> getNeighbors(int v) {}
   bool hasEdge(int v1, int v2) {}
   void display() {}
\end{cppcode}

\bigbreak \noindent 
\subsubsection{Adding edges}
\bigbreak \noindent 
\begin{cppcode}
    void addEdge(int v1, int v2) {
        if (std::find(adjlist[v1].begin(), adjlist[v1].end(), v2) == adjlist[v1].end()) {
            adjlist[v1].push_back(v2);
        }
        if (std::find(adjlist[v2].begin(), adjlist[v2].end(), v1) == adjlist[v2].end()) {
            adjlist[v2].push_back(v1);
        }
    }
\end{cppcode}
\bigbreak \noindent 
Adding edges is pretty simple in an adjacency list, we simply add vertex$_{1}$ to the map as the key, and add vertex$_{2}$ to vertex$_1$s vector. If vertex$_1$ doesn't exist, the map will add it for us. If the vertex already exists in the vector, we don't add it.
\bigbreak \noindent 
We could also use a set instead of a vector, but vectors are generally faster to traverse.

\bigbreak \noindent 
\subsubsection{Adding nodes}
\bigbreak \noindent 
\begin{cppcode}
    void addNode(int v) {
        if (adjlist.find(v) == adjlist.end()) {
            adjlist[v] = {};
        }
    }
\end{cppcode}
\bigbreak \noindent 
If the node does not exist as a key in the map, we add it.

\bigbreak \noindent 
\subsubsection{Getting the neighbors}
Pretty straightforward
\bigbreak \noindent 
\begin{cppcode}
    vector<int> getNeighbors(int v) {
        vector<int> ret{};
        for (const auto& item : adjlist[v]) {
            ret.push_back(item);
        }
        return ret;
    }
\end{cppcode}

\bigbreak \noindent 
\subsubsection{Checking if an edge exists}
\bigbreak \noindent 
\begin{cppcode}
    bool hasEdge(int v1, int v2) {
        if (adjlist.find(v1) != adjlist.end() && std::find(adjlist[v1].begin(), adjlist[v1].end(), v2) != adjlist[v1].end()) {
            return true;
        } return false;
    }
\end{cppcode}

\pagebreak 
\subsubsection{Breadth first traversal}
\bigbreak \noindent 
\begin{cppcode}
   void bft(int start) {
        bool visited[size()]; 
        std::fill(visited, visited + size(), 0);
        queue<int> q;

        if (adjlist.find(start) == adjlist.end()) {
            cout << "Starting point does not exist" << endl;
            return;
        }
        q.push(start);
        visited[start] = 1;

        while (!q.empty()) {
            int curr = q.front();
            cout << curr << " ";
            q.pop();

            for (const auto& item : adjlist[curr]) {
                if (!visited[item]) {
                    q.push(item);
                    visited[item] = 1;
                }
            }
        }
   }
\end{cppcode}

\pagebreak 
\subsubsection{Breadth first search to find the shortest path}
\bigbreak \noindent 
\begin{cppcode}
    void bfs(int start, int end) {
        vector<bool> visited(size(), 0);
        unordered_map<int,int> parent;
        queue<int> q;

        q.push(start);
        visited[start] = 1;
        parent[start] = -1;

        while (!q.empty()) {
            int curr = q.front();
            q.pop();

            if (curr == end) {
                vector<int> path;

                for (int v = end; v != -1; v = parent[v]) {
                    path.push_back(v);
                }
                std::reverse(path.begin(), path.end());

                for (const auto& item : path) {
                    cout << item << " -> ";
                }
                cout << endl;
                return;
            }


            for (const auto& item : adjlist[curr]) {
                if (!visited[item]) {
                    q.push(item);
                    visited[item] = 1;
                    parent[item] = curr;
                }
            }
        }
    }
\end{cppcode}

\pagebreak 
\subsubsection{Depth first traversal}
\bigbreak \noindent 
\begin{cppcode}
    void r_dft(int v, vector<bool>& visited) {
        cout << v << " ";

        for (const auto& item : adjlist[v]) {
            if (!visited[item]) {
                visited[item] =  1;
                r_dft(item, visited);
            }
        }
    }

    void dft(int start) {
        if (adjlist.find(start) == adjlist.end()) {
            cout << "Starting point does not exist" << endl;
            return;
        }
        vector<bool> visited(size(), 0);
        visited[start] = 1;
        r_dft(start, visited);
    }
\end{cppcode}

\pagebreak 
\subsection{Undirected graph in c++ with adjacency matrix}
\bigbreak \noindent 
\subsubsection{Setup}
\bigbreak \noindent 
\begin{cppcode}
struct graph {
    vector<vector<int>> matrix;
    int numVertices{0};

    graph(int n) : numVertices(n) {
        matrix.resize(n, vector<int>(n,0));
    }
};
\end{cppcode}

\bigbreak \noindent 
\subsubsection{Adding an edge}
\bigbreak \noindent 
\begin{cppcode}
    void addEdge(int v1, int v2) {
        if (v1 >=0 && v1 < numVertices && v2 >= 0 && v2 < numVertices) {
            matrix[v1][v2] = 1;
            matrix[v2][v1] = 1;
        } 
    }
\end{cppcode}

\bigbreak \noindent 
\subsubsection{Removing an edge}
\bigbreak \noindent 
\begin{cppcode}
    void removeEdge(int v1, int v2) {
        if (v1 >= 0 && v1 < numVertices && v2 >= 0 && v2 < numVertices) {
            matrix[v1][v2] = 0; 
            matrix[v2][v1] = 0;
        } 
    }
\end{cppcode}

\bigbreak \noindent 
\subsubsection{Getting neighbors}
\bigbreak \noindent 
\begin{cppcode}
    vector<int> getNeighbors(int v) {
        vector<int> ret{};
        int row = v; 
        for (int j=0; j<numVertices; ++j) {
            if (matrix[row][j]) ret.push_back(j);
        }
        return ret;
    }
\end{cppcode}

\bigbreak \noindent 
\subsubsection{Checking if an edge exists}
\bigbreak \noindent 
\begin{cppcode}
    bool hasEdge(int v1, int v2) {
        if (v1 >= 0 && v1 < numVertices && v2 >= 0 && v2 < numVertices) {
            return matrix[v1][v2] == 1;
        } 
        return false;
    }
\end{cppcode}

\pagebreak 
\subsubsection{Breadth-first traversal}
\bigbreak \noindent 
\begin{cppcode}
    void bft(int start) {
        if (start < 0 || start >= numVertices) {
            cout << "Starting point does not exist" << endl;
            return;
        }
        vector<bool> visited(numVertices,0);
        queue<int> q;
        q.push(start);
        visited[start] = 1;

        while (!q.empty()) {
            int curr = q.front();
            q.pop();
            cout << curr << " ";

            int i = curr;
            for (int j=0; j<numVertices; ++j) {
                if (matrix[i][j] && !visited[j]) {
                    q.push(j);
                    visited[j] = 1;
                }
            }
        }
    }
\end{cppcode}

\pagebreak 
\subsubsection{Depth-first traversal}
\bigbreak \noindent 
\begin{cppcode}
    void r_dft(int v, vector<bool>& visited) {
        cout << v << " ";

        int i=v;
        for (int j=0; j<numVertices; ++j) {
            if (matrix[i][j] && !visited[j]) {
                visited[j] = 1;
                r_dft(j, visited);
            }
        }
    }

    void dft(int start) {
        if (start < 0 || start >= numVertices) {
            return;
        }
        vector<bool> visited(numVertices,0);
        visited[start] = 1;
        r_dft(start, visited);
    }
\end{cppcode}

\pagebreak 
\subsection{Weighted graph in C++ with an adjacency matrix}
\bigbreak \noindent 
We can represent a weighted graph in c++ using an adjacency matrix, where the entry at $[i][j]$ represents the weight of the edge from node $i$ to $j$. If there is no edge, we can use some specical value like $-1,0$, or \textit{INF}.
\bigbreak \noindent 
\subsubsection{Setup}
\bigbreak \noindent 
\begin{cppcode}
const int INF = std::numeric_limits<int>::max();

struct graph {
    int n = 0;
    vector<vector<int>> matrix;

    graph() = default;
    graph(int n) : n(n) {
        matrix = vector<vector<int>>(n, vector<int>(n,0));
    }
};
\end{cppcode}
\bigbreak \noindent 
\subsubsection{Adding and removing edges}
\bigbreak \noindent 
\begin{cppcode}
    void addEdge(int v, int u, int weight) {
        if (v >=0 && v < n && u >= 0 && u < n) {
            matrix[v][u] = weight;
            matrix[u][v] = weight;
        }
    }

    void removeEdge(int v, int u) {
        if (v >=0 && v < n && u >= 0 && u < n) {
            matrix[v][u] = 0;
            matrix[u][v] = 0;
        }
    }
\end{cppcode}

\pagebreak 
\subsubsection{Dijkstra}
\bigbreak \noindent 
\begin{cppcode}
    void dijkstra(int start) {
        priority_queue<std::pair<int,int>, vector<std::pair<int,int>>, std::greater<>> pq;
        vector<int> dist(n, INF);
        vector<int> parent(n,-1);
        dist[start] = 0;

        pq.emplace(0,start);

        while (!pq.empty()) {
            auto [d,u] = pq.top();
            pq.pop();

            if (d > dist[u]) continue;

            for (int v = 0; v<n; ++v) {
                if (matrix[u][v] > 0) {
                    int weight = matrix[u][v];
                    if (dist[u] + weight < dist[v]) {
                        dist[v] = dist[u] + weight;
                        pq.emplace(dist[v], v);
                        parent[v] = u;
                    }
                }
            }
        }
        for (int i=0; i<n; ++i) {
            if (dist[i] == INF) {
                cout << "Distance " << start << " -> " << i << ": " << "not reachable" << endl;
            } else {
                cout << "Distance " << start << " -> " << i << ": " << dist[i] << endl;
                cout << "Path: ";
                vector<int> path;
                for (int v = i; v != -1; v = parent[v]) {
                    path.push_back(v);
                }
                std::reverse(path.begin(), path.end());

                for (const auto& item : path) {
                    cout << item << " -> ";
                }
                cout << endl << endl;
            }
        }
    }
\end{cppcode}

\pagebreak 
\subsection{Weighted graph in c++ with structure}
\bigbreak \noindent 
\begin{cppcode}
typedef pair<int,int> Vpair;

struct graph {
    int V, E;
    vector<pair<int,Vpair>> edges;

    graph(int v, int e) : V(v), E(e) {}

    void addEdge(int u, int v, int w) {
        edges.push_back({w, {u,v}});
    }
};
\end{cppcode}

\pagebreak 
\subsection{Topological sort (DFS approach)}
\bigbreak \noindent 
Topological sort is a linear ordering of vertices in a directed acyclic graph (DAG) such that for every directed edge $u\to v$, vertex $u$ appears before $v$ in the ordering. It is used in applications where certain tasks must be performed before others, such as task scheduling, build systems, and dependency resolution.
\begin{itemize}
    \item \textbf{Applies only to DAGs:} A topological sort is possible only for directed acyclic graphs. If there are cycles, a topological ordering does not exist.
    \item \textbf{Not unique:} A graph can have multiple valid topological orderings if some vertices are not constrained by dependencies.
\end{itemize}
\bigbreak \noindent 
We perform a DFS, and add nodes to a stack after visiting all their neighbor

\bigbreak \noindent 
\subsubsection{C++ Setup}
\bigbreak \noindent 
\begin{cppcode}
struct graph {
    unordered_map<int, vector<int>> list;

    void addEdge(int v, int u) {
        list[v].push_back(u);
    }
};
\end{cppcode}
\bigbreak \noindent 
\subsubsection{Topological sort with dfs helper}
\bigbreak \noindent 
\begin{cppcode}
    void topologicalSort() {
        unordered_set<int> visited; // Set to keep track of visited nodes
        stack<int> s;               // Stack to store the topological order

        // Visit each node that hasn't been visited yet
        for (const auto& [node, _] : list) {
            if (visited.find(node) == visited.end()) {
                dfs(node, visited, s);
            }
        }

        // Print the nodes in topological order
        cout << "Topological Sort: ";
        while (!s.empty()) {
            cout << s.top() << " ";
            s.pop();
        }
        cout << endl;
    }

    // Helper function for DFS
    void dfs(int node, unordered_set<int>& visited, stack<int>& s) {
        visited.insert(node); // Mark the current node as visited

        // Visit all the neighbors
        for (int neighbor : list[node]) {
            if (visited.find(neighbor) == visited.end()) {
                dfs(neighbor, visited, s);
            }
        }

        // Push the current node onto the stack after visiting all its neighbors
        s.push(node);
    }
\end{cppcode}

\pagebreak \bigbreak \noindent 
\subsection{Union-Find Data Structure (Disjoint Set Union - DSU)}
\bigbreak \noindent 
The Union-Find data structure, also known as Disjoint Set Union (DSU), is used to efficiently handle dynamic connectivity queries. It is particularly useful in applications such as Kruskal's algorithm for finding the Minimum Spanning Tree (MST) and cycle detection in graphs.
\bigbreak \noindent 
Union-Find keeps track of a partition of a set into disjoint subsets. It supports two primary operations:
\begin{enumerate}
    \item \textbf{Find(x):} Determines which set the element x belongs to.
    \item \textbf{Union(x, y):} Merges the sets containing x and y.
\end{enumerate}
\bigbreak \noindent 
Each subset is represented as a tree, where each element points to a parent. The root of the tree represents the set.

\bigbreak \noindent 
\subsubsection{Class structure and Fields}
\bigbreak \noindent 
\begin{cppcode}
    class UnionFind {
        vector<int> parent;

    public:
        UnionFind(int n) {
            parent.resize(n,0);
            for (int i=0; i<n; ++i) {
                parent[i] = i;
            }
        }

        int find(x);
        void unionsets(int x, int y)
    };
\end{cppcode}

\bigbreak \noindent 
\subsubsection{Find}
\bigbreak \noindent 
Find returns the representative of the group. It does this by moving up the tree until it reaches the root, in which case it then simply returns the root.
\bigbreak \noindent 
\begin{cppcode}
    int find(x) {
        if (parent[x] != x) {
            return find(parent[x]);
        } else {
            return x;
        }
    }
\end{cppcode}

\bigbreak \noindent 
\subsubsection{Union}
\bigbreak \noindent 
Finds the root of both elements and merges one set into the other
\bigbreak \noindent 
\begin{cppcode}
    void unionSets(int x, int y) {
        int rootX = find(x);  // Find root of x
        int rootY = find(y);  // Find root of y

        if (rootX != rootY)   // If they are in different sets, merge them
            parent[rootY] = rootX;  // Make rootX the parent of rootY
    }
\end{cppcode}

\bigbreak \noindent 
\subsubsection{Find and union optimizations}
\bigbreak \noindent 
To efficiently perform Find and Union, we use two optimizations:
\begin{itemize}
    \item Path Compression (for efficient Find operations)
    \item Union by Rank (for efficient Union operations)
\end{itemize}
\bigbreak \noindent 
When calling Find(x), we update each node along the path to point directly to the root. This reduces the tree depth and speeds up future queries.
\bigbreak \noindent 
\begin{cppcode}
    int find(x) {
        if (parent[x] != x) {
            parent[x] = find(parent[x]);
        } 
        return parent[x];
    }
\end{cppcode}
\bigbreak \noindent 
Instead of arbitrarily merging trees, we attach the smaller tree under the larger tree to keep the structure balanced. We make an additional field \textit{rank}, which is a vector<int>. Note that we must resize it to $n$ in the constructor, and fill with ones.
\bigbreak \noindent 
The rank vector is used to keep track of the depth of each set's tree. This helps in balancing the trees when performing unionSets(x, y), preventing them from becoming too deep.
\bigbreak \noindent 
\begin{cppcode}
    // Union by rank
    void unite(int x, int y) {
        int rootX = find(x);
        int rootY = find(y);

        if (rootX != rootY) {
            if (rank[rootX] > rank[rootY]) {
                parent[rootY] = rootX; // Attach smaller tree to larger one
            } else if (rank[rootX] < rank[rootY]) {
                parent[rootX] = rootY;
            } else {
                parent[rootY] = rootX;
                rank[rootX]++; // Increase rank if same
            }
        }
    }
\end{cppcode}
\bigbreak \noindent 
Since the rank tracks the depth, we only increase the rank if both trees have the same depth. If we only merge the smaller one into the larger one, then the larger one will always "obsorb" the smaller one without increasing the depth. Thus, the depth only will increase if they have the same rank (depth).

\bigbreak \noindent 
\subsection{Using UnionFind}
\bigbreak \noindent 
\begin{cppcode}
    int main() {
        UnionFind uf(5); // Create a DSU with 5 elements (0 to 4)

        uf.unionSets(0, 1);
        uf.unionSets(1, 2);

        cout << (uf.find(0) == uf.find(2)) << endl; // 1 (true, same set)
        cout << (uf.find(3) == uf.find(4)) << endl; // 0 (false, different sets)

        uf.unionSets(3, 4);
        cout << (uf.find(3) == uf.find(4)) << endl; // 1 (true, now same set)

        return 0;
    }
\end{cppcode}


\pagebreak \bigbreak \noindent 
\subsection{Minimum spanning trees}
\bigbreak \noindent 
A spanning tree is a subset of Graph G, such that all the vertices are connected using minimum possible number of edges. Hence, a spanning tree does not have cycles and a graph may have more than one spanning tree.
\begin{itemize}
    \item A Spanning tree does not exist for a disconnected graph.
    \item For a connected graph having \(N\) vertices, the number of edges in the spanning tree for that graph will be \(N-1\).
    \item A Spanning tree does not have any cycle.
    \item We can construct a spanning tree for a complete graph by removing \(E-N+1\) edges, where \(E\) is the number of edges and \(N\) is the number of vertices.
    \item \textbf{Cayley's Formula:} It states that the number of spanning trees in a complete graph with \(N\) vertices is \(N^{N-2}\).
    \begin{itemize}
        \item For example: \(N=4\), then the maximum number of spanning trees possible is \(4^{4-2} = 16\) (shown in the above image).
    \end{itemize}
\end{itemize}
\bigbreak \noindent 
A minimum spanning tree (MST) or minimum weight spanning tree for a weighted, connected, undirected graph is a spanning tree with a weight less than or equal to the weight of every other spanning tree. To learn more about Minimum Spanning Tree
\bigbreak \noindent 
In an unweighted graph, all edges have the same "weight" (implicitly 1). Thus, the concept of a "minimum" spanning tree becomes more about ensuring a spanning tree that connects all vertices with the minimum number of edges.
\bigbreak \noindent 
Since all edges are equal in weight, every spanning tree of the graph is an MST because there is no way to differentiate one tree from another based on edge weights.
\bigbreak \noindent 
An MST is unique if and only if all edge weights in the graph are distinct
\bigbreak \noindent 
If the graph contains edges with equal weights, multiple MSTs may exist.

\bigbreak \noindent 
\subsubsection{Kruskal's algorithm}
\bigbreak \noindent 
Given an undirected, connected and weighted graph, we can find the Minimum Spanning Tree (MST) of the graph using Kruskal’s algorithm.
\bigbreak \noindent 
Below are the steps for finding MST using Kruskal’s algorithm
\begin{enumerate}
    \item Sort all the edges in non-decreasing order of their weight.
    \item Pick the smallest edge. Check if it forms a cycle with the spanning tree formed so far. If cycle is not formed, include this edge. Else, discard it.
    \item Repeat step#2 until there are (V-1) edges in the spanning tree.
\end{enumerate}
\bigbreak \noindent 
Note that step two uses union find.

\bigbreak \noindent 
The graph structure is
\begin{cppcode}
typedef pair<int, int> iPair; 

struct Graph { 
    int V, E; 
    vector<pair<int, iPair>> edges; 

    Graph(int V, int E) : V(V), E(E) {}

    void addEdge(int u, int v, int w) { 
        edges.push_back({w, {u, v}}); 
    } 

    int kruskalMST(); 
}; 
\end{cppcode}

\pagebreak \bigbreak \noindent 
\begin{cppcode}
int Graph::kruskalMST() 
{ 
    int mst_wt = 0; // Initialize result 
  
    // Sort edges in increasing order on basis of cost 
    sort(edges.begin(), edges.end()); 
  
    // Create disjoint sets 
    DisjointSets ds(V); 
  
    // Iterate through all sorted edges 
    vector< pair<int, iPair> >::iterator it; 
    for (it=edges.begin(); it!=edges.end(); it++) 
    { 
        int u = it->second.first; 
        int v = it->second.second; 
  
        int set_u = ds.find(u); 
        int set_v = ds.find(v); 
  
        // Check if the selected edge is creating 
        // a cycle or not (Cycle is created if u 
        // and v belong to same set) 
        if (set_u != set_v) 
        { 
            // Current edge will be in the MST 
            // so print it 
            cout << u << " - " << v << endl; 
  
            // Update MST weight 
            mst_wt += it->first; 
  
            // Merge two sets 
            ds.merge(set_u, set_v); 
        } 
    } 
  
    return mst_wt; 
} 
\end{cppcode}
\bigbreak \noindent 
Note that DisjointSets uses the union find data structure described earlier.

\pagebreak 
\unsect{Prefix and suffix sums, products, etc}
\bigbreak \noindent 
The prefix sum algorithm computes an array where each element at index $i$ is the sum of all elements from the start of the input array up to $i$. Given an array $A$ of length $n$, the prefix sum array $P$ is defined as:
\begin{align*}
    P[i] = \sum_{j=0}^{i}A[j]
\end{align*}
\bigbreak \noindent 
\begin{cppcode}
    vector<int> prefixSum(vector<int>& v) {
        vector<int> res(v.size(),0);

        res[0] = v[0];
        for (int i=1; i<(int)v.size(); ++i) {
            res[i] = res[i-1] + v[i];
        }
        return res;
    }
\end{cppcode}
\bigbreak \noindent 
We could also do a prefix product, where
\begin{align*}
    P[i] = \prod_{j=0}^{i} A[i]
\end{align*}
\bigbreak \noindent 
\begin{cppcode}
    vector<int> prefixProd(vector<int>& v) {
        vector<int> res(v.size(),1);

        res[0] = v[0];
        for (int i=1; i<(int)v.size(); ++i) {
            res[i] = res[i-1] * v[i];
        }
        return res;
    }
\end{cppcode}

\bigbreak \noindent 
\subsection{Suffix sums and products}
\bigbreak \noindent 
We define the suffix sum
\begin{align*}
    S[i] = \sum_{j=0}^{(n-1)-i} a_{n-j}
\end{align*}
Where $n$ is the size of the input array. The suffix product is therefore defined as
\begin{align*}
    S[i] = \prod_{j=0}^{(n-1)-i}a_{n-j}
\end{align*}
\bigbreak \noindent 
\begin{cppcode}
    vector<int> suffixProd(vector<int>& v) {
        vector<int> res(v.size(),1);

        res[v.size()-1] = v[v.size()-1];
        for (int i=v.size()-2; i>=0; --i) {
            res[i] = res[i+1] * v[i];
        }
        return res;
    }
\end{cppcode}

\pagebreak 
\unsect{Two pointer}
\bigbreak \noindent 
The two-pointer algorithm is an optimization technique used for solving problems involving searching, sorting, and iterating through arrays or linked lists efficiently. It involves using two pointers (indices or iterators) that move through the data structure in a coordinated manner to improve time complexity.
\begin{itemize}
    \item \textbf{Opposite Direction (Left and Right Pointers)}:
        \begin{itemize}
            \item Used when searching for pairs that satisfy a condition (e.g., Two Sum in a sorted array).
            \item One pointer starts from the beginning, the other from the end, moving toward each other.
        \end{itemize}
    \item \textbf{Same Direction (Fast and Slow Pointers)}:
        \begin{itemize}
            \item Used when modifying or filtering data (e.g., removing duplicates in an array).
            \item The slow pointer keeps track of the valid part of the array, while the fast pointer scans ahead.
        \end{itemize}
\end{itemize}

\bigbreak \noindent 
\subsection{Two Sum II - Input Array Is Sorted}
\bigbreak \noindent 
Given a 1-indexed array of integers numbers that is already sorted in non-decreasing order, find two numbers such that they add up to a specific target number. Let these two numbers be numbers[index1] and numbers[index2] where $1 \leq \text{index1} < \text{index2} \leq \text{numbers.length}$
\bigbreak \noindent 
The idea is that we maintain two pointers to the array, one at the front and one at the back. If the sum of the pointers values is greater than the target, we decrement the right pointer. If the sum of the pointers values is less than the target, increase the front pointer
\bigbreak \noindent 
\begin{cppcode}
    class Solution {
        public:
        vector<int> twoSum(vector<int>& numbers, int target) {
            auto front = numbers.begin(), back = numbers.end() - 1;

            while (front < back) {
                if ((*front + *back) > target) {
                    --back;
                } else if ((*front + *back) < target) {
                    ++front;
                } else {
                    return {(int)std::distance(numbers.begin(), front) + 1, (int)std::distance(numbers.begin(), back) + 1};
                }
            }
            return {};
        }
    };
\end{cppcode}
\bigbreak \noindent 
Note we add one to the results of both std::distances because the input array is 1-indexed

\pagebreak 
\subsection{Is valid palindrome}
\bigbreak \noindent 
\begin{cppcode}
    class Solution {
        public:
        bool isPalindrome(string s) {
            auto left = s.begin(), right = s.end() - 1;

            while (left < right) {
                if (!isalnum(*left)) { ++left; continue; }
                if (!isalnum(*right)) { --right; continue; }

                if ((char)tolower(*left) != (char)tolower(*right)) return false;
                ++left, --right;
            }
            return true;
        }
    };
\end{cppcode}
\bigbreak \noindent 
We simply maintain two pointers, one at the start and one at the end. If the value of the front pointer does not match the value of the back pointer, we return false. Otherwise, we increment the left pointer and decrease the left pointer.
\bigbreak \noindent 
We ignore non alphanumeric characters, and convert uppercase characters to lowercase. If either the left or right pointer detects an invalid character, we move that pointer either left or right and continue to the next iteration. This is quicker than constructing a new string with all bad characters removed and all uppercase characters converted to lower case. Although we could easily do that will a string stream
\bigbreak \noindent 
\begin{cppcode}
    static string augmentString(const string& s) {
        ostringstream os;

        for (const auto& item : s) {
            if (isalnum(item)) os << (char)tolower(item);
        }
        return os.str();
    }
\end{cppcode}


\pagebreak 
\unsect{Greedy algorithms}
\bigbreak \noindent 
A greedy algorithm is a problem-solving approach that builds up a solution piece by piece, always choosing the option that seems best at the moment (i.e., the locally optimal choice) in the hope that this approach will lead to a globally optimal solution.
\begin{itemize}
    \item \textbf{Greedy Choice Property}: A locally optimal choice leads to a globally optimal solution.
        \bigbreak \noindent 
        Example: Choosing the largest coin first in a coin change problem.
    \item \textbf{Optimal Substructure}: A problem has an optimal substructure if an optimal solution to the problem contains optimal solutions to its subproblems.
        \bigbreak \noindent 
        Example: The shortest path problem in graphs follows this property.

\end{itemize}

\pagebreak 
\unsect{Subarray problems}
\bigbreak \noindent 
\subsection{Brute force subarray generation ($O(n^{2})$)}
\bigbreak \noindent 
\begin{cppcode}
    vector<vector<int>> subsets(vector<int>& v) {
        vector<vector<int>> res;
        vector<int> sub;

        for (int i=0; i<(int)v.size(); ++i) {
            sub.clear();
            for (int j=i; j<(int)v.size(); ++j) {
                sub.push_back(v[j]);
                res.push_back(sub);
            }
        }
        return res;
    }
\end{cppcode}
\bigbreak \noindent 
\subsection{Max subarray sum: Kadane's algorithm ($O(n)$)}
\bigbreak \noindent 
\begin{cppcode}
// Function to find the maximum subarray sum
int maxSubarraySum(vector<int> &arr) {
    int res = arr[0];
    int maxEnding = arr[0];

    for (int i = 1; i < arr.size(); i++) {
      
        // Find the maximum sum ending at index i by either extending 
        // the maximum sum subarray ending at index i - 1 or by
        // starting a new subarray from index i
        maxEnding = max(maxEnding + arr[i], arr[i]);
      
        // Update res if maximum subarray sum ending at index i > res
        res = max(res, maxEnding);
    }
    return res;
}
\end{cppcode}

\pagebreak 
\unsect{Divide and conquer}
\bigbreak \noindent 


\pagebreak 
\unsect{Range queries}
\bigbreak \noindent 
\subsection{Difference arrays}
\bigbreak \noindent 
Difference arrays are a useful technique in data structures and algorithms that allow for efficient range updates on an array.
\bigbreak \noindent 
A difference array is derived from an original array. It stores the difference between consecutive elements. For an array $A$ of size $n$, the difference array $D$ is defined as:
\begin{align*}
    D[0] &= A[0] \\
    D[i] &= A[i] - A[i-1]  \quad \text{ for} i \in \{1,2,...n-1\}
\end{align*}
\bigbreak \noindent 
The key idea is that if you want to add a constant value $x$ to a range from index $l$ to $r$ in $A$, you can update the difference array $D$ as follows
\begin{align*}
    D[l] &+= x \\
    D[r+1] &-= x \quad \text{ if $r+1$ in bounds}
\end{align*}
After processing all such range updates, you can reconstruct the final array by computing the prefix sum of $D$
\begin{align*}
    A[0] &= D[0] \\
    A[i] &= A[i-1] + D[i] \quad \text{ for } i\in \{1,2,...,n-1\} 
\end{align*}
\bigbreak \noindent 
Range updates that would normally take $O(n)$ time each can be done in $O(1)$ time
\bigbreak \noindent 
Consider an example
\begin{align*}
    A = [1,2,3,4,5]
\end{align*}
Thus,
\begin{align*}
    D = [1,1,1,1,1]
\end{align*}
\bigbreak \noindent 
With query $Q(1,3,3)$, of form $Q(l_{i}, r_{j}, v)$. Thus, $Q(1,3,3)  = A[i]+=3$ for $i\in \{1,2,3\}$
\bigbreak \noindent 
Applying $D[l] = D[1] +=3$, $D[r+1]  = D[4]-=3$, we get
\begin{align*}
    D = [1,4,1,1,-2]
\end{align*}
Then we reconstruct $A$ using prefix sums $A[i] = A[i-1] + D[i]$, and we get
\begin{align*}
    A = [1,5,6,7,5] 
\end{align*}
\bigbreak \noindent 
if $r$ is the last index in the original array, then $r+1$ will be out of bounds. In that case, you simply omit the subtraction step because there's no element beyond the last index. This naturally means that the update applies to all elements from $l$ to the end of the array. Consider the query $Q(1,4,3)$ on the array $A = [1,2,3,4,5]$. The difference array is 
\begin{align*}
    D = [1,4,1,1,1]
\end{align*}
Notice that since $D[r+1] = D[5]$ is out of bounds, we only need to do $D[l] = D[1]+=3$. The final array is 
\begin{align*}
    A[1,5,6,7,8]
\end{align*}

\bigbreak \noindent 
\subsubsection{Why it works}
\bigbreak \noindent 
Let's first consider the property between the difference array $D$ and the array $A$
\begin{align*}
    D[0] &= A[0] \\
    D[i] &= A[i] - A[i-1]
\end{align*}
Thus,
\begin{align*}
    A[i] &= D[i] + A[i-1]
\end{align*}
Suppose we have the array $A[a,b,c,d,e]$, and we want to add $k$ to $A[i]$, for $i \in \{1,2,3\} $, we have
\begin{align*}
    A[a,b+k, c+k, d+k, e]
\end{align*}
Then, 
\begin{align*}
    &D[a, b+k-a, b+k-c-k, d+k-c-k, e-d-k] \\
    &=[a, b-a+k, b-c, d-c, e-k]
\end{align*}
We see the only changes to $D$ are $D[l]$, and $D[r+1]$

\begin{align*}
    1 + 2 + 3 + \cdots
\end{align*}












\pagebreak 
\unsect{Math algorithms}
\bigbreak \noindent 
\subsection{Euclidean GCD Algorithm}
\bigbreak \noindent 
The GCD of two integers $a$ and $b$ (with $a \leq b$) is the largest integer that divides both $a$ and $b$. The Euclidean algorithm is based on the principle that
\begin{align*}
    \text{gcd}(a,b) = \text{gcd}(b, a\ \text{mod } b)
.\end{align*}
\bigbreak \noindent 
This means that the GCD of two numbers doesn't change if the larger number is replaced by its remainder when divided by the smaller number. You keep repeating this until the remainder is 0, and the GCD will be the last non-zero remainder
\bigbreak \noindent 
\begin{cppcode}
    int gcd(int a, int b)  {
        if (!b) return a;

        return gcd(b, a%b);
    }
    // An iterative approach
    int gcd(int a, int b ) {
        if (b < a) {
            b = std::exchange(a,b);
        }
        while (b != 0) {
            a = std::exchange(b, a % b);
        }
        return a;
    }
\end{cppcode}
\bigbreak \noindent 
\subsubsection{GCD of some integer collection}
\bigbreak \noindent 
We can apply the above algorithm to find the GCD of a collection of elements
\bigbreak \noindent 
\begin{cppcode}
    int gcd_set(const vector<int>& v) {
        if (v.empty()) return 0;
        int result = v[0];
        for (int i=1; i<(int)v.size(); ++i) {
            result = gcd(result, v[i]);
            // Return early
            if (result == 1) return result;
        }
        return result;
    }
\end{cppcode}

\bigbreak \noindent 
\subsection{Fibonacci numbers in constant time}
\bigbreak \noindent 
The formula for the $n$th Fibonacci number $F(n)$ can be derived using Binet's formula, which expresses the Fibonacci sequence in terms of powers of the golden ratio.
\begin{align*}
    F(n) = \frac{\phi^{n} - \psi^{n}}{\sqrt{5}}
.\end{align*}
Where $\phi$ is the golden ration $\frac{1+\sqrt{5}}{2} \approx 1.618 $, and $\psi$ is the conjugate of the golden ration $\psi = \frac{1- \sqrt{5}}{2} \approx -0.618 $

\bigbreak \noindent 
\subsection{Sterlings factorial approximation}
\bigbreak \noindent 
Stirling's Approximation provides an approximation for factorials, particularly useful for large values of $n$. The formula is:
\begin{align*}
    n! \approx  \sqrt{2\pi n}\left(\frac{n}{e}\right)^{n}
.\end{align*}

\bigbreak \noindent 
\subsection{square roots}
\bigbreak \noindent 
Given a non-negative integer $x$, return the square root of $x$ rounded down to the nearest integer. The returned integer should be non-negative as well.
\bigbreak \noindent 
\textbf{Note:} You must not use any built-in exponent function or operator.
\bigbreak \noindent 
\subsubsection{Naive $O(\sqrt{n})$}
\bigbreak \noindent 
\begin{cppcode}
    typedef long long dword; 
    class Solution {
        public:
        int mySqrt(int x) {
            dword last{};
            for (dword i=0; i<=x; ++i) {
                if (i*i == x) return i;
                else if (i*i > x) return last;
                last = i;
            }
            return 0;
        }
    };
\end{cppcode}
\bigbreak \noindent 
For each number $i \in [0,x]$, we simply check either the square of $i$ is precisely $x$, or $i^{2}> x$, in which case we return the value of $i$ before the current iteration.
\bigbreak \noindent 
This algorithm runs in $O(\sqrt{n})$ time because the for loop runs at most $\sqrt{x}$ times.
\bigbreak \noindent 
Note that we start $i$ at one and go up to including $x$ specifically to handle the case where $x=1$. If $x=0$, the for loop never runs and the default initialized dword gets returned (which is correct)
\bigbreak \noindent 
\subsubsection{$O(\lg(n))$ binary search approach}
\bigbreak \noindent 
\begin{cppcode}
    typedef long long dword;
    class Solution {
        public:
        int mySqrt(int x) {
            dword start{}, end = x, mid, res;
            while (start <= end) {
                mid = (start + end) / 2;

                if (mid * mid > x) {
                    end = mid-1;
                } else if (mid * mid < x) {
                    start = mid + 1;
                    res = mid;
                } else {
                    return mid;
                }
            }
            return res;
        }
    };
\end{cppcode}

\pagebreak 
\subsection{Fast exponentiation ($O(\lg(n))$)}
\bigbreak \noindent 
To perform exponentiation faster than $O(n)$, use Exponentiation by Squaring, which runs in $O(logn)$ time.
\bigbreak \noindent 
This method efficiently computes $a^{b}$ using the following rules:
\begin{enumerate}
    \item If $b$ is even
        \begin{align*}
            a^{b} = \left(a^{\frac{b}{2}}\right)^{2} = \left(a^{2}\right)^{\frac{b}{2}}
        \end{align*}
    \item If $b$ is odd
        \begin{align*}
            a^{b} = a \cdot a^{b-1}
        \end{align*}
\end{enumerate}
\bigbreak \noindent 
\begin{cppcode}
    long long fastExponentation(long long a, long long b) {
        long long result = 1;
        while (b > 0) {
            if (b % 2 == 1) {
            result *= a;
        } 
        a*=a;
        b/=2;
    }
    return result;
}
\end{cppcode}

\bigbreak \noindent 
\subsubsection{Recursive}
\bigbreak \noindent 
\begin{cppcode}
    class Solution {
        public:
        double r_fe(double& x, long long& n, double& res) {
            if (n <= 0) return res;

            if (n%2 == 1) {
                res*=x;
                --n;
            }
            n/=2;
            x*=x;

            return r_fe(x,n,res);
        }

        double myPow(double x, long long n) {
            double res{1.0};
            if (n < 0) {
                n = -n;
                return 1 / r_fe(x,n,res);
            }
            return r_fe(x,n,res);
        }
    };
\end{cppcode}

\pagebreak 
\subsection{Bitwise power of two}
\bigbreak \noindent 
To check if a positive integer $n$ is a power of two. That is, $n = 2^{x}$ for some $x\in\mathbb{N}$, we check
\bigbreak \noindent 
\begin{cppcode}
    if (n & n-1 == 0) {
        // n is a power of two
    }
\end{cppcode}
\bigbreak \noindent 
If $n$ is a power of two, it can be written as $2^{k}$ . Its binary form will have a single '1' followed by $k$ zeros
\bigbreak \noindent 
When you subtract 1 from a power of two, the binary representation becomes a sequence of $k$ ones.
\bigbreak \noindent 
Consider $1000 - 0001$. We can convert $0001$ to its twos complement representation, and change the subtraction to addition. $0001$ in its two complement representation is $1110$. Thus, we have
\begin{align*}
    \begin{array}{ccccc} &1&0&0&0 \\ -&0&0&0&1 \\ \hline \end{array} = \begin{array}{ccccc} 1 & 0 & & & \\ & 1 & 0 & 0 & 0 \\ +&1&1&1&1 \\ \hline &0&1&1&1 \end{array}
\end{align*}
\bigbreak \noindent 
The operation $n\&(n-1)$ compares each bit of $n$ and $n-1$. Since $n$ has a single '1' in a position where $n-1$ has a '0' (and vice versa for all lower bits), none of the corresponding bits are '1' at the same time. Therefore, the result is 0.
\bigbreak \noindent 
If $n $ is not a power of two, its binary representation will have more than one '1'. Subtracting 1 will not clear all the '1' bits when you perform the AND operation, so the result will be non-zero.
\bigbreak \noindent 
If $n =0$, $n \& (n-1)$ will be zero, but $0$ is not a power of two, so we require $n >0$.

\pagebreak 
\unsect{Number Theory}
\bigbreak \noindent 


\pagebreak 
\unsect{STL}
\bigbreak \noindent 
\subsection{<algorithm>}
\bigbreak \noindent 
\subsubsection{stable\_partition}
\bigbreak \noindent 
\begin{cppcode}
it stable_partition(begin, end, p)
\end{cppcode}
\bigbreak \noindent 
 Reorders the elements in the range [first, last) in such a way that all elements for which the predicate p returns true precede the elements for which predicate p returns false. Relative order of the elements is preserved.
 \bigbreak \noindent 
 Consider an example where we want to move all elements that are equal to zero to the end of a vector
 \bigbreak \noindent 
 \begin{cppcode}
     vector<int> v{1,0,4,3,0,6,2};
     stable_partition(v.begin(), v.end(), [](int x) -> bool {
        return x != 0;
     });
     for (const auto& item : v) cout << item << " ";
     // 1 4 3 6 2 0 0
 \end{cppcode}
 \bigbreak \noindent 
 Returns iterator to the first element of the second group.

 \bigbreak \noindent 
 \subsubsection{partition}
 \bigbreak \noindent 
 \begin{cppcode}
    it partition(begin, end, p) 
 \end{cppcode}
 \bigbreak \noindent 
 Reorders the elements in the range [first, last) in such a way that all elements for which the predicate p returns true precede all elements for which predicate p returns false. Relative order of the elements is not preserved.
 \bigbreak \noindent 
 Returns iterator to the first element of the second group.

 \bigbreak \noindent 
 \subsubsection{partition\_copy}
 \bigbreak \noindent 
 \begin{cppcode}
 pair<it, it> partition_copy(first, last, d_first_true, d_first_false, p)
 \end{cppcode}
 Where
 \begin{itemize}
     \item first, last	-	the pair of iterators defining the source range of elements to copy from
     \item d\_first\_true	-	the beginning of the output range for the elements that satisfy p
     \item d\_first\_false	-	the beginning of the output range for the elements that do not satisfy p
 \end{itemize}
 \bigbreak \noindent 
 Copies the elements from the range [first, last) to two different ranges depending on the value returned by the predicate p.
 \bigbreak \noindent 
 Returns a std::pair constructed from the iterator to the end of the d\_first\_true range and the iterator to the end of the d\_first\_false range.

 \bigbreak \noindent 
 \subsubsection{is\_partitioned}
 \bigbreak \noindent 
 \begin{cppcode}
 bool is_partitioned(firs, last, p)
 \end{cppcode}
 \bigbreak \noindent 
 Checks whether [first, last) is partitioned by the predicate p: all elements satisfy p appear before all elements that do not.

 \bigbreak \noindent 
 \subsubsection{partition\_pont}
 \bigbreak \noindent 
 Examines the partitioned range [first, last) and locates the end of the first partition, that is, the first element that does not satisfy p or last if all elements satisfy p.
 \bigbreak \noindent 
    If the elements elem of [first, last) are not partitioned with respect to the expression bool(p(elem)), the behavior is undefined
    \bigbreak \noindent 
    Returns the iterator past the end of the first partition within [first, last) or last if all elements satisfy p.











\end{document}
