\documentclass{report}

\input{~/dev/latex/template/preamble.tex}
\input{~/dev/latex/template/macros.tex}

\title{\Huge{}}
\author{\huge{Nathan Warner}}
\date{\huge{}}
\fancyhf{}
\rhead{}
\fancyhead[R]{\itshape Warner} % Left header: Section name
\fancyhead[L]{\itshape\leftmark}  % Right header: Page number
\cfoot{\thepage}
\renewcommand{\headrulewidth}{0pt} % Optional: Removes the header line
%\pagestyle{fancy}
%\fancyhf{}
%\lhead{Warner \thepage}
%\rhead{}
% \lhead{\leftmark}
%\cfoot{\thepage}
%\setborder
% \usepackage[default]{sourcecodepro}
% \usepackage[T1]{fontenc}

% Change the title
\hypersetup{
    pdftitle={Math 3}
}

\begin{document}
    % \maketitle
        \begin{titlepage}
       \begin{center}
           \vspace*{1cm}
    
           \textbf{Undergraduate Topics in Mathematics 3} \\
           Proof writing, The theory of sets, Axiomatic geometry, Numerical analysis
    
           \vspace{0.5cm}
            
                
           \vspace{1.5cm}
    
           \textbf{Nathan Warner}
    
           \vfill
                
                
           \vspace{0.8cm}
         
           \includegraphics[width=0.4\textwidth]{~/niu/seal.png}
                
           Computer Science \\
           Northern Illinois University\\
           United States\\
           
                
       \end{center}
    \end{titlepage}
    \tableofcontents
    \pagebreak 
    \unsect{Proofs}
    \bigbreak \noindent 
    \subsection{Intro to proof writing, intuitive proofs}
    \begin{itemize}
        \item \textbf{Intro to definitions, propositions and proofs: the chessboard problem}: Suppose you have a chessboard (8$\times$8 grid of squares) and a bunch of dominoes (2$\times$1 block of squares), so each domino can perfectly cover two squares of the chessboard.
            \bigbreak \noindent 
            Note that with 32 dominoes you can cover all 64 squares of the chessboard. There are many different ways you can place the dominoes to do this, but one way is to cover the first column by 4 dominoes end-to-end, cover the second column by 4 dominoes, and so on
            \bigbreak \noindent 
            Math runs on definitions, so let’s give a name to this idea of covering all the squares. Moreover, let’s not define it just for 8 $\times$ 8 boards — let’s allow the definition to apply to boards of other dimensions
            \bigbreak \noindent 
            \textbf{Definition.} A perfect cover of an $m\times n$ board with 2 $\times$ 1 dominoes is an arrangement of those dominoes on the chessboard with no squares left uncovered, and no dominoes stacked or left hanging off the end.
            \bigbreak \noindent 
            As we demonstrated above, there exist perfect covers of the 8 $\times$ 8 chessboard. This is a book about proofs, so let’s write this out as a proposition (something which is true and requires proof) and then let’s write out a formal proof of this fact.
            \bigbreak \noindent 
            \textbf{Proposition.} There exists a perfect cover of an 8 $\times$ 8 chessboard.
            \bigbreak \noindent 
            This proposition is asserting that “there exists” a perfect cover. To say “there exists” something means that there is at least one example of it. Therefore, any proposition like this can be proven by simply presenting an example which satisfies the statement.
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} Observe that the following is a perfect cover.
            \bigbreak \noindent 
            \fig{.7}{./figures/1.png}
            \bigbreak \noindent 
            We have shown by example that a perfect cover exists, completing the proof. $\blacksquare$
            \bigbreak \noindent 
            We typically put a small box at the end of a proof, indicating that we have completed our argument. This practice was brought into mathematics by Paul Halmos, and it is sometimes called the Halmos tombstone
            \bigbreak \noindent 
            One apocryphal story is that Halmos regarded proofs as living until proven. Once proven, they have been defeated — killed. And so he wrote a little tombstone to conclude his proof
            \bigbreak \noindent 
            What if I cross out the bottom-left and top-left squares, can we still perfectly cover the 62 remaining squares?
            \bigbreak \noindent 
            As you can probably already see, the answer is yes. For example, the first column can now be covered by 3 dominoes and the other columns can be covered by 4 dominoes each.
            \bigbreak \noindent 
            What if I cross out just one square, like the top-left square? Can this be perfectly covered? 
            \bigbreak \noindent 
            The answer is no
            \bigbreak \noindent 
            \textbf{Proposition.} If one crosses out the top-left square of an 8 $\times$ 8 chessboard, the remaining squares can not be perfectly covered by dominoes.
            \bigbreak \noindent 
            \textbf{Proof Idea}. The idea behind this proof is that one domino, wherever it is placed, covers two squares. And two dominoes must cover four squares. And three cover six. In general, the number of squares covered — 2, 4, 6, 8, 10, etc. — is always an even number. This insight is the key, because the number of squares left on this chessboard is 63— an odd number
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} Since each domino covers 2 squares and the dominoes are non-overlapping, if one places our k dominoes on the board, then they will cover $2k$ squares, which is always an even number. Therefore, a perfect cover can only cover an even number of squares. Notice, though, that the board has 63 remaining squares, which is an odd number. Thus, it can not be perfectly covered.
            \bigbreak \noindent 
            What if I take an 8$\times$8 chessboard and cross out the top-left and the bottom-right squares? Then can it be covered by dominoes?
            \bigbreak \noindent 
            \textbf{Proposition.} If one crosses out the top-left and bottom-right squares of an 8 $\times$ 8 chessboard, the remaining squares can not be perfectly covered by dominoes.
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} Observe that the chessboard has 62 remaining squares, and since every domino covers two squares, if a perfect cover did exist it would require
            \begin{align*}
                \frac{62}{2} = 31 \text{ dominoes}
            .\end{align*}
            \bigbreak \noindent 
            Also observe that every domino on the chessboard covers exactly one white square and exactly one black square
            \bigbreak \noindent 
            Thus, whenever you place 31 non-overlapping dominoes on a chessboard, they will collectively cover 31 white squares and 31 black squares.
            \bigbreak \noindent 
            Next observe that since both of the crossed-out squares are white squares, the remaining squares consist of 30 white squares and 32 black squares. Therefore, it is impossible to have 31 dominoes cover these 62 squares. $\blacksquare$
        \item \textbf{Naming Results}: So far, all of our results have been called “propositions.” Here’s the run-down on the naming of results:
            \begin{itemize}
                \item A theorem is an important result that has been proved.
                \item A proposition is a result that is less important than a theorem. It has also been proved.
                \item A lemma is typically a small result that is proved before a proposition or a theorem, and is used to prove the following proposition or theorem.
                \item A corollary is a result that is proved after a proposition or a theorem, and which follows quickly from the proposition or theorem. It is often a special case of the proposition or theorem.
            \end{itemize}
            All of the above are results that have been proved — a conjecture, though, has not.
            \begin{itemize}
                \item A conjecture is a statement that someone guesses to be true, although they are not yet able to prove or disprove it.
            \end{itemize}
        \item \textbf{Conjectures and counterexamples}: As an example of a conjecture, suppose you were investigating how many regions are formed if one places $n$ dots randomly on a circle and then connects them with lines.
            \bigbreak \noindent 
            \fig{.7}{./figures/2.png}
            \bigbreak \noindent 
            At this point, if you were to conjecture how many regions there will be for the $n = 6$ case, your guess would probably be 32 regions — the number of regions certainly seems to be doubling at every step. In fact, if it kept doubling, then with a little more thought you might even conjecture a general answer: that n randomly placed dots form $2^{n-1}$ regions;
            \bigbreak \noindent 
            Surprisingly, this conjecture would be incorrect. One way to disprove a conjecture is to find a counterexample to it. And as it turns out, the $n = 6$ case is such a counterexample
            \bigbreak \noindent 
            \fig{.8}{./figures/3.png}
            \bigbreak \noindent 
            This counterexample also underscores the reason why we prove things in math. Sometimes math is surprising. We need proofs to ensure that we aren’t just guessing at what seems reasonable. Proofs ensure we are always on solid ground. Further, proofs help us understand why something is true — and that understanding is what makes math so fun
            \bigbreak \noindent 
            Lastly, we study proofs because they are what mathematicians do
        \item \textbf{The pingeonhole principal}
            \bigbreak \noindent 
            \textbf{principal}. The principal has a simple form and a general form. Assume $k$ and $n$ are positive integers
            \bigbreak \noindent 
            \textbf{Simple form:} If $n + 1$ objects are placed into $n$ boxes, then at least one box has at least two objects in it.
            \bigbreak \noindent 
        \textbf{General form:} If $kn + 1$ objects are placed into $n$ boxes, then at least one box has at least $k + 1$ objects in it.
            \bigbreak \noindent 
            \textbf{Birthday example}: If there are 330 million people in the united states, how many U.S. residents are guaranteed to have the same birthday according to the pigeonhole principal?
            \bigbreak \noindent 
            To determine this, let’s see what would happen if each date of the year had exactly the same number of people born on it
            \begin{align*}
                \frac{330\times10^{6}}{366} = 901,639.344
            .\end{align*}
            \bigbreak \noindent 
            Since 901,639.344 people are born on an average day of the year, we should be able round up and say that at least one day of the year has had at least 901,640 people born on it. That is, with the pigeonhole principal we should be able to prove that there are at least 901,640 people in the USA with the same birthday
            \bigbreak \noindent 
            \textbf{Solution.} Imagine you have one box for each of the 366 dates of the (leap) year, and each person in the U.S. is considered an object. Put each person in the box corresponding to their birthday. By the general form of the pigeonhole principal (with $n = 366$ and $k = 901, 639$ and thus $k + 1 = 901, 640$), any group of
            \begin{align*}
                (901, 639)(366) + 1
            .\end{align*}
            \bigbreak \noindent 
            people is guaranteed to contain 901,640 people which have the same birthday.
        \item \textbf{Another pingeonhole example}:
            \bigbreak \noindent 
            \textbf{Proposition.} Given any five numbers from the set $\{1, 2, 3, 4, 5, 6, 7, 8\}$, two of the chosen numbers will add up to 9.
            \bigbreak \noindent 
            We may think to start by listing the pairs that sum to 9. We have
            \begin{align*}
                1 &+ 8 \\ 
                2 &+ 7 \\
                3 &+ 6 \\
                4 &+ 5 
            .\end{align*}
            And of course $8+1,7+2,..$ etc. We see we have four sums, we choose these sums as our boxes. If each of the four sums is a box, and each number is an object, then we are placing five objects into four boxes 
            \bigbreak \noindent 
            \textbf{Proof.} Let one box correspond to the numbers 1 and 8, a second box correspond to 2 and 7, another to 3 and 6, and a final box to 4 and 5. Notice that each of these pairs adds up to 9.
            \bigbreak \noindent 
            Given any five numbers from $\{1, 2, 3, 4, 5, 6, 7, 8\}$, place each of these five numbers in the box to which it corresponds; for example, if your first number is a 6, then place it in the box labeled “3 and 6.” Notice that we just placed five numbers into four boxes. Thus, by the simple form of the pigeonhole principal, there must be some box which contains two numbers in it. These two numbers add up to 9, as desired
        \item \textbf{Another pingeonhole example}:
            \bigbreak \noindent 
            \textbf{Proposition.} Given any collection of 10 points from inside the following square (of side-length 3), there must be at least two of these points which are of distance at most $\sqrt{2}$
            \bigbreak \noindent 
            \fig{1}{./figures/4.png}
            \bigbreak \noindent 
            \textbf{Proof.} Divide the $3\times 3$ square into nine $1\times 1$ boxes. Placing 10 arbitrary points amongst the boxes gaurantees that at least one box will have at least two points. We observe that the farthest these two points can be from each other is when they sit in two corners such that a diagonal line through the box hits both points. The length of this line is given by
            \begin{align*}
                \sqrt{1^{2} + 1^{2}} = \sqrt{2}
            .\end{align*}
            Thus, we observe that the maximum distance of these two points is $\sqrt{2}$ $\blacksquare$
        \item \textbf{Another pingeonhole example}:
            \bigbreak \noindent 
            \textbf{Proposition.} Given any 101 integers from $\{1, 2, 3, . . . , 200\}$, at least one of these numbers will divide another
            \bigbreak \noindent 
            \textbf{Solution.} As we ponder about how to construct 100 boxes from the properties of the set, we may wonder how the even and odd members partition this set. Call $S = \{1,2,3,...,200\} $, $E=\{2,4,6,...,200\} $, and $O = \{1,3,5,...,199\} $. Note that $E \cup O = S$. We notice that these two sets are arithmetic sequences, each with difference two. If $a_{n} = a_{1} +  (n-1)d$, then 
            \begin{align*}
                n &= \frac{a_{n} - a_{1}}{2} + 1 \\
                \implies n&= 100
            .\end{align*}
            \bigbreak \noindent 
            Let's make the odd numbers are boxes. We note that any even number $\ell$ can be written as $\ell = 2^{k}m$, where $m$ is odd, and $k$ is the highest power of two that divides $\ell$. Thus, in box $m$, we place any number of the form $2^{k}m$
            \bigbreak \noindent 
            \fig{.5}{./figures/5.png}
            \bigbreak \noindent 
            For any pair of numbers in the same box, the smaller divides the larger. Picking 101 numbers from the set $S$, and only 100 boxes... by the pigeonhole principal we must have atleast two numbers in the same box, and thus the smaller divides the larger. $\blacksquare$.
            \bigbreak \noindent 
            \textbf{Formal proof.} 
            \textbf{Proof.} For each number $n$ from the set $\{1, 2, 3, \dots, 200\}$, factor out as many 2's as possible, and then write it as $n = 2^k \cdot m$, where $m$ is an odd number. So, for example, $56 = 2^3 \cdot 7$, and $25 = 2^0 \cdot 25$. Now, create a box for each odd number from 1 to 199; there are 100 such boxes.
            \bigbreak \noindent 
            Remember that we are given 101 integers and we want to find a pair for which one divides the other. Place each of these 101 integers into boxes based on this rule:
            \bigbreak \noindent 
            \begin{quote}
                If the integer is $n$, then place it in Box $m$ if $n = 2^k \cdot m$ for some $k$.
            \end{quote}
            \bigbreak \noindent 
            For example, $72 = 2^3 \cdot 9$ would go into Box 9, because that's the largest odd number inside it.
            \bigbreak \noindent 
            Since 101 integers are placed in 100 boxes, by the pigeonhole principal (principal 1.5) some box must have at least 2 integers placed into it; suppose it is Box $m$. And suppose these two numbers are $n_1 = 2^k \cdot m$ and $n_2 = 2^\ell \cdot m$, and let’s assume the second one is the larger one, meaning $\ell > k$. Then we have now found two integers where one divides the other; in particular $n_1$ divides $n_2$, because:
            \[
                \frac{n_2}{n_1} = \frac{2^\ell \cdot m}{2^k \cdot m} = 2^{\ell - k}.
            \]
            This completes the proof.$\blacksquare$
        \item \textbf{Another pigeonhole example}
            \bigbreak \noindent 
            \textbf{Proposition}. Suppose $G$ is a graph with $n \geq 2$ vertices. Then $G$ contains two vertices which have the same degree.
            \bigbreak \noindent 
            We start by observing that the minimum degree is zero, and the maxmium is $n-1$. It could happen that a vertex is connected to no other vertices, and a vertex could be connected to all other vertices. If a vertex is connected to all other vertices, than it has degree $n-1$, because it has an edge going to all vertices but itself. Thus, we have our boxes. But you may notice that we have $n$ boxes for $n$ vertices. This may seem like a problem, but after some thought you may see that it is not possible for the zero box and the $n-1$ box to both be used for a specific graph $G$. Thus, we have only $n-1$ boxes for $n$ vertices.
            \bigbreak \noindent 
            The rest of the proof is left as an exercise for the reader.
        \item \textbf{Classic Geometry Theorem}. Given any two points on the sphere, there is a great circle that passes through those two points. 
            \bigbreak \noindent 
            Given a sphere, there are infinitely many ways to cut it in half, and each of these paths of the knife is called a great circle
            \bigbreak \noindent 
            \fig{.5}{./figures/6.png}
        \item \textbf{Final pigeonhole example}
            \bigbreak \noindent 
            \textbf{Proposition}. If you draw five points on the surface of an orange in marker, then there is always a way to cut the orange in half so that four points (or some part of the point) all lie on one of the halves.
            \bigbreak \noindent 
            \textbf{\textit{Proof}}. Consider an orange with five points drawn on it. Pick any two of these points, and call them $p$ and $q$. By the Classic Geometry Theorem, there exists a great circle passing through these points; angle your knife to cut along this great circle. Because the points are drawn in marker, they are wide enough so that part of these two points appear on both halves.
            \bigbreak \noindent 
            Now consider the remaining three points and the two halves that you just cut the orange into. Consider these three points to be objects and the halves to be boxes; by the simple form of the pigeonhole principal, at least two of these three points are on the same orange half. These two, as well a portion of $p$ and of $q$, give four points or partial points, as desired $\quad \blacksquare $



    \end{itemize}

    \pagebreak 
    \subsection{Direct proofs}
    \begin{itemize}
        \item \textbf{Fact about integers}: The sum of integers is an integer, the difference of integers is an integer, and the product of integers is an integer. Also, every integer is either even or odd.
            \bigbreak \noindent 
            We are calling these facts because, while they are true and one could prove them, we will not be proving them here
        \item \textbf{Even and odd integers}: An integer $n$ is \textit{even} if $n=2k$ for some integer $k $
            \bigbreak \noindent 
            An integer $n$ is \textit{odd} if $n=2k+1$ for some integer $k$
        \item \textbf{Sum of two even integers}
            \bigbreak \noindent 
            \textbf{Proposition.} The sum of two even integers is even
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} Assume $n$ and $m$ are even integers, then $n = 2a$, and $m = 2b$ for some integers $a$ and $b$. Furthermore,
            \begin{align*}
                n + m &= 2a + 2b = 2(a+b)
            .\end{align*}
            \bigbreak \noindent 
            Since the sum of two integers is itself an integer, then we have two times an integer, which satisfies the definition of an even number. Hence, the sum $n + m$ is even, where $n$ and $m$ are even. $\int$
        \item \textbf{More on propositions}: We can rewrite our propositions to take the form
            \begin{quote}
               if \textit{statement} is true, then \textit{other statement} is also true 
            \end{quote}
            For example, 
            \begin{quote}
               if $m$ and $n$ are even, then $m+n$ is also even
            \end{quote}
            \bigbreak \noindent 
            Another way to summarize such statements is this:
            \begin{quote}
               \textit{some statement} is true implies \textit{some other statement} is true. 
            \end{quote}
            Which allows us to use the implies symbol $\implies$. For example, 
            \begin{quote}
               $m$ and $n$ being even $\implies$ $m+n$ is even 
            \end{quote}
            We have the general form $P \implies Q$, where $P$  and $Q$ are statements
            \bigbreak \noindent 
             However, when writing formally, like when writing up the final draft of your homework, these symbols are rarely used. You should write out solutions with words, complete sentences, and proper grammar. Pick up any of your math textbooks, or look online at math research articles, and you will find that such practices are standard.
        \item \textbf{The structure of direct proofs}: A direct proof is a way to prove a “$P \Rightarrow Q$” proposition by starting with $P$ and working your way to $Q$. The “working your way to $Q$” stage often involves applying definitions, previous results, algebra, logic, and techniques. Here is the general structure of a direct proof:
            \bigbreak \noindent 
            \begin{mdframed}
                \textbf{Proposition}. $P\implies Q$
                \bigbreak \noindent 
                \textbf{\textit{Proof.}} Assume $P$
                \bigbreak \noindent 
                \hspace{1cm}\textit{Explain what $P $ means by applying definitions and/or other results}
                \begin{align*}
                    &\vdots \quad \text{Apply algebra,} \\
                    &\vdots \quad \text{logic techniques}
                .\end{align*}
                \bigbreak \noindent 
                \hspace{1cm} \textit{Hey look, that's what $Q$ means}
                \bigbreak \noindent 
                Therefore $Q$ \hspace{10cm} $\blacksquare $
            \end{mdframed}
        \item \textbf{Proof by cases}: A related proof strategy is proof by cases. This is a “divide and conquer” strategy where one breaks up their work into two or more cases 
            \bigbreak \noindent 
            The below example of proof by cases will also give us more practice with direct proofs involving definitions. Indeed, when you break up a problem in two parts, those two parts still need to be proven, and a direct proof is often the way to tackle each of those parts
            \bigbreak \noindent 
            \textbf{Proposition.} If $n$ is an integer, then $n^{2} + n + 6$ is even.
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} Assume $n$ is an integer, then either $n$ is even or it is odd.
            \begin{tcolorbox}[penv]
                \textit{Case I}. Assume $n$ is even, then $n=2m$ for some integer $m$. Thus, we have
                \begin{align*}
                    n^{2} + n + 6 &= (2m)^{2} + 2m + 6 \\
                      &=4m^{2} + 2m + 6 \\
                      &= 2 (2m^{2} + m + 3)
              .\end{align*}
              \bigbreak \noindent 
              Observe that $2m^{2} + m + 3 \in \mathbb{Z}$. Thus, we have two times an integer, which satisfies the definition of an even number.
              \bigbreak \noindent 
              \textit{Case 2.} Assume $n$ is odd, then $n=2m+1$ for some integer $m$. Thus,
              \begin{align*}
                  n^{2} + n + 6 &= (2m+1)^{2} + 2m + 1 + 6 \\
                                &=4m^{2} + 4m + 1 + 2m + 7 \\
                                &= 4m^{2} + 6m + 8 \\
                                &= 2(2m^{2} + 3m + 4)
              .\end{align*}
              \bigbreak \noindent 
              Since $m$ is an integer, $2m^{2} + 3m +4$ is an integer, and we again have two times an integer, which is an even integer.
              \bigbreak \noindent 
              We have shown that $n^{2} + n  + 6 $ is even whether $n$ is even or odd. Combined, this shows that $n^{2} + n + 6$ is even for all integers $n$ $\quad \blacksquare$
                
            \end{tcolorbox}
        \item \textbf{Proof by exhaustion (brute force proof)}: A proof by cases cuts up the possibilities into more manageable chunks. If the theorem refers to a collection of elements and your proof is simply checking each element individually, then it is called a \textit{proof by exhaustion} or a \textit{brute force proof}
        \item \textbf{Divisibility}: An integer \(a\) is said to divide an integer \(b\) if \(b = ak\) for some integer \(k\). When \(a\) does divide \(b\), we write \(a \mid b\), and when \(a\) does not divide \(b\), we write \(a \nmid b\).
            \bigbreak \noindent 
            \textbf{Note:} A common mistake is to see something like “$2 \mid 8$” and think that this equals 4. The expression “$a \mid  b$” is either true or false
            \bigbreak \noindent 
            \textbf{Remark.} $a\mid 0$ for any integer $a$, because $0 = a \cdot 0$ for every such $a$
            \bigbreak \noindent 
            $0\nmid b$ for any nonzero integer $b$, because for any such $b$, we have $b\ne 0 \cdot k $ for any integer $k$
        \item \textbf{The transitive property of divisibility}:
            \bigbreak \noindent 
            \textbf{Proposition}. Let $a,b$, and $c$ be integers, if $a\mid b$ and $b \mid c$, then $a\mid c$
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} Assume $a,b$, and $c$ are integers. Further assume that $a\mid b$, and $b\mid c$
            \penv {
                By the definition of divisibility, $a\mid b$ and $b \mid c$ implies $b = ak$ for some integer $k$, and $c = bs$ for some integer $s$
                \bigbreak \noindent 
                If $a\mid c$, we require that $c = ar$ for some integer $r$
                \bigbreak \noindent 
                \begin{align*}
                    b &= ak  \\
                    \implies c &= (ak)s \\
                    \implies c&= a(ks)
                .\end{align*}
                \bigbreak \noindent 
            }
            Since $k$ and $s$ are integers, then their product $ks$ is itself an integer. Let $r = ks$. Then $c  = ar$, which is precisely the definition of divisiblity, and we conclude that $a\mid c$. $\quad \blacksquare$
        \item \textbf{The division algorithm}:
            \bigbreak \noindent 
            \textbf{Theorem.} For all integers $a$ and $m $ with $m>0 $, there exist unique integers $q $ and $r $ such that
            \begin{align*}
                a = mq + r
            .\end{align*}
            Where $0 \leq r < m$. We call $q$ the \textit{quotient} and $r$ the \textit{remainder}
        \item \textbf{Common divisor, greatest common divisor}:
            Let $a$ and $b$ be integers. If $c \mid a$ and $c \mid b$, then $c$ is said to be a common divisor of $a$ and $b$.
            \bigbreak \noindent 
            The greatest common divisor of $a$ and $b$ is the largest integer $d$ such that $d \mid a$ and $d \mid b$. This number is denoted $\text{gcd}(a, b)$.
            \bigbreak \noindent 
            Note that there is one pair of integers that does not have a greatest common divisor; if $a = 0$ and $b = 0$, then every positive integer $d$ is a common divisor of $a$ and $b$. This means that no divisor is the greatest divisor, since you can always find a bigger one. Thus, in this one case, $gcd(a, b)$ does not exist
        \item \textbf{Bezout's identity}: If $a$ and $b$ are positive integers, then there exist integers $k$ and $\ell$ such that
            \begin{align*}
                \gcd{(a, b)} = ak + b\ell
            .\end{align*}
            \bigbreak \noindent 
            As an example, suppose $a=12$ and $b=20$, then $\gcd{(12,20)} =4$, and we have
            \begin{align*}
                4 &= 12k + 20 \ell  \\
                \implies \ell &= \frac{1}{5}-\frac{3}{5}k
            .\end{align*}
            Let $k=2$, then we see $\ell = -1$. We see that there are infinitely many solutions, $k=2, \ell = -1$ is just one of them. Nevertheless, this theorem simply says that at least one solution must exist. 
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} Assume $a$ and $b$ are fixed positive integers, notice that the expression $ax + by$ can take many values for integers $x$ and $y$. Let $d$ be the \textit{smallest positive integer} that $ax + by$ can be equal. Let $k$ and $\ell$ be the $x$ and $y$ that obtain this $d$. That is, 
            \begin{align*}
                d = ak + b\ell
            .\end{align*}
            \penv{
               We now must show that $d$ is a common divisor of $a$ and $b$, and then that it is the \textit{greatest common divisor}
               \bigbreak \noindent 
               \textit{Part 1 (common divisor)}. $d$ is a common divisor of $a$ and $b$ if $d\mid a$ and $d\mid b$. To see that $d \mid a$, we examine the division algorithm. We know that there exsits unique integers $q $ and $r $ such that
               \begin{align*}
                   a = dq + r
               .\end{align*}
               With $0 \leq r < d$. We have
               \begin{align*}
                   r &= a-dq \\
                     &=a-(ak + b\ell)q \\
                     &=a-akq -b\ell q \\
                     &= a(1-kq) + b(-\ell q)
               .\end{align*}
               Observe that $1-kq$, and $-\ell q$ are both integers, Since $r$ is written in the form $ax + by$, $0 \leq r < d$, and $d$ is the smallest positive integer that this form can produce (with the given $a,b$), it must be that $r=0$. Thus,
               \begin{align*}
                   a = dq + 0 = dq
               .\end{align*}
               And we see that $d\mid a$. A similar argument will show that $d\mid b$ as well. This proves that $d$ is a common divisor of $a$ and $b$.
               \bigbreak \noindent 
           }
           \penv{
               \textit{Part 2 (gcd)}. Assume that $d^{\prime}$ is some other common divisor of $a$ and $b$. We must show that $d^{\prime} \leq d$. If $d^{\prime}$ is a common divisor of $a$ and $b$, then $d^{\prime} \mid a$ and $d^{\prime} \mid b$, which implies $a = d^{\prime}n$, and $b = d^{\prime} m$, for some integers $n$ and $m$. If $d = ak + b\ell$, then
               \begin{align*}
                   d &= d^{\prime}nk + d^{\prime}m\ell \\
                   &=d^{\prime}(nk + m\ell) \\
                   \implies d^{\prime} &=\frac{d}{nk + m\ell}
               .\end{align*}
               Since $n,k,m,\ell \in \mathbb{Z}$, it follows that $nk +m\ell \in \mathbb{Z}$. Thus, $d^{\prime} \leq d$.
           }
           \bigbreak \noindent 
           Therefore, we have shown that $d$ is not only a common divisor of $a$ and $b$, but that it is also the largest, and hence the $gcd$. Thus,
           \begin{align*}
               \gcd{(a,b)} = d = ak + b \ell
           .\end{align*}
           $\blacksquare$
           \bigbreak \noindent 
           A corollary from this result is that $\gcd{(ma, mb)} = m \gcd{(a,b)}$. If $\gcd{(a,b)} = ak + b\ell$, we have
           \begin{align*}
               \gcd{(ma,mb)} &= mak + mb\ell  \\
                             &=m(ak + b\ell) \\
                             &=m\gcd{(a,b)}
           .\end{align*}
       \item \textbf{Modulo and congruence}: 
           For integers \(a\), \(r\), and \(m\), we say that \(a\) is congruent to \(r\) modulo \(m\) and we write \(a \equiv r \pmod{m}\) if \(m \mid (a - r)\).
           \bigbreak \noindent 
           For example, $18 \equiv  4 \pmod{7}$ because $18 = 7(2) +4 $, we see that $7 \mid (18-4)$
           \bigbreak \noindent 
           If \(a\) divided by \(m\) leaves a remainder of \(r\), then \(a \equiv r \pmod{m}\). However, this is not the only way to have \(a \equiv r \pmod{m}\) — it is not required that \(r\) be the remainder when \(a\) is divided by \(m\); all that is required is that \(a\) and \(r\) have the same remainder when divided by \(m\). For example:
           \begin{align*}
               18 =11 \pmod{7}
           .\end{align*}
        \item \textbf{Properties of modular congruence}: Assume that $a, b, c, d$
            and $m$ are integers, $a \equiv b \pmod{m}$ and $c \equiv d\pmod{m}$. Then
            \begin{enumerate}[label=(\roman*)]
                \item $a + c  \equiv b + d \pmod{m} $ 
                \item $a - c  \equiv b - d \pmod{m} $ 
                \item $a \cdot  c  \equiv b \cdot  d \pmod{m} $ 
            \end{enumerate}
            \bigbreak \noindent 
            \textbf{\textit{Proof of property $i$}}. Assume that $a \equiv b \pmod{m}$, and $c \equiv d \pmod{m}$, we must show that $a + c \equiv b + d \pmod{m}$
            \penv{
                If $a\equiv b \pmod{m}$, then $m \mid a-b$, which implies $a-b = mk$ for some $k\in \mathbb{Z}$. Similarly, $c \equiv d \pmod{m} \implies m \mid c-d \implies c-d = m\ell$, for some $\ell \in \mathbb{Z}$. Adding these two equations yields
                \begin{align*}
                    (a-b) + (c-d) &= mk + m\ell \\
                    \implies (a+c) - (b + d) &= m(k+\ell)
                .\end{align*}
        }
        Since $k+\ell \in \mathbb{Z}$, then by the definition of divisibility
        \begin{align*}
            m \mid (a+c) - (b+d)
        .\end{align*}
        Which then by the definition of congruence
        \begin{align*}
            a+c \equiv b+d \pmod{m}
        .\end{align*}
        $\blacksquare$
        \bigbreak \noindent 
        \textbf{\textit{Proof of property $iii$}}. Assume $a \equiv b \pmod{m}$, and $c \equiv d \pmod{m}$
        \penv{
            From above we know it follows that $a - b = mk $, and $c-d = m\ell$, for $k,\ell \in \mathbb{Z}$. If $ ac \equiv bd \pmod{m}$, it must be that $ac -bd = ms$, for some $s\in \mathbb{Z}$. Let's see if we can derive $ac-bd$ in terms of what we know, namely $a-b$ and $c-d$. Amazingly,
            \begin{align*}
                ac -bd &= (a-b)c + (c-d)b \\
                &= mkc + m\ell b \\
                &= m(kc +\ell b)
            .\end{align*}
        }
        It then follows that
        \begin{align*}
            m \mid ac-bd
        .\end{align*}
        Thus,
        \begin{align*}
            ac \equiv bd \pmod{m}
        .\end{align*}
        $\blacksquare$
    \item \textbf{Prime and composite integers}: An integer $p \geq 2$ is prime if its only positive divisors are 1 and $p$. An integer $n \geq 2$ is composite if it is not prime. Equivalently, $n$ is composite if it can be written as $n = st$, where $s$ and $t$ are integers and $1 < s, t < n$.
        \bigbreak \noindent 
        \textbf{Note:} To be clear, “$1 < s, t < n$” means that both $s$ and $t$ are between 1 and $n$.
    \item \textbf{Properties of primes and divisibility}:
        \bigbreak \noindent 
        \textbf{Lemma}. Let \( a, b \) and \( c \) be integers, and let \( p \) be a prime:
        \begin{enumerate}[label=(\roman*)]
            \item If \( p \nmid a \), then \( \gcd(p, a) = 1 \).
            \item If \( a \mid bc \) and \( \gcd(a, b) = 1 \), then \( a \mid c \).
            \item If \( p \mid bc \), then \( p \mid b \) or \( p \mid c \) (or both).
        \end{enumerate}
        \bigbreak \noindent 
        \textbf{\textit{Proof of property $i$}}. Assume that $p$ does not divide $a$, then $p$ cannot possibly be a common divisor of $a$ and $p$, because it is not a divisor of $a$. 
        \bigbreak \noindent 
        Since $p \in \mathbb{P}\footnote{Where $\mathbb{P}$ is the family of primes}$, then the only divisors of $p$ are one and itself. Thus, the only option left is one. Hence, the greatest common divisor is one. $\blacksquare$
        \pagebreak \bigbreak \noindent 
        \textbf{\textit{Proof of property $ii$}}. Assume $a\mid bc$, and $\gcd{(a,b)} = 1$. Then, $bc = ar$ for some integer $r$, and by Bezout's identity, there exist some integers $k, \ell$ such that
        \begin{align*}
            \gcd{(a,b)} &= ak + b\ell \\
            \implies 1&= ak + b\ell 
        .\end{align*}
        If $a\mid c$, we require $c = as$, for some integer $s$. If we multiply the above expression by c, we get
        \begin{align*}
            c &= cak + cb\ell
        .\end{align*}
        Since we assumed $a\mid bc$, then it must be that $bc = ar$, for $r\in \mathbb{Z}$. Thus, we have
        \begin{align*}
            c &= cak + ar\ell \\
              &= a(ck + r\ell)
        .\end{align*}
        Since $c,k,r,\ell \in \mathbb{Z}$, the expression $ck+r\ell$ is also an integer, and by the definition of divisibility, it must be that $a\mid c$  $\quad \blacksquare $
        \bigbreak \noindent 
        \textbf{\textit{Proof of property $iii$}}. Assume that $p \mid bc$. Then there are two cases, either $p\mid b$, or $p\nmid b$.
        \bigbreak \noindent 
        \textit{Case I}. If $p\mid b$, then the statement is true and we are done
        \bigbreak \noindent 
        \textit{Case II}. If $p\nmid b$, then by property $i$, it must be that $\gcd{(p,b)} = 1$. By property $ii$, if $p \mid bc$, and $\gcd{(p,b)} = 1$, then it must be that $p \mid c$. $\quad \blacksquare$.
    \item \textbf{More on properties of congruence}: We return to congruence to examine the statement
        \begin{align*}
            ak \equiv bk \pmod{m} \stackrel{?}{\implies} a\equiv b \pmod{m}
        .\end{align*}
        \bigbreak \noindent 
        \textbf{Proposition (\textit{modular cancellation law})}. Let $a,b,k,m$ be integers. If $ak \equiv bk \pmod{m}$, and $\gcd{(m,k)} = 1$, then $a \equiv b \pmod{m}$
        \bigbreak \noindent 
        \textbf{\textit{Proof}}. Assume $ak \equiv bk\pmod{m}$, and $\gcd{(m,k)} =1$, then $m \mid ak-bk$, and $ak-bk = m\ell$, for some integer $\ell$. 
        \bigbreak \noindent 
        \penv{
            If $a\equiv b\pmod{m}$, then $m\mid a-b$, and $a-b = mr$, for some integer $r$. Since $ak \equiv bk\pmod{m}$, then it must be that
            \begin{align*}
                ak-bk &= m\ell \\
                \implies k(a-b) &= m\ell \\
                \implies a-b &= \frac{m\ell}{k}
            .\end{align*}
            Thus, we require $\frac{\ell}{k}$ to be an integer, it then follows that the proposition holds true.
            \bigbreak \noindent 
            We know that if $a\mid bc$, and $\gcd{(a,b)} = 1$, then $a\mid c$. Thus, since $k\mid m\ell$, and $\gcd{(m,k)} = 1$, it must be that $k\mid \ell$. Hence, $ \frac{\ell}{k} \in \mathbb{Z}$, and 
            \begin{align*}
                a-b = m\left(\frac{\ell}{k}\right)
            .\end{align*}
            And by the definition of divisibility, $m\mid a-b$, which implies $a \equiv b \pmod{m}$ $\quad \blacksquare$.
        }
    \item \textbf{Fermat's little theorem}: If $a$ is an integer and $p$ is a prime which does not divide $a$, then
        \begin{align*}
            a^{p-1} \equiv 1 \pmod{p}
        .\end{align*}
        \bigbreak \noindent 
        \textbf{\textit{Proof.}} 
        Assume that $a$ is an integer and $p$ is a prime which does not divide $a$. We begin by proving that when taken modulo $p$,
        \[
            \{a, 2a, 3a, \dots, (p-1)a\} \equiv \{1, 2, 3, \dots, p-1\}.
        \]
        To do this, observe that the set on the right has every residue modulo $p$ except $0$, and each such residue appears exactly once. Therefore, since both sets have $p-1$ elements listed, in order to prove that the left set is the same as the right set, it suffices to prove this:
        \begin{enumerate}
            \item No element in the left set is congruent to $0$, and
            \item Each element in the left set appears exactly once.
        \end{enumerate}
        In doing so, we will twice use the modular cancellation law (Proposition 2.18) to cancel out an $a$, and so we note at the start that by Lemma 2.17 part (i) we have $\gcd(p, a) = 1$.
        \bigbreak \noindent 
        \textbf{Step 1.} First we show that none of the terms in $\{a, 2a, 3a, \dots, (p-1)a\}$, when considered modulo $p$, are congruent to $0$. To do this, we will consider an arbitrary term $ia$, where $i$ is anything in $\{1, 2, 3, \dots, p-1\}$. Indeed, if we did have some
        \[
            ia \equiv 0 \pmod{p},
        \]
        which is equivalent to
        \[
            ia \equiv 0a \pmod{p},
        \]
        then by the modular cancellation law (Proposition 2.18) we would have
        \[
            i \equiv 0 \pmod{p}.
        \]
        That is, in order to have $ia \equiv 0 \pmod{p}$, that would have to have $i \equiv 0 \pmod{p}$. Therefore we are done with Step 1, since no $i$ from $\{1, 2, 3, \dots, p-1\}$ is congruent to $0$ modulo $p$.
        \bigbreak \noindent 
        \textbf{Step 2.} Next we show that every term in $\{a, 2a, 3a, \dots, (p-1)a\}$, when considered modulo $p$, does not appear more than once in that set. Indeed, if we did have
        \[
            ia \equiv ja \pmod{p},
        \]
        for $i$ and $j$ from $\{1, 2, 3, \dots, p-1\}$, then by the modular cancellation law (Proposition 2.18) we have
        \[
            i \equiv j \pmod{p}.
        \]
        And since $i$ and $j$ are both from the set $\{1, 2, 3, \dots, p-1\}$, this means that $i = j$. In other words, each term in $\{a, 2a, 3a, \dots, (p-1)a\}$ is not congruent to any other term from that set — it is only congruent to itself. This completes Step 2.
        \bigbreak \noindent 
        We have succeeded in proving that when taken modulo $p$,
        \[
            \{a, 2a, 3a, \dots, (p-1)a\} \equiv \{1, 2, 3, \dots, p-1\},
        \]
        even though the numbers in these sets may be in a different order. But since the order does not matter when multiplying numbers, we see that
        \[
            a \cdot 2a \cdot 3a \cdot 4a \cdot \dots \cdot (p-1)a \equiv 1 \cdot 2 \cdot 3 \cdot 4 \cdot \dots \cdot (p-1) \pmod{p}.
        \]
        Then, since $\gcd(2, p) = 1$ by Lemma 2.17 part (i), by the modular cancellation law (Proposition 2.18) we may cancel a $2$ from both sides:
        \[
            a \cdot 3a \cdot 4a \cdot \dots \cdot (p-1)a \equiv 1 \cdot 3 \cdot 4 \cdot \dots \cdot (p-1) \pmod{p}.
        \]
        Then, since $\gcd(3, p) = 1$ by Lemma 2.17 part (i), by the modular cancellation law (Proposition 2.18) we may cancel a $3$ from both sides:
        \[
            a \cdot a \cdot 4a \cdot \dots \cdot (p-1)a \equiv 1 \cdot 4 \cdot \dots \cdot (p-1) \pmod{p}.
        \]
        Continuing to do this for the $4, 5, \dots, (p-1)$ on each side (each of which has a greatest common divisor of $1$ with $p$, by Lemma 2.17 part (i)), by the modular cancellation law (Proposition 2.18) we obtain
        \[
            \underbrace{a \cdot a \cdot a \cdot \dots \cdot a}_{p-1 \text{ copies}} \equiv 1 \pmod{p},
        \]
        which is equivalent to what we sought to prove:
        \[
            a^{p-1} \equiv 1 \pmod{p}.
        \]
    \item \textbf{Bonus proof}:
        \bigbreak \noindent 
        \textbf{Proposition.} If $x$ and $y$ are positive integers, and $x \geq y$, then $\sqrt{x} \geq \sqrt{y} $
        \bigbreak \noindent 
        \textbf{\textit{Proof.}} Assume $x$ and $y$ are positive integers, and $x \geq y$. Then
        \begin{align*}
            x &\geq y \\
            \implies x -y & \geq 0  \\
        .\end{align*}
        Since $x,y \geq 0$, $\sqrt{x^{2}} = \abs{x} = x$, and $\sqrt{y^{2}} = \abs{y} = y $. Thus,
        \begin{align*}
            x-y &\geq 0 \\
            \implies \sqrt{x^{2}} - \sqrt{y^{2}} &\geq 0  \\
            \implies (\sqrt{x} - \sqrt{y})(\sqrt{x} + \sqrt{y}) &\geq 0 \\
            \implies \sqrt{x} - \sqrt{y} &\geq 0  \quad \quad \blacksquare
        .\end{align*}
    \item \textbf{The AM-GM inequality}:
        \bigbreak \noindent 
        \textbf{Theorem (\textit{AM-GM inequality})}. If $x,y \geq 0 \in \mathbb{Z}$, then $\sqrt{xy} \leq \frac{x+y}{2} $
        \bigbreak \noindent 
        \textbf{\textit{Proof.}} Assume $x,y \geq 0 \in \mathbb{Z}$. Consider
        \begin{align*}
            0 \leq (x-y)^{2}
        .\end{align*}
        Which we know to be true, squaring an integer is always positive, and we know $x-y$ to be an integer. It then follows that
        \begin{align*}
            0 \leq x^{2} -2xy + y^{2}
        .\end{align*}
        If we add $4xy$ to both sides, we get
        \begin{align*}
            4xy &\leq x^{2} + 2xy + y^{2} \\
            \implies 4xy &\leq (x + y)^{2} \\
        .\end{align*}
        Now let's take the square root of both sides
        \begin{align*}
            2\sqrt{xy} \leq \abs{x+y}
        .\end{align*}
        Since $x,y \geq 0$, $\abs{x+y} = x+y$. Thus,
        \begin{align*}
            2\sqrt{xy} \leq x + y \\
            \therefore \sqrt{xy} \leq \frac{x+y}{2}
        .\end{align*}
        \bigbreak \noindent 
        \textbf{Note:} Some of the steps taken in this proof may seem a bit random, but if we start at the proposition $\sqrt{xy} \leq \frac{x+y}{2}$ and work backwards algebraically, we see
        \begin{align*}
            \sqrt{xy} &\leq \frac{x+y}{2} \\
            2\sqrt{xy} &\leq x+y \\
            4xy &\leq (x+y)^{2} \\
            4xy &\leq x^{2} + 2xy + y^{2} \\
            0 &\leq x^{2} + 2xy + y^{2} - 4xy \\
            0 &\leq x^{2} - 2xy + y^{2} \\
            0 &\leq (x-y)^{2}
        .\end{align*}
        \bigbreak \noindent 
        We see that we have derived a starting point, and were just working backwards in the proof.
            
    \end{itemize}

    \pagebreak 
    \subsection{Sets}
    \begin{itemize}
        \item \textbf{Vacuous truth}: a vacuous truth is a conditional or universal statement (a universal statement that can be converted to a conditional statement) that is true because the antecedent cannot be satisfied.[1] It is sometimes said that a statement is vacuously true because it does not really say anything. For example, the statement "all cell phones in the room are turned off" will be true when no cell phones are present in the room. In this case, the statement "all cell phones in the room are turned on" would also be vacuously true, as would the conjunction of the two: "all cell phones in the room are turned on and turned off", which would otherwise be incoherent and false.
        \item \textbf{Review: Proper subset}: If $A = B$, then $A \subseteq B$. In the case that $A \subseteq B$ and $A \ne B$, we say that $A$ is a proper subset of $B$. the correct notation for this is “$A \subset B$.”
        \item \textbf{Proving $A \subseteq B $}
            \bigbreak \noindent 
            \textbf{Definition}. Suppose $A$ and $B$ are sets. If every element in $A$ is also an element of $B$, then $A$ is a subset of $B $, which is denoted $A \subseteq B$
            \bigbreak \noindent 
            \textbf{Note:} For every set $B$, it is true that $\varnothing \subseteq B $. 
            To see it, first note that, because there are no elements in $\varnothing$, it would be true to say  
            "for any $x \in \varnothing$, $x$ is a purple elephant that speaks German.'' It’s vacuously\footnote{A statement is vacuously true if it asserts something about all elements of the empty set.} true!  
            You certainly can’t disprove it, right? You can’t present to me any element in $\varnothing$ that is not a purple elephant that speaks German.
            \bigbreak \noindent 
            By this reasoning, I could switch out "is a purple elephant that speaks German" for any other statement, and it would still be true! And this includes the subset criteria: if $x \in \varnothing$, then $x \in B$, which by definition means that $\varnothing \subseteq B$.  
            Again, you certainly can not present to me any $x \in \varnothing$ which is not also an element of $B$, can you?
            \bigbreak \noindent 
            in order to prove that $A \subseteq B$, what we would have to show is this:
            \begin{align*}
                \text{If } x\in A, \text{ then } x\in B 
            .\end{align*}
            In other words, for any arbitrary element in $A$, that same element is also in $B$
            \bigbreak \noindent 
            \textbf{Proposition.} It is the case that 
            \begin{align*}
                \{n \in \mathbb{Z}:\ 12 \mid n\} \subseteq \{n\in\mathbb{Z}:\ 3\mid n\}
            .\end{align*}
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} Let $A = \{n\in\mathbb{Z}:\ 12\mid n\} $, and $B =\{n\in\mathbb{Z}:\ 3\mid n\} $. Assume $a\in A$
            \penv{
                Since $a\in A$, then $12 \mid a$, which implies $a = 12k$, for some $k\in \mathbb{Z}$. If $a\in B$, then $3\mid a \implies a = 3\ell$
                \bigbreak \noindent 
                Since $a =12k$, and $a=3\ell$, then $12k=3\ell \implies \ell = 4k$. Thus, we have
                \begin{align*}
                    a = 3(4k)
                .\end{align*}
                Which by the definition of divisiblity, and since $4k \in \mathbb{Z}$, we have $3\mid a$. 
                \bigbreak \noindent 
                Therefore, $a \in B \quad \blacksquare$
            }
        \item \textbf{Proving $A = B$}:
            Recall that, for sets $A$ and $B$, to say that ``$A = B$'' is to say that these two sets contain \textit{exactly} the same elements. Said differently, it means these two things:
            \begin{enumerate}
                \item Every element in $A$ is also in $B$ (which means $A \subseteq B$), and
                \item Every element in $B$ is also in $A$ (which means $B \subseteq A$).
            \end{enumerate}
            Indeed, a slick way to prove that $A = B$ is to prove both $A \subseteq B$ and $B \subseteq A$, both of which can be done using the approach discussed above.
        \item \textbf{Review of set operations}:
            \begin{itemize}
                \item The \textit{union} of sets $A$ and $B$ is the set $A \cup B = \{x : x \in A \text{ or } x \in B\}$.
                \item The \textit{intersection} of sets $A$ and $B$ is the set $A \cap B = \{x : x \in A \text{ and } x \in B\}$.
                \item Likewise, if $A_1, A_2, A_3, \dots, A_n$ are all sets, then the union of all of them is the set
                    \[
                        A_1 \cup A_2 \cup \cdots \cup A_n = \{x : x \in A_i \text{ for some } i\}.
                    \]
                    This set is also denoted
                    \[
                        \bigcup_{i=1}^n A_i.
                    \]
                \item Likewise, if $A_1, A_2, A_3, \dots, A_n$ are all sets, then the intersection of all of them is the set
                    \[
                        A_1 \cap A_2 \cap \cdots \cap A_n = \{x : x \in A_i \text{ for all } i\}.
                    \]
                    This set is also denoted
                    \[
                        \bigcap_{i=1}^n A_i.
                    \]
            \end{itemize}
            \bigbreak \noindent 
            Assume $A$ and $B$ are sets and ``$x \notin B$'' means that $x$ is not an element of $B$.
            \begin{itemize}
                \item The \textit{subtraction} of $B$ from $A$ is $A \setminus B = \{x : x \in A \text{ and } x \notin B\}$.
                \item If $A \subseteq U$, then $U$ is called a \textit{universal set} of $A$. The \textit{complement} of $A$ in $U$ is $A^c = U \setminus A$.
            \end{itemize}
            \bigbreak \noindent 
            Furthermore, 
            \begin{itemize}
                \item The \textit{power set} of a set $A$ is $\mathcal{P}(A) = \{X : X \subseteq A\}$.
                \item The \textit{cardinality} of a set $A$ is the number of elements in the set, and it is denoted $|A|$.
            \end{itemize}
            \bigbreak \noindent 
            Assume $A$ and $B$ are sets, The Cartesian product of A and B is 
            \begin{align*}
                A \times B = \{(a, b):\  a \in A and b \in B\}.
            .\end{align*}
            \pagebreak \bigbreak \noindent 
        \item \textbf{More on power sets}: 
            \bigbreak \noindent 
            \textbf{Proposition.} Suppose $A$ and $B $ are sets. If $\mathcal{P}(A)\subseteq \mathcal{P}(B)$, then $A\subseteq B$.
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} Assume $A$ and $B$ are sets, and $\mathcal{P}(A) \subseteq \mathcal{P}(B)$.
            \bigbreak \noindent 
            \penv{
                Choose $x\in \mathcal{P}(A)$, which means $x \subseteq A$. Since $\mathcal{P}(A) \subseteq \mathcal{P}(B)$, it follows that $x \in \mathcal{P}(B)$, which means $x \subseteq B$. Let $x=A$, since $A\in\mathcal{P}(A)$. Since $x \subseteq B$, then $A \subseteq B $
            }
            \bigbreak \noindent 
            Therefore, $A\subseteq B \quad \blacksquare$
        \item \textbf{De Morgan's law}:
            \bigbreak \noindent 
            \textbf{Theorem}. Suppose $A$ and $B$ are subsets of a universal set $U$. Then,
            \begin{align*}
                (A \cup B)^{C} &= A^{C} \cap B^{C} \tag{1}
            .\end{align*}
            And
            \begin{align*}
                (A \cap B)^{C} &= A^{C} \cup B^{C} \tag{2}
            .\end{align*}
            \bigbreak \noindent 
            \textbf{\textit{Proof (1)}}. Assume $A$ and $B$ are subsets of a universal set $U$, since $(A \cup B)^{C}$, and $A^{C} \cap B^{C} $ are sets, we show equality by showing $(A\cup B)^{C} \subseteq A^{C}\cap B^{C}$, and $A^{C} \cap B^{C} \subseteq (A\cup B)^{C}$. It then follows that $(A \cup B)^{C} = A^{C} \cap B^{C} $
            \penv{
                Choose $x\in (A\cup B)^{C} $, by the definition of the complement, we have $x\not\in(A\cup B)$, which by the definition of the union means $x$ cannot be in $A$, and it cannot be in $B$. In other words, $x\not\in A$ and $x\not\in B \implies x \in A^{C}$ and $x\in B^{C} $. Therefore,
                \begin{align*}
                    x\in A^{C} \cap B^{C}
                .\end{align*}
                Which by the definition of the subset, means $(A\cup B)^{C} \subseteq A^{C} \cap B^{C}$
                \bigbreak \noindent
                Next, let $x\in A^{C} \cap B^{C}$, then $x \in A^{C}$ and $x\in B^{C}$, which means $x\not\in A$ and $x\not\in B$, which implies $x\not\in (A\cup B) \implies x\in (A\cup B)^{C}$.
                \bigbreak \noindent 
                Therefore, since $x\in A^{C} \cap B^{C} \implies x\in (A\cup B)^{C}$, by the definition of a subset, we have $A^{C} \cap B^{C} \subseteq (A\cup B)^{C} $
            }
            Since both $(A\cup B)^{C} \subseteq A^{C} \cap B^{C}$, and $A^{C} \cap B^{C} \subseteq (A \cup B)^{C}$, it must be the case that $(A\cup B)^{C} = A^{C} \cap B^{C} \quad \blacksquare$
            \bigbreak \noindent 
            It should be addressed that this proof can be done by simply manipulating the set builder notation. We have
            \begin{align*}
                A^{C} \cap B^{C} &= \{x \in \mathbb{R}:\ x \in A^{C} \text{ and } x\in B^{C}\} \\
                                 &=\{x\in\mathbb{R}:\ x\not\in A \text{ and } x\not\in B\} \\
                                 &=\{x\in\mathbb{R}:\ x\not\in (A\cup B)\}  \\
                                 &=\{x\in\mathbb{R}:\ x\in (A\cup B)^{C}\}
            .\end{align*}
            $\blacksquare$
        \item \textbf{Proving $a\in A$}: Consider the set $\{x\in S:\ P(x)\}$, where $P(x)$ is some condition on $x$
            \bigbreak \noindent 
            Given a set of this form, if you are presented with a specific $a$ and you wish to prove that $a \in A$, then you must show that
            \begin{enumerate}
                \item $a\in S$
                \item $P(a)$ is true
            \end{enumerate}
            \bigbreak \noindent 
            For example, Let $A = \{(x,y) \in \mathbb{Z} \times \mathbb{N}:\ x\equiv y\pmod{5}\} $, then $(17,2) \in A$
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} First, note that $(17,2) \in\mathbb{Z} \times \mathbb{N}$ because $17\in \mathbb{Z}$, and $2 \in \mathbb{N} $, Next, observe that
            \begin{align*}
                17 \equiv 2 \pmod{5}
            .\end{align*}
            Because $5\mid (17-2)$
        \item \textbf{Indexed Families of Sets}: Consider a set $\mathcal{F}$, If every element of $\mathcal{F} $ is itself a set, then $\mathcal{F}$ is called a \textit{family of sets}. Then, one can ask questions about such a family, — like, what is the union of all of the sets in $\mathcal{F}$. That is,
            \begin{align*}
                \bigcup_{S \in \mathcal{F}} S &= \{x:\ x\in S \text{ for some } S \in \mathcal{F}\}
            .\end{align*}
            Likewise, 
            \begin{align*}
                \bigcap_{S\in \mathcal{F}} S &= \{x:\ x\in S \text{ for every } S \in \mathcal{F}\}
            .\end{align*}
        \item \textbf{Bonus example I}.
            \bigbreak \noindent 
            \textbf{Proposition}. It is the case that 
            \begin{align*}
                \{n \in \mathbb{Z}:\ 12\mid n\} = \{n\in\mathbb{Z}:\ 3\mid n\} \cap \{n\in \mathbb{Z}:\ 4\mid n\}
            .\end{align*}
            \textbf{\textit{Proof.}} Let $A = \{n \in \mathbb{Z}:\ 12\mid n\}$, $B =\{n\in\mathbb{Z}:\ 3\mid n\}$, and $C= \{n\in \mathbb{Z}:\ 4\mid n\}$
            \penv{
                \textit{Part i.)} Choose $x\in A$, we then have $ 12\mid x$, and $x = 12k$, for some $k\in \mathbb{Z}$. Thus,
                \begin{align*}
                    x = 12k = 3(4k) = 4(3k)
                .\end{align*}
                Which by the definition of divisibility implies both $3\mid x $ and $4\mid x$, since both $4k$ and $3k \in \mathbb{Z}$. Hence, $x\in B \cap C$
                \bigbreak \noindent 
                \textit{Part ii.)} Choose $x\in B\cap C$, then both $x = 3r$ and $x=4s$, for $r,s\in\mathbb{Z} $. We have
                \begin{align*}
                    3r = 4s
                .\end{align*}
                Which implies $3\mid 4s$, since $r \in \mathbb{Z}$. Because $3\in \mathbb{P}$, we know that either $3\mid 4$ or $3\mid s$. Since it is clear that $3\nmid 4$, it must be the case that $3\mid s$, and thus $s = 3\ell$ for an integer $\ell$. It then follows that
                \begin{align*}
                    x=4s = 4(3\ell) = 12\ell
                .\end{align*}
                Which by the definition of divisibility implies $12\mid x$, and thus $x\in A$
            }
            Since choosing an $x\in A \implies x\in B\cap C$, it must be that $A\subseteq B\cap C$, and choosing an $x\in B\cap C \implies x\in A$, it must also be that $B\cap C \subseteq A$. With these two facts, we can assert that $A = B \cap C \quad \blacksquare$
        \item \textbf{The Cardinality of the Power Set}: Suppose $A$ is a set with $n$ elements. How many subsets of $A$ are there? Said differently, what is $\abs{P(A)}$?
            \bigbreak \noindent 
            We could check the first few cases by hand
            \begin{center}
                \begin{tabular}{c|c|c}
                    $A$& $\abs{A} = n$ & $\abs{\mathcal{P}(A)}$ \\
                    \hline
                    $\{1\}$ & $1$ & 2 \\
                    $\{1,2\}$ & 2 & 4 \\
                    $\{1,2,3\}$ & 3 & 8 \\
                    $\{1,2,3,4\}$ & 4 & 16
                \end{tabular}
            \end{center}
            \bigbreak \noindent 
            It sure looks like if $|A| = n$, then $|P(A)| = 2^n$.  Why would this be true? There is actually a pretty slick way to see it. Every subset  of $\{1, 2, 3\}$ can be thought of by asking whether or not each element is included in the  subset. For example, $\{1, 3\}$ can be thought of as $\langle \text{yes, no, yes} \rangle$, since 1 was included,  2 was not, and 3 was.
            \bigbreak \noindent 
            Suppose you’re trying to generate a subset of $\{1, 2, 3\}$. You could think about  doing so by asking three yes/no questions, the answers to which uniquely determine  your set. With 2 options for the first element, 2 for the second, and 2 for the third, in  total there are $2 \times 2 \times 2 = 8$ ways to answer the three questions, and hence 8 subsets!
            \bigbreak \noindent 
            With $n$ straight yes/no questions, there are $2 \times 2 \times \cdots \times 2 = 2^n$ ways to answer  the questions, each corresponding uniquely to a subset of $A$. Thus, if $|A| = n$, then  $|P(A)| = 2^n$.
        \item \textbf{A consequence of the above fact}:
            \bigbreak \noindent 
            \textbf{Proposition}. Given any $A \subseteq \{1, 2, 3, \ldots, 100\}$ for which $|A| = 10$, there  exist two different subsets $X \subseteq A$ and $Y \subseteq A$ for which the sum of the elements  in $X$ is equal to the sum of the elements in $Y$.
            \bigbreak \noindent 
            For example, consider the set $\{6, 23, 30, 39, 44, 46, 62, 73, 90, 91\}$, If we let 
            \begin{align*}
                X = \{6, 23, 46, 73, 90\} \text{ and } Y = \{30, 44, 73, 91\}
            .\end{align*}
            then the elements in both sets sum to $238$:
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} We prove this fact using the pigeonhole principal. Consider the smallest and largest possible subset sums. If $A = \varnothing \subseteq \{1,2,3,...,100\} $, then the sum is $0$. If $A = \{91,92,93,94,95,96,97,98,99,100\} $, then the subset sum is $955$. Thus, there are no more than $956$ possible subset sums for the set $A \subseteq \{1,2,3,...,100\} $, for which $\abs{A} = 10$.
            \bigbreak \noindent 
            Consider $956$ boxes, each representing a unique subset sum. Since we have $2^{\abs{A}} = 2^{10} = 1024$ subsets and only $956$ boxes to place each subset in, there must be a box containing two subsets $A$, which means they must have the same sum $\quad \blacksquare$.
        \item \textbf{The symmetric difference of sets}. The \textit{symmetric difference} of two sets $A$ and $B$, denoted $A \Delta B $, or $A \ominus B$, is the set which contains the elements which are either in set $A$ or in set $B$ but not in both 

    \end{itemize}

    \pagebreak 
    \subsection{Induction}
    \begin{itemize}
        \item \textbf{Dominoes}: Consider a line of dominoes, perfectly arranged, just waiting to be knocked over. Dominoes stacked up like this have the following properties:
            \begin{enumerate}
                \item If you give the first domino a push, it will fall (in particular, it will fall into the second domino, knocking it over).
                \item Moreover, every domino, when it’s knocked over, falls into the next one and knocks it over.
            \end{enumerate}
            Given these two properties, it must be the case that if you knock over the first domino, then every domino will eventually fall. The first premise gets the process going, as it implies that the first domino will fall. And then the second premise keeps it going: Applying the second premise means that the falling first domino will cause the second domino to fall. Applying the second premise again means that the second falling domino will cause the third domino to fall. Applying the second premise again means that the third falling domino will cause the fourth domino to fall. And so on.
        \item \textbf{Sum of the first $n$  odd numbers}: Take a look at the following
            \begin{align*}
                1 = 1 &= 1^{2} \\
                1 + 3 = 4 &= 2^{2}\\
                1 + 3 + 5 = 9 &= 3^{2}\\
                1 + 3 + 5 + 7 = 16 &= 4^{2}\\
                1 + 3 + 5 + 7 + 9 = 25 &= 5^{2}\\
                1 + 3 + 5 + 7 + 9 + 11 = 36 &= 6^{2}\\
                1 + 3 + 5 + 7 + 9 + 11 + 13 = 49 &= 7^{2}
            .\end{align*}
            \bigbreak \noindent 
            It sure looks like the sum of the first $n$ odd numbers is $n^{2}$. But how can we prove that it’s true for every one of the infinitely many $n$? The trick is to use the domino idea. Imagine one domino for each of the above statements.
            \bigbreak \noindent 
            \fig{.7}{./figures/7.png}
            \bigbreak \noindent 
            Suppose we do the following:
            \begin{itemize}
                \item Show that the first domino is true (this is trivial, since obviously $1=1^{2} $).
                \item Show that any domino, if true, implies that the following domino is true too
            \end{itemize}
            Given these two, we may conclude that all the dominoes are true. It’s exactly the same as noting that all the dominoes from earlier will fall. This is a slick way to prove infinitely many statements all at once, and it is called the \textit{principal of mathematical induction}, or, when among friends, it is simply called \textit{induction}.
        \item \textbf{Induction}: Consider a sequence of mathematical statements, $S_{1}, S_{2}, S_{3}, . . . .$
            \begin{itemize}
                \item Suppose $S_{1}$ is true, and
                \item Suppose, for each $k \in \mathbb{N}$, if $S_{k}$ is true then $S_{k+1}$ is true.
            \end{itemize}
            Then, $S_{n} $ is true for every $n\in \mathbb{N}$.
        \item \textbf{Induction framework}:
            \bigbreak \noindent 
            \textbf{Proposition}. $S_{1}, S_{2}, S_{3},... $ are all true
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} \textit{General setup or assumptions if needed}
            \penv{
                \textit{Base case.} $\left\langle\left\langle \text{Demonstration that $S_{1}$ is true} \right\rangle\right\rangle$
                \bigbreak \noindent 
                \textit{Inductive hypothesis}. Assume that $S_{k}$ is true
                \bigbreak \noindent 
                \textit{Induction step}. $\left\langle \left\langle \text{Proof that $S_{k}$ implies $S_{k+1} $} \right\rangle \right\rangle $
            }
            \textit{Conclusion}. Therefore, by induction, all the $S_{n}$ are true. \hspace{5cm} $\blacksquare $
        \item \textbf{Induction example 1}: Let’s simply sum the first $n$ natural numbers: $1 + 2 + 3 + 4 + · · · + n$. These sums are called the triangular numbers since they can be pictured as the number of balls in the following triangles.
            \bigbreak \noindent 
            \fig{.6}{./figures/8.png}
            \bigbreak \noindent 
            \textbf{Proposition.} For any $n\in \mathbb{N}$, $\sum_{i=1}^{n} i = 1 + 2 + 3 + \ldots + n = \frac{n(n+1)}{2} $
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} We proceed by induction
            \penv{
                \underline{Base case:} The base case is when $n=1$, and
                \begin{align*}
                    1 = \frac{1(1+1)}{2} = 1
                .\end{align*}
                \bigbreak \noindent 
                \underline{Inductive hypothesis}: Let $k\in \mathbb{N}$, assume 
                \begin{align*}
                    1 + 2 + 3 + ... + k = \frac{k(k+1)}{2}
                .\end{align*}
                \bigbreak \noindent 
                \underline{Inductive step}: We aim to show that the result holds for $k+1$. Thus,
                \begin{align*}
                    1+ 2 + 3 + ... + k+k+1 = \frac{(k+1)((k+1)+1)}{2} 
                .\end{align*}
                We have
                \begin{align*}
                    1 + 2 + 3 + ... + k + k+1 &= \frac{(k+1)(k+2)}{2}  \\
                    \implies \frac{k(k+1)}{2} + k+1 &= \frac{(k+1)(k+2)}{2} \\
                    \implies \frac{k^{2} + k + 2k + 1}{2} &= \frac{k^{2} + 2k + k + 2}{2}
                .\end{align*}
            }
            Therefore, by induction, $1+2+3+...+n = \frac{n(n+1)}{2} $ for all $n\in \mathbb{N} \quad \blacksquare$
        \item \textbf{Induction example 2}: 
            \bigbreak \noindent 
            \textbf{Proposition}. Let $S_{n}$ be the sum of the first $n$ natural numbers. Then, for any $n \in \mathbb{N}$,
            \begin{align*}
                S_{n} + S_{n+1} = (n+1)^{2}
            .\end{align*}
            \bigbreak \noindent 
            We will prove this proposition twice. The first proof is a direct proof, the second will be by induction.
            \bigbreak \noindent 
            \textbf{\textit{Direct proof.}} We have
            \begin{align*}
                S_{n} + S_{n+1} &= \frac{n(n+1)}{2} + \frac{(n+1)((n+1)+1)}{2} \\
                                &= \frac{n^{2} + n}{2} + \frac{n^{2} + 2n + n + 2}{2} \\
                                &= \frac{n^{2} + n + n^{2} + 3n + 2}{2} \\
                                &= \frac{2n^{2} + 4n + 2}{2} \\
                                &= \frac{2(n^{2} + 2n + 1)}{2} \\
                                &= n^{2} + 2n + 1 \\
                                &= (n+1)^{2} \quad \blacksquare
            .\end{align*}
            \pagebreak \bigbreak \noindent 
            \textbf{\textit{Proof by induction}}. We proceed by induction
            \penv{
                \underline{Base case}: The base case is when $n=1$, and 
                \begin{align*}
                    S_{1} + S_{2} = 1 + 3 = 4 = (1+1)^{2}
                .\end{align*}
                as desired
                \bigbreak \noindent 
                \underline{Inductive hypothesis}. Let $k\in\mathbb{N}$, and assume that 
                \begin{align*}
                    S_{k} + S_{k+1} = ( k+1)^{2}
                .\end{align*}
                \bigbreak \noindent 
                \underline{Inductive step}. We aim to prove that the result holds for $k+1$. That is, 
                \begin{align*}
                    S_{k+1} + S_{k+2} = (k+2)^{2}
                .\end{align*}
                For this, we use the fact that $S_{k+1} $ is the sum of the first $k+1$ natural numbers, thus we can write it as $S_{k} + (k+1)$. Likewise, $S_{k+2} = S_{k+1} + (k+2)$. Thus,
                \begin{align*}
                    S_{k+1} + S_{k+2} &= S_{k} + (k+1) + S_{k+1} + (k+2) \\
                                      &= S_{k} + S_{k+1} + 2k+3 \\
                                      &=(k+1)^{2} + 2k + 3\\
                                      &= k^{2} + 2k + 1 + 2k + 3 \\
                                      &= k^{2} + 4k + 4 \\
                                      &= (k+2)^{2}
                .\end{align*}
            }
            \underline{Conclusion.} Therefore, by induction, the proposition holds for all $n\in \mathbb{N} \quad \blacksquare$
        \item \textbf{A quick note about induction}: For some proof techniques, adding a sentence at the end of your proof is nice but not required. For induction, though, it really is required. You can prove that the first domino will fall, and you can prove that each domino — if fallen— will knock over the next domino, but why does this mean they all fall? Because induction says so! Until you say “by induction. . . ” your work will not officially prove the result
        \item \textbf{Induction example 3.}
            \bigbreak \noindent 
            \textbf{Proposition.} For every $n \in N$, the product of the first $n$ odd natural numbers equals $\frac{(2n)!}{2^{n}n!} $. That is, 
            \begin{align*}
                1 \cdot  3 \cdot 5 \cdot ... \cdot (2n-1) = \frac{(2n)!}{2^{n}n!}
            .\end{align*}
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} We proceed by induction.
            \bigbreak \noindent 
            \underline{Base case:} The base case occurs when $n=1$, 
            \begin{align*}
                1 = \frac{(2(1))!}{2^{1}1!} = 1
            .\end{align*}
            As desired
            \bigbreak \noindent 
            \underline{Inductive hypothesis}. Let $k\in \mathbb{N}$, assume
            \begin{align*}
                1 \cdot 3 \cdot 5 \cdot ... \cdot (2k-1) = \frac{(2k)!}{2^{k}k!}
            .\end{align*}
            \bigbreak \noindent 
            \underline{Inductive step}. We aim to prove that the result holds for $k+1$. Thus, we wish to show
            \begin{align*}
                1 \cdot 3 \cdot 5 \cdot ... \cdot (2k-1) \cdot (2(k+1)-1) &= \frac{(2(k+1))!}{2^{k+1}(k+1)!}\\
                                                                          &=\frac{(2k+2)!}{2^{k+1}(k+1)!}
            .\end{align*}
            By the inductive hypothesis, we have
            \begin{align*}
                1 \cdot 3 \cdot 5 \cdot ... \cdot (2k-1) \cdot (2k+1) &= \frac{(2k)!}{2^{k}k!}(2k+1) \\
                                                                      &= \frac{(2k)!(2k+1)}{2^{k}k!} \\
                                                                      &= \frac{(2k+1)!}{2^{k}k!} \\
                                                                      &=\frac{(2k+1)!}{2^{k}k!} \cdot \frac{(2k+2)}{(2k+2)} \\
                                                                      &= \frac{(2k+2)!}{2^{k}k!(2k+2)} \\
                                                                      &= \frac{(2k+2)!}{2^{k}k! \cdot 2(k+1)} \\
                                                                      &= \frac{(2k+2)!}{2^{k+1}(k+1)!}
            .\end{align*}
            Therefore, by induction, the proposition holds for all $n\in \mathbb{N} \quad \blacksquare$
        \item \textbf{Induction example 4}. 
            \bigbreak \noindent 
            \textbf{Proposition.} For every $n \in \mathbb{N}$, if any one square is removed from a $2^n \times 2^n$ chessboard, the result can be perfectly covered with $\text{L}$-shaped tiles.
            \bigbreak \noindent 
            The tiles cover three squares and look like this:
            \bigbreak \noindent 
            \fig{1}{./figures/9.png}
            \bigbreak \noindent 
            Since the proposition refers to something being true “for every \( n \in \mathbb{N} \),” that’s a pretty good indication that induction is the way to proceed. The base case (when \( n = 1 \)) will be fine. For the inductive hypothesis, we will be assuming that any \( 2^k \times 2^k \) board, with one square removed, can be perfectly covered by L-shaped tiles.
            \bigbreak \noindent 
            In the induction step we are going to consider a $2^{k+1} \times 2^{k+1}$ board — a board that is twice as big in each dimension— with one square missing.
            \pagebreak \bigbreak \noindent 
            \textbf{\textit{Proof.}} We proceed by induction
            \bigbreak \noindent 
            \penv{
                \underline{Base Case}. The base case is when $n = 1$, and among the four possible squares that one can remove from a $2 \times 2$ chessboard, each leaves a chessboard which can be perfectly covered by a single $L$-shaped tile:
                \bigbreak \noindent 
                \fig{.9}{./figures/10.png}
                \bigbreak \noindent 
                \underline{Inductive Hypothesis}. Let $k \in \mathbf{N}$, and assume that if any one square is removed from a $2^{k} \times 2^{k}$ chessboard, the result can be perfectly covered with $L$–shaped tiles.
                \bigbreak \noindent 
                \underline{Induction Step.} Consider a $2^{k+1} \times 2^{k+1}$ chessboard with any one square removed.  Cut this chessboard in half vertically and horizontally to form four $2^k \times 2^k$ chessboards.  One of these four will have a square removed, and hence, by the induction hypothesis, can be perfectly covered.
                \bigbreak \noindent 
                Next, place a single $L$-shaped tile so that it covers one square from each of the other three $2^{k} × 2^{k}$ chessboards, as shown in the picture below.
                \bigbreak \noindent 
                \fig{.7}{./figures/11.png}
                \bigbreak \noindent 
                Each of these other three $2^k \times 2^k$ chessboards can be perfectly covered by the  inductive hypothesis, and hence the entire $2^{k+1} \times 2^{k+1}$ chessboard can be perfectly covered.
                \bigbreak \noindent 
            }
            \textbf{Conclusion.} By induction, for every $n \in \mathbb{N}$, if any one square is removed from a  $2^n \times 2^n$ chessboard, the result can be perfectly covered with L-shaped tiles.
        \item \textbf{Another note about induction}: So far, in all of our examples we proved that a statement holds from all $n \in \mathbb{N}$.  
            The base case was $n = 1$ and in the inductive hypothesis we assumed that the result holds for some $k \in \mathbb{N}$.  
            \bigbreak \noindent 
            There are times where one instead wants to prove that a statement holds for only the natural numbers past some point.  
            For example, it is possible to prove the $p$-test by induction, a result that you might remember from your calculus class:
            \[
                \sum_{i=1}^\infty \frac{1}{i^n} \text{ converges for all integers } n \geq 2.
            \]
            To prove this result, the base case would be $n = 2$ and in the inductive hypothesis we would assume that the result holds for some $k \in \{2, 3, 4, 5, \ldots\}$.  
            \bigbreak \noindent 
            At other times, you may want to prove that a result holds for more than just the natural numbers.  
            For example, a result from combinatorics is that
            \[
                \sum_{i=1}^n \binom{n}{i} = 2^n \text{ holds for all integers } n \geq 0.
            \]
            Here, the base case is $n = 0$, and the inductive hypothesis is the assumption that this holds for some $k \in \{0, 1, 2, 3, \ldots\}$.
        \item \textbf{Strong induction idea}: The idea behind strong induction is that at the point when the 100th domino is the  next to get knocked down, you know for sure that all of the first 99 dominoes have  fallen, not just the 99th. Likewise, when you are proving some sequence of statements  $S_1, S_2, S_3, S_4, \ldots$, instead of just assuming that $S_k$ is true in order to prove $S_{k+1}$,  why not just assume that $S_1, S_2, \ldots, S_k$ are all true in order to prove $S_{k+1}$ — because  by the time you are proving $S_{k+1}$, you have shown them all to be true!
        \item \textbf{Strong induction}: Consider a sequence of mathematical statements, $S_{1}, S_{2}, S_{3}, ...$
            \begin{itemize}
                \item Suppose $S_1$ is true, and  
                \item Suppose, for any $k \in \mathbb{N}$, if $S_1, S_2, \ldots, S_k$ are all true, then $S_{k+1}$ is true.
            \end{itemize}
            Then $S_n$ is true for every $n \in \mathbb{N}$.
            \bigbreak \noindent 
            \textbf{Note:} In regular induction, you essentially use $S_1$ to prove $S_2$, and then $S_2$ to prove $S_3$,  and then $S_3$ to prove $S_4$, and so on. With strong induction, you use $S_1$ to prove $S_2$,  and then $S_1$ and $S_2$ to prove $S_3$, and then $S_1, S_2$, and $S_3$ to prove $S_4$, and so on.
        \item \textbf{Fundemental theorem of arithmetic}: If $n$ is an integer and $n \geq 2$,  then $n$ is either prime or composite. An integer $p$ is prime if $p \geq 2$ and its only  positive divisors are $1$ and $p$. A positive integer $n \geq 2$ that is not prime is called  composite, and is therefore one that can be written as $n = st$, where $s$ and $t$ are  integers smaller than $n$ but larger than $1$. And with that, it is time for a really big  and important result.
            \bigbreak \noindent 
            \textbf{Theorem 4.8 (Fundamental Theorem of Arithmetic).}  Every integer $n \geq 2$ is either prime or a product of primes.
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} We proceed by strong induction
            \pagebreak \bigbreak \noindent 
            \penv{
                \underline{Base case.} The base case occurs when $n=2$. Observe that $2\in \mathbb{P} $
                \bigbreak \noindent 
                \underline{Inductive hypothesis}. Let $k\in \mathbb{N}$ such that $k \geq 2$. Assume that the integers $2,3,4,...,k$ are either prime or a product of primes.
                \bigbreak \noindent 
                \underline{Induction step}. Next, we consider $k+1$. We aim to show that $k+1$ is either prime or a product of primes. Since $k+1$ is larger than one, it is either prime or composite. Consider these two cases separately. Case 1 is that $k+1$ is prime. In this case, our goal is achieved.
                \bigbreak \noindent 
                Case 2 is that $k+1$ is composite; that is, $k+1$ has positive factors other than one and itself. Say, $k+1 = st$, where $s,t$ are positive integers greater than zero, and  
                \begin{align*}
                    1 < s < k+1 \quad 1<t<k+1
                .\end{align*}
                By the inductive hypothesis, both $s$ and $t$ can be written as a product of primes, say
                \begin{align*}
                    s &= p_{1} \cdot p_{2} \cdot ... \cdot p_{m} \\
                    t &= q_{1} \cdot q_{2} \cdot  ... \cdot q_{\ell}
                .\end{align*}
                Where each $p_{i}, q_{j} \in \mathbb{P}$, then 
                \begin{align*}
                    k + 1 = st = (p_{1} \cdot p_{2} \cdot ... \cdot p_{m})(q_{1} \cdot q_{2} \cdot  ... \cdot q_{\ell})
                .\end{align*}
                \bigbreak \noindent 
                Is written as a product of primes
                \bigbreak \noindent 
                Note that if $s$ or $t$ where prime, then $m$ or $\ell$ would be one. Say $s$ was prime, then $s = p_{1} $
                \bigbreak \noindent 
                \textbf{Conclusion}. By strong induction, every positive integer larger than 2 can be written as a product of primes.
            }
        \item \textbf{Chocolate bar example}:
            \bigbreak \noindent 
            \textbf{Proposition.} Suppose you have a chocolate bar that is an $m \times n$ grid of squares. The entire bar, or any smaller rectangular piece of that bar, can be broken along the vertical or horizontal lines separating the squares. 
             
            The number of breaks to break up that chocolate bar into individual squares is precisely $mn - 1$.
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} We proceed by strong induction
            \penv{
                \underline{Base case}: The base case occurs when $n=1$, which is an $1\times 1$ chocolate bar. Since the number of breaks needed to break the bar into individual squares is clearly zero, we have
                \begin{align*}
                    0 = 1(1) -1 = 0
                .\end{align*}
                As desired
                \bigbreak \noindent 
                \underline{Inductive hypothesis}: Let $k\in\mathbb{N}$, assume that all bars with at most $k$ squares satisfy the proposition.
                \bigbreak \noindent 
                \underline{Induction step}: Consider now any bar with $k+1$ squares, suppose this bar has dimensions $m\times n $. Consider an arbitrary first break, and suppose the two smaller bars have $a$ squares and $b$ squares, respectively. Note that we must have $a + b = mn$, because the number of squares in the smaller bars must add up to the number of squares in the original $m \times n$ bar.
                \bigbreak \noindent 
                By the inductive hypothesis, the bar with $a$ squares will require $a − 1$ breaks to completely break it up, and the bar with $b$ breaks will require $b−1$ breaks. Therefore, to break up the $m \times n$ bar, we must make a first break, followed by $(a − 1) + (b − 1)$ additional breaks. The total number of breaks is then
                \begin{align*}
                    1 + (a-1) + (b-1) &= a+b-1 \\
                                      &=mn - 1
                .\end{align*}
                And $mn − 1$ is indeed one less than the number of squares in the $m \times n$ bar.
                \bigbreak \noindent 
            }
            \underline{Conclusion}. By strong induction, a chocolate bar of any size requires one break less than its number of squares to break it up into individual squares $\quad \blacksquare $
            \bigbreak \noindent 
            \textbf{Note:} What if the pieces were in the shape of a triangle? If it had $T$ squares would it still require $T - 1$ breaks?
            \bigbreak \noindent 
            What about other shapes? What if there are pieces missing in the middle? Interestingly, the answer is $T - 1$ no matter the bar’s shape, and even if pieces are missing! As long as each of your “breaks” divides one chunk into two, that’s the answer.
            \bigbreak \noindent 
            Here is some intuition for that: No matter the shape, the bar starts out as a  
            single “chunk” of chocolate, and after your sequence of breaks the bar is broken into  
            $T$ chunks of chocolate — the $T$ individual squares. How many breaks does it take to  
            move from 1 chunk to $T$ chunks? Notice that every break increases the number of  
            chunks by 1. So after 1 break, there will be 2 chunks. After 2 breaks, there will be 3  
            chunks. And so on. Thus, after $T - 1$ breaks there will be $T$ chunks, which is why  
            $T - 1$ breaks is guaranteed to be the answer, no matter which shape you started with.

        \item \textbf{Multiple base cases}: When proving the $(k + 1)$st case within the induction step, strong induction allows  you to apply not just the $k$th step, but any of the steps $1, 2, 3, \ldots, k$. In the previous  two examples, you had no idea which earlier steps you will need, so it was vital that  you assumed them all. At times, though, you really only need, say, the previous two  steps. The $k$th step is perhaps not enough, but the $(k - 1)$st step and the $k$th step is  guaranteed to be enough.
            \bigbreak \noindent 
            If you rely on the two previous steps, then that is analogous to saying that it takes the previous two dominoes to knock over the next one. Thus, if you knock over dominoes 1 and 2, then they will collectively knock over the third. Then, since the second and third have fallen, those two will collectively knock over the fourth. Then the third and fourth will knock over the fifth. And so on. Thus, the induction relies on two base cases, because without knocking over the first two the third won’t fall and the process won’t begin
            \bigbreak \noindent 
            \textbf{Example:} 
            \bigbreak \noindent 
            \textbf{Proposition.} Every $n \in N$ with $n \geq 11$ can be written as $2a + 5b$ for some natural numbers $a$ and $b$.
            \bigbreak \noindent 
            \textbf{Base Cases.} In the induction step, we will need two cases prior, so we show two base  
            cases here: $n = 11$ and $n = 12$. Both of these can be written as asserted:
            \[
                11 = 2 \cdot 3 + 5 \cdot 1 \\
                12 = 2 \cdot 1 + 5 \cdot 2.
            \]
            \textbf{Inductive Hypothesis.} Assume that for some integer $k \geq 12$, the results hold for  
            \[
                n = 11, 12, 13, \ldots, k.
            \]
            \textbf{Induction Step.} We aim to prove the result for $k + 1$. By the inductive hypothesis,  
            \[
                k - 1 = 2a + 5b
            \]
            for some $a, b \in \mathbb{N}$. Adding 2 to both sides,
            \[
                k + 1 = 2(a + 1) + 5b.
            \]
            Observe that $(a + 1) \in \mathbb{N}$ and $b \in \mathbb{N}$, proving that this is indeed a representation of  
            $(k + 1)$ in the desired form.
            \bigbreak \noindent 
            \textbf{Conclusion.} Therefore, by strong induction, every integer $n \geq 11$ can be written as  
            the proposition asserts. \(\blacksquare\)
            \pagebreak 
        \item \textbf{False proofs with induction}: 
            \bigbreak \noindent 
            \textbf{Proposition}. Everyone on Earth has the same name
            \bigbreak \noindent 
            \textit{Fake Proof.} We will consider groups of $n$ people at a time, and by induction we will  
            ``prove'' that for every $n \in \mathbb{N}$, every group of $n$ people must have everyone with the  
            same name.
            \penv{
                \textbf{Base Case.} If $n = 1$, then of course everyone in the group has the same name, since  
                there’s only one person in the group!
                \bigbreak \noindent 
                \textbf{Inductive Hypothesis.} Let $k \in \mathbb{N}$, and assume that any group of $k$ people all have  
                the same name.
                \bigbreak \noindent 
                \textbf{Induction Step.} Consider a group of $k + 1$ people.
                \bigbreak \noindent 
                \fig{.7}{./figures/12.png}
                \bigbreak \noindent 
                But notice that we can look at the first $k$ of these people and then the last $k$ of these people, and to each of these groups we can apply the inductive hypothesis:
                \bigbreak \noindent 
                \fig{.7}{./figures/13.png}
                \bigbreak \noindent 
                And the only way that this can all happen, is if all $k + 1$ people have the same name.
            }
            Conclusion. This “proves” by induction that for every $n \in N$, every group of $n$ people must have the same name. So if you let $n$ be equal to the number of people on Earth, this “proves” that everyone has the same name.
            \bigbreak \noindent 
            For $k+1$ people, the proof assumes that you can take the first $k$ people and the last  
            $k$ people, and both of these subsets must have the same name because the induction  
            hypothesis applies to them individually.  
            \bigbreak \noindent 
            However, this reasoning fails when $k+1 = 2$. For $k+1 = 2$, the first subset has one  
            person, and the second subset also has one person. These subsets do not overlap, so  
            there is no logical connection ensuring that these two people share the same name.
            \bigbreak \noindent 
            The induction relies on overlapping subsets of $k$ people to conclude that all $k+1$ people  must have the same name. However, this overlap only works if $k+1 > 2$, meaning the proof  doesn't actually establish the result for $k+1 = 2$, which breaks the induction chain.  Without the foundation for $n = 2$, the argument fails for all larger $n$.
        \item \textbf{Induction bonus example 1}.
            \bigbreak \noindent 
            \textbf{Lemma 4.13}. For every $n\in \mathbb{N}_{0} $,
            \begin{align*}
                1 + 2 + 4 + 8 + ... + 2^{n} = 2^{n+1}-1
            .\end{align*}
            For example, 
            \begin{align*}
                1 &= 2^{1}-1 \\
                1 + 2 &= 2^{2}-1 \\
                1 + 2 + 4 &= 2^{3}-1 \\
                1 + 2 + 4 + 8 &= 2^{4}-1
            .\end{align*}
            \penv{
                \underline{Base case}. The base case occurs when $n=1$, we have
                \begin{align*}
                    1 = 2^{1}-1 = 1
                .\end{align*}
                As desired
                \bigbreak \noindent 
                \underline{Inductive hypothesis}. Let $k\in \mathbb{N}_{0}$, assume that
                \begin{align*}
                    1 + 2 + 4 +... + 2^{k} = 2^{k+1}-1
                .\end{align*}
                \bigbreak \noindent 
                \underline{Induction step}. We wish to show that the result holds for $k+1$. That is, 
                \begin{align*}
                    1 + 2 + 4+ ... + 2^{k} + 2^{k+1} = 2^{(k+1)+1}-1 = 2^{k+2} -1
                .\end{align*}
                By the inductive hypothesis, we have
                \begin{align*}
                    1 + 2 + 4 + ... + 2^{k} + 2^{k+1} &= 2^{k+1}-1 + 2^{k+1} \\
                                                      &=2(2^{k+1})-1 \\
                                                      &=2^{k+2} -1
                .\end{align*}
                As desired

            }
            \bigbreak \noindent 
            Therefore, by induction, the proposition holds for all $n\in \mathbb{N}_{0} $
            \bigbreak \noindent 
        \item \textbf{Induction bonus example 2}. \textbf{Proof.} We proceed by strong induction.
            \textbf{Base Case.} Our base case is when \( n = 1 \). Note that 1 can be written as \( 2^0 \), and this is the only way to write 1 as a sum of distinct powers of 2, because all other powers of 2 are larger than 1.
            \bigbreak \noindent 
            \textbf{Inductive Hypothesis.} Let \( k \in \mathbb{N} \), and assume that each of the integers \( 1, 2, 3, \dots, k \) can be expressed as a sum of distinct powers of 2 in precisely one way.
            \bigbreak \noindent 
            \textbf{Induction Step.} We now aim to show that \( k+1 \) can be expressed as a sum of distinct powers of 2 in precisely one way.
            \bigbreak \noindent 
            Let \( 2^m \) be the largest power of 2 such that \( 2^m \leq k+1 \). We now consider two cases: the first is if \( 2^m = k+1 \), and the second is if \( 2^m < k+1 \).
            \bigbreak \noindent 
            \textbf{Case 1:} \( 2^m = k+1 \). If this occurs, then \( 2^m \) itself is a way to express \( k+1 \) as a (one-term) sum of distinct powers of 2. Moreover, there is no other way to express \( k+1 \) as a sum of distinct powers of 2, because by Lemma 4.13 all smaller powers of 2 sum to \( 2^m - 1 = k \). Thus, even by including all smaller powers of 2, we are unable to reach \( k+1 \). So, in Case 1, there is precisely one such expression for \( k+1 \).
            \bigbreak \noindent 
            \textbf{Case 2:} \( 2^m < k+1 \). In order to apply the inductive hypothesis, we will consider \( (k+1) - 2^m \). First, note that \( (k+1) - 2^m \) is less than \( 2^m \), because otherwise \( k+1 \) would have two copies of \( 2^m \) within it, implying that \( 2^m + 2^m \leq k+1 \). However, since \( 2^m + 2^m = 2 \cdot 2^m = 2^{m+1} \), this would mean \( 2^{m+1} \leq k+1 \). This can't be, since \( 2^m \) was chosen to be the largest power of 2 that is at most \( k+1 \). Thus, it must be the case that \( (k+1) - 2^m < 2^m \).
            \bigbreak \noindent 
            Next, by the inductive hypothesis, \( (k+1) - 2^m \) can be expressed as a sum of distinct powers of 2 in precisely one way, and since \( (k+1) - 2^m < 2^m \), this unique expression for \( (k+1) - 2^m \) will not contain a \( 2^m \). Thus, by adding a \( 2^m \) to it, we obtain an expression for \( k+1 \) as a sum of powers of 2. And this expression is unique because \( (k+1) - 2^m \) is unique according to the inductive hypothesis, and the \( 2^m \) portion is unique because, again by Lemma 4.13, even if you summed all of the smaller powers of 2, you will not reach \( 2^m \).
            \bigbreak \noindent 
            \textbf{Conclusion.} By strong induction, every \( n \in \mathbb{N} \) can be expressed as a sum of distinct powers of 2 in precisely one way. \(\Box\)
        \item \textbf{Induction bonus example 3.}
            \bigbreak \noindent 
            \textbf{Theorem 4.15 (\textit{The binomial theorem})}. For $x,y \in \mathbb{R}$, and $n\in \mathbb{N}_{0} $
            \begin{align*}
                (x+y)^{n} = \sum_{m=0}^{n}\binom{n}{m} x^{n-m}y^{m}
            .\end{align*}
            Here, when $n \geq m$, the binomial coefficient $\binom{n}{m}$ is defined to be 
            \[
                \binom{n}{m} = \frac{n!}{m!(n-m)!},
            \]
            which one can show is always an integer. The binomial coefficients can also be defined combinatorially: $\binom{n}{m}$ is equal to the number of ways to choose $m$ elements from an $n$-element set; in fact, $\binom{n}{m}$ is read "n choose m." For example, 
            \[
                \binom{4}{2} = 6
            \]
            because there are six subsets of the set $\{1, 2, 3, 4\}$ containing two elements:
            \[
                \{1, 2\}, \{1, 3\}, \{1, 4\}, \{2, 3\}, \{2, 4\}, \{3, 4\}.
            \]
            Binomial coefficients can be computed iteratively using \textit{Pascal's rule}, which says that
            \[
                \binom{n}{r} = \binom{n-1}{r-1} + \binom{n-1}{r},
            \]
            as well as the fact that
            \[
                \binom{n}{0} = 1 \quad \text{and} \quad \binom{n}{n} = 1 \quad \text{for all } n \in \mathbb{N}_0.
            \]
            A beautiful way to combine these facts is called \textit{Pascal's triangle}:
            \bigbreak \noindent 
            \fig{.6}{./figures/15.png}
            \bigbreak \noindent 
            Indeed, we can even prove the binomial theorem by induction, by making use of Pascal’s rule. Here is a sketch of that proof:
            \bigbreak \noindent 
            \textbf{\textit{Proof sketch}}. The base case is when $n = 0$, and indeed $(x + y)^0 = 1$. The next couple cases are more interesting, and you can check that $(x + y)^1 = x + y$ and $(x + y)^2 = x^2 + 2xy + y^2$ do indeed match the theorem. The inductive hypothesis will be
            \[
                (x + y)^k = x^k + \binom{k}{1}x^{k-1}y + \binom{k}{2}x^{k-2}y^2 + \cdots + \binom{k}{k-1}xy^{k-1} + y^k.
            \]
            For the induction step, we perform easy algebra, then apply the inductive hypothesis, then perform hard algebra, then apply Pascal's rule:
            \[
                (x + y)^{k+1} = (x + y)(x + y)^k
            \]
            \[
                = (x + y) \left[ x^k + \binom{k}{1}x^{k-1}y + \binom{k}{2}x^{k-2}y^2 + \cdots + \binom{k}{k-1}xy^{k-1} + y^k \right]
            \]
            \[
                = x^{k+1} + \left[\binom{k}{0}\right]x^k y + \left[\binom{k}{1}\right]x^{k-1}y^2 + \cdots + \left[\binom{k}{k}\right]xy^k + y^{k+1}
            \]
            \[
                = x^{k+1} + \binom{k+1}{1}x^k y + \binom{k+1}{2}x^{k-1}y^2 + \cdots + \binom{k+1}{k}xy^k + y^{k+1}.
            \]
            And that—a few boring algebraic details omitted—is the proof.
            \bigbreak \noindent 
            The binomial theorem tells us that in order to expand $(x + y)^5$ you can just look at the 5th row of Pascal’s triangle (where the top element counts as the $0$th row, so the 5th row is $1 \ 5 \ 10 \ 10 \ 5 \ 1$):
            \[
                (x + y)^5 = 1x^5 + 5x^4y + 10x^3y^2 + 10x^2y^3 + 5xy^4 + 1y^5.
            \]
            Moreover, by plugging in special values for $x$ and $y$, all sorts of neat identities pop out. There are loads of examples of this, but here are just three:
            \begin{itemize}
                \item By plugging in $x = 1$, $y = 1$, we prove $\sum_{k=0}^n \binom{n}{k} = 2^n$.
                \item By plugging in $x = 2$, $y = 1$, we prove $3^n = \sum_{k=0}^n \binom{n}{k}2^k$.
                \item By plugging in $x = -1$, $y = 1$, we prove $0 = \sum_{k=0}^n (-1)^k \binom{n}{k}$.
            \end{itemize}
        % \item \textbf{Fermat's little theorem with 4.15}:
        %     \bigbreak \noindent 
        %     \textbf{Theorem}. If $a$ is a natural number and $p$ is a prime which does not divide $a$, then
        %     \begin{align*}
        %         a^{p} \equiv a \pmod{p}
        %     .\end{align*}
        %     \textbf{Note: } Written just slightly differently by multiplying each side of the congruence by a, which can also be undone by using the cancellation law



    \end{itemize}

    \pagebreak 
    \subsection{Logic}
    \begin{itemize}
        \item \textbf{Statements}: A statement is a sentence or mathematical expression that is either true or false. If the logic is valid and the statements are true, then it is called sound
            \bigbreak \noindent 
            Every theorem/proposition/lemma/corollary is a (true) statement; Every conjecture is a statement (of unknown truth value); and Every incorrect calculation is a (false) statement.
        \item \textbf{Open sentence}: 
            A related notion is that of an \textit{open sentence}, which refers to sentences or mathematical expressions that:
            \begin{enumerate}
                \item do not have a truth value,
                \item depend on some unknown, like a variable $x$ or an arbitrary function $f$, and
                \item when the unknown is specified, the open sentence becomes a statement (and thus has a truth value).
            \end{enumerate}
            Their truth value depends on the specific value of $x$ or $f$ that is chosen.
            \bigbreak \noindent 
            Typically, we use capital letters for statements, like $P$, $Q$ and $R $. Open sentences are often written the same, or perhaps like $P(x)$, $Q(x)$ or $R(x)$ when one wishes to emphasize the variabl
        \item \textbf{And, or, not}: Let $P$ and $Q$ be statements or open sentences.
            \begin{enumerate}
                \item $P \land Q$ means "P and Q".
                \item $P \lor Q$ means "P or Q (or both)".
                \item $\sim P$ means "not P".
            \end{enumerate}
        \item \textbf{Implies, iff}:
            Let $P$ and $Q$ be statements or open sentences.
            \begin{enumerate}
                \item $P \implies Q$ means "P implies Q".
                \item $P \iff Q$ means "P if and only if Q".
            \end{enumerate}
            \bigbreak \noindent 
            Let’s now discuss a subtle aspect of implications: Translating them to and from English. Language can be complicated,\footnote{Language nuances can make logical translation challenging.} and we in fact have many different ways in English to say “$P$ implies $Q$.” Here are some examples:
            \begin{itemize}
                \item If $P$, then $Q$
                \item $Q$ if $P$
                \item $P$ only if $Q$
                \item $Q$ whenever $P$
                \item $Q$, provided that $P$
                \item Whenever $P$, then also $Q$
                \item $P$ is a sufficient condition for $Q$
                \item For $Q$, it is sufficient that $P$
                \item For $P$, it is necessary that $Q$
            \end{itemize}
            For example, “If it is raining, then the grass is wet” has the same meaning as “The grass is wet if it is raining.” These also mean the same as “The grass is wet whenever it is raining” or “For the grass to be wet, it is sufficient that it is raining.”
            \bigbreak \noindent 
            Next, here are some ways to say “$P$ if and only if $Q$”:
            \begin{itemize}
                \item $P$ is a necessary and sufficient condition for $Q$.
                \item For $P$, it is necessary and sufficient that $Q$.
                \item $P$ is equivalent to $Q$.
                \item If $P$, then $Q$, and conversely.
                \item $P$ implies $Q$ and $Q$ implies $P$.
                \item Shorthand: $P$ iff $Q$.
                \item Symbolically: $(P \implies Q) \land (Q \implies P)$.
            \end{itemize}
            The fact that ``$P$ implies $Q$'' is the same as ``If $P$, then $Q$'' or ``$Q$ if $P$'' is sometimes intuitive to students. But the fact that these are all the same as ``$P$ only if $Q$'' is often confusing. Most people's guts tell them that ``$P$ implies $Q$'' should be the same as ``$Q$ only if $P$.''
            \bigbreak \noindent 
            The answer is ``$P$ only if $Q$'', and the way to think about it is that ``$P$ implies $Q$'' means that whenever $P$ is true, $Q$ must also be true. And ``$P$ only if $Q$'' means that $P$ can only be true if $Q$ is true\ldots that is, whenever $P$ is true, it must be the case that $Q$ is also true\ldots that is, $P \implies Q$.
        \item \textbf{Conditional, biconditional statements}: Now, if $P$ and $Q$ are statements, then ``$P \implies Q$'' and ``$P \iff Q$'' are also statements, meaning they must also be either true or false. The statement $P \implies Q$ is called a conditional statement, whereas $P \iff Q$ is called a biconditional statement. These are minor definitions, but the following is an important definition.

        \item \textbf{Converse}: The \textit{converse} of $P \implies Q$ is $Q\implies P $
            \bigbreak \noindent 
            \textbf{Note:} If $P \implies Q$, it is not necessarily the case that $Q \implies P$
        \item \textbf{Truth tables for and, or, and not}: A truth table models the relationship between the truth values of one or more statements, and that of another
            \bigbreak \noindent 
            \begin{center}
                \begin{tabular}{c|c|c}
                    $P$& $Q$ & $P\land Q$ \\
                    \hline
                    True & True  & True \\
                    True & False & False \\
                    False &  True & False \\
                    False & False & False
                \end{tabular}
            \end{center}
            For for “$P$ and $Q$” to be a true statement, both $P$ and $Q$ must be independently true
            \bigbreak \noindent 
            Here’s how the truth values for $P$ and for $Q$ affect the truth value for $P \lor Q$.
            \begin{center}
                \begin{tabular}{c|c|c}
                    $P$ & $Q$ & $P \lor Q$ \\
                    \hline
                    True  & True & True \\
                    True  & False & True \\
                    False  & True  & True \\
                    False & False & False
                \end{tabular}
            \end{center}
            It is sufficient that either $P$ is true or that $Q$ is true (or both).
            \bigbreak \noindent 
            Finally, here is how the truth values for $P$ affects that of $\neg P$.
            \begin{center}
                \begin{tabular}{c|c}
                    $P$ &  $\neg P$ \\
                    \hline
                    True & False\\
                    False & True
                \end{tabular}
            \end{center}
            In order for ``not $P$'' to be true, it is required that $P$ be false. By applying this reasoning twice, this also implies that $\sim\sim P$ and $P$ always have the same truth value.
            \bigbreak \noindent 
            One last example shows how we proceed with more complicated statements
            \[
                \begin{array}{c|c|c|c|c|c}
                    \hline
                    P & Q & P \vee Q & P \wedge Q & \neg (P \wedge Q) & (P \vee Q) \wedge \neg (P \wedge Q) \\
                    \hline
                    \text{True} & \text{True} & \text{True} & \text{True} & \text{False} & \text{False} \\
                    \text{True} & \text{False} & \text{True} & \text{False} & \text{True} & \text{True} \\
                    \text{False} & \text{True} & \text{True} & \text{False} & \text{True} & \text{True} \\
                    \text{False} & \text{False} & \text{False} & \text{False} & \text{True} & \text{False} \\
                \end{array}
            \]
        \item \textbf{De Morgan’s Logic Laws}: Take a loot at the truth tables for $\neg(P \land Q)$ and $\neg P \lor \neg Q$, side by side:
            \begin{multicols}{2}
                \[
                    \begin{array}{c|c|c|c}
                        P & Q & P \land Q & \neg(P \land Q) \\
                        \hline
                        \text{True} & \text{True} & \text{True} & \text{False} \\
                        \text{True} & \text{False} & \text{False} & \text{True} \\
                        \text{False} & \text{True} & \text{False} & \text{True} \\
                        \text{False} & \text{False} & \text{False} & \text{True} \\
                    \end{array}
                \]

                \[
                    \begin{array}{c|c|c|c|c}
                        P & Q & \neg P & \neg Q & \neg P \lor \neg Q \\
                        \hline
                        \text{True} & \text{True} & \text{False} & \text{False} & \text{False} \\
                        \text{True} & \text{False} & \text{False} & \text{True} & \text{True} \\
                        \text{False} & \text{True} & \text{True} & \text{False} & \text{True} \\
                        \text{False} & \text{False} & \text{True} & \text{True} & \text{True} \\
                    \end{array}
                \]
            \end{multicols}
            Since the final columns are the same, if one is true, the other is true; if one is false, the other is false; that is, there is no way to select $P$ and $Q$ without these two agreeing. When two statements have the same final column in their truth tables, like in the example above, they are said to be logically equivalent (one is true if and only if the other is true), which we denote with an ``$\iff$'' symbol. De Morgan’s logic law, for example, can be written like this:
            \[
                \neg(P \land Q) \iff (\neg P \lor \neg Q)
            \]
            \bigbreak \noindent 
            “$P$ and $Q$ are not both true” is the same as “$P$ is false or $Q$ is false.”
            \bigbreak \noindent 
            \textbf{Theorem}: If $P$ and $Q$ are statements, then
            \[
                \neg(P \land Q) \iff \neg P \lor \neg Q \quad \text{and} \quad \neg(P \lor Q) \iff \neg P \land \neg Q.
            \]
        \item \textbf{$P$, $Q$, and their names}:
            In logical statements involving \( P \) and \( Q \), the terms \( P \) and \( Q \) are referred to as propositions or statements. Depending on the logical operator used, they may also have more specific names:
            \begin{enumerate}
                \item \textbf{In a conjunction (\( P \land Q \)):}
                    \begin{itemize}
                        \item \( P \) and \( Q \) are called \textbf{conjuncts}.
                    \end{itemize}
                \item \textbf{In a disjunction (\( P \lor Q \)):}
                    \begin{itemize}
                        \item \( P \) and \( Q \) are called \textbf{disjuncts}.
                    \end{itemize}
                \item \textbf{In an implication (\( P \implies Q \)):}
                    \begin{itemize}
                        \item \( P \) is called the \textbf{antecedent} (or \textbf{hypothesis}, \textbf{premise}).
                        \item \( Q \) is called the \textbf{consequent} (or \textbf{conclusion}).
                    \end{itemize}
                \item \textbf{In a biconditional (\( P \iff Q \)):}
                    \begin{itemize}
                        \item \( P \) and \( Q \) are called \textbf{equivalents} (since \( P \iff Q \) means \( P \) and \( Q \) are logically equivalent).
                    \end{itemize}
                \item \textbf{In negation (\( \neg P \)):}
                    \begin{itemize}
                        \item \( P \) is simply the proposition being negated.
                    \end{itemize}
            \end{enumerate}

        \item \textbf{Implications}: We call the conditional statemests, $P \implies Q$ \textit{implications}. They are called implications because they express a logical relationship where one statement (the premise, $P$) ``implies'' or leads to another statement (the conclusion, $Q$). The word ``implication'' comes from the Latin root \textit{implicare}, meaning ``to entwine'' or ``to involve,'' reflecting the idea that $P$ is connected to $Q$.
            \bigbreak \noindent 
            A biconditional statement combines two implications, $P \implies Q$ AND $Q\implies P $
        \item \textbf{Truth Tables with Implications}: Consider the truth table for the implication $P\implies Q$
            \begin{center}
                \begin{tabular}{c|c|c}
                    $P$& $Q$ & $P\implies Q$ \\
                    \hline 
                    True & True  & True \\
                    True  & False & False \\
                    False & True & True \\
                    False & False & True
                \end{tabular}
            \end{center}
            The results of the first two rows are trivial, but the last two may be hard to grasp.
            \bigbreak \noindent 
            Why is the implication true if the assumption, $P$, is false? It’s kind of like how we said that this is true: “If $x \in \varnothing$, then $x$ is a purple elephant that speaks German.” Since there is nothing in the empty set, if you suppose $x \in \varnothing$, you can then claim anything you want about $x$ and it is inherently true — you certainly cannot present to me any element in the empty set that is not a purple elephant that speaks German. In the set theory chapter, we called such a claim \textit{vacuously true}. 
            \bigbreak \noindent 
            Likewise, in a universe where $P$ is true, the statement $P \implies Q$ has some real meaning that needs to be proven or disproven: Does $P$ being true imply $Q$ is true, or not? But in a universe where $P$ is not true, it claims nothing, and hence $P \implies Q$ is \textit{vacuously true}.
            \bigbreak \noindent 
            “If unicorns exist, then they can fly” can certainly not be considered false, because unicorns do not exist, so any claim about them is considered vacuously true. Indeed, the way to falsify that proposition would be to locate a unicorn that cannot fly, which is impossible to do. Every unicorn in existence can indeed fly! Also, every unicorn in existence cannot fly! Neither can be disproven!
        \bigbreak \noindent 
        Let's now consider the truth table for the statement $P \iff Q $
        \begin{center}
            \begin{tabular}{c|c|c}
                $P$& $Q$ & $P\iff Q$ \\
                \hline
                True &True & True \\
                True & False & False \\
                False & True & False \\
                False & False & True
            \end{tabular}
        \end{center}
        We can see this by writing $P\iff Q $ as $(P\implies Q) \land (Q\implies P)$
    \item \textbf{Quantifiers}: Consider the sentence
        \begin{center}
           $n$ is even 
        \end{center}
        Which is not a statement because it is neither true nor false. One way to turn a sentence like this into a statement is to give n a value. For example,
        \begin{center}
           If $n=5$, then $n$ is even 
        \end{center}
        What I’d like to discuss now are two other basic ways to turn “$n$ is even” into a statement: add quantifiers. A quantifier is an expression which indicates the number (or quantity) of our objects
        \begin{center}
          $\forall \ n\in \mathbb{N}$, $n$ is even \\
          $\exists \ n \in \mathbb{N}$ such that $n$ is even
        \end{center}
        Where $\forall$ means "for all", and $\exists$ means "there exists". The symbol $\forall$ is known as the \textit{universal quantifer}. Whereas $\exists$ is known as the \textit{existential quantifier.}
        \bigbreak \noindent 
        \textbf{Note}: We also have $\not\exists $ "there does not exist", and $\exists! $ "there exists a unique"
    \item \textbf{Rules of negating}: We have the following rules for negating statements
        \begin{itemize}
            \item $\neg\land = \lor $ 
            \item $\neg\lor = \land $
            \item $\neg\forall = \exists $ 
            \item $\neg\exists = \forall $ 
        \end{itemize}
        Consider the statement, $R: $ for every real number $x$, there is some real number $y$ such that $y^{3} = x $. Symbolically, we have
        \begin{align*}
            \forall\ x\in \mathbb{R},\ \exists \ y\in \mathbb{R} \text{ such that } y^{3} = x
        .\end{align*}
        Then,
        \begin{align*}
            \neg(\forall\ x\in \mathbb{R},\ \exists \ y\in \mathbb{R} \text{ such that } y^{3} = x)
        .\end{align*}
        Is equivalent to the statement
        \begin{align*}
            \exists \ x\in \mathbb{R}, \text{ such that } \forall \ y\in \mathbb{R},\ y^{3} \ne x
        .\end{align*}
    \item \textbf{Negations with implications}: First, recall the truth table for $P\implies Q$
        \begin{center}
            \begin{tabular}{c|c|c}
                $P$& $Q$ & $P\implies Q $ \\
                \hline
                True  & True & True \\
                True & False & False \\
                False & True & True \\
                False & False & True
            \end{tabular}
        \end{center}
        \bigbreak \noindent 
        The only way for $P \implies Q$ to be false is for both $P$ to be true and for $Q$ to be false. This shows that
        \begin{align*}
            \neg(P \implies Q) \Leftrightarrow P \land \neg Q
        .\end{align*}
        Consider the statement
        \begin{align*}
            S:\ \forall \ n \in \mathbb{N}, (3\mid n) \implies (6\mid n)
        .\end{align*}
        Then,
        \begin{align*}
            \neg S:\ &\neg( \forall \ n \in \mathbb{N}, (3\mid n) \implies (6\mid n)) \\
                     &\eq \exists \ n \in \mathbb{N} \text{ such that } (3\mid n) \land (6\nmid n)
        .\end{align*}
    \item \textbf{The contrapositive (and the inverse)}: The \textit{contrapositive} of $P\implies Q$ is $\neg Q \implies \neg P $
        \bigbreak \noindent 
        \textbf{Note:} The \textit{inverse} of $P \implies Q$ is $\neg P \implies \neg Q $
        \bigbreak \noindent 
        \textbf{Theorem}: An implication is logically equivalent to its contrapositive. That is,
        \begin{align*}
            P \implies Q \eq \neg Q \implies \neg P
        .\end{align*}
        The truth table easily verifys this
    \item \textbf{Proving quantified statements: Existential proofs}: To prove an existence statement, it suffices to exhibit an example satisfying the criteria. The above strategy is called a constructive proof — you literally construct an example. There are also non-constructive ways to prove something exists. Often (but not always!) non-constructive proofs make use of some other theorem. 
    \item \textbf{Proving quantified statements: Universal proofs}: To prove a universal statement, it suffices to choose an arbitrary case and prove it works there. We have seen several examples of this. For example, if you were asked to prove that “For every odd number $n$, it follows that $n + 1$ is even,” your proof wouldn’t explicitly check 1 and 3 and 5 and so on. Rather, you would say “Since $n$ is odd, $n = 2a + 1$ for some $a \in \mathbb{Z}$.” Then you would note that 
        \[
            n + 1 = (2a + 1) + 1 = 2(a + 1)
        \]
        is even. The point here is that by letting $n = 2a + 1$, you were essentially selecting an arbitrary odd number, and operating on that. Every odd number can be written in that form, and every odd number can have 1 added to it and then factored like we did. Since our $n$ was completely arbitrary, everything we did could be applied to any particular odd number. Proving something holds for an arbitrary element of a set, proves that it in turn holds for every element in that set.
    \item \textbf{Proving biconditional statements}: In order to prove a statement in the form $P\implies Q$, we must prove both directions. That is, $P\implies Q$ and $Q\implies P $


    \end{itemize}

    \pagebreak 
    \subsection{Proof using the contrapositive}
    \begin{itemize}
        \item \textbf{Proof outline}:
            \bigbreak \noindent 
            \textbf{Proposition.} $P\implies Q$
            \bigbreak \noindent 
            \textbf{\textit{Proof}}. We will use the contrapositive. Assume not-$Q$
            \penv{
                $\left\langle \left\langle \text{ An explanation of what not-$Q$ means } \right\rangle \right\rangle $, use definitions, and/or other results
                \begin{align*}
                    &\vdots  \quad \text{ Apply algebra,}\\
                    &\vdots  \quad \text{ logic, techniques}
                .\end{align*}
                $\left\langle \left\langle \text{ Hey look, that's what not-$P$  means } \right\rangle \right\rangle $
                \bigbreak \noindent 
                Therefore not-$P$
            }
            Since not-$Q \implies $ not-$P$, by the contrapositive $P\implies Q $ \hspace{2.5cm} $\blacksquare $
        \item \textbf{Contrapositive proof 1.}
            \bigbreak \noindent 
            \textbf{Proposition}. Suppose $n\in \mathbb{N}$, if $n^{2}$ is odd, then $n$ is odd.
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} We will use the contrapositive. The statement, $\forall \ n \in \mathbb{N},\ n^{2} = 2k+1 \implies n = 2\ell + 1,\ k,\ell \in \mathbb{Z}$ has the logically equivalent contrapositive $\forall n \in \mathbb{N},\ n \ne 2\ell + 1 \implies n^{2}\ne 2k+1$. Since $n\in \mathbb{N}$, if $n, n^{2}$  is not odd, then it must be even. Thus, the statement becomes $\forall n \in \mathbb{N},\ n = 2\ell \implies n^{2} = 2k,\ k,\ell \in \mathbb{N}$ which becomes much easier to proof. For some extra practice negating statements, here is the negation
            \begin{align*}
                \neg(&\forall n \in \mathbb{N},\ n^{2} = 2k+1 \implies n  = 2\ell  + 1,\ k,\ell \in \mathbb{N}) \\
                =\ &\exists n \in \mathbb{N} \text{ such that } n^{2}= 2k+1 \land n \ne 2\ell + 1
            .\end{align*}
            Recall $\neg(P\implies Q) = P \land \neg Q $
            \bigbreak \noindent 
            Assume $n\in \mathbb{N}$, and that $n$ is even. Since $n$ is even, it must be that $n = 2\ell$, for some integer $\ell$. Squaring both sides, we get
            \begin{align*}
                n^{2} &= (2\ell)^{2} \\
                      &=4\ell^{2} =2(2\ell^{2})
            .\end{align*}
            Since $\ell \in \mathbb{Z}$, we know $2\ell^{2} \in \mathbb{Z} $, and thus $n^{2}$ is even.
            \bigbreak \noindent 
            Therefore, since $n$ not being odd implies $n^{2}$ is also not odd, we have shown by the contrapositive that if $n^{2}$ is odd, $n$ is also odd $\quad \blacksquare $
        \item \textbf{Contrapositive proof 2.} 
            \bigbreak \noindent 
            \textbf{Proposition}. Suppose $n \in N$. Then, $n$ is odd if and only if $3n + 5$ is even
            \pagebreak \bigbreak \noindent 
            \textbf{\textit{Proof.}} We will prove this in two parts
            \penv{
                \underline{Part 1: If $n$ is odd then $3n+5$ is even}. Assume $n\in \mathbb{N}$ is odd, then $n = 2k+1$, for $k\in \mathbb{N}_{0}$. Thus,
                \begin{align*}
                    3n+5 &= 3(2k+1) + 5 \\
                         &= 6k + 3 + 5 = 6k + 8\\
                         &= 2(3k+4)
                     .\end{align*}
                     Thus even.
                     \bigbreak \noindent 
                     \underline{Part 2: $3n+5$ being even implies $n$ is odd}. We prove this by use of the contrapositive. The given statement has the following contrapositive...
                     \begin{align*}
                         n = 2k \implies 3n+5 = 2\ell + 1,\ k,\ell \in \mathbb{N}_{0}
                     .\end{align*}
                     Thus,
                     \begin{align*}
                         3n+5 &= 3(2k) + 5 \\
                              &= 6k+5 = 6k + 4 + 1 \\
                              &= 2(3k+2) + 1
                    .\end{align*}
                    Thus odd.
                    \bigbreak \noindent 
            }
            Since $P \implies Q$, and $Q\implies P$, it must be that $P \iff Q$ is true. Thus, we assert for $n\in \mathbb{N}$, $n$ is odd if and only if $3n+5$ is even.
        \item \textbf{Contrapositive proof 3.}:
            \bigbreak \noindent 
            \textbf{Proposition}. Let $a,b\in \mathbb{Z}$, and $p \in \mathbb{P}$. If $p\nmid ab$, then $p\nmid a$ and $p\nmid b $
            \textbf{Proof.} Suppose $a, b \in \mathbb{Z}$ and $p$ is a prime. We will use the contrapositive. Suppose that it is not true that $p \nmid a$ and $p \nmid b$. By the logic form of De Morgan’s law (Theorem 5.9), this is equivalent to saying it is not true that $p \nmid a$ \textit{or} it is not true that $p \nmid b$. That is, $p \mid a$ \textit{or} $p \mid b$. Let’s consider these two cases separately.
            \penv{
                \textbf{Case 1.} Suppose $p \mid a$, which by the definition of divisibility (Definition 2.8) means that $a = pk$ for some $k \in \mathbb{Z}$. Thus,
                \[
                    ab = (pk)b = p(kb).
                \]
                Since $k, b \in \mathbb{Z}$, also $(kb) \in \mathbb{Z}$. And so, by the definition of divisibility (Definition 2.8), $p \mid ab$.
                \bigbreak \noindent 
                \textbf{Case 2.} Suppose $p \mid b$, which by the definition of divisibility (Definition 2.8) means that $b = p\ell$ for some $\ell \in \mathbb{Z}$. Thus,
                \[
                    ab = a(p\ell) = b(a\ell).
                \]
                Since $a, \ell \in \mathbb{Z}$, also $(a\ell) \in \mathbb{Z}$. And so, by the definition of divisibility (Definition 2.8), $p \mid ab$.
                \bigbreak \noindent 
                In either case, we concluded that $p \mid ab$, which is equivalent to saying that it is not true that $p \nmid ab$.
                \bigbreak \noindent 

            }
            We proved that if it is not true that $p \nmid a$ and $p \nmid b$, then it is not true that $p \nmid ab$. Hence, by the contrapositive, this implies that if $p \mid ab$, then $p \mid a$ and $p \mid b$. \(\square\)
            \bigbreak \noindent 
            \textbf{Note:} Mathematicians have agreed that we should be allowed to skip essentially-identical cases
            \bigbreak \noindent 
            If you have two cases, like $p \mid a$ and $p \mid b$, and there is literally no mathematical distinction between them, then you are allowed to say “without loss of generality, assume $p \mid a$.” This allows you to skip the “$p \mid b$” case entirely.
            \bigbreak \noindent 
            \textbf{Condensed, Elder-Approved Proof.} Suppose $a, b \in \mathbb{Z}$ and $p$ is a prime. We will use the contrapositive. Suppose that it is not true that $p \nmid a$ and $p \nmid b$. By the logic form of De Morgan’s law (Theorem 5.9), this is equivalent to saying it is not true that $p \nmid a$ \textit{or} it is not true that $p \nmid b$. That is, $p \mid a$ \textit{or} $p \mid b$. Without loss of generality, assume $p \mid a$.
            \bigbreak \noindent 
            By the definition of divisibility (Definition 2.8), this means that $a = pk$ for some $k \in \mathbb{Z}$. Thus,
            \[
                ab = (pk)b = p(kb).
            \]
            Since $k, b \in \mathbb{Z}$, also $(kb) \in \mathbb{Z}$. And so, by the definition of divisibility (Definition 2.8), $p \mid ab$.
            \bigbreak \noindent 
            We proved that if it is not true that $p \nmid a$ and $p \nmid b$, then it is not true that $p \nmid ab$. Hence, by the contrapositive, this implies that if $p \mid ab$, then $p \mid a$ and $p \mid b$. \(\square\)
        \item \textbf{Contrapositive proof 4.}
            \bigbreak \noindent 
            \textbf{Proposition}. Let $a,b,n \in \mathbb{N}$. If $36a \not\equiv 36b \pmod{n} $, then $n\nmid 36 $
            \bigbreak \noindent 
            \textbf{Proof idea}. The fact that this proposition says a lot of things are not happening is one indication that the contrapositive could be worthwhile. The contrapositive states For $a,b,n\in \mathbb{N}$,  If $ n\mid 36$, then $36a\equiv 36b \pmod{n}$
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} Assume $a,b,n\in \mathbb{N}$, and $n\mid36$. In this case, we have $36 = nk$, for $k\in \mathbb{Z}$. We require $36a-36b = n\ell$, for $\ell \in \mathbb{Z}$. We then examine the quantity $36a-36b$. Since $36 = nk$, we have
            \begin{align*}
                36a - 36b &= nka - nkb \\
                          &=n(ka-kb)
            .\end{align*}
            Which is precisely the definition of divisibility, since it is clear that $ka-kb \in \mathbb{Z}$. Thus, we have $n \mid 36a -36b $, and by the definition of modular congruence $36a  \equiv 36b \pmod{n}$.
            \bigbreak \noindent 
            Therefore, by the contrapositive, $36a \not\equiv 36b \pmod{n}$ implies that $n\nmid 36 \quad \blacksquare $
        \item \textbf{Lemma 6.6} This lemma has two parts
            \begin{enumerate}[label=(\roman*)]
                \item If $m\in\mathbb{Z} $, then $m^{2}  + m $ is even 
                \item If $a\in \mathbb{Z}$, and $a^{2}$ is even, then $a$ is even
            \end{enumerate}
            This proof is trivial and will not be shown. Proving $i$ is simply a proof by cases. To prove $ii$, we can use the contrapositive, instead proving that if $a$ is odd, then $a^{2}$ is odd. Which, by the contrapositive shows that if $a^{2}$ is even, then $a$ must also be even.
        \item \textbf{Contrapositive proof 5.} 
            \bigbreak \noindent 
            \textbf{Proposition}. If $a$ is an odd integer, then $x^{2} + x - a^{2}=  0$ has no integer solution.
            \bigbreak \noindent 
            \textbf{Proof idea.} We will use the contrapositive, which states if $x^{2} + x - a^{2} = 0$ has an integer solution, then $a$ is even.
            \bigbreak \noindent 
            \textbf{Note:} Negating $Q$ in this case ($x^{2} + x - a^{2} = 0 $ has no integer solution) does not given $x^{2} +x - a^{2} \ne 0 $... It is important to question what it means for the given statement to be false in order to properly negate. The negation of the statement is "it is false that $x^{2} + x  -a^{2} = 0 $ has no integer solutions", which must mean that some integer $m$ exists such that $m^{2} + m -a^{2} = 0$.
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} Suppose that \(a\) is an odd integer. We will use the contrapositive. Assume that it is false that \(x^2 + x - a^2 = 0\) has no integer solutions; that is, assume that there is some integer \(m\) such that
            \begin{align*}
                m^2 + m - a^2 = 0
            .\end{align*}
            By the quadratic formula\(^9\) and then some algebra,
            \begin{align*}
                m &= \frac{-1 \pm \sqrt{1^2 - 4(1)(-a^2)}}{2(1)} \\
                m &= \frac{-1 \pm \sqrt{1 + 4a^2}}{2} \\
                2m &= -1 \pm \sqrt{1 + 4a^2} \\
                2m + 1 &= \pm \sqrt{1 + 4a^2} \\
                4m^2 + 4m + 1 &= 1 + 4a^2 \\
                m^2 + m &= a^2.
            .\end{align*}
            Next, observe that \(m^2 + m\) is guaranteed to be even, by Lemma 6.6 part (i). Thus, since we just deduced that \(m^2 + m = a^2\), this means that \(a^2\) must be even. And since \(a\) is an integer, \(a^2\) being even implies that \(a\) is even, by Lemma 6.6 part (ii). In particular, this means that \(a\) is not odd.

            We have shown that if it is false that \(x^2 + x - a^2 = 0\) has no integer solutions, then it is also false that \(a\) is an odd integer. By the contrapositive, if \(a\) is an odd integer, then \(x^2 + x - a^2 = 0\) has no integer solution. \(\Box\)







    \end{itemize}

    \pagebreak 
    \subsection{Contradiction}
    \begin{itemize}
        \item \textbf{The idea}: The big idea is this: If you start with something true and apply correct logic to it, you will never arrive at something false. So it can’t be true that Carmen stole the bag, if that would imply the falsity that she can be in two places at once. Indeed, if your assumptions imply something false, then something you assumed had to be false as well.
            \bigbreak \noindent 
            Suppose we had a theorem $P \implies Q$. Throughout the problem, we assume $P$ to be true. The goal is to show that $Q$ is also true. By the truth tables, either $Q$ is true or $\neg Q$ is true, not both. This gives two options.
            \begin{enumerate}
                \item $P$ is true and $Q$ is true $(P \land Q) $
                \item $P$ is true and $\neg Q$ is true $(P \land \neg Q) $
            \end{enumerate}
            If $P \land \neg Q $ implies anything false, that can't be the correct option. That is, it must be $P \land Q$. Thus, we have shown $P\implies Q $
            \bigbreak \noindent 
            Notice that the only way that \(P \implies Q\) can be false is if \(P\) is true and \(Q\) is false.
            \bigbreak \noindent 
            \begin{center}
                \begin{tabular}{c|c|c}
                    $P$ &$Q$ &$P \implies  Q$ \\
                    \hline
                    True &True &True \\
                    True &False &False \\
                    False &True &True \\
                    False &False &True
                \end{tabular}
            \end{center}
            Thus, this is the only case we have to rule out in order to prove our theorem: that \(P \implies Q\) is true. So, if you assume that \(P\) is true and \(Q\) is false, and manage to use that to deduce a contradiction, then you will have ruled out the one and only bad case, which in turn means that the theorem must be true!
            \bigbreak \noindent 
            In other words, if $P \land \neg Q$ cannot be, then it must be that $P \implies Q$
        \item \textbf{Contradiction example 1.} 
            \bigbreak \noindent 
            \textbf{Proposition.} There does not exist a largest natural number
            \bigbreak \noindent 
            \textbf{Proof Idea.} One quick note: This proposition is not phrased explicitly as 
            ``\(P \implies Q\),'' but you are probably starting to see how to rephrase propositions 
            in this form. For example, this proposition could instead be stated as: 
            ``If \(N\) is the set of natural numbers, then \(N\) does not have a largest element.'' 
            Or, equivalently: ``If \(N\) is larger than every natural number, then \(N \notin \mathbb{N}\)'' 
            Or, equivalently: ``If \(N\) is a natural number, then there exists a natural number 
            larger than \(N\).''
            \bigbreak \noindent 
            For our proof by contradiction, we will assume that there \emph{is} a largest natural number, 
            and then deduce a contradiction. There are several ways to do this, but one way is to assume 
            that \(N\) is the largest and then show that \(N + 1\) must be larger—if it weren’t, we could 
            deduce that \(0 \geq 1\), which is clearly a contradiction. Here’s that:
            \bigbreak \noindent 
            \textbf{Proof.} Assume for a contradiction that there is a largest element of \(\mathbb{N}\), and call this number \(N\). Being larger than every other natural number, \(N\) has the property that \(N \geq m\) for all \(m \in \mathbb{N}\).
            \bigbreak \noindent 
            Observe that since \(N \in \mathbb{N}\), also \((N + 1) \in \mathbb{N}\). And so, by assumption,
            \[
                N \geq N + 1.
            \]
            Subtracting \(N\) from both sides,
            \[
                0 \geq 1.
            \]
            This is a contradiction\(^1\) since we know that \(0 < 1\), and therefore there must not be a largest element of \(\mathbb{N}\). \(\Box\)

        \item \textbf{Contradiction example 2.}
            \bigbreak \noindent 
            \textbf{Proposition}. There does not exist a smallest positive rational number.
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} Assume for the sake of contradiction that there does exist a smallest positive rational number. Call this number $q$. Since $q\in \mathbb{Q}$, we have
            \begin{align*}
                q = \frac{a}{b}
            .\end{align*}
            Where $a,b \in \mathbb{Z}$, and $a,b > 0$. Since $q$ is the smallest, than for all $r \in \mathbb{Q}$, we have $q \leq r$. Let $r = \frac{a}{2b} $. Then,
            \begin{align*}
                \frac{a}{b} &\leq \frac{a}{2b} \\
                \implies 2ab &\leq ab \\
                \implies 2 &\leq 1
            .\end{align*}
            This is a contradiction, since we know $2 > 1$. It must be that there is no smallest positive rational number.
        \item \textbf{Proof by contradiction general form}: 
            \bigbreak \noindent 
            \textbf{Proposition.} $P\implies Q$
            \bigbreak \noindent 
            \textbf{Proof.} Assume for the sake of contradiction $P$ and $\neg Q$
            \penv{
                $\left\langle \left\langle \text{ An explanation of what these mean } \right\rangle \right\rangle $
                \begin{align*}
                    &\vdots \quad \text{ Apply algebra,} \\
                    &\vdots \quad \text{ logic, techniques} 
                .\end{align*}
                $\left\langle \left\langle \text{ Hey look, that contradicts something we know to be true } \right\rangle \right\rangle $
            }
            We obtained a contradiction, therefore $P\implies Q \quad \blacksquare$
        \item \textbf{Proof by contradiction example 3.}
            \bigbreak \noindent 
            \textbf{Proposition.} If $A,B$ are sets, then $A \cap (B \setminus A) = \varnothing $
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} Assume for the sake of contradiction, that $A \cap (B\setminus A) \ne \varnothing $
            \penv{
                Since $A\cap (B \setminus A) \ne \varnothing$, then $\exists x \in A \cap (B \setminus A) $. Thus, $x\in A \ \land \ x\in (B\setminus A) $. Rewrite $B\setminus A$ as $B \cap A^{C}$. Thus, $x\in B \ \land \ x\in A^{C}$. Since $x\in A^{C}$, it must be that $x \not\in A$. Thus, we have $x\in A$, $x\in B$, and $x\not\in A $
            }
            Therefore, since $x \in A $ and $x \not\in A$ is a contradiction, it must be that if $A$, and $B$ are sets, then $A \cap (B \setminus A) = \varnothing $ $\quad \blacksquare $.
        \item \textbf{Proof by contradiction example 4.}
            \bigbreak \noindent 
            \textbf{Proposition.} There does not exists integers $m,n$ such that $15m + 35n = 1 $
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} Assume for the sake of contradiction there does exist integers $m,n $ such that $15m + 35n = 1$, since $m,n \in \mathbb{Z}$, $3m + 7n \in \mathbb{Z} $, but
            \begin{align*}
                15m + 35n &=  1\\
                \implies 3m + 7n &= \frac{1}{5}
            .\end{align*}
            Since $3m+7n \not\in \mathbb{Z} $, we have a contradiction. Thus, it must be that there does not exist integers $m,n$ such that $15m + 35n = 1$.
            \bigbreak \noindent 
            Alternatively, we could have done
            \begin{align*}
                15m + 35n &= 1 \\
                \implies 5(3m + 7n) &= 1
            .\end{align*}
            Which implies $5\mid 1$. But it is clearly the case that $5\nmid 1$, since there exists no $k\in \mathbb{Z}$ such that $1 = 5k$. Thus, another way to arrive at a contradiction. $\quad \blacksquare $
        \item \textbf{Proof by contradiction example 5.}
            \bigbreak \noindent 
            \textbf{Proposition.} There are infinitely many primes.
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} Suppose for the sake of contradiction that there are finitely many primes, say $k$ in total. Let $p_{1}, p_{2},p_{3},...,p_{k}$ be the complete list. Consider the number $N = p_{1} \cdot p_{2} \cdot p_{3} \cdot ...\cdot p_{k}$. Next, consider $N + 1 $. That is, $p_{1}p_{2}p_{3}...p_{k} + 1$. Either $N + 1 $ is prime or it is composite, we consider both cases separately
            \bigbreak \noindent 
            \underline{Case 1: $N+1$ is prime.} In this case, $N+1$ is prime and greater than all the $p_{i}$s we have previously considered. Thus, we have found a new prime.
            \bigbreak \noindent 
            \underline{Case 2: $N+1$ is composite}. We begin by showing that no such $p_{i}$ divides $N+ 1$. Because we know that $p_{i} \mid N$, we have
            \begin{align*}
                N \equiv 0 \pmod{p_{i}}
            .\end{align*}
            Adding one to both sides, we get
            \begin{align*}
                N+1 \equiv 1 \pmod{p_{i}}
            .\end{align*}
            Hence, it must be that $p_{i}\nmid N+1$. Since $p_{i}$ was arbitrary, this shows that none of our $k$ primes divide $N+1 $
            \bigbreak \noindent 
            We assumed that \( p_1, p_2, \ldots, p_k \) was the complete list of prime numbers. 
            And recall that \( N + 1 \) is assumed to be composite, which means it is a product of primes. 
            But since none of the \( p_i \) divide \( N + 1 \), there must be some other prime number, \( q \),
            which divides \( N + 1 \). And hence, we have again found a new prime.
            \bigbreak \noindent 
            In either case, we have contradicted the claim that \( p_1, p_2, \ldots, p_k \) was an exhaustive
            list of the prime numbers. Therefore, there must be infinitely many primes. $\quad \blacksquare $
        \item \textbf{Proof by contradiction example 6.}
            \bigbreak \noindent 
            \textbf{Proposition} The number $\sqrt{2}$ is irrational
            \bigbreak \noindent 
            \textbf{Proof.} Assume for a contradiction that $\sqrt{2}$ is rational. Then there must be some non-zero integers $p$ and $q$ where
            \[
                \sqrt{2} = \frac{p}{q}.
            \]
            Moreover, we may assume that this fraction is written in \textit{lowest terms}, meaning that $p$ and $q$ have no common divisors. Then,
            \[
                \sqrt{2}q = p.
            \]
            By squaring both sides,
            \[
                2q^2 = p^2.
            \]
            Since $q^2 \in \mathbb{Z}$, by the definition of divisibility, this implies that $2 \mid p^2$, and hence $2 \mid p$ by Lemma 2.17 part (iii). By a second application of the definition of divisibility, this means that $p = 2k$ for some non-zero integer $k$. Plugging this in:
            \begin{align*}
                2q^2 &= p^2,\\
                2q^2 &= (2k)^2,\\
                2q^2 &= 4k^2,\\
                q^2 &= 2k^2
            \end{align*}
            Therefore, $2 \mid q^2$, and hence $2 \mid q$, again by Lemma 2.17 part (iii). But this is a contradiction: We had assumed that $p$ and $q$ had no common factors, and yet we proved that $2$ divides each. Therefore, $\sqrt{2}$ cannot be rational, meaning it is irrational.
            \bigbreak \noindent 
            The following is a geometric proof that $\sqrt{2} \in \bar{\mathbb{Q}}$. Recall that $\bar{\mathbf{Q}}$ is the set of irrational numbers.
            \bigbreak \noindent 
            Assume for a contradiction that $\sqrt{2} = \frac{p}{q}$ where $p, q \in \mathbb{N}$ and the fraction is written in lowest terms. This implies that 
            \[
                2q^2 = p^2,
            \]
            but this time let’s think about this as 
            \[
                p^2 = 2q^2.
            \]
            Or, better yet,
            \[
                p^2 = q^2 + q^2.
            \]
            Since $p$ and $q$ are integers, $p^2$ represents the area of a square with side length $p$, and each $q^2$ represents the area of a square with side length $q$.
            \bigbreak \noindent 
            \fig{.6}{./figures/15.png}
            \bigbreak \noindent 
            Recall that $\sqrt{2} = \frac{p}{q}$ was written in lowest terms. In particular, this means that there do not exist any smaller integers $a$ and $b$ for which $\sqrt{2} = \frac{a}{b}$. Our contradiction will be to find such $a$ and $b$.
            \bigbreak \noindent 
            Getting back to the squares above, we are now going to imagine each square is a piece of paper and we are going to place the two $q^{2}$ squares on top of the $p^{2}$ square. If one $q^{2}$ square is placed in the lower-left, and the other is placed in the upper-right, this happens
            \bigbreak \noindent 
            \fig{.6}{./figures/17.png}
            \bigbreak \noindent 
            Notice that there is one square region in the middle that was covered twice, and two small squares in the upper-left and lower-right that were not covered at all. And remember: The amount of area in the $p^2$ square is equal to the amount of area in the two $q^2$ squares. Therefore, the area that was covered twice must equal the area that was not covered at all! Let’s suppose the middle square has dimensions $a \times a$, and the two corner squares have dimensions $b \times b$. Then, this reasoning shows that
            \bigbreak \noindent 
            \fig{.8}{./figures/18.png}
            \bigbreak \noindent 
            And those $a$ and $b$ must also be integers, since they are the difference of integers from the overlap picture:
            \bigbreak \noindent 
            \fig{.8}{./figures/19.png}
            \bigbreak \noindent 
            We had assumed that $p$ and $q$ were the smallest integers for which $\sqrt{2} = \frac{p}{q}$, and yet the above image shows that $a$ and $b$ are also integers, and since $a^2 = b^2 + b^2$, which implies $2b^2 = a^2$, we have $2 = \frac{a^2}{b^2}$. And so, finally, by taking the square root of each side, we see that
            \[
                \sqrt{2} = \frac{a}{b}.
            \]
            We have shown that $a$ and $b$ are integers with the above property. The picture above also shows that $a$ is smaller than $p$, and $b$ is smaller than $q$. Combined, this contradicts our assumption that $p$ and $q$ are the smallest integers where $\sqrt{2} = \frac{p}{q}$.
        \item \textbf{The irrational numbers}:
            The fact that irrational numbers exist explains why we need the real numbers $\mathbb{R}$—the rational numbers $\mathbb{Q}$ are clearly not enough! Next, note that while $\sqrt{2}$ is not a ratio of integers, it is a root of $x^2 - 2 = 0$, which is a polynomial with integer coefficients.
            \bigbreak \noindent 
            \textbf{Big Question:} Is every irrational number a root of a polynomial with integer coefficients? 
            \bigbreak \noindent 
            \textbf{Big Answer:} Nope! In 1844, Joseph Liouville proved that
            \[
                \sum_{k=1}^\infty \frac{1}{10^{k!}} = 0.11000100000000000000000100\ldots
            \]
            is not the root of any polynomial with integer coefficients.
            \bigbreak \noindent 
            The irrational numbers were thus partitioned into \textit{algebraic numbers}, which are the roots of such polynomials, and \textit{transcendental numbers}, which are not. Today, $\pi$ and $e$ are the most famous numbers which have been proved to be transcendental.
        \item \textbf{Proof of the halting problem}:
            \bigbreak \noindent 
            \textbf{Theorem}. Assume that $P$ is an arbitrary program and $i$ is a possible input of $P$; we write $P(i)$ to be the result of plugging input $i$ into the program $P$. There does not exist a program $H(P(i))$ which determines whether $P(i)$ will eventually halt.
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} Assume for a contradiction that such a program $H$ did exist. Create a new program $T(x)$; its input, $x$, is itself a program with some input. Now, we define the program $T(x)$ as follows:
            \bigbreak \noindent 
            \begin{cppcode}
            Input: A program |$x$|, with its own input
            Run |$H(x)$|
            if |$H(x)$| answers |\textit{Program $x$ will halt} \textbf{then}|
                begin an infinite loop
            else halt
            \end{cppcode}
            \bigbreak \noindent 
            The program $T$ is designed to run counter to $x$: If the input program $x $ was going to halt, then $T$ begins an infinite loop. And if the input program was going to run forever, then $T$ says to halt
            \bigbreak \noindent 
            The program $T$ accepts as input any program. And since $T$ is itself a program, we are allowed to \textit{plug $T$ into itself!} What is the result? Well, since $T(T)$ is a program, like any program either $T(T)$ contains an infinite loop or it does not. Let’s consider each of these two cases.
            \bigbreak \noindent 
            \underline{Case 1:} Observe that if $T(T)$ has an infinite loop, then like all programs with infinite loops, it will not halt — but by looking at the above pseudocode for $T$, it is clear that if $T(T)$ has an infinite loop, then it will halt! This is a contradiction.
            \bigbreak \noindent 
            \underline{Case 2:} Conversely, if $T(T)$ does not have an infinite loop, then like all programs without an infinite loop it must eventually halt — but by looking at the above pseudocode for $T$, it is clear that if $T(T)$ will eventually halt, then it will begin an infinite loop which will prevent it from halting! This is again a contradiction.
            \bigbreak \noindent 
            Whether $T$ does or does not have an infinite loop, we have reached a contradiction. And since $T$ was built from $H $, our assumption that there exists a halting program $H$ must have been incorrect. This concludes the proof. $\quad \blacksquare $
        \item \textbf{Proof by contradiction example 7}:
            \bigbreak \noindent 
            \textbf{Proposition.} Every natural number is interesting
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} Assume for a contradiction that not every natural number is interesting. Then, there must be a smallest uninteresting number, which we call $n$. But being the smallest uninteresting number is a very interesting property for a number to have! So $n$ is both uninteresting and interesting, which gives the contradiction. Therefore, every natural number must be interesting. $\quad \blacksquare $
        \item \textbf{Proof by minimal counterexample}: We proved that every natural number is interesting. The way we did this was by assuming for a contradiction that not every number is interesting. Under this assumption, there exist uninteresting natural numbers, and so there must exist a smallest uninteresting natural number.
            \bigbreak \noindent 
            Despite it being a silly example, there is an important idea behind it which is sometimes called \textit{proof by minimal counterexample}. Consider a theorem which asserts something is true for every natural number, and you are attempting to prove it by contradiction. Then you would assume for a contradiction not every natural number satisfies the result — that is, you’re assuming there is at least one counterexample. Well, among all of the counterexamples, one of them must be the smallest. And thinking about that smallest counterexample — such as the smallest uninteresting number — can at times be a powerful variant of proof by contradiction.
            \bigbreak \noindent 
            We used strong induction to prove the fundamental theorem of arithmetic. But there’s another slick proof of this theorem that uses a proof by minimal counterexample
            \bigbreak \noindent 
            \textbf{Theorem (\textit{Fundemental theorem of arithmetic})}. Every integer $n \geq 2 $ is either prime or a product of primes.
            \bigbreak \noindent 
            Recall that every integer $n \geq 2$ is either prime or composite, and being composite means it is a product of smaller integers
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} 
            Assume for a contradiction that this is not true. Then there must be a minimal counterexample; let’s say $N$ is the smallest natural number at least 2 which is neither prime nor the product of primes. The fact that it is not prime means that it is composite: $N = ab$ for some $a, b \in \{2, 3, \ldots, N - 1\}$.
            \bigbreak \noindent 
            We now make use of the fact that $N$ is assumed to be the minimal counterexample to this result — which means that everything smaller than $N$ must satisfy the result. In particular, since $a$ and $b$ are smaller than this smallest counterexample, $a$ and $b$ must each be prime or a product of primes.
            \bigbreak \noindent 
            And this gives us a contradiction: Since $N = ab$, if $a$ and $b$ are each prime or a product of primes, then their product — which equals $N$ — must be as well. This contradicts our assumption that $N$ was a counterexample, completing the proof.
            \bigbreak \noindent 
            Another way to think about this proof is that it argues that if $N$ were a counterexample, then since $N = ab$, it can’t possibly be that both $a$ and $b$ are primes or a product of primes, since as we just saw, that would produce a contradiction. And therefore, it must be the case that either $a$ or $b$ is also a counterexample. This implies that every counterexample produces a smaller counterexample — every $N$ produces an $a$ or a $b$. But this is a contradiction, since you can not repeatedly find smaller and smaller natural numbers — at some point you reach the bottom.
        \item \textbf{Proof of the division algorithm}
            \bigbreak \noindent 
            \textbf{Theorem (\textit{The division algorithm})}: For all integers $a$ and $m$ with $m > 0$, there exist unique integers $q$ and $r$ such that
            \[
                a = mq + r,
            \]
            where $0 \leq r < m$.
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} 
            \textbf{Existence.} First, note that if $a = 0$, then by simply choosing $q = 0$ and $r = 0$, the theorem follows. Thus, we may assume that $a \neq 0$.
            \bigbreak \noindent 
            Next, we will argue that if the theorem holds for all positive $a$, then it also holds for all negative $a$. Indeed, assume that $a > 0$, and suppose $a$ and $m$ can be expressed as
            \[
                a = mq + r,
            \]
            where $0 \leq r < m$. Then, $-a$ has an expression as well. In particular, if we let $q' = -q - 1$ and $r' = m - r$, then
            \[
                mq' + r' = m(-q - 1) + (m - r) = -mq - m + m - r = -(mq + r) = -a.
            \]
            Therefore, for these integers $q'$ and $r'$,
            \[
                -a = mq' + r',
            \]
            where $0 \leq r' < m$. Because of this, any expression for $a > 0$ immediately produces one for $-a$. Thus, we need only prove the case where $a$ is a positive integer.
            \bigbreak \noindent 
            We will implement a proof by minimal counterexample in order to prove the case where $a$ is positive. Fix any $m > 0$, and assume for a contradiction that not every $a \in \mathbb{N}$ satisfies the theorem, which in turn means that there is a smallest $a$ for which the theorem fails. Consider three cases.
            \bigbreak \noindent 
            \textbf{Case 1:} $a < m$. In this case, we can simply let $q = 0$ and $r = a$, and we have obtained
            \[
                a = m \cdot q + r,
            \]
            with $0 \leq r < m$, and the theorem is satisfied.
            \bigbreak \noindent 
            \textbf{Case 2:} $a = m$. In this case, we can simply let $q = 1$ and $r = 0$, and we have obtained
            \[
                a = m \cdot q + r,
            \]
            with $0 \leq r < m$, and the theorem is satisfied.
            \bigbreak \noindent 
            \textbf{Case 3:} $a > m$. Recall that the theorem assumes that $m > 0$, and so in this case we have $a > m > 0$. In particular, note that $a > a - m$ and also $a - m > 0$.
            \bigbreak \noindent 
            Since $a$ is the smallest positive counterexample to this theorem, and $a - m$ is both positive and less than $a$, the integer $a' = a - m$ must satisfy this theorem! That is, there must exist integers $d$ and $s$ for which
            \[
                (a - m) = m \cdot d + s,
            \]
            with $0 \leq s < m$. By moving the $m$ on the left side over,
            \[
                a = m \cdot d + s + m.
            \]
            By factoring,
            \[
                a = m \cdot (d + 1) + s.
            \]
            Thus, by letting $q = d + 1$ and $r = s$, we have shown that our smallest counterexample is not a counterexample at all:
            \[
                a = m \cdot q + r,
            \]
            with $0 \leq r < m$. Since there cannot exist a smallest counterexample, there cannot exist any counterexample. Thus, for each $a$ and $m$, there must exist a $q$ and $r$ as the theorem asserts.
            \bigbreak \noindent 
            \textbf{Uniqueness.} Assume for a contradiction that for our fixed $a$ and $m$, the $q$ and $r$ are not unique. That is, assume there exist two different representations of $a$:
            \[
                a = mq + r \quad \text{and} \quad a = mq' + r',
            \]
            where $q, r, q', r' \in \mathbb{Z}$ and $0 \leq r, r' < m$. Then,
            \[
                mq + r = mq' + r'.
            \]

            By some algebra, we find:
            \[
                r - r' = mq' - mq,
            \]
            which means
            \[
                r - r' = m(q' - q).
            \]
            \bigbreak \noindent 
            Since $q$ and $q'$ are integers, so is $q - q'$ (by Fact 2.1), which means the above expression matches the definition of divisibility (Definition 2.8)! That is, $m \mid (r - r')$.
            \bigbreak \noindent 
            Notice that since $0 \leq r, r' < m$, the difference $r - r'$ would have these restrictions:
            \[
                -m < r - r' < m.
            \]
            And the only number in this range which is divisible by $m$ is zero. That is, $r - r' = 0$, or $r = r'$.
            \bigbreak \noindent 
            Next, since $r = r'$, the fact that $r - r' = m(q - q')$ implies that
            \[
                0 = m(q - q').
            \]
            Since $m > 0$, we may divide both sides by $m$, which means $0 = q - q'$, or $q = q'$.
            \bigbreak \noindent 
            We assumed that
            \[
                a = mq + r \quad \text{and} \quad a = mq' + r'
            \]
            were two different representations of $a$ and $m$, but we have proven that $q = q'$ and $r = r'$, proving that they are in fact the same representation, giving the contradiction and concluding the proof.


    \end{itemize}

    \pagebreak 
    \subsection{Functions}
    \begin{itemize}
        \item \textbf{The definition of a function}: Given a pair of sets \( A \) and \( B \), suppose that each element \( x \in A \)
            is associated, in some way, to a unique element of \( B \), which we denote \( f(x) \). Then
            \( f \) is said to be a function from \( A \) to \( B \). This is often denoted
            \( f : A \to B \).
            \bigbreak \noindent 
            Furthermore, \( A \) is called the \textbf{domain} of \( f \), and \( B \) is called the \textbf{codomain} of \( f \).
            \bigbreak \noindent 
            The set \( \{f(x) : x \in A\} \) is called the \textbf{range} of \( f \).
        \item \textbf{The \textit{Existence}, and \textit{uniqueness} property of functions}: When discussing functions, the ideas of existence and uniqueness will come up repeatedly. We defined a function \( f : A \to B \) to be a rule which sends each \( x \in A \) to some \( f(x) \in B \). What this means is that \( f(x) \) must exist (it must be equal to some \( b \in B \)), and it must be unique (it must be equal to only one \( b \in B \)).
            \bigbreak \noindent 
            For example, defining $f: \mathbb{R} \to \mathbb{R}$, $f(x) = \ln{(x)}$ fails the \textit{existence} requirement of functions, because the natural logarithm function $\ln{(x)}$ is not defined for negative values of $x$ or $x=0 $. his means that the function $\ln(x)$ would fail the requirement of existence for all elements in the domain $\mathbb{R}$.
            \bigbreak \noindent 
            To make $f(x) = \ln(x) $ a valid function, we must adjust the domain to only include values for which $\ln(x)$ is defined. The correct domain is $(0,\infty)$, the set of positive real numbers. Thus, we would write
            \begin{align*}
                f: (0, \infty) \to \mathbb{R}
            .\end{align*}
            A "function" that fails the uniqueness requirement of functions would assign a single element in the domain to more than one element in the codomain.
            \bigbreak \noindent 
            Consider a rule $f: A \to B $ defined as 
            \begin{align*}
                f(x) = \begin{cases}
                    b_{1} & \text{ if } x= a \\     
                    b_{2} & \text{ if } x= a 
                \end{cases}
            .\end{align*}
            Where $b_{1} \ne b_{2}$, and $a\in A $. This rule clearly violates the \textit{uniqueness} criterion, and is therefore not a function.
            \bigbreak \noindent 
            In high school you were probably taught the \textit{vertical line test} to check whether a graph corresponds to a function. The vertical line test says that if every vertical line hits the graph in one (existence) and only one (uniqueness) spot, then the graph corresponds to a function
        \item \textbf{Injections, Surjections and Bijections}: A function $f : A \to B$ is injective (or one-to-one) if $f(a_{1}) = f(a_{2})$ implies that $a_{1} = a_{2}$.
            \bigbreak \noindent 
            The contrapositive of the second half states, A function $f: A \to B$ is \textit{injective} if $a_{1} \ne a_{2}$ implies that $f(a_{1}) \ne f(a_{2}) $
            \bigbreak \noindent 
            A function $f:\  A \to B$ is surjective (or onto) if, for every $b \in B$, there exists some $a \in A$ such that $f(a) = b$
            \bigbreak \noindent 
            Let’s take a look at another way to define this same idea, by again applying the contrapositive (and doing a little rearranging).
            \bigbreak \noindent 
            A function $f:\ A \to B$ is surjective (or onto) if there does not exist any $b \in B$ for which $f(a) \ne b$ for all $a \in A$.
            \bigbreak \noindent 
            When defining a function \( f : A \to B \), the ideas of existence and uniqueness were focused on \( A \) — for every \( x \in A \), we demanded that \( f(x) \) exist and be unique. To be injective and surjective, the attention shifts to \( B \). To be surjective means that \( B \) has an existence criterion (for every \( b \in B \), there exists some \( a \in A \) that maps to it). And to be injective means that \( B \) has a uniqueness-type criterion (for every \( b \in B \), there is at most one \( a \in A \) that maps to it).
            \bigbreak \noindent 
            A function $f:\  A \to B$ is \textit{bijective} if it is both injective and surjective.
            \bigbreak \noindent 
            Defining a function \( f : A \to B \) placed existence and uniqueness criteria on \( A \). If \( f \) is both injective and surjective, then this adds existence and uniqueness criteria to \( B \). Thus, if \( f \) is a bijection, then it has these criteria on both sides: Every \( a \in A \) is mapped to precisely one \( b \in B \), and every \( b \in B \) is mapped to by precisely one \( a \in A \). In effect, this pairs up each element of \( A \) with an element of \( B \); namely, \( a \) is paired with \( f(a) \) in this way.
        \item \textbf{Proving $x$jectiveness for $x\in \{\text{in,sur,bi}\}$}: Based on its definition, this is the outline to prove a function is injective.
            \bigbreak \noindent 
            \begin{mdframed}
                \textbf{Proposition}. $f:\ A \to B$ is an injection
                \bigbreak \noindent 
                \textbf{\textit{Proof.}} Assume $x,y \in A $, and $f(x) = f(y)$
                \begin{align*}
                    &\vdots \quad \text{ Apply algebra}, \\
                     &\vdots \quad \text{ logic, techniques}
                .\end{align*}
                Therefore, $x=y $
                \bigbreak \noindent 
                Since $f(x) =f(y)$ implies $x=y$, $f$ is injective $\quad \blacksquare $
            \end{mdframed}
            \bigbreak \noindent 
            Alternatively, one could use the contrapositive, which would mean one starts by assuming $x \ne y$, and then concludes that $f(x) \ne f(y)$.
            \bigbreak \noindent 
            Next, here’s the outline for a surjective proof.
            \begin{mdframed}
                \textbf{Proposition.} $f:\ A \to B$ is a surjection
                \bigbreak \noindent 
                \textbf{\textit{Proof.}} Assume $b\in B $
                \begin{align*}
                    &\vdots \quad \text{ Magic  to find an $a\in A$}\\
                    &\vdots \quad \text{ where $f(a) = b $}
                .\end{align*}
                Since every $b\in B$ has an $a\in A$ where $f(a) = b $, $f$ is surjective $\quad \blacksquare $
            \end{mdframed}
        \item \textbf{Proving jectiveness examples}
            \begin{itemize}
                \item \( f : \mathbb{R} \to \mathbb{R} \) where \( f(x) = x^2 \) is not injective, surjective, or bijective.
                \item \( g : \mathbb{R}^+ \to \mathbb{R} \) where \( g(x) = x^2 \) is injective, but not surjective or bijective.
                \item \( h : \mathbb{R} \to \mathbb{R}^+ \) where \( h(x) = x^2 \) is surjective, but not injective or bijective.
                \item \( k : \mathbb{R}^+ \to \mathbb{R}^+ \) where \( k(x) = x^2 \) is injective, surjective, and bijective.
            \end{itemize}
            \bigbreak \noindent 
            \textbf{\textit{Proof (part a).}} Observe that $f(-2) = f(2) =4 $, while $-2 \ne 2 $. Thus, $f$ is not injective. Next, notice that $f(x) = x^{2} > 0$. Thus, there is no such $a\in \mathbb{R}$ such that $f(a) = -4$. Since $-4$ is in the codomain and is not hit, $f$ is not surjective. Since $f$ is not both injective and surjective, it is therefore not bijective. 
            \bigbreak \noindent 
            \textbf{Part b}. Let $a_{1}, a_{2} \in \mathbb{R}^{+}$, assume $g(a_{1}) = g(a_{2})$. Thus,
            \begin{align*}
                a_{1}^{2} &= a_{2}^{2} \\
                \implies a_{1} &= \pm a_{2}
            .\end{align*}
            But, for all $a\in \mathbb{R}^{+}$, $a >0$. Thus, $a_{1} = a_{2}$ and $g $ is injective. Observe that again there is no such value in the domain of $g$ such that $g(x) = -4$. Since $-4$ is in the codomain of $g$, it is not surjective, and is therefore not bijective.
            \bigbreak \noindent 
            \textbf{Part c.} Observe that $h(-2) = h(2) = 4$, while $-2 \ne 2$. Thus, $ h$ is not injective. Further, let $b\in \mathbb{R}^{+}$, then
            \begin{align*}
                h(a) &= b \\
                \implies a^{2} &= b \\
                \implies a &= \pm b
            .\end{align*}
            But, the codomain is restricted to positive values, thus $a=b$ and $h $ is surjective. Since $h$ is not injective, it is not bijective.
            \bigbreak \noindent 
            \textbf{Part d.} Let $a_{1}, a_{2} \in \mathbb{R}^{+}$, assume $f(a_{1}) = f(a_{2}) $, which implies
            \begin{align*}
                a_{1}^{2} &= a_{2}^{2} \\
                \implies a_{1} &= \pm a_{2}
            .\end{align*}
            Again, since the domain is restricted to positive values, we have $a_{1} = a_{2}$ and $f$ is injective. Next, let $b\in \mathbb{R}^{+}$, then
            \begin{align*}
                f(a) &= b \\
                \implies a^{2} &= b \\
                \implies a &= \pm b
            .\end{align*}
            But since the codomain is restricted to positive values, $a=b$ and the function is surjective. Since the function is both onto and one-to-one, the function is bijective (invertible). $\quad \blacksquare $
        \item \textbf{Proving jectiveness example 2.} Show $f: (\mathbb{Z} \times \mathbb{Z}) \to (\mathbb{Z} \times \mathbb{Z})$, with $f(x,y) = (x+2y, 2x+3y) $ is a bijection.
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} First, we show injectiveness. Let $(a,b), (c,d) \in \mathbb{Z}^{2}$. Assume $f(a,b) = f(c,d) $. Thus,
            \begin{align*}
                &(a+2b,2a+3b) = (c+2d, 2c+3d) \\
                \implies &\begin{cases} a+2b &=c+2d \\ 2a+3b &=2c+3d \end{cases} \\
                \implies &\begin{cases} a+2b-2a-3b &=0 \\ 2a+3b-2c-3d &=0 \end{cases} 
            .\end{align*}
            We then solve this system,
            \begin{align*}
                \begin{array}{cccc|c} 1 & 2 & -1 & -2 & 0\\ 2 & 3 & -2 & -3 & 0 \end{array} \implies \begin{array}{cccc|c} 1 & 0 & -1 & 0 & 0 \\ 0 &1 & 0 & -1 & 0\end{array}
            .\end{align*}
            Which implies 
            \begin{align*}
                \begin{cases}
                    a &= c \\
                    b &=d
                \end{cases}
           \end{align*}
            As desired. Thus, $f$ is injective. Next, let $(c,d) \in \mathbb{Z}^{2}$. Require $f(a,b) = (c,d)$ for some $(a,b) \in \mathbb{Z}^{2} $. Thus,
            \begin{align*}
                &(a+2b, 2a+3b) = (c,d) \\
                \implies &\begin{cases} a + 2b &= c\\ 2a+3b &=d \end{cases}
            .\end{align*}
            Solving this system yields
            \begin{align*}
                \begin{array}{cc|c} 1 & 2 & c \\ 2 & 3 & d \end{array} \implies \begin{array}{cc|c} 1 & 0 & -3c + 2d \\0&1 &2c-d \end{array}
            .\end{align*}
            Thus, $(a,b) = (-3c +2d, 2c-d)$ and the function is surjective. Because the function is both injective and surjective, it is therefore bijective.
            \bigbreak \noindent 
            Alternatively, observe that $f:\ \mathbb{Z}^{2} \to \mathbb{Z}^{2}$, $f(x,y) = (x+2y, 2x+3y) $ is given by the matrix representation $A\vec{\mathbf{x}} = \vec{\mathbf{b}} $
            \begin{align*}
                \begin{pmatrix} 1 & 2\\2&3 \end{pmatrix}\begin{pmatrix} x \\ y \end{pmatrix} &= \begin{pmatrix} a \\b \end{pmatrix}
            .\end{align*}
            Thus, since $A$ is square, we can simply check its determinant. \footnote{Common linear algebra $W$} 
            \begin{align*}
                \det\begin{pmatrix} 1 & 2 \\ 2 & 3 \end{pmatrix} &= 1(2)-2(3) = -1
            .\end{align*}
            Since $\det(A) \ne 0$, the function is invertible
        \item \textbf{The func-y pigeonhole principal}:
            \bigbreak \noindent 
            \textbf{Theorem 8.10 (The func-y pigeonhole principal)}: Suppose $A$ and $B$ are finite sets and $f:\  A \to B$ is any function.
            \begin{enumerate}[label=(\alph*)]
                \item If $|A| > |B|$, then $f$ is not injective.
                \item If $|A| < |B|$, then $f$ is not surjective.
            \end{enumerate}
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} \textbf{Part (a).} Consider each element in \( A \) to be an object and each element of
            \( B \) to be a box. Given an \( a \in A \), place object \( a \) into box \( b \) if \( f(a) = b \). 
            Since there are more objects than boxes, by the pigeonhole principal at least one box has at least
            two objects in it. That is, \( f(a_1) = f(a_2) \) for some distinct \( a_1 \) and \( a_2 \), implying that
            \( f \) is not injective.
            \bigbreak \noindent 
            \textbf{Part (b).} Since \( f \) is a function, each \( a \in A \) is mapped to only one \( b \in B \). 
            Thus, \( k \) elements in \( A \) can map to at most \( k \) elements of \( B \). 
            And so the \( |A| \) elements in \( A \) can map to at most \( |A| \) elements in \( B \). 
            However, since \( |A| < |B| \), there must be some elements not hit, meaning that \( f \) is not surjective.
            \bigbreak \noindent 
            It is again useful to think about what the contrapositive tells us:
            \begin{itemize}
                \item[(a)] If \( f \) is injective, then \( |A| \leq |B| \).
                \item[(b)] If \( f \) is surjective, then \( |A| \geq |B| \).
            \end{itemize}
            Viewing the statements this way is beneficial for another reason: It demonstrates
            clearly that in order for \( f \) to be a bijection—meaning an injection and a surjection—we
            would need \( |A| = |B| \).
            \bigbreak \noindent 
            It is also worth mentioning that this theorem still holds true in the case that \( |A| \)
            and/or \( |B| \) are infinite.\footnote{But proving this to be the case would take us too far afield.}
        \item \textbf{The Composition}: Let \( A \), \( B \), and \( C \) be sets, \( g : A \to B \), and \( f : B \to C \). Then the
            composition function is denoted \( f \circ g \) and is defined as follows:
            \[
                (f \circ g) : A \to C \quad \text{where} \quad (f \circ g)(a) = f(g(a)).
            \]
            Suppose 
            \begin{align*}
                &g:\ \mathbb{R} \to \mathbb{R},\ g(x) = x+1 \\
                &f:\ \mathbb{R} \to \mathbb{R}^{+},\ f(x) = x^{2}
            .\end{align*}
            Then,
            \begin{align*}
                (f\circ g):\ \mathbb{R} \to \mathbb{R}^{+},\ (f\circ g)(x) = (x+1)^{2}
            .\end{align*}
        \item \textbf{Property of injective functions under composition}:
            \bigbreak \noindent 
            \textbf{Theorem 8.13}. Suppose $A, B$ and $C$ are sets, $g:\ A \to B$ is injective, and $f:\ B \to C$ is injective. Then $f \circ g$ is injective
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} Since $(f\circ g):\ A \to C$, to show that is an injection we must show that for all $a_{1}, a_{2}\in A$, $(f\circ g)(a_{1}) = (f\circ g)(a_{2}) $ implies $a_{1} = a_{2}$. Assume $a_{1}, a_{2} \in A$, and $(f\circ g)(a_{1}) = (f\circ g)(a_{2})$. Using the definition of the composition, we have
            \begin{align*}
                f(g(a_{1})) = f(g(a_{2}))
            .\end{align*}
            Since $f$ is injective, we know that for any $b_{1}, b_{2} \in B$, $f(b_{1}) = f(b_{2}) $ implies $b_{1} = b_{2}$. Since $g(a_{1}), g(a_{2}) \in B$, we have
            \begin{align*}
                g(a_{1}) = g(a_{2})
            .\end{align*}
            Likewise, since $g$ is injective, it must be that $a_{1}  = a_{2}$
            \bigbreak \noindent 
            Thus, we have shown that for any $a_{1}, a_{2} \in A$, if $(f\circ g)(a_{1}) = (f\circ g)(a_{2})$, then $a_{1} = a_{2}$. Therefore, $(f\circ g)$ is an injection. $\quad \blacksquare $
        \item \textbf{Property of surjective functions under composition}:
            \bigbreak \noindent 
            \textbf{Theorem 8.14}: Suppose $A$, $B$ and $C$ are sets, $g:\ A \to B$ is surjective, and $f:\ B \to C$ is surjective. Then $f \circ g$ is surjective.
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} Since $(f\circ g):\ A \to C$, to show that $f\circ g$ is surjective, we must show that for all $c\in C$, there exists some $a\in A$ such that $(f\circ g)(a) = c$. To start, since $f$ is surjective, then for all $c\in C$, there exists some $b\in B$ such that $f(b) = c$. Further, we know that $g$ is surjective. Thus, for all $b\in B$, there exists some $a\in A$ such that $g(a) = b$. 
            \bigbreak \noindent 
            Thus, for an arbitrary $c\in C$, we have found an $a\in A$ such that
            \begin{align*}
                (f\circ g)(a) = f(g(a)) = f(b) = c
            .\end{align*}
            Completing the proof $\quad \blacksquare $
        \item \textbf{A corollary from the above two results}: Suppose $A$, $B$ and $C$ are sets, $g:\  A \to B$ is bijective, and $f:\ B \to C$ is bijective. Then $f \circ g$ is bijective.
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} By Theorem 8.13, \( f \circ g \) is an injection. By Theorem 8.14, \( f \circ g \) is a surjection. Thus, by the definition of a bijection (Definition 8.7), \( f \circ g \) is a bijection.
        \item \textbf{Note about compositions}: Notice that in our definition of function composition (Definition 8.11) we had functions \( g \) and \( f \) where \( g : A \to B \), and \( f : B \to C \). Notice that we don’t really need the codomain of \( g \) to equal the domain of \( f \). If we had \( g : A \to B \) and \( f : D \to C \) where \( B \subseteq D \), that would be enough (for the definition, and for these last two theorems). As long as \( g(a) \) is a part of \( f \)’s domain, then \( f(g(a)) \) will make sense, which is all we need.
        \item \textbf{Identity function and invertibility}: For a set \( A \), the identity function on \( A \) is the function 
            \begin{align*}
                i_A : A \to A  \text{ where } i_A(x) = x  \text{ for every }  x \in A 
            \end{align*}
            \bigbreak \noindent 
            The inverse of a function \( f : A \to B \), if it exists, is the function \( f^{-1} : B \to A \) such that \( f^{-1} \circ f = i_A \) and \( f \circ f^{-1} = i_B \).
            \bigbreak \noindent 
            For example, if \( f : \mathbb{R} \to \mathbb{R} \) where \( f(x) = x + 1 \), then \( f^{-1} : \mathbb{R} \to \mathbb{R} \) is the function
            \( f^{-1}(x) = x - 1 \). To see this, simply note that
            \[
                (f \circ f^{-1})(x) = f(f^{-1}(x)) = f(x - 1) = (x - 1) + 1 = x
            \]
            and
            \[
                (f^{-1} \circ f)(x) = f^{-1}(f(x)) = f^{-1}(x + 1) = (x + 1) - 1 = x.
            \]
        \item \textbf{Arctan and the natural logarithm}: this is a great opportunity to mention a couple important functions — $\text{arctan}(x)$ and $\ln(x)$ — which are defined as the inverses to other important function.
            \begin{itemize}
                \item If \( \tan : (-\pi/2, \pi/2) \to \mathbb{R} \) is the tangent function, then its inverse is defined to be \( \arctan : \mathbb{R} \to (-\pi/2, \pi/2) \), and is called the arctangent function.\footnote{}
                \item If \( \exp : \mathbb{R} \to \mathbb{R}^+ \) is the exponential function (that is, \( \exp(x) = e^x \)), then its inverse is defined to be \( \ln : \mathbb{R}^+ \to \mathbb{R} \), and is called the natural logarithm function.
            \end{itemize}
        \item \textbf{When does an inverse exist}:
            \bigbreak \noindent 
            \textbf{Theorem}: A function $f:\ A \to B$ is invertible if and only if $f$ is a bijection.
            \bigbreak \noindent 
            \textbf{Proof.} First, suppose that \( f : A \to B \) is invertible. We will prove that \( f \) is both
            an injection and a surjection, which will prove that \( f \) is a bijection. To see that \( f \)
            is a surjection, choose any \( b \in B \). We aim to find an \( a \in A \) such that \( f(a) = b \). To 
            this end, let \( a = f^{-1}(b) \), which exists and is in \( A \) because \( f^{-1} : B \to A \). Now simply
            observe that the definition of an invertible function (Definition 8.16) implies
            \[
                f(a) = f(f^{-1}(b)) = b.
            \]
            This proves that \( f \) is a surjection.
            \bigbreak \noindent 
            To see that \( f \) is an injection, let \( a_1, a_2 \in A \) and assume \( f(a_1) = f(a_2) \). Note that 
            \( f(a_1) \) (and hence \( f(a_2) \), since they're equal) is an element of \( B \) due to the fact that 
            \( f : A \to B \). And so, since \( f^{-1} : B \to A \), we may apply \( f^{-1} \) to both sides:
            \[
                f(a_1) = f(a_2)
            \]
            \[
                f^{-1}(f(a_1)) = f^{-1}(f(a_2))
            \]
            \[
                a_1 = a_2,
            \]
            by the definition of the inverse. Thus, \( f \) is an injection. And since we already showed 
            that \( f \) is a surjection, it must be a bijection. This concludes the forward direction of 
            the theorem.
            \bigbreak \noindent 
            As for the backwards direction, assume that \( f \) is a bijection. For \( b \in B \), we will 
            now define \( f^{-1}(b) \) like this:
            \[
                f^{-1}(b) = a \quad \text{if} \quad f(a) = b.
            \]
            That is, we are defining \( f^{-1} \) to act as an inverse from \( B \) to \( A \) should act, without yet 
            claiming that \( f^{-1} \) is a function. Our goal now is to demonstrate that this definition 
            of \( f^{-1} \) satisfies the conditions to be a function, which would prove that \( f \) is invertible. 
            To do so, recall that to be a function there is an existence condition (\( f^{-1}(b) \) must be 
            equal to some \( a \in A \)) and a uniqueness condition (\( f^{-1}(b) \) must be equal to only one 
            \( a \in A \)). We will check these separately.
            \bigbreak \noindent 
            \textbf{Existence:} Let \( b \in B \). Since \( f \) is surjective, there must be some \( a \in A \) such that 
            \( f(a) = b \). Hence, by our definition of \( f^{-1} \), we have \( f^{-1}(b) = a \). We have shown that 
            for every \( b \in B \) there exists at least one \( a \in A \) for which \( f^{-1}(b) = a \), which concludes 
            the existence portion of this argument.
            \bigbreak \noindent 
            \textbf{Uniqueness:} Suppose \( f^{-1}(b) = a_1 \) and \( f^{-1}(b) = a_2 \), for some \( b \in B \) and \( a_1, a_2 \in A \). 
            By the definition of \( f^{-1} \), this means that \( f(a_1) = b \) and \( f(a_2) = b \). But since \( f \) is 
            injective, this means that \( a_1 = a_2 \). We have shown that \( f^{-1}(b) \) can not be equal to 
            two different elements of \( A \), which concludes the uniqueness portion of this argument.
            \bigbreak \noindent 
            Combined, these two parts show that \( f^{-1} : B \to A \) is a function, hence proving 
            that \( f \) is invertible.
            \bigbreak \noindent 
            We have proved the forwards and backwards directions of Theorem 8.17, which 
            completes its proof. \(\Box\)
        \item \textbf{The image and inverse image}: Let \( f : A \to B \) be a function, and assume \( X \subseteq A \) and \( Y \subseteq B \).
            The \textit{image} of \( A \) is
            \[
                f(X) = \{y \in B : y = f(x) \text{ for some } x \in X\},
            \]
            and the \textit{inverse image} of \( Y \) is
            \[
                f^{-1}(Y) = \{x \in A : f(x) \in Y\}.
            \]
        \item \textbf{The bijection principal}:
            \bigbreak \noindent 
            \textbf{principal (\textit{The bijection principal.})} Two sets have the same size if and only if there is a bijection between them.
        \item \textbf{Hilbert's hotel}: We begin by talking about the set of problems related to the so-called Hilbert’s Hotel. Assume that there is a hotel, called Hilbert’s Hotel, which has infinitely many rooms in a row.
            \bigbreak \noindent 
            \fig{.7}{./figures/20.png}
            \begin{itemize}
                \item Assume every room has someone in it, and so the “No Vacancy” sign has been
                    turned on. With most hotels, this would mean that if someone else arrives at
                    the hotel, they will not be given a room. But this isn’t the case with Hilbert’s
                    Hotel. If, for \( n \in \mathbb{N} \), the patron in room \( n \) moves to room \( n + 1 \), then nobody
                    is left without a room and suddenly room 1 is completely open! So the new
                    customer can go to room 1. We created a room out of nothing!
                \item Now imagine 2 people arrived at the hotel. Can we accommodate them?
                    Certainly! Now, just have everyone move from room \( n \) to room \( n + 2 \). This
                    leaves rooms 1 and 2 open to the newcomers, and we are again good-to-go.

                \item What if, however, we have infinitely many people lined up wanting a room?
                    Can we accommodate all of them? Yes! We still can! Just have the person in
                    room \( n \) move to room \( 2n \). Then all of the odd-numbered rooms are vacant and
                    the infinite line of people can take these rooms.
            \end{itemize}
            The first point of this exercise is to simply realize that weird stuff can happen
            when dealing with the infinite. The second point, though, is to realize that each time
            the people switched rooms, those same exact people got new rooms. So in the first
            example when they each just moved one room down, that should mean that there
            are just as many rooms from 1 to $\infty$ as there are from 2 to $\infty$. . . And likewise for the
            others.
        \item \textbf{Cardinality and infinite sets}:
            \bigbreak \noindent 
            \textbf{Example} There are the same number of natural numbers as there are natural
            numbers larger than 1 (that is, \( |\mathbb{N}| = |\{2, 3, 4, \dots \}| \)). What’s the bijection that shows
            this? Let
            \[
                f : \mathbb{N} \to \{2, 3, 4, \dots \} \quad \text{where} \quad f(n) = n + 1.
            \]

            In other (non-)words, this is the pairing
            \[
                1 \leftrightarrow 2 \quad 2 \leftrightarrow 3 \quad 3 \leftrightarrow 4 \quad 4 \leftrightarrow 5 \quad \dots
            \]
            \bigbreak \noindent 
            \textbf{The Moral}. Two sets can have the same size even though one is a proper subset of the other.
            \bigbreak \noindent 
            \textbf{Example}. There are the same number of natural numbers as even natural
            numbers (that is, \( |\mathbb{N}| = |2\mathbb{N}| \)). What’s the bijection that shows this? Let
            \[
                f : \mathbb{N} \to \{2, 4, 6, 8, \dots\} \quad \text{where} \quad f(n) = 2n.
            \]
            In other (non-)words, this is the pairing
            \[
                1 \leftrightarrow 2 \quad 2 \leftrightarrow 4 \quad 3 \leftrightarrow 6 \quad 4 \leftrightarrow 8 \quad \dots
            \]
            \textbf{The Moral.} Two sets can have the same size even though one is a proper subset of the other 
            and the larger one even has \textit{infinitely many more elements} than the smaller one.
            \bigbreak \noindent 
            And in a similar way, one can prove that \( |\mathbb{N}| = |\mathbb{Z}| \). Indeed, a bijection 
            \( f : \mathbb{N} \to \mathbb{Z} \) can be given by following this pattern:
            \[
                f(1) = 0, \quad f(2) = 1, \quad f(3) = -1, \quad f(4) = 2, \quad f(5) = -2, \quad f(6) = 3, \quad \dots
            \]
            \bigbreak \noindent 
            One way to write such a function is this:
            \[
                f : \mathbb{N} \to \mathbb{Z} \quad \text{where} \quad 
                f(n) = 
                \begin{cases} 
                    \frac{n}{2} & \text{if } n \text{ is even}; \\
                    -\frac{(n-1)}{2} & \text{if } n \text{ is odd}.
                \end{cases}
            \]


    \end{itemize}

    \pagebreak 
    \subsection{Relations}
    \begin{itemize}
        \item \textbf{Set partitions}: A partition of a set $A$ is a collection of non-empty subsets of $A$ for which each element of $A$ is in one and only one of the subsets.
            \bigbreak \noindent 
            Formally, a partition is a collection of non-empty sets $\{P_{i}\}_{i\in S} $ such that
            \begin{enumerate}
                \item $P_{i} \subseteq A $ for all $i$
                \item $\bigcup_{i\in S} P_{i} = A$
                \item $P_{i} \cap P_{j} = \varnothing$ for all $i\ne j $
            \end{enumerate}
            \bigbreak \noindent 
            A partition of \( \mathbb{Z} \) is the set of evens and the set of odds. Another partition of \( \mathbb{Z} \) is the positive integers, the negative integers, and \(\{0\}\). Another is the non-\(17\) integers and \(\{17\}\). Another is the five sets in the Mod-5 Property section on the previous page. And the simplest partition of \( \mathbb{Z} \) is simply \( \mathbb{Z} \) — a partition with only one part.
        \item \textbf{Index sets}: In the formal definition of a partition, $S$ is the index set that labels or indexes the subsets $P_{i}$ in the partition. 
            \bigbreak \noindent 
            $S$ can be any set (e.g., $N,\{1,2,...,n\}$, or any other index set), as long as it provides unique labels for each subset $P_{i}$
        \item \textbf{Equivalence Relations}: An \emph{equivalence relation} on a set \( A \) is an ordered relationship between pairs of elements of \( A \) for which the pair is either \emph{related} or is \emph{not related}. If \( a, b \in A \), we denote \( a \sim b \) if \( a \) is related to \( b \), and \( a \not\sim b \) if \( a \) is not related to \( b \).
            \bigbreak \noindent 
            For \( \sim \) to be an equivalence relation, it also must satisfy the following three properties:
            \begin{itemize}
                \item \textbf{Reflexive:} \( a \sim a \) for all \( a \in A \);
                \item \textbf{Symmetric:} If \( a \sim b \), then \( b \sim a \) for all \( a, b \in A \); and
                \item \textbf{Transitive:} If \( a \sim b \) and \( b \sim c \), then \( a \sim c \) for all \( a, b, c \in A \).
            \end{itemize}
            Lastly, if \( \sim \) is an equivalence relation and \( a \in A \), define the \emph{equivalence class} containing \( a \) to be the set
            \[
                \{ b \in A : a \sim b \}.
            \] 
        \item \textbf{Relations}: A relation on a set \( A \) is any ordered relationship between pairs of elements of \( A \) for which the pair is either \emph{related} or is \emph{not related}. If \( a, b \in A \), we denote \( a \sim b \) if \( a \) is related to \( b \), and \( a \not\sim b \) if \( a \) is not related to \( b \).
            \bigbreak \noindent 
            Lastly, if \( \sim \) is a relation and \( a \in A \), define the class containing \( a \) to be the set
            \[
                \{ b \in A : a \sim b \}.
            \]
        \item \textbf{Equivalence relations and partitions}:
            \bigbreak \noindent 
            \textbf{Theorem 9.5.} Assume $\sim$ is a relation on $A$. The relation $\sim$ partitions the elements of $A$ into classes if and only if $\sim$ is an equivalence relation.
            \bigbreak \noindent 
            Before we prove this theorem, we first define some notation. We denote the equivalence class of an element $a\in A$, $\{x\in A:\ a \sim x\} $ by $[a]$. 
            \bigbreak \noindent 
            Next, a lemma.
            \bigbreak \noindent 
            \textbf{Lemma 9.10}. Suppose $\sim$ is an equivalence relation on a set $A$, and let $a, b \in A$. Then,
            \begin{align*}
                [a] = [b] \text{ if and only if } a\sim b
            \end{align*}
            \bigbreak \noindent 
            \textbf{\textit{Proof of lemma 9.10}}. For the (straight)forward direction, assume that \([a] = [b]\). Observe that since \( \sim \) is reflexive, \( b \sim b \) and so \( b \in [b] \). And since \([a] = [b]\), this in turn means that \( b \in [a] \), which by Notation 9.9 implies \( a \sim b \). This concludes the forward direction.
            \bigbreak \noindent 
            As for the backward direction, we begin by assuming \( a \sim b \), and we aim to prove that \([a] = [b]\). This will be accomplished by demonstrating that \([a] \subseteq [b]\) and \([b] \subseteq [a]\). To prove the former, choose any \( x \in [a] \); we will show that \( x \in [b] \). By assumption we have \( a \sim b \), and because \( x \in [a] \) we have \( a \sim x \). That is,
            \[
                a \sim b \quad \text{and} \quad a \sim x.
            \]
            By the symmetry property of \( \sim \),
            \[
                b \sim a \quad \text{and} \quad a \sim x.
            \]
            By the transitivity property of \( \sim \),
            \[
                b \sim x.
            \]
            And so, by Notation 9.9,
            \[
                x \in [b].
            \]
            We have shown that \( x \in [a] \) implies \( x \in [b] \), and hence \([a] \subseteq [b]\).
            \bigbreak \noindent 
            The reverse direction is nearly the same. Let \( x \in [b] \), which means \( b \sim x \). Combining this, the transitivity of \( \sim \), and our assumption that \( a \sim b \), we get \( a \sim x \), which means \( x \in [a] \). And since \( x \in [b] \) implies \( x \in [a] \), we have \([b] \subseteq [a]\).
            \bigbreak \noindent 
            We have shown that \([a] \subseteq [b]\) and \([b] \subseteq [a]\), which proves that \([a] = [b]\). This concludes the backward direction, and hence the proof.
            \qed
            \bigbreak \noindent 
            We now proceed to the proof of theorem 9.5

        \item \textbf{Equivalence relation example 1}: Let $\sim$ be the relation on $\mathbb{R}$ where
            \begin{align*}
                a \sim b \text{ if } \floor{a} = \floor{b}
            \end{align*}
            We can verify that \( \sim \) is an equivalence relation by checking that it satisfies the three criteria. It is reflexive because certainly \( \floor{a}= \floor{a} \) for any \( a \in \mathbb{R} \); it is symmetric because if \( \floor{a} = \floor{b} \), then certainly \( \floor{b} = \floor{a} \); and it is transitive because if \( \floor{a} = \floor{b} \) and \( \floor{b} = \floor{c} \), then \( \floor{a} = \floor{c} \). Each of these is immediate because the equal sign already has these properties.
            \bigbreak \noindent 
            This means that the equivalence classes must then partition all of $\mathbb{R}$, and indeed they do.
            The class of all numbers that are equivalent to \( 12.4 \) is the set of numbers in the interval \([12, 13)\); that is, all numbers \( x \) such that \( 12 \leq x < 13 \). Indeed, the equivalence classes for \( \sim \) are all intervals of the form \([n, n+1)\) for \( n \in \mathbb{Z} \). 
            \bigbreak \noindent 
            Moreover, by Theorem 9.5 this means that the equivalence classes must then partition all of \( \mathbb{R} \), and they do: every \( x \in \mathbb{R} \) is in precisely one of these intervals:
            \[
                \ldots, [2, 3), [3, 4), [4, 5), [5, 6), [6, 7), \ldots.
            \]
            \qed
    \end{itemize}

    \pagebreak 
    \unsect{Elementary fields, groups, and rings}
    \begin{itemize}
        \item \textbf{Modular congruence and congruence classes}: Recall that two integers $a$ and $b$ are said to be congruent modulo $n$ if they leave the same remainder when divided by $n$. Mathematically, this is written as
            \begin{align*}
                a\equiv b \pmod{n}
            .\end{align*}
            Which means
            \begin{align*}
                n \mid a-b
            .\end{align*}
            When an integer $a$ is divided by $n$
            \begin{align*}
                a = q_{1}n + r_{1} \quad \text{ with} 0 \leq r_{1} < n
            .\end{align*}
            Similarly, for an integer $b$ divided by $n$
            \begin{align*}
                b = q_{2}n + r_{} \quad \text{ with} 0 \leq r_{2} < n
            .\end{align*}
            Subtracting $b$ from $a$
            \begin{align*}
                a - b &= (q_{1} - q_{2})n + (r_{1} - r_{2}) \tag{1}
            .\end{align*}
            If $n\mid (a-b)$, 
            \begin{align*}
                a-b=nk, \quad k\in \mathbb{Z}
            .\end{align*}
            By (1) above, we have
            \begin{align*}
                (q_{1} - q_{2})n + (r_{1} - r_{2}) &= kn
            .\end{align*}
            For this to hold, we require $r_{1}-r_{2}$ to be a multiple of $n$, since $q_{1} - q_{2}$ is already a multiple of $n$. Since $r_{1}, r_{2}$ satisfy $0 \leq r_{1}, r_{2} < n$. It must be that $-n < r_{1}- r_{2} < n$. In this case, for $n$ to divide $r_{1} - r_{2}$. It must be that
            \begin{align*}
                r_{1} - r_{2} &= 0 
            .\end{align*}
            Which implies $r_{1} = r_{2}$. Hence, $a$ and $b$ have the same remainder when divided by $n$ when $n\mid a-b$.
            \bigbreak \noindent 
            A congruence class modulo $n$ is the set of all integers that are congruent to a particular integer $a$ modulo $n$. This set is denoted as
            \begin{align*}
                [a]_{n} = \{x\in \mathbb{Z} \mid x \equiv a \pmod{n}\}
            .\end{align*}
            For example, $[0]_{3}$ is 
            \begin{align*}
                \{x\in \mathbb{Z}:\ x \equiv 0 \pmod{3}\} \\
            .\end{align*}
            Which is the integers $x$ such that $3 \mid x-0  $. In other words, it describes the set of integers that are divisible by 3.
            \bigbreak \noindent 
            The set $[1]_{3}$ is the set
            \begin{align*}
                [1]_{3} = \{x\in \mathbb{Z}:\ x \equiv 1 \pmod{3}\}
            .\end{align*}
            Which implies $3\mid x-1$, and thus $x = 3k + 1$, for $k\in \mathbb{Z}$. In words, it is the set of integers that leave a remainder of one when divided by three.
            \bigbreak \noindent 
            The modulus $n$ partitions the integers into $n$ distinct congruence classes:
            \begin{align*}
                [0]_{n}, [1]_{n}, ...,[n-1]_{n}
            .\end{align*}
            Every integer belongs to exactly one of these classes.
            \bigbreak \noindent 
            Arithmetic operations can be performed within the framework of congruence classes
            \begin{itemize}
                \item \textbf{Addition}: If $a\equiv b \pmod{n}$ and $c \equiv d \pmod{n}$, then  
                    \begin{align*}
                        a + c \equiv b + d \pmod{n}
                    .\end{align*}
                \item \textbf{Multiplication}: If $a\equiv b \pmod{n}$ and $c \equiv d \pmod{n}$, then  
                    \begin{align*}
                        ac \equiv bd \pmod{n}
                    .\end{align*}
            \end{itemize}
        \item \textbf{Groups}: A group is a collection of objects $G$, together with one operation $\oplus$, which has the following properties:
            \begin{itemize}
                \item \textbf{Associativity}: $a \oplus (b\oplus c) = (a\oplus b) \oplus c$
                \item \textbf{Identity}: There is an element $e\in G$ such that $e \oplus g = g \oplus e = g$ for all $g \in G $
                \item \textbf{Inverse}: For every $g\in G$, there exists $g^{-1} \in G$ such that $g \oplus g^{-1} = g^{-1}\oplus g = e $
            \end{itemize}
            For example, $\mathbb{Z}$ is a group under addition.
            \begin{itemize}
                \item \textbf{Associativity}: Two integers $a,b$ are associative, $a + (b + c)  = (a+b) +c $
                \item \textbf{Identity}: Zero is the identity element, since $0 \in \mathbb{Z}$ and $0 + a = a+ 0 = a $
                \item \textbf{Inverse}: $a + (-a) = (-a) + a = 0$
            \end{itemize}
            \textbf{Note:} A group is said to be \textit{abelian} if it is commutative under its operation. In other words, $x \oplus y = y\oplus x$ for all $x,y \in G $
        \item \textbf{Rings}: A ring is a set $R$, together with two operations $\oplus $ and $* $, which has the following properties
            \begin{itemize}
                \item $R$ is an abelian group under $\oplus $
                \item $R$ is associative under $*$
                % \item There exists an element 1 such that $r * 1 =  1 * r  = r $ for all $r\in R $
                \item The operation $*$ distributes over $\oplus$
                    \begin{align*}
                        a * (b\oplus c) &= (a*b) \oplus a * c \\
                        (a\oplus b) * c &= (a*c) \oplus (b*c)
                    .\end{align*}
            \end{itemize}
            For example, $\mathbb{Z}$ is a ring under addition and multiplication. First note that $\mathbb{Z}$ is an abelian group under addition. Further, for $a,b\in \mathbb{Z}$, $a\cdot b = b\cdot a$. 
            \bigbreak \noindent 
            $1\in \mathbb{Z}$ is the identity, $1 \cdot a = a \cdot 1 =a$ for all $a\in \mathbb{Z} $, and we know that multiplication distributes over addition
            \begin{align*}
                a \cdot (b+c) &= a\cdot b + a\cdot c \\
                (a+b) \cdot c &= a\cdot c + b\cdot c
            .\end{align*}
        \item \textbf{Fields}: A field is a set $F$, together with two operations $\oplus$ and $* $, which has the following properties
            \begin{itemize}
                \item $F$ is a commutative ring under $\oplus$ and $* $ 
                \item Every nonzero $f\in F$ has a multplicative inverse, that is, some element $g\in F$ for which
                    \begin{align*}
                        f*g = g*f = 1
                    .\end{align*}
            \end{itemize}
            The sets $\mathbb{Q}, \mathbb{R}$, and $\mathbb{C}$ under addition and multiplication are examples of fields. The set of integers $\mathbb{Z}$ is not. Although it is a commutative ring under addition and multiplication, not every element has a multiplicative inverse. For example, there is no such $a\in \mathbb{Z}$ such that $2 \cdot  g = 1 $
        \item \textbf{Vector spaces}: A vector space is a set of vectors $V$, together with a set of scalars $F$, with the following properties
            \begin{itemize}
                \item $V$ is a abelian group under vector addition
                \item  $F$ is a field under multiplication
                \item For each $s\in F$, and $\mathbf{v}\in V$, scalar multiplication gives a unique element $s\cdot \mathbf{v} \in V $
                \item Additional properties
                    \begin{align*}
                        1\mathbf{v} &= \mathbf{v} \\
                        a(b\mathbf{v}) &= (ab)\mathbf{v} \\
                        a(\mathbf{u} + \mathbf{v}) &= a\mathbf{u} + a\mathbf{v} \\
                        (a+b) \mathbf{v} &= a\mathbf{v} + b\mathbf{v}
                    .\end{align*}
            \end{itemize}



    \end{itemize}


    \pagebreak 
    \unsect{Combinatorics}
    \bigbreak \noindent 
    \subsection{Introduction}
    \begin{itemize}
        \item \textbf{What is combinatorics?}: Combinatorics is a collection of techniques and a language for the study of finite or countably infinite discrete structures. Given a set of elements and possibly some structure on that set, typical questions are
            \begin{itemize}
                \item Does a specific arrangement of the elements exists?
                \item How many such arrangemets are there?
                \item What properties do these arrangements have?
                \item Which one of the arrangemetns is maximal, minimal, or optimal according to some criterion?
            \end{itemize}
        \item \textbf{Counting the number of subsets for a set}: Let $[n] = \{1,2,...,n\} $, and let $f(n)$ be the number of subsets of $[n]$. Then $f(n) = 2^{n}$. For any particular subset of $[n]$, each element is either in that subset or not. Thus, to construct a subset, we have to make one of two choices for each element of $[n]$. Furthermore, these choices are independent of each other. Hence, the total number of choices, and consequently the total number of subsets is 
            \begin{align*}
                \underbrace{2\times2\times...\times2}_{n} = 2^{n}
            .\end{align*}
        \item \textbf{Number of subsets without consecutive integers}: For a sequence $[n] = \{1,...,n\}$ we can count the number of subsets given by $f(n)$, that do not contain consecutive integers with the recurrence relation
            \begin{align*}
                f(n) = f(n-1) + f(n-2)
            .\end{align*}
            We consider two cases
            \begin{enumerate}
                \item $n$ in not included in the subsets
                \item $n$ is included in the subsets. In this case, we build the subsets considering the subsequence $[n-2]  = \{1,...,n-2\}$. Note that if we include $n$, we must exclude $n-1$, because $n-1$ and $n$ are consecutive, this will become cleare in the upcoming example.
            \end{enumerate}
            \bigbreak \noindent 
            Consider the sequence $[n] = \{1,2,3,4\}$. By the relation above, 
            \begin{align*}
                f(4) &= f(3) + f(2) 
            .\end{align*}
            Before we are able to compute this, we must define our base cases. 
            \begin{align*}
                f(n) &= \begin{cases}
                    3 & \text{if } n = 2 \\
                    2 & \text{if } n =1
                \end{cases}
            .\end{align*}
            If $n=2$, we have $\{1,2\}$, and the allowed subsets are $\varnothing, \{1\}, \{2\} $. If we have $n=1$, the subsets are $\{\varnothing, \{1\}\} $. Thus
            \begin{align*}
                f(4) &= f(3) + f(2) = f(2) + f(1) + f(2)  \\
                     &= 3 + 2 + 3 = 8               
            .\end{align*}
            Let's explicitly break up the given sequence so we can see whats going on. In the first case, $n$ is excluded, thus the sequence becomes $\{1,2,3\}$. If $n$ is included, the sequence becomes $\{1,2\}$, where we build the subsets of $\{1,2\}$, and then add 4 to each one. Thus,
            \begin{align*}
                \{1,2,3\} + \{1,2\} &= \{1,2,3\}  + \varnothing + \{1\} + \{2\} \\
                                    &= \{1,2,3\} + \{4\} + \{1,4\} + \{2,4\}
            .\end{align*}
            Since the sequence $\{1,2,3\}$ in not a base case, we must split this one up aswell, we have
            \begin{align*}
                \{1,2,3\} &= \{1,2\} + \{1\}  + \{4\} + \{1,4\} + \{2,4\} \\
                          &=  \varnothing + \{1\} + \{2\} + \varnothing + \{1\} + \{4\} + \{1,4\} + \{2,4\} \\
                          &= \varnothing + \{1\} + \{2\} + \{3\} + \{1,3\} + \{4\} + \{1,4\} + \{2,4\} 
            .\end{align*}
            Thus, we conclude all "good" subsets of $[n]$ either have $n$ or don't have $n$. The ones that don't have $n$ are exactly the "good" subsets of $[n-1]$. The "good" subsets of $[n]$ that include $n$ are exactly the "good" subsets of $[n-2]$ together with $n$. Thus $f(n) = f(n-1) + f(n-2) $ $\blacksquare$
            
    \end{itemize}

    \pagebreak 
    \subsection{Induction and recurrence relations}
    \begin{itemize}
        \item \textbf{Principal of Mathematical Induction}: Given an infinite sequence of propositions
            \begin{align*}
                P_{1}, P_{2}, P_{3},...P_{n},...,
            .\end{align*}
            In order to prove that all of them are true, it is enough to show two things
            \begin{enumerate}
                \item \textbf{The base case:} $P_{1}$ is true
                \item \textbf{The inductive step}: For all positive integers $k$, if $P_{k}$ is true, then so is $P_{k+1}$
            \end{enumerate}
            \bigbreak \noindent 
            \textbf{Example}: Show that 
            \begin{align*}
                1 + 2 + 3 + ... + n = \frac{n(n+1)}{2}
            .\end{align*}
            \textbf{Base case}:
            \begin{align*}
                1 &= \frac{1(1+1)}{2} = \frac{2}{2} = 1
            .\end{align*}
            \bigbreak \noindent 
            \textbf{Inductive step}: $P_{k}$ is given by
            \begin{align*}
                1 + 2 + 3 + ... + k = \frac{k(k+1)}{2}
            .\end{align*}
            \bigbreak \noindent 
            $P_{k+1}$ is given by
            \begin{align*}
                1 + 2 + 3 + ... + k + k+1 = \frac{k+1(k+2)}{2}
            .\end{align*}
            If $1+2+3+...+k  = \frac{k(k+1)}{2}$, then
            \begin{align*}
                1 + 2 + 3 + ... + k + k+1 &= \frac{k+1(k+2)}{2} \\
                \frac{k(k+1)}{2} + k + 1 &= \frac{k+1(k+2)}{2} \\
                \frac{k(k+1) + 2k + 2}{2} &= \frac{k^{2} + 3k + 2}{2} \\
                \frac{k^{2} + 3k  + 2}{2} &= \frac{k^{2} + 3k + 2}{2}
            .\end{align*}
            Thus, we have showed that $P_{k} \implies P_{k+1}$ $\blacksquare$.
            \bigbreak \noindent 
            \textbf{Note}: Our aim is not to directly prove $P_{k+1}$, but to prove that $P_{k}$ implies $P_{k+1}$. In the inductive step we assume $P_{k}$ to be true, then show under this assumption, $P_{k+1}$ is also true.
        \item \textbf{Understanding gauss's formula for the sum of the first $n$ natural numbers}: Suppose we want to find the sum $1+2+3+...+(n-1)+n$. We could have discovered the formula that we proved above by first writing the sum twice
            \begin{align*}
                &1 + 2  + 3 + ... + (n-1) + n \\
                &n + (n-1) + (n-2) + ... + 2 + 1
            .\end{align*}
            The sum of the two numbres in each column is $n+1$, and there are $n$ columns, so the total sum is $n(n+1)$, it then follows that the actual sum is $\frac{1}{2}n(n+1)$
        \item \textbf{Trianglular numbers}: The sequence of integers
            \begin{align*}
                &1
                &3 = 1+2 \\
                &6 = 1 + 2  + 3 \\
                &10 = 1 + 2 + 3 + 4 \\
                &15  = 1 + 2 + 3 + 4 + 5  \\
                &...
            .\end{align*}
            Are called \textit{triangular numbers}. If you were to make a triangle of dots out of the sum, where the highest number is the base, the second highest is the layer ontop of the base, etc, you would form a triangle.
        \item \textbf{Strong induction}: Given an infinite sequence of propositions
            \begin{align*}
                P_{1}, P_{2}, P_{3}, ..., P_{n}
            .\end{align*}
            In order to demonstrate that all of them are true, it is enough to know two things.
            \begin{enumerate}
                \item \textbf{The base case}: $P_{1}$ is true
                \item \textbf{The inductive step}: For all integers $k \geq 1$, if $P_{1}, P_{2}, P_{3},...,P_{k}$ are true, then so is $P_{k+1}$
            \end{enumerate}
        \item \textbf{Pingala-fibonacci numbers}: Define a sequence of positive integers as follows: $F_{0} = 0, F_{1} = 1$, and for $n=2,3,... $ we have
            \begin{align*}
                F_{n} = F_{n-2} + F_{n-1}
            .\end{align*}
            This sequence is also known as \textit{the fibonacci sequence}.
        \item \textbf{Lucas numbers}: Change the initial values on the fibonacci sequence. Let $L_{0} = 2, L_{1} = 1$, and $L_{n} = L_{n-2} + L_{n-1}$. Then, we get the \textit{Lucas numbers}
            \begin{align*}
                2,1,3,4,7,11,18,29,47,...
            .\end{align*}
        \begin{align*}
            \mathcal{L}
        .\end{align*}

    \end{itemize}

    \pagebreak 
    \unsect{Axiomatic geometry}
    \subsection{Euclids elements and the question of parallels}
    \begin{itemize}
        \item \textbf{Mathematical axioms and postulates}: Axioms are general truths or statements accepted without proof. Postulates are assumptions specific to a particular mathematical framework, often geometry. They serve as starting points for reasoning within that system.
            \bigbreak \noindent 
            In short, axioms are universal truths in mathematics. Postulates are subject-specific assumptions.
        \item \textbf{Euclids definitions}:
            \begin{enumerate}
                \item \textbf{Point:} That which has no part.
                \item \textbf{Line:} Breadthless length.
                \item The ends of a line are points.
                \item \textbf{Straight line:} A line which lies evenly with the points on itself.
                \item \textbf{Surface:} That which has length and breadth only.
                \item The edges of a surface are lines.
                \item \textbf{Plane surface:} A surface which lies evenly with the straight lines on itself.
                \item \textbf{Angle:} The inclination to one another of two lines in a plane which meet one another and do not lie in a straight line.
                \item \textbf{Right angle:} When a straight line set up on another straight line makes the adjacent angles equal to one another, each of the equal angles is a right angle.
                \item \textbf{Perpendicular:} A straight line standing on another straight line to form right angles with it.
                \item \textbf{Obtuse angle:} An angle greater than a right angle.
                \item \textbf{Acute angle:} An angle less than a right angle.
                \item \textbf{Boundary:} That which is the extremity of anything.
                \item \textbf{Figure:} That which is contained by any boundary or boundaries.
                \item \textbf{Circle:} A plane figure contained by one line (the circumference) such that all straight lines falling upon it from one point among those lying within the figure are equal to one another.
                \item \textbf{Center of a circle:} The point from which all straight lines drawn to the circumference are equal.
                \item \textbf{Diameter of a circle:} Any straight line drawn through the center and terminated in both directions by the circumference.
                \item \textbf{Semicircle:} The figure contained by the diameter and the circumference cut off by it. The center of the semicircle is the same as that of the circle.
                \item \textbf{Segment of a circle:} The figure contained by a straight line and the circumference it cuts off.
                \item \textbf{Rectilineal figure:} A figure contained by straight lines.
                \item \textbf{Trilateral figure:} A rectilineal figure contained by three straight lines (a triangle).
                \item \textbf{Quadrilateral figure:} A rectilineal figure contained by four straight lines.
                \item \textbf{Multilateral figure (polygon):} A rectilineal figure contained by more than four straight lines.
                \item \textbf{Equilateral triangle:} A triangle with three equal sides.
                \item \textbf{Isosceles triangle:} A triangle with two equal sides.
                \item \textbf{Scalene triangle:} A triangle with three unequal sides.
                \item \textbf{Right-angled triangle:} A triangle with one right angle.
                \item \textbf{Obtuse-angled triangle:} A triangle with one obtuse angle.
                \item \textbf{Acute-angled triangle:} A triangle with three acute angles.
                \item \textbf{Parallel lines:} Straight lines which, being in the same plane and being produced indefinitely in both directions, do not meet one another in either direction.
            \end{enumerate}
        \item \textbf{Euclids postulates}
            \begin{enumerate}
                \item To draw a straight line from any point to any point
                \item To produce a finite straight line continuously in a straight line
                \item To describe a circle with any center and distance
                \item That all right angles are equal to one another
                \item That, if a straight line falling on two straight lines makes the interior angles on the same side less than two right angles, the two straight lines, if produced indefinitely, meet on that side on which are the angles less than the two right angels
            \end{enumerate}
        \item \textbf{Euclids axioms}:
            \begin{enumerate}
                \item Things which are equal to the same thing are also equal to one another
                \item If equals be added to equals, the wholes are equal
                \item If equals be subtracted from equals, the remainders are equal.
                \item Things which coincide with one another are equal to one another
                \item The whole is greater than the part
            \end{enumerate}
        \item \textbf{Definitions rephrased}:
            \begin{enumerate}
                \item \textbf{Point:} A location that has no size or dimension.
                \item \textbf{Line:} A one-dimensional object that has length but no width.
                \item \textbf{Endpoints of a line:} The points where a line begins or ends.
                \item \textbf{Straight line:} A line that does not curve and lies evenly between its endpoints.
                \item \textbf{Surface:} A two-dimensional object that has length and width but no thickness.
                \item \textbf{Edges of a surface:} The boundaries of a surface are lines.
                \item \textbf{Plane surface:} A flat surface where any straight line connecting two points on it lies entirely on the surface.
                \item \textbf{Angle:} The measure of the inclination or separation between two lines that meet at a point but are not aligned.
                \item \textbf{Right angle:} An angle formed when one line meets another to create two equal angles (90 degrees each).
                \item \textbf{Perpendicular lines:} Two lines that meet to form a right angle.
                \item \textbf{Obtuse angle:} An angle larger than a right angle (greater than 90 degrees).
                \item \textbf{Acute angle:} An angle smaller than a right angle (less than 90 degrees).
                \item \textbf{Boundary:} The edge or limit of an object.
                \item \textbf{Figure:} A shape that is enclosed by boundaries.
                \item \textbf{Circle:} A shape where all points on the boundary (the circumference) are the same distance from a central point.
                \item \textbf{Center of a circle:} The point that is equidistant from every point on the circle’s boundary.
                \item \textbf{Diameter of a circle:} A straight line passing through the center of a circle that touches the boundary on both sides.
                \item \textbf{Semicircle:} Half of a circle, defined by dividing a circle along its diameter.
                \item \textbf{Segment of a circle:} A region of a circle bounded by a chord (a straight line) and the arc it cuts off.
                \item \textbf{Polygon (rectilinear figure):} A shape enclosed by straight lines.
                \item \textbf{Triangle:} A polygon with three sides.
                \item \textbf{Quadrilateral:} A polygon with four sides.
                \item \textbf{Polygon (multilateral figure):} A shape with more than four sides.
                \item \textbf{Equilateral triangle:} A triangle where all three sides are equal in length.
                \item \textbf{Isosceles triangle:} A triangle where two sides are equal in length.
                \item \textbf{Scalene triangle:} A triangle where all three sides are of different lengths.
                \item \textbf{Right triangle:} A triangle with one right angle (90 degrees).
                \item \textbf{Obtuse triangle:} A triangle with one obtuse angle (greater than 90 degrees).
                \item \textbf{Acute triangle:} A triangle where all angles are acute (less than 90 degrees).
                \item \textbf{Parallel lines:} Two straight lines in the same plane that, no matter how far extended, will never meet
            \end{enumerate}
        \item \textbf{Postulates rephrased}
            \begin{enumerate}
                \item It is possible to draw a straight line connecting any two points.
                \item A finite straight line can be extended indefinitely in a straight line.
                \item A circle can be drawn with any center and any radius.
                \item All right angles are equal to each other.
                \item If a straight line intersects two straight lines such that the interior angles on one side add up to less than two right angles, then the two straight lines, if extended indefinitely, will meet on the side where the angles are less than two right angles.
            \end{enumerate}
        \item \textbf{Axioms rephrased}:
            \begin{enumerate}
                \item Things equal to the same thing are equal to each other.
                \item If equals are added to equals, the results are equal.
                \item If equals are subtracted from equals, the remainders are equal.
                \item Things that overlap or coincide exactly are equal.
                \item The whole is greater than any of its parts.
            \end{enumerate}
        \item \textbf{More on Euclids 5th postulate}: Unlike the other four postulates, the 5th postulate is more complex and less intuitive. It essentially describes the behavior of parallel lines, but its wording led mathematicians to wonder if it could be derived from the other postulates.
            \bigbreak \noindent 
            For centuries, mathematicians like Proclus, Ptolemy, and others tried to prove the 5th postulate as a theorem based on the other four postulates. These attempts were unsuccessful, as the postulate is independent.
            \bigbreak \noindent 
            In the 19th century, mathematicians like Lobachevsky, Bolyai, and Gauss explored what happens if the 5th postulate is replaced with different assumptions. This led to the development of non-Euclidean geometries:
            \begin{itemize}
                \item \textbf{Hyperbolic geometry:} There are infinitely many parallel lines through a point not on a given line.
                \item \textbf{Elliptic geometry:} No parallel lines exist.
            \end{itemize}
            The questioning of the 5th postulate revolutionized mathematics, leading to a broader understanding of geometry and the realization that Euclidean geometry is just one of many possible systems.
            \bigbreak \noindent 
            Observe Euclids 5th postulate
            \bigbreak \noindent 
            \fig{.2}{./figures/21.png}
        \item \textbf{ Playfair's Postulate}: Is an equivalent form of Euclid's 5th postulate which states 
            \begin{quote}
                "Through a given point not on a line, there is exactly one line parallel to the given line"
            \end{quote}
    \end{itemize}

    \pagebreak 
    \subsection{Five examples}
    \begin{itemize}
        \item \textbf{The Euclidean plane}: The Euclidean plane is a two-dimensional geometric space that forms the foundation of Euclidean geometry, as described in Euclid's Elements. It is characterized by the following properties
            \begin{enumerate}
                \item \textbf{Flat Surface:} The Euclidean plane is flat, meaning it has no curvature.
                \item \textbf{Points and Lines:} It consists of an infinite set of points. Straight lines can be drawn to connect any two points, and these lines extend infinitely in both directions.
                \item \textbf{Distance and Angles:} Distance between points is measured using the Euclidean distance formula. Angles are measured in degrees or radians.
                \item \textbf{Postulates:} The plane follows Euclid’s postulates, including the 5th (parallel postulate), which ensures the uniqueness of parallel lines.
                \item \textbf{Coordinate Representation:} Often represented using the Cartesian coordinate system, where every point is defined by an ordered pair $(x,y)$
                \item \textbf{Dimensions:} It has two dimensions: length and width.
            \end{enumerate}
            Note that the 2-dimensional cartesian plane is a mathematical representation of the Euclidean plane using a coordinate system.
            The Euclidean plane is a more general geometric concept, while the Cartesian plane provides a numerical framework (coordinates) for working with Euclidean geometry. In practical applications, the Cartesian plane is often used to model the Euclidean plane
            \bigbreak \noindent 
            Let $\mathbb{E}$ denote the Euclidean plane. 
            \bigbreak \noindent 
            \textbf{Coordinates}: The points in $\mathbb{E}$ are in one-to-one correspondence with the ordered pairs of real numbers. Each point $A$ corresponds to a pair of real numbers $(x,y)$, called the \textit{coordinates} of $A$, where the pair is assigned in the familiar way
            \bigbreak \noindent 
            \begin{figure}[ht]
                \centering
                \incfig{e1}
                \label{fig:e1}
            \end{figure}
            We often identify $A$ with its pair of coordinates $(x,y)$
            \bigbreak \noindent 
            \textbf{Equations of lines}: Each \textit{nonvertical line} $\ell$ in $\mathbb{E}$ consists of all points $(x,y)$, where $y = mx + b$ for some fixed $m$ and $b$. each \textit{vertical line} $\ell$ consists of all $(x,y)$, where $x=a$ for some fixed $a$
            \bigbreak \noindent 
            For any two points $A(x_{1}, y_{1})$ and $B(x_{2}, y_{2})$, the \textit{slope} of the line $\ell$ through $A$ and $B$ is 
            \begin{align*}
                m  = \frac{y_{2} - y_{1}}{x_{2} - x_{1}} \quad (\text{if } x_{2} \ne x_{1})
            \end{align*}
            And an equation for $\ell$ is given by 
            \begin{align*}
                y-y_{1} = m(x - x_{1}) \quad (\text{if } x_{2} \ne x_{1})
            \end{align*}
            The \textit{Euclidean distance} $e(AB)$ between $A$ and $B$ satisfies the formula 
            \begin{align*}
                e(AB) = \sqrt{(x_{2} - x_{1})^{2} + (y_{2} - y_{1})^{2}}
            \end{align*}
            \begin{figure}[ht]
                \centering
                \incfig{e2}
                \label{fig:e2}
            \end{figure}
            \bigbreak \noindent 
            Then 
            \begin{align*}
                (e(AB))^{2} &= (x_{2} -x_{1})^{2} + (y_{2} - y_{1})^{2} \\
                e(AB) &= \sqrt{(x_{2} - x_{1})^{2} + (y_{2} - y_{1})^{2}}
            \end{align*}
        \item \textbf{More on Euclidean distance}
            \bigbreak \noindent 
            \textbf{Proposition 1.1} If $A(x_{1}, y_{1})$ and $B(x_{2}, y_{2}) $ are on the line $y=mx+b$, then $e(AB) = \abs{x_{1} - x_{2}}\sqrt{m^{2} + 1}$
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} Assume $A(x_{1}, y_{1})$ and $B(x_{2}, y_{2}) $ are on the line $y=mx+b$, and $e(AB) = \sqrt{(x_{2} - x_{1})^{2} + (y_{2} - y_{1})^{2}}$. Observe that the slope $m$ of the line is given by
            \begin{align*}
                m = \frac{y_{2} - y_{1}}{x_{2} - x_{1}}
            \end{align*}
            Which implies 
            \begin{align*}
                y_{2} - y_{1} = m(x_{2} - x_{1})
            \end{align*}
            Plugging this expression for $y_{2} -y_{1}$ into $e(AB)$ yields
            \begin{align*}
                e(AB) &= \sqrt{(x_{2} - x_{1})^{2} + (m(x_{2}-x_{1}))^{2}} \\
                      &= \sqrt{(x_{2} - x_{1})^{2} + (m^{2}(x_{2}-x_{1})^{2})} \\
                      &= \sqrt{(x_{2}-x_{1})^{2}[1 + m^{2}]} \\
                      &= \sqrt{(x_{2} - x_{1})^{2}} \cdot \sqrt{m^{2} + 1} \\
                      &= \abs{x_{2} - x_{1}} \sqrt{m^{2} + 1} 
            \end{align*}
            As desired \hspace*{\fill}$\blacksquare$ 
        \item \textbf{The Minkowski plane, or taxicab plane}: Let $\mathbb{M}$ denote the Minkowski plane. $\mathbb{M}$ has the same points, lines, and coordinates as $\mathbb{E}$, but distance is different. For any $A(x_{1}, y_{1})$ and $B(x_{2}, y_{2})$, define the \textit{Minkowski distance} $d_{\mathbb{M}}$ as
            \begin{align*}
                d_{\mathbb{M}} = \abs{x_{2} - x_{1}} + \abs{y_{2} - y_{1}}
            \end{align*}
            Thus, the \textit{Minkowski distance} $d_{\mathbb{M}}(AB)$ is defined as the sum of the horizontal and vertical "ordinary distances"
            \bigbreak \noindent 
            For example, consider $A(1,2), B(-1,-3)$, then 
            \begin{align*}
                d_{\mathbb{M}}(AB) &= \abs{-1-1} + \abs{-3-2} = 7
            \end{align*}
        \item \textbf{More on Minkowski distance}:
            \bigbreak \noindent 
            \textbf{Proposition 1.2} If $A(x_{1}, y_{1})$ and $B(x_{2}, y_{2}) $ are on the line $y=mx+b$, then $d_{\mathbb{M}}(AB) = \abs{x_{1} - x_{2}}(1+\abs{m})$
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} Assume $A(x_{1}, y_{1})$ and $B(x_{2}, y_{2}) $ are on the line $y=mx+b$, and $d_{\mathbb{M}} = \abs{x_2-x_1} + \abs{y_{2} - y_{1}}$. Observe that the slope $m$ of the line is given by
            \begin{align*}
                m = \frac{y_{2}-y_{1}}{x_{2} - x_{1}}
            \end{align*}
            Which implies 
            \begin{align*}
                y_{2} - y_{1} = m(x_{2} - x_{1})
            \end{align*}
            Plugging this expression for $y_{2} -y_{1}$ into $d_{\mathbb{M}}$ yields
            \begin{align*}
                d_{\mathbb{M}} &= \abs{x_{2} - x_{1}} + \abs{y_{2} - y_{1}} \\
                               &= \abs{x_{2} - x_{1}} + \abs{m(x_{2} - x_{1})} \\
                               &= \abs{x_{2} - x_{1}} + \abs{m}\abs{x_{2} - x_{1}} \\
                               &= \abs{x_{2} - x_{1}}(1 + \abs{m}) \\
                               &= \abs{-(x_{1} - x_{2})} (1 + \abs{m}) \\
                               &= \abs{-1}\abs{x_{1} - x_{2}} (1 + \abs{m}) \\
                               &= \abs{x_{1} - x_{2}} (1 + \abs{m}) \\
            \end{align*}
            As desired \hspace*{\fill} $\blacksquare$
        \item \textbf{The spherical plane}: Let $\mathbb{S}(r)$ denote the surface of the sphere of radius $r$; that is, the \textit{spherical plane}.
            \bigbreak \noindent 
            Once $r$ is fixed, we shorten the notation to $\mathbb{S}$. We shall assume that our spheres are centered at the origin $(0,0,0)$ in three-dimensional space. Then $\mathbb{S}$ is the set of all $(x,y,z)$ such that $x^{2} + y^{2} + z^{2} = r^{2} $. Points are as usual, and lines on $\mathbb{S}$ are defined to be the \textit{great circles}. A great circle is the intersection of the sphere with a plane that cuts the sphere in half. Then, any two points have a unique line joining them, unless they are oppoosite (antipodes). In this case, they have infinitely many lines joining them.
            \bigbreak \noindent 
            \textbf{Distance in $\mathbb{S}$}: For points $A,B$ on $\mathbb{S}$, define distance
            \begin{align*}
                d_{\mathbb{S}}(AB) = &\text{ length of the minor (shorter) arc of the} \\
                                     &\text{ great circle (line) through $A$ and $B$}
            \end{align*}
            To compute $d_{\mathbb{S}}(AB)$ more easily, we must recall the forumula for the \textit{arc length in a circle of radius $r$}. Let $\theta$ be the radian measure of $\angle POQ$. The angle that sweeps out the full circle has measure $2\pi$, and the circumference is $2\pi r $. The sector formed by $\angle POQ$ makes up $\frac{\theta}{2\pi}$ of the full circle, so
            \begin{align*}
                \text{arc length } PQ = \frac{\theta }{2\pi} \cdot 2\pi r = \theta  r
            \end{align*}
            \bigbreak \noindent 
            An explicit formula for the spherical distance between two points, in terms of their coordinates, is given next. It follows from the distance formula for three-dimensional space and the Law of Cosines.
            \bigbreak \noindent 
            If $P(a,b,c)$ and $Q(x,y,z)$ are points on the surface of the sphere of radius $r$ centered at $(0,0,0)$ then
            \begin{align*}
                d_{\mathbb{S}} = r\cos^{-1}{\left(\frac{ax+by+cz}{r^{2}}\right)}
            \end{align*}
            \bigbreak \noindent 
            First, recall the law of cosines
            \begin{remark}
               \textit{(Law of Cosines.)} 
            \end{remark}
            \begin{figure}[ht]
                \centering
                \incfig{loc}
                \label{fig:loc}
            \end{figure}
            \bigbreak \noindent 
            In trigonometry, the \textbf{law of cosines} (also known as the \textit{cosine formula} or \textit{cosine rule}) relates the lengths of the sides of a triangle to the cosine of one of its angles. For a triangle with sides $a$, $b$, and $c$, opposite respective angles $\alpha$, $\beta$, and $\gamma$ (see Fig.~1), the law of cosines states:
            \[
                c^2 = a^2 + b^2 - 2ab \cos \gamma,
            \]
            \[
                a^2 = b^2 + c^2 - 2bc \cos \alpha,
            \]
            \[
                b^2 = a^2 + c^2 - 2ac \cos \beta.
            \]
            The law of cosines generalizes the Pythagorean theorem, which holds only for \textbf{right triangles}: if $\gamma$ is a right angle then $\cos \gamma = 0$, and the law of cosines reduces to:
            \[
                c^2 = a^2 + b^2.
            \]
            The law of cosines is useful for solving a triangle when all three sides or two sides and their included angle are given. $\qed $
            \bigbreak \noindent 
            Consider the points $P(a,b,c)$ and $Q(x,y,z)$ and the line (great circle) connecting them
            \bigbreak \noindent 
            \begin{figure}[ht]
                \centering
                \incfig{gc}
                \label{fig:gc}
            \end{figure}
            \bigbreak \noindent 
            Let $d$ be the Euclidean distance $PQ$ and $\theta$ be the radian measure of $\angle POQ$. By the law of cosines,
            \begin{align*}
                d^{2} &= r^{2} + r^{2} - 2r^{2}\cos{\left(\theta \right)} \\
                \implies \cos{\left(\theta \right)} &= \frac{d^{2} - r^{2} - r^{2}}{-2r^{2}} = \frac{d^{2} - 2r^{2}}{-2r^{2}} = \frac{2r^{2}-d^{2}}{2r^{2}}
            \end{align*}
            The Euclidean distance $d$ is given by
            \begin{align*}
                d &= \sqrt{(x-a)^{2} + (y-b)^{2} + (z-c)^{2}} \\
                  &= \sqrt{x^{2} + y^{2} + z^{2} + a^{2} + b^{2} + c^{2} - 2ax -2by - 2cz}
            \end{align*}
            Thus,
            \begin{align*}
                \cos{\left(\theta \right)} &= \frac{2r^{2} - \left(\sqrt{x^{2} + y^{2} + z^{2} + a^{2} + b^{2} + c^{2} - 2ax -2by - 2cz}\right)^{2}}{2r^{2}} \\
                                           &= \frac{2r^{2} -x^{2}-y^{2}-z^{2}-a^{2}-b^{2}-c^{2}+2ax+2by+2cz}{2r^{2}}
            \end{align*}
            Observe that since points $P,Q$ lie on a sphere, they must obey the equations
            \begin{align*}
                x^{2} + y^{2} + z^{2} = r^{2}
            \end{align*}
            Thus, since $P$ is given by the pair $(a,b,c)$, and $Q$ is given by $(x,y,z)$, we have
            \begin{align*}
                a^{2} +b^{2} + c^{2} &=r^{2} \\
                x^{2} + y^{2} + z^{2} &= r^{2}
            \end{align*}
            Thus,
            \begin{align*}
                \cos{\left(\theta \right)}  &= \frac{2r^{2} -x^{2}-y^{2}-z^{2}-a^{2}-b^{2}-c^{2}+2ax+2by+2cz}{2r^{2}} \\
                                        &= \frac{r^{2} + r^{2} -x^{2}-y^{2}-z^{2}-a^{2}-b^{2}-c^{2}+2ax+2by+2cz}{2r^{2}} \\
                                        &= \frac{a^{2} + b^{2} + c^{2} + x^{2} + y^{2} + z^{2} -x^{2}-y^{2}-z^{2}-a^{2}-b^{2}-c^{2}+2ax+2by+2cz}{2r^{2}} \\
                                        &= \frac{2ax+2by+2cz}{2r^{2}} \\
                                        &=\frac{2(ax+by+cz)}{2r^{2}} \\
                                        &= \frac{ax+by+cz}{r^{2}}
            \end{align*}
            Since $d_{\mathbb{S}} = r\theta$, we finally arrive at the expression
            \begin{align*}
                d_{\mathbb{S}} &= r\theta  = r\cos^{-1}{\left(\frac{ax+by+cz}{r^{2}}\right)}
            \end{align*}
            As desired \hspace*{\fill} $\blacksquare$
            \bigbreak \noindent 
            \textbf{Note:} There are no parallel lines in $\mathbb{S}$, any two great circles meet at a pair of antipodes.
        \item \textbf{The chords of a circle}: A chord of a circle is a straight line segment whose endpoints lie on the circle. In other words, it is a line segment that connects two points on the circumference of a circle
        \item \textbf{The hyperbolic plane (Poincare disk model)}: Let $\mathbb{H}$ denote the hypebolic plane, which is the set of all points inside (but not on) the unit circle in $\mathbb{E}$. That is, all $(x,y)$ with $x^{2} + y^{2} < 1 $
            \bigbreak \noindent 
            Lines in $\mathbb{H}$ are defined to be the chords of the circle.
            \bigbreak \noindent 
            \textbf{Distance:} If $A,B$ are two points in $\mathbb{H}$, define $d_{\mathbb{H}}(AB)$, the distance between them in $\mathbb{H}$ as follows: Draw the chord $AB$, and let $M,N$ be the points where the chord meets the unit circle ($M,N$ are in $\mathbb{E}$ but not $\mathbb{H}$). label so that $B $ separates $A$ and $N$.
            \bigbreak \noindent 
            \begin{figure}[ht]
                \centering
                \incfig{hyper}
                \label{fig:hyper}
            \end{figure}
            \bigbreak \noindent 
            Let $e(PQ)$ denote the usual Euclidean distance between points, and define
            \begin{align*}
                d_{\mathbb{H}}(AB) = \ln{\left(\frac{e(AN)e(BM)}{e(AM)e(BN)}\right)}
            \end{align*}
            Since $e(AN)>e(BN)$ and $e(BM) > e(AM)$, we have $\frac{e(AN)}{e(BN)} > 1$ and $\frac{e(BM)}{e(AM)} > 1$. Hence $\frac{e(AN)e(BM)}{e(AM)e(BN)}  = \frac{e(AN)}{e(BN)} \cdot \frac{e(BM)}{e(AM)}> 1$. It follows from a property of ln that $d_{\mathbb{H}}(AB) > 0$. Note that $d_{\mathbb{H}}(AB) = d_{\mathbb{H}(BA)}$. Also,
            \begin{align*}
                d_{\mathbb{H}}(AB) = \bigg\lvert \ln{\left(\frac{e(AN)e(BM)}{e(AM)e(BN)}\right)} \bigg\rvert = \bigg\lvert \ln{\left(\frac{e(AM)e(BN)}{e(AN)e(BM)}\right)} \bigg\rvert
            \end{align*}
            So if absolute value is used in this way, then we need not worry about which point on the unit circle is marked $M$ and which is marked $N$.
            \bigbreak \noindent 
            If $A=B$ in $\mathbb{H}$, take any chord through $A$ and let $M,N$ be as previously. Since $\frac{e(AN)e(AM)}{e(AM)e(AN)} =1$, it is consistent with the preceding definition to set $d_{\mathbb{H}}(AA) = 0$.
            \bigbreak \noindent 
            We note that $N$ using the distance formula above is always the point from $A$ through $B$, and the point $M$ is the point from $B$ through $A$. With this in mind, it is clear that $\frac{e(AN)e(BM)}{e(AM)e(BN)} \to \infty$ as we move $A$ and $B$ closer to the opposing sides of the unit circle. Since $\ln:\ (0,\infty) \to \mathbb{R}$, and we noted earlier that $\frac{e(AN)e(BM)}{e(AM)e(BN)} > 1$, distances in the hyperbolic plane can get arbitrary large or small, without bound.
            \bigbreak \noindent 
            Further, Euclids 5th postulate/Playfairs postulate is  false on the hyperbolic plane. Observe
            \bigbreak \noindent 
            \begin{figure}[ht]
                \centering
                \incfig{e5}
                \label{fig:e5}
            \end{figure}
            \fc{Euclids fifth postulate does not hold on the hyperbolic plane}
            \bigbreak \noindent 
            These lines will never meet, because they are stopped by the unit circle boundary. Further, they will in a sense continue on forever, because distances can get arbitrarily large
            \bigbreak \noindent 
            Also,
            \bigbreak \noindent 
            \begin{figure}[ht]
                \centering
                \incfig{play}
                \label{fig:play}
            \end{figure}
            \bigbreak \noindent 
            We see that given a point $P$ not on the line $\ell$, there are many lines through $P$ that are parallel to $\ell$. All of these lines are parallel to $\ell$, because they will never intersect with $\ell$
        \item \textbf{The gap plane}: Let $\mathbb{G}$ denote the \textit{gap}, or \textit{missing strip} plane. The points of $\mathbb{G}$ are all those of $\mathbb{E}$ except those $(x,y)$ with $0< x \leq 1$
            \bigbreak \noindent 
            \begin{figure}[ht]
                \centering
                \incfig{gap}
                \label{fig:gap}
            \end{figure}
            So the $y$-axis is part of $\mathbb{G}$, but the line $x=1$ is not (and neither is any vertical line $x=a$ for $0 < a < 1$)
            \bigbreak \noindent 
            Lines in $\mathbb{G}$ are defined to be the same as in $\mathbb{E}$, except that for any nonvertical line $y=mx+b$, the part in the missing strip is deleted. So a typical nonvertical line $\ell$ consists of all $(x,y)$ with $y=mx+b$ ($m,b$ fixed) and with $x \leq 0$ or $x > 1$
            \bigbreak \noindent 
            Behold a line in $\mathbb{G}$
            \bigbreak \noindent 
            \begin{figure}[ht]
                \centering
                \incfig{behold}
                \label{fig:behold}
            \end{figure}
            \bigbreak \noindent 
            \textbf{Distance:} For points $A,B$ in $\mathbb{G}$, we define $d_{\mathbb{G}}(AB)$ as follows. First; if $A$ and $B$ lie on opposite sides of the gap, let $C$ be the point where segment $\overline{AB} $ meets the $y$-axis, and $D$ the point where $\overline{AB}$ meets the vertical line $x=1$ ($D$ is not in $\mathbb{G}$)
            \bigbreak \noindent 
            \begin{figure}[ht]
                \centering
                \incfig{ex1}
                \label{fig:ex1}
            \end{figure}
            \bigbreak \noindent 
            Now define
            \begin{align*}
                d_{\mathbb{G}}(AB) = \begin{cases}
                    e(AB) & \text{ for $A,B$ on the same side of the gap} \\     
                    e(AB) - e(CD) & \text{ for $A,B$ on the opposite sides of the gap} 
                \end{cases}
            \end{align*}
        \item \textbf{Interior and Exterior angles}: Interior angles are the angles inside the triangle. Each vertex of the triangle has one interior angle. The sum of the interior angles of a triangle is always $180^{\circ}$
            \bigbreak \noindent 
            Exterior angles are the angles formed outside the triangle when one side of the triangle is extended.
            At each vertex, an exterior angle is supplementary to the interior angle (they add up to $180^{\circ} $
            \bigbreak \noindent 
            If an interior angle at a vertex is $A$, the corresponding exterior angle $E$ is:
            \begin{align*}
                E = 180^{\circ} - A
            \end{align*}
            \bigbreak \noindent 
            The sum of the exterior angles of a triangle (one at each vertex) is always $360^{\circ}$ , regardless of the shape of the triangle.
            \bigbreak \noindent 
            \fig{.8}{./figures/23.png}
        \item \textbf{Remote angles}: Remote angles refer to the interior angles of a triangle that are not adjacent to a given exterior angle
        \item \textbf{More on points}:
            \begin{itemize}
                \item \textbf{Collinear points}: Points that lie on the same straight line.
                \item \textbf{Noncollinear points}: Points that do not lie on the same straight line.
                \item \textbf{Coplanar points}: Points that lie on the same plane.
                \item \textbf{Concurrent Points}: Points where three or more lines intersect.
                \item \textbf{Equidistant Points}: Points that are all the same distance from a particular point or object.
                \item \textbf{Lattice Points}:  Points with integer coordinates.
                \item \textbf{Interior points:} Points that lie inside a given shape.
                \item \textbf{Exterior points:} Points that lie outside a given shape.
            \end{itemize}
        \item \textbf{Congruent triangles}: Congruent triangles are triangles that are exactly the same in shape and size. This means that all corresponding sides and angles of one triangle are equal to those of the other triangle.
        \item \textbf{Vertical (opposite) angles}: Vertical angles (also called opposite angles) are the angles that are formed by two intersecting lines and are opposite to each other
        \item \textbf{Reading angle notation}: Suppose you have an angle $\angle ABC$. This angle refers to the angle formed at vertex $B$ by the two line segments or rays:
            \bigbreak \noindent 
            One extending from $B$ to $A$, the other extending from $B$ to $C$. The middle letter, $B$, always represents the vertex of the angle (the point where the two lines meet).
            \bigbreak \noindent 
            \textbf{Note:} If there’s no ambiguity about which angle is being referred to, the angle might simply be denoted as $\angle B$.
        \item \textbf{Potential dangers and the exterior angle inequality}:
            \bigbreak \noindent 
            \textbf{Theorem (\textit{Exterior angle inequality})}: An exterior angle of a triangle is greater than either remote interior angle. That is, if $\triangle ABC$ is a any triangle, and point $D$ is on the extension of segment $\overline{BC}$ through $C$, then
            \begin{align*}
                \angle ACD > \text{ both } \angle A \text{ and } \angle B
            \end{align*}
            \bigbreak \noindent 
            \textbf{Background facts that are ok for both $\mathbb{E}$ and $\mathbb{S}$}
            \begin{enumerate}
                \item \textbf{Triangles:} Line segments that join any three noncollinear points
                \item \textbf{Angle measures}: Are defined for every angle
                \item \textbf{Vertical angles}: Have equal measure
                \item \textbf{side-angle-side}: Criterion for congruent triangles, If two sides and the angle between them in one triangle are equal to the corresponding parts in another triangle, the triangles are congruent.
            \end{enumerate}
            \bigbreak \noindent 
            Consider the triangle 
            \bigbreak \noindent 
            \begin{figure}[ht]
                \centering
                \incfig{tri3}
                \label{fig:tri3}
            \end{figure}
            \bigbreak \noindent 
            \textbf{\textit{Euclid's proof of EAI}}: Let $ M$ be the midpoint of $\overline{AC}$ so $\overline{AM} = \overline{CM}$. Next, extend $\overline{BM}$ through $M$ to point $E$ such that $\overline{MB} = \overline{ME}$
            \bigbreak \noindent 
            \begin{figure}[ht]
                \centering
                \incfig{tri4}
                \label{fig:tri4}
            \end{figure}
            \bigbreak \noindent 
            Notice that since $\angle AMB$ and $\angle CME$ are vertical, they must be equal. That is, $\angle AMB = \angle CME$. Since 
            \begin{enumerate}
                \item $AM = CM $
                \item $MB = ME$
                \item $\angle AMB = \angle CME $
            \end{enumerate}
            We have met the side-angle-side criterion for congruent triangles. Thus, $\triangle AMB \cong \triangle CME$. Consequently, we have $\angle BAM = \angle ECM $. Further, notice that
            \begin{align*}
                \angle ACD &= \angle ACE + \angle ECD \\
                            &= \angle ECM + \angle ECD \\
                           &= \angle BAM + \angle ECD > \angle BAM = \angle A
            \end{align*}
            Thus, $\angle ACD > \angle A$. To show $\angle ACD > \angle B$, first, extend $AC$ through $C$ to point $F$, forming  $\angle BCF$. Notice that since $\angle ACD$ and $\angle BCF$ are vertical, they must be equal. That is, $\angle ACD = \angle BCF$ 
            \bigbreak \noindent 
            Next, let $N$ be the midpoint of $BC$ such that $BN = CN$. Extend $A$ through $N$ to point $G$ such that $AN=GN$.
            \bigbreak \noindent 
            \begin{figure}[ht]
                \centering
                \incfig{tri5}
                \label{fig:tri5}
            \end{figure}
            \bigbreak \noindent 
            Note that since $\angle ANB$ and $\angle CNG$ are vertical, they are equal. That is, $\angle ANB = \angle CNG$. Further, since we have
            \begin{enumerate}
                \item $\angle ANB = \angle CNG $
                \item $AN = GN $
                \item $BN = CN$
            \end{enumerate}
            We have congruence, $\triangle ANB \cong \triangle CNG$. Thus, $\angle ABN = \angle NCG$. Therefore,
            \begin{align*}
                \angle ACD = \angle BCF &= \angle BCG + \angle GCF \\
                                        &= \angle NCG + \angle GCF \\
                                        &= \angle ABN + \angle GCF > \angle ABN = \angle B
            \end{align*}
            \bigbreak \noindent 
            Thus, we have shown that $\angle ACD > \angle A$ and $\angle B$ \hspace*{\fill} $\blacksquare$
            
    \end{itemize}

    \pagebreak 
    \subsection{Intro to geometric proofs and some set theory}
    \begin{itemize}
        \item \textbf{Transversal}: a transversal is a line that passes through two lines in the same plane at two distinct points.
        \item \textbf{Relationship of angles}: Consider the transversal configuration
            \bigbreak \noindent 
            % \begin{figure}[ht]
            %     \centering
            %     \incfig{tc}
            %     \label{fig:tc}
            % \end{figure}
            \bigbreak \noindent 
            We see that we get eight formed angles.
            \begin{itemize}
                \item \textbf{Interior angles}: Interior angles are the angles that are inside the transversal configuration. Angles $a,b,c,d$ are interior
                \item \textbf{Exterior angles}: Exterior angles are the angles that are inside the transversal configuration. Angles $e,f,g,h$ are exterior
                \item \textbf{Consecutive interior angles}: Pairs of interior angles that are on the same side of the transversal. Angles $c,d$ are consecutive interior, and $a,b$ are consecutive interior
                \item \textbf{Consecutive exterior angles}: Pairs of exterior angles that are on the outside of the transversal configuration. Angles $e,g$ are consecutive exterior, angles $f,h$ are consecutive exterior
                \item \textbf{Alternate interior angles}: Pairs of interior angles that are on opposite sides but not complementary, angles $b,d$ and $a,c$ are alternate interior
                \item \textbf{Alternate exterior angles}: Pairs of exterior angles that are on opposite sides but not complementary, angles $e,h$, and $f,g$ are alternate exterior
                \item \textbf{Vertical angles}: Angles that are opposite each other, formed when two lines intersect. Vertical angles are of equal measure. Pairs $d,h$ - $a,g$ - $e,b$ - and $f,c$ are vertical
                \item \textbf{Supplementary angles}: Angle pairs that sum to 180, pairs $a,h$ - $d,g$ - $f,b$ - and $e,c$ are supplementary
                \item \textbf{Complementary angles}: Angle pairs that sum to 90, none in the transversal configuration
            \end{itemize}
            \bigbreak \noindent 
            \textbf{Proposition (Equal alternate interior angles)}. Suppose $a + b = 180$, then $b = d$, and $c=a $.
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} Consider the transversal configuration shown above. Assume $a+b = 180$, then $a=180-b$. Since vertical angles are equal, we have $d=h$. But since $a,h$ are supplementary, we have $a + h = 180$, which implies $h = 180 -a$. Thus,
            \begin{align*}
                d = h = 180 - a
            \end{align*}
            Since $a+b = 180$ implies $ b = 180-a$, we have
            \begin{align*}
                 d = h = 180 - a = b
            \end{align*}
            \bigbreak \noindent 
            Thus, $d=b$. Next, we show that $c=a$. Since $c$ and $f$ are vertical, we have $c = f$. Further, since $a + b =180$, we have $a = 180 -b$. Notice that $b$ and $f$ are supplementary, which implies $b + f = 180$, or $f = 180 - b $. So, since $c=f = 180 -b$, and $a = 180-b$, we have $c = f = 180 - b = a$. Thus, $c=a$
            \bigbreak \noindent 
            Therefore, we conclude that if $a+ b =180$, $b = d$ and $c = a $ \hspace*{\fill}$\blacksquare$
        \item \textbf{Background on Euclid's plane without the fifth postulate}:
            \bigbreak \noindent 
            \textbf{Assumptions}:
            \begin{itemize}
                \item Two points determine a unique line
                \item Distances between points on a line include all positive real numbers
                \item Angles are measured
                    \bigbreak \noindent 
                    \begin{figure}[ht]
                        \centering
                        \incfig{supp}
                        \label{fig:supp}
                \end{figure}
                We say that $\alpha$ and $\beta$ are \textit{supplementary} because $\alpha + \beta = 180^{\circ}$. Note that two angles that are supplementary to each other do not have to be next to each other, only the sums of their angles must be $180^{\circ}$. 
                \bigbreak \noindent 
                As a side note, recall that \textit{complementary} angles are angles that sum to $90^{\circ}$
            \end{itemize}
            \bigbreak \noindent 
            \textbf{Definitions}:
            \begin{itemize}
                \item \textbf{Angles}: An angle is formed when two rays meet at a common endpoint, called the vertex.
                \item \textbf{Vertical angles}: Vertical angles (or opposite angles) are the angles formed when two lines intersect.
                \item \textbf{Triangle}: A triangle is a polygon with three sides, three vertices, and three angles.
                    \bigbreak \noindent 
                    The sum of the interior angles of a triangle is always $180^{\circ}$
                    \bigbreak \noindent 
                    A triangle is a closed geometric figure formed by three line segments connecting three non-collinear points
                \item \textbf{Congruent}: Congruent refers to figures or shapes that are identical in size and shape.
                    \bigbreak \noindent 
                    Two triangles are congruent if their corresponding sides and angles are equal (e.g., by SSS, SAS, ASA, or AAS congruence criteria).
                    \bigbreak \noindent 
                    We generally use the side-angle-side criterion to determine congruent triangles.
            \end{itemize}
            \bigbreak \noindent 
            Also, recall the exterior angle theorem proved above.
        \item \textbf{Example of proof by contradiction}: Suppose we are on Euclid's plane without the fifth postulate
            \bigbreak \noindent 
            \textbf{Proposition 1}. Suppose that line $\ell$ crosses $m$ and $n$ so that  the interior angles on one side of $\ell$ add to more than $180^{\circ}$
            \bigbreak \noindent 
            \begin{figure}[ht]
                \centering
                \incfig{p1}
                \label{fig:p1}
            \end{figure}
            \bigbreak \noindent 
            Then, $m,n$ do not meet on that side of $\ell$
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} Assume for the sake of contradiction that the statement is false. That is, suppose $m,n$ meet on that side of $\ell$. Then, we must have
            \bigbreak \noindent 
            \begin{figure}[ht]
                \centering
                \incfig{p2}
                \label{fig:p2}
            \end{figure}
            \bigbreak \noindent 
            Call the point where they meet $C$, since we have three noncollinear points $A,B,C$, $\triangle ABC$ is formed.
            \bigbreak \noindent 
            Define $\angle CBD$ as the exterior angle for $\triangle ABC$, call it measure $\gamma$
            \bigbreak \noindent 
            \begin{figure}[ht]
                \centering
                \incfig{p4}
                \label{fig:p4}
            \end{figure}
            \bigbreak \noindent 
            $\beta$ and $\gamma$ are supplementary, so $\beta + \gamma = 180^{\circ}$. Thus, $\gamma = 180^{\circ} - \beta$. By the EAI, $\gamma > \alpha$, which means $180^{\circ} - \beta > \alpha$. Thus, we have $180^{\circ} > \alpha + \beta$. But, we stated that $\alpha + \beta > 180^{\circ}$, which is a contradiction. 
            \bigbreak \noindent 
            Therefore, by contradiction, are assumption that $m,n$ meet on that side is false, and therefore $m,n$ must not meet on that side.  \hspace*{\fill} $\blacksquare$
            \bigbreak \noindent
        \item \textbf{Upper bounds}: Suppose $S$ is a set of real numbers, we define $b \in \mathbb{R}$ as an \textit{upper bound} for $S$ if for all $x\in S, x \leq b$
            \bigbreak \noindent 
            The negation of this definition is, there exists $x\in S$ such that $x \nleq b$, or $x > b$. Thus, to prove some $b$ is not an upper bound for $S$, we can show that some element of $S$ is greater than $b$
            \bigbreak \noindent 
            There are of course  sets that do not have any upper bounds. Consider the set $S = \{n:\ n\in \mathbb{N} \text{ and } n>0\} $. This set has no upper bound.
            \bigbreak \noindent 
            If $S = \varnothing$, then every $b \in \mathbb{R}$ is an upper bound for $S$. This statement is vacuously true.
        \item \textbf{Least upper bound (supremum)}: $c\in \mathbb{R}$ is a \textit{least upper bound} of a set $S$ of real numbers if 
            \begin{enumerate}
                \item $c$ is an upper bound for $S$
                \item $c \leq b$ for all upper bounds $b$ of $S$
            \end{enumerate}
            \bigbreak \noindent 
            \textbf{Note:} The supremum of a set $S$ is denoted $ b = \text{sup}(S)$, where $b$ is the supremum of the set
        \item \textbf{Least upper bound property of $\mathbb{R}$}: If $S$ is a nonempty set of real numbers that has an upper bound in $\mathbb{R}$, then $S$ has a least upper bound (l.u.b) in $\mathbb{R} $
            \bigbreak \noindent 
            This justifies, among other things, that infinite decimals exist as real numbers, since an infinite decimal can be defined as the least upper bound of the set of all its finite truncations. For example, suppose $S$ is the set of all finite decimal expansions of $\pi$.
            \begin{align*}
                S = \{3,3.1,3.14,3.141, 3.1415,...\}
            \end{align*}
            Then, $S$ as an $l.u.b$ $\pi$, and $\pi\not\in S $
        \item \textbf{Least upper bound proposition}
            \bigbreak \noindent 
            \textbf{Proposition.} Let $S$ be a nonempty set of real numbers that has a least upper bound $b \in \mathbb{R}$. Let $t \in \mathbb{R}$ such that $t < b$. Then, there exists some $s \in S$ such that $t < s \leq b$.
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} Assume $S$ is a nonempty subset of the real numbers with a least upper bound $b$. Let $t\in \mathbb{R}$ such that $t<b$. Since $b$ is a least upper bound of $S$, we have
            \begin{align*}
                \forall \ s \in S,\ s \leq b 
            \end{align*}
            Since $t< b$, $t$ cannot be an upper bound for $S$. If it were, then that would contradict $b$ being the least upper bound. Since $t$ is not an upper bound of $S$, then this implies the existence of some $s\in S$ such that $ t< s$. If this were not the case, then the negation which states, for all $s\in S$, $t \geq s$ would be true. Since the negation implies that $t$ is an upper bound, which we know can't be the case, there must exist some $s\in S$ such that $t < s$. 
            \bigbreak \noindent 
            Since $s \leq b$ for all $s\in S$, and we know that there exists some $s \in S$ such that $t < s$, there must be at least one $s$ that satisfies
            \begin{align*}
                t < s \leq b
            \end{align*}
            \hspace*{\fill} $\blacksquare$
        \item \textbf{Lower bounds}: Let $S$ be a nonempty set of real numbers. Then $g\in \mathbb{R}$ is a \textit{lower bound} for $S$ if $g \leq x$ for all $x\in S$.
        \item \textbf{Greater lower bounds (Infimum)}: $h\in \mathbb{R}$ is a \textit{greatest lower bound}, also called the \textit{infimum}, or \textit{inf} for $S$ if $ h$ is a lower bound for $S$ and $h \geq g$ for all lower bounds $g$ of $S$
        \item \textbf{Infimum proposition}
            \bigbreak \noindent 
            \textbf{Proposition}. Let $S$ be a nonempty set or real numbers that has a lower bound in $\mathbb{R}$. Then $S$ has a infimum in $\mathbb{R}$
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} Assume $S \subseteq \mathbb{R}$, $S\ne \varnothing$ that has a lower bound in $\mathbb{R}$.
            \bigbreak \noindent 
            Let $B$ be the set of all lower bounds of $S$. Since $S$ has a lower bound, $B$ is nonempty. Define
            \begin{align*}
                B = \{b \in \mathbb{R}: b \leq s \ \forall \ s \in S\}
            \end{align*}
            \bigbreak \noindent 
            We first note that every $s \in S$ serves as an upper bound for $B$. This is because for any $b\in B,\ b \leq s$ for all $s\in S$, thus satisfying the definition of an upper bound
            \bigbreak \noindent 
            Since $B$ is nonempty and bounded above by all elements of $S$, $B$ has a least upper bound (supremum) in $\mathbb{R}$. Let $\lambda$ be this supremum. That is, $\lambda = \text{sup}\ B$. To show that this supremum is precisely the infimum for $S$ is to show two things
            \begin{enumerate}
                \item $\lambda \in B$. That is, $\lambda$ is a lower bound for $S$
                \item $\lambda \geq b$ for all lower bounds $b $ of $S$
            \end{enumerate}
            We begin by showing that $\lambda \in B$. If $\lambda \in B$, then by definition of $B$, $ \lambda \leq s \ \forall \ s \in S$. Assume for the sake of contradiction that there exists some $s \in S$ such that $\lambda > s$. This would contradict the fact that $\lambda$ is the least upper bound for $B$ because then $s$ would be an upper bound for $B$ smaller than $\lambda$. Thus, there are no such $s\in S$ such that $s < \lambda$, and $\lambda$ must therefore be in $B$
            \bigbreak \noindent 
            Next, we show that $\lambda$ is truly the greatest lower bound of $S$, that $\lambda \geq b $ for all lower bounds $b$ of $S$. Assume for the sake of contradiction that there exists some $b \in B$ such that $\lambda < b$. This would mean $\lambda$ is not actually an upper bound for $B$  which again contradicts the fact that $\lambda$ is the supremum of $B$
            \bigbreak \noindent 
            Thus, since $\lambda \in B$, and $\lambda \geq b$ for all $b\in B$. We have that $\lambda$ is the greatest lower bound of $S$, or $\lambda = \text{inf } S $ \hspace*{\fill}$\blacksquare$


    \end{itemize}

    \pagebreak 
    \subsection{An axiom system for geometry: First steps.}
    \begin{itemize}
        \item \textbf{What is projective geometry?} Projective geometry is a branch of geometry where any two distinct lines intersect in exactly one point, meaning there are no parallel lines. It extends Euclidean geometry by adding "points at infinity" to ensure this property holds. Projective geometry focuses on incidence relations (how points and lines are related) rather than distances or angles.
        \item \textbf{What is incidence?} In geometry, "incident" means that a point lies on a line (or a plane, in higher dimensions), or that a line passes through a point. More generally, it describes a fundamental relationship between geometric objects in an incidence structure.
            \bigbreak \noindent 
            For example:
            \begin{itemize}
                \item A point is incident to a line if it lies on that line.
                \item A line is incident to a point if it passes through that point.
                \item In projective geometry, two lines are incident to the same point if they intersect at that point.
            \end{itemize}
            \bigbreak \noindent 
            It is a basic, undefined term in axiomatic geometry, meaning it is taken as a fundamental concept rather than being defined in terms of simpler notions.
        \item \textbf{What is incidence geometry}: Incidence geometry is the study of geometric structures based only on points, lines, and their incidence relations (which points lie on which lines). It focuses on which objects are connected rather than distances, angles, or measurements. The main rules are typically:
            \begin{enumerate}
                \item Any two distinct points determine a unique line.
                \item Any two distinct lines intersect in at most one point.
                \item There exist at least four points, not all on the same line (to avoid trivial cases).
            \end{enumerate}
            \bigbreak \noindent 
            It includes Euclidean, affine, and projective geometries as special cases.

        \item \textbf{The Fano plane}: The Fano plane is a \textit{projective plane of order two.}
            \bigbreak \noindent 
            When we say that the Fano Plane is a projective plane of order two, we mean that
        \begin{itemize}
            \item \textbf{It is a finite projective plane}: – A projective plane is a type of incidence geometry satisfying specific axioms
                \begin{enumerate}
                    \item Any two distinct points determine a unique line.
                    \item Any two distinct lines intersect in a unique point.
                    \item There exist four points, no three of which are collinear (this ensures it is not a degenerate geometry).
                \end{enumerate}
            \item \textbf{Order two ($q = 2$)}: The order of a finite projective plane is a parameter $q$ that determines its structure. The order $q$ is defined by the number of points on each line minus one. In the Fano Plane:
                \begin{enumerate}
                    \item Every line contains exactly  $q+1=3$ points
                    \item Every point is on exactly $q+1 = 3$ lines
                    \item The total number of points is $q^{2} + q + 1 = 2^{2} + 2 + 1 = 7 $
                    \item The total number of lines is $q^{2} + q + 1 = 2^{2} + 2  + 1 = 7 $
                \end{enumerate}
        \end{itemize}
        Since the Fano Plane satisfies these properties for $q=2$, it is called a projective plane of order two.
    \item \textbf{More on the Fano plane}: There are seven points $\{A,B,C,D,E,F,G\}$, and there are seven lines $\{A,B,D\}, \{C,D,F\}, \{A,F,E\}, \{A,C,G\}, \{B,C,E\}, \{B,F,G\}, \{D,E,G\}$
        \bigbreak \noindent 
        There are three points on each line, and three points through each line
        \bigbreak \noindent 
        \begin{figure}[ht]
            \centering
            \incfig{fano}
            \label{fig:fano}
        \end{figure}
        \bigbreak \noindent 
        Which points on which line? Write points in alphabetical order in three rows, start with $A$, then $B$, then with $D$
        \begin{align*}
            &A \ B \ C \ D \ E \ F \\
            &B \ C \ D \ E \ F \ A\\
            &D \ E \ F \ A B \ C \ \\
        \end{align*}
        Note that the columns give the lines
        \bigbreak \noindent 
        \textbf{Note:} The triangle picture is a good visual aid, but the Fano plane is not part of the Euclidean plane.
    \item \textbf{Coordinates for the Fano plane}: Each point is an ordered triple $(x,y,z) $, where $x,y,z$ are integers mod $2$
        \begin{align*}
            \begin{cases}
            0 & \text{ stands for all even numbers}     \\
            1 & \text{ stands for all odd numbers}     
            \end{cases}
        \end{align*}
        We further note that $\text{odd } + \text{ odd} = \text{ even}$. Or, $1 +1 = 0 $. Other than that it is business as usual... $0+0 =0,\ 1+0 = 0 + 1 =  1$
        \bigbreak \noindent 
        We have the points
        \begin{align*}
            &A(1,0,0) \quad B(1,1,0) \quad D(0,1,0) \quad E(0,0,1) \\
            &C(1,1,1) \quad F(1,0,1) \quad G(0,1,1) \quad \text{No point }:\ (0,0,0)
        \end{align*}
        \bigbreak \noindent 
        Given points $P,Q$, find the third point collinear with $P,Q$. We simply add the coordinate triples for $P,Q$. For example, suppose $A(1,0,0), B(1,1,0)$. Then,
        \begin{align*}
            (1,0,0) + (1,1,0) = (0,1,0) = D
        \end{align*}
    \item \textbf{Distance on the Fano plane}: We define distance for Fano points, but its not Euclidean distance
        \bigbreak \noindent 
        Given points $P,Q$,
        \begin{align*}
            d(PQ) = \text{ number of different respective coordinates}
        \end{align*}
        \bigbreak \noindent 
        For example, $B(1,1,0), G(0,1,1)$ implies $d(BG) = 2 $
    \item \textbf{General finite projective plane}:In general, for a finite projective plane of order  $q$
        \begin{enumerate}
            \item There are $q^{2} + q + 1$ points
            \item There are $q^{2} + q + 1$ lines
            \item Every line contains $q+1$ points
            \item Every point is contained in $q+1$ lines
        \end{enumerate}
        \bigbreak \noindent 
        And satisfies
        \begin{enumerate}
            \item Any two distinct points determine a unique line.
            \item Any two distinct lines intersect at a unique point.
            \item There exist at least four points, no three of which are collinear. (This ensures non-triviality.)
        \end{enumerate}
        Thus, the Fano Plane is the smallest projective plane, and it uniquely exists for order 2.
        \bigbreak \noindent 
        \textbf{Note:} The "projective" part in the name projective plane comes from its connection to projective geometry, which generalizes Euclidean geometry by removing the notion of parallel lines.
    \item \textbf{Fine projective plane with order one?}: a finite projective plane cannot have order $q=1$ because it would not satisfy the axioms of a projective plane.
        \bigbreak \noindent 
        If $q=1$:
        \begin{enumerate}
            \item \textbf{Number of points}: $1^{2} + 1 + 1 =3$
            \item \textbf{Number of lines}: $1^{2} + 1 + 1 =2$
            \item \textbf{Each line has}: $1+1 = 2$ points
            \item \textbf{Each point is on}: $1+1=2$ lines
        \end{enumerate}
        \bigbreak \noindent 
        This configuration forms a triangle, but therefore fails the requirement that a finite projective plane has at least four points (it only has three)
    \item \textbf{Some extra planes}
        \begin{itemize}
            \item \textbf{$\hat{\mathbb{E}}$: The bumpy plane}: Which is $\mathbb{E}$, but warped. Has bumps and depressions, not always flat.
            \item \textbf{$\mathbb{R}^{3}$:} Points, lines, distance of usual 3-dimensional space.
            \item \textbf{$\varnothing$}: Has the components necessary for a plane vacuously
        \end{itemize}
    \item \textbf{Define a plane}: Let's define a plane called $*$, 
        \begin{align*}
            \mathbb{P} &= \{A,B,C,D\} \quad \text{(4 points)} \\
            \mathbb{L} &= \{A,B,C\}, \{A,C,D\}, \{B,D\} \quad \text{3 lines}
        \end{align*}
        With distance function
        \begin{align*}
            \begin{array}{c|cccc}
               &A&B&C&D \\
               \hline
                A&0&1&2&\frac{1}{2} \\
                B&1&0&\frac{3}{2} &\frac{1}{2} \\
                C&2&\frac{3}{2}&0&\frac{3}{2} \\
                D&\frac{1}{2} &\frac{1}{2}&\frac{3}{2}&0
            \end{array}
        \end{align*}
    \item \textbf{Axiom system for plane geometry}: 
        \bigbreak \noindent 
        \textbf{Undefined terms:}
        \begin{itemize}
            \item \textbf{$\mathbb{P}$:} Set of elements, called \textbf{points.}
            \item \textbf{$\mathbb{L}$:} Collection of subsets of $\mathbb{P}$, called \textbf{lines}
            \item A function $d:\ \mathbb{P}\times \mathbb{P} \to \mathbb{R} $, called a \textbf{distance function}
        \end{itemize}
        \bigbreak \noindent 
        Call anything with these components a \textbf{plane}
        \bigbreak \noindent 
        \textbf{Notation, terminology}
        \begin{itemize}
            \item A line is a set of points
            \item If $P$ is on the line $m$ ($P\in m$), we say that "$P$ is on $m$", or "$m$ goes through $P$"
            \item If two or more points are on the same line, we say they are \textbf{collinear}
            \item Denote distance $d(P,Q)$, or $d(PQ)$, or just $PQ$
        \end{itemize}
        \bigbreak \noindent 
        \textbf{Axiom of distance}: For all points $P,Q$
        \begin{enumerate}
            \item $PQ \geq 0 $
            \item $PQ = 0 \iff P=Q $
            \item $PQ = QP $
        \end{enumerate}
        These are true for all planes mentioned so far, even $*$ and $\varnothing $
        \bigbreak \noindent 
        \textbf{The distance set}: Define $\mathbb{D} = \{PQ:\ P,Q \in \mathbb{R}\}$. This is the set of all distances that occur between points of $\mathbb{P}$, with respect to the given distance function.
        \bigbreak \noindent 
        \textbf{The diameter of the plane $\mathbb{P}$, $\omega $}
        \begin{align*}
            \begin{cases}
                \omega = \text{sup}\ \mathbb{D} & \text{ if $\mathbb{D}$ has an upper bound in $\mathbb{R} $}      \\
                \omega = \infty & \text{ if $\mathbb{D}$ has no an upper bound in $\mathbb{R} $}      \\
            \end{cases}
        \end{align*}
        \bigbreak \noindent 
        Note that $\infty$ is not a real number, but we still say $r < \infty$ for all $r\in \mathbb{R} $
        \bigbreak \noindent 
        \[
            \begin{array}{|c|c|c|}
                \hline
                \mathbb{P} & \mathbb{D} & \omega \\
                \hline
                \mathbb{E} & [0, \infty) & \infty \\
                \mathbb{M} & [0, \infty) & \infty \\
                \mathbb{S}(r) & [0, \pi r] & \pi r \\
                \mathbb{H} & [0, \infty) & \infty \\
                \mathbb{G} & [0, \infty) & \infty \\
                \text{Fano} & \{0, 1, 2, 3\} & 3 \\
                \hat{\mathbb{E}} & [0, \infty) & \infty \\
                \mathbb{R}^3 & [0, \infty) & \infty \\
                \varnothing & \varnothing & \times \\
                (*) & \{0, \frac{1}{2}, 1, \frac{3}{2}, 2\} & 2 \\
                \hline
            \end{array}
        \]
        \bigbreak \noindent 
        \textbf{Note:} Whether $\omega$ is a finite number or $ \infty$, each distance $PQ$ is a nonnegative, finite real number
        \bigbreak \noindent 
        Why not assume that two points determine a unique line? That two points are together in exactly one line? The sphere $\mathbb{S}$, which we want to include as a plane, has many lines through two points, when the points are antipodes. These are the points $P,Q$ where $PQ = \pi r = \omega $.
        \bigbreak \noindent 
        Thus, our axioms will allow multiple lines through two points, but only if their distance apart is precisely $\omega$, the diameter of the plane. Note that $P,Q$, with $PQ = \omega$ \textbf{may or may not} have more than one line through them.
        \bigbreak \noindent 
        \textbf{Axioms of incidence}
        \begin{enumerate}
            \item There are at least two different lines
            \item Each line contains at least two different points
            \item Each pair of points are together in at least one line
            \item Each pair of points $P,Q$, with $PQ < \omega$ are together in at most one line
        \end{enumerate}
        \bigbreak \noindent 
        \textbf{Note:} These are true for all discussed planes except $\varnothing$. 1 and 2 are false for $\varnothing $
        \bigbreak \noindent     
    \item \textbf{So what exactly is a plane?}: Based on the provided axioms, the definition of a plane in this system is simply a structure consisting of
        \begin{itemize}
            \item A set of points $\mathbb{P}$
            \item A collection of subsets of $\mathbb{P}$ called lines $\mathbb{L}$
            \item A distance function $d:\ \mathbb{P}\times\mathbb{P}\to\mathbb{R}$.
        \end{itemize}
        \bigbreak \noindent 
        Thus, a set $\mathbb{P}$ and a set of lines $\mathbb{L}$ can be called a plane as long as they fit this definition, regardless of whether they satisfy the axioms of distance or incidence.
        \bigbreak \noindent 
        However, for a plane to behave in a meaningful way in axiomatic geometry (i.e., to be one of the discussed geometric planes like $\mathbb{E}, \mathbb{M}, \mathbb{S}(r)$, etc...) it must satisfy the axioms of distance and incidence. These axioms impose necessary geometric structure, ensuring that distances behave as expected and that lines and points interact according to the incidence rules.
        \bigbreak \noindent 
        Thus, a plane can exist without satisfying the axioms, but to be a meaningful model of geometry, it is typically expected to satisfy them.
    \item \textbf{Plane example}: Consider the plane with $\mathbb{P}:\ $ all points inside the unit circle in $\mathbb{E}$, and $\mathbb{L}$ be the set of all chords inside the circle
        \bigbreak \noindent 
        For points $P,Q$ in $\mathbb{P}$, define $d(PQ) = PQ = e(PQ)$. Ie the Euclidean distance
        \bigbreak \noindent 
        Note that the seven axioms are true statements for this example. 
        \bigbreak \noindent 
        We have $\mathbb{D} = [0,2)$, so $\omega = 2$, but $PQ < 2$ for all $P,Q\in \mathbb{P}$
    \item \textbf{Trivial discrete model (TDM)}: Let $\mathbb{P}$ be any set of at least three elements. Let $\mathbb{L}$ be the collection of all two element subsets of $\mathbb{P} $
        \bigbreak \noindent 
        Define distance as follows: For all $x\ne y \in \mathbb{P}$, 
        \begin{align*}
            \begin{cases}
                xy &= 1 \\
                xx &= 0
            \end{cases}
        \end{align*}
        The seven axioms are true for the TDM. We have $\mathbb{D} = \{0,1\}$, thus $\omega = 1 $
        \bigbreak \noindent 
        For example, if $\mathbb{P} = \{A,B,C\} $, which implies $\mathbb{L} = \{A,B\}, \{A,C\}, \{B,C\} $, which forms a triangle where all sides are of length one.
    \item \textbf{White stripes model (ws)}: Let $\ell, m$ be two parallel lines in $\mathbb{E}$
        \bigbreak \noindent 
        \begin{figure}[ht]
            \centering
            \incfig{twolines}
            \label{fig:twolines}
        \end{figure}
        \bigbreak \noindent 
        Define $\mathbb{P} = \{\text{all points on $\ell $}\} \cup \{\text{all points on $m$}\}$, and $\mathbb{L} = \ell,m$, and all two point sets $\{P,Q\}$ where $P$ on $\ell$, $Q$ on $m$. Define distance $d = $ Euclidean distance $e(PQ)$
        \bigbreak \noindent 
        Note that the seven axioms are true statements for $ws$, and $\mathbb{D} = [0,\infty),\ \omega = \infty$
        \bigbreak \noindent 
        \begin{figure}[ht]
            \centering
            \incfig{twolines2}
            \label{fig:twolines2}
        \end{figure}





    \end{itemize}

    \pagebreak 
    \subsection{Betweenness, segements, and rays}
    \begin{itemize}
        \item \textbf{Betweenness}: Let $\mathbb{P}$ be a plane with points, lines, distance, and satisfy the seven axioms (3 distance, 4 incidence). Define
            \bigbreak \noindent 
            \textbf{Definition.} Point $B$ lies \textbf{between} points $A$ and $C$, denoted $A-B-C$ provide that
            \begin{enumerate}
                \item $A,B$, and $C$ are different and collinear
                \item $AB + BC = AC $
            \end{enumerate}
            \bigbreak \noindent 
            \begin{figure}[ht]
                \centering
                \incfig{between2}
                \label{fig:between2}
            \end{figure}
        \item \textbf{Betweenness example 1}: \begin{align*}
                P &= \{A, B, C, D\} \\
                L &= \{\{A, B, C\}, \{A, C, D\}, \{B, D\}\}
            \end{align*}
            \textbf{Distance:}
            \[
                \begin{array}{c|cccc}
  & A & B & C & D \\
  \hline
                    A & 0 & 1 & 2 & \frac{1}{2} \\
                    B & 1 & 0 & \frac{3}{2} & \frac{1}{2} \\
                    C & 2 & \frac{3}{2} & 0 & \frac{3}{2} \\
                    D & \frac{1}{2} & \frac{1}{2} & \frac{3}{2} & 0 \\
                \end{array}
            \]
            \textbf{On line \(\{A, C, D\}\):}
            \begin{align*}
                AC &= 2, \quad AD = \frac{1}{2}, \quad DC = \frac{3}{2} \\
            \end{align*}
            $AD + DC = AC$. Thus, $A - D - C$.
            \bigbreak \noindent 
            \textbf{On line \(\{A, B, C\}\):}
            \begin{align*}
                AB &= 1, \quad AC = 2, \quad BC = \frac{3}{2} \\
            \end{align*}
            No two of these add to the third, so there is \textbf{no betweenness relation} among \(A, B, C\).
        \item \textbf{Betweenness on the Fano plane}: We have the collinear points $A(1,0,0), B(1,1,0), D(0,1,0)$, with 
            \begin{align*}
                AB =1, \quad BD = 1, \quad AD =2
            \end{align*}
            We see $AB + BD = AD$. Thus, $A-B-D$
        \item \textbf{Betweenness on the spherical plane}: Consider points $A, C$, with $A \ne C$, and distance $AC < \omega = \pi r$. So, $A,C$ determine unique great circle (line) $\overleftrightarrow{AC}$. Let $A^{*}$ be the antipode of $A$, and $C^{*}$ be the antipode of $C$. We check all points $B$ on $\overleftrightarrow{AC}$  and see in which locations there is betweenness $A-B-C$.
            \bigbreak \noindent 
            First, consider $B$ on minor arc $\overset{\frown}{AC}$. Notice that the minor arc $\overset{\frown}{AB}$ plus the minor arc $\overset{\frown}{BC}$ equals the minor arc $\overset{\frown}{AC}$. Thus,
            \begin{align*}
                d_{\mathbb{S}}(AB) + d_{\mathbb{S}}(BC) = d_{\mathbb{S}}(AC)
            \end{align*}
            Thus, $A-B-C$
            \bigbreak \noindent 
            \begin{figure}[ht]
                \centering
                \incfig{minor1}
                \label{fig:minor1}
            \end{figure}
            \bigbreak \noindent 
            Next, let $B$ be on the minor arc $\overset{\frown}{A^{*}C}$. Observe that
            \begin{align*}
               d_{\mathbb{S}}(AC) + d_{\mathbb{S}}(CB) =  d_{\mathbb{S}}(AB)
            \end{align*}
            Thus, $A-C-B $
            \bigbreak \noindent 
            \begin{figure}[ht]
                \centering
                \incfig{overset}
                \label{fig:overset}
            \end{figure}
            \bigbreak \noindent 
            Next, let $B$ be on the minor arc $\overset{\frown}{C^{*}A}$. Observe that
            \begin{align*}
                d_{\mathbb{S}}(BA) + d_{\mathbb{S}}(AC) = d_{\mathbb{S}}(BC)
            \end{align*}
            Thus, $B-A-C $
            \bigbreak \noindent 
            \begin{figure}[ht]
                \centering
                \incfig{minorarc3}
                \label{fig:minorarc3}
            \end{figure}
            \bigbreak \noindent 
            Next, let $B$ be on the minor arc $\overset{\frown}{A^{*}C^{*}}$, any two of $d_{\mathbb{S}}(AB), d_{\mathbb{S}}(BC), d_{\mathbb{S}}(AC)  $ add to more than $\pi r $, hence more than any distance on $\mathbb{S} $. Therefore, no two add to the third and $A,B,C$ have no betweenness relation
            \bigbreak \noindent 
            \begin{figure}[ht]
                \centering
                \incfig{marc1}
                \label{fig:marc1}
            \end{figure}
            \bigbreak \noindent 
            Finally, consider two points $A, A^{*}$, where $A^{*}$ is $A $'s antipode. Let $B$ be any point not equal to $A$ or $A^{*}$. Then, $B$ is collinear with $A, A^{*}$. Observe that
            \begin{align*}
                d_{\mathbb{S}}(AB) + d_{\mathbb{S}}(BA^{*}) = \pi r = d_{\mathbb{S}}(AA^{*})
            \end{align*}
            Thus, $A-B-A^{*} $
            \bigbreak \noindent 
            \begin{figure}[ht]
                \centering
                \incfig{mior4}
                \label{fig:mior4}
            \end{figure}
            \pagebreak \bigbreak \noindent 
        \item \textbf{Betweenness theorem 1}: 
            \bigbreak \noindent 
            \textbf{Theorem 6.1 (Symmetry of betweenness)}. For a general plane $\mathbb{P}$ with points, lines, distance, and satisfy the seven axioms, $A-B-C \iff C-B-A$
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} Suppose that $A-B-C$, by definition, $A,B,C$ are different and collinear. Hence, $C,B,A$ are different and collinear, and $AB + BC = AC $
            \bigbreak \noindent 
            By distance axiom three, $AB = BA$, $BC = CB$, and $AC = CA$. Thus,
            \begin{align*}
                AB + BC &= AC \\
                \implies BA + CB &= CA
            \end{align*}
            But by the commutative property of $+$ in $\mathbb{R} $
            \begin{align*}
                BA + CB &= CA \\
                \implies CB + BA &= CA 
            \end{align*}
            Therefore, by the definition of betweenness, $C-B-A$. Thus, by similar steps, if $C-B-A$, then $A-B-C$ \hspace*{\fill} $\blacksquare$
        \item \textbf{Uniqueness Middle Theorem (UMT)}:
            \bigbreak \noindent 
            \textbf{Theorem 6.2 (UMT)}: If $A-B-C$ then $B-A-C$ and $A-C-B$ are false.
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} Assume $A-B-C$, then $A,B,C$ are different, collinear, and $AB + BC = AC$. We know by distance axioms $1$ and $2$ that each of $AB, BC$, and $AC$ are greater than zero. Thus,
            \begin{align*}
                AC > AB \quad \text{ and } AC > BC
            \end{align*}
            Suppose for the sake of contradiction that $B-A-C$ is also true. Thus, $BC$ would be larger than both $BA=AB$ and $AC$. 
            \bigbreak \noindent 
            Since this contradicts the fact that $AC > BC$, which must be true if $A-B-C$, it must be that $B-A-C$ is false. By similar steps, $A-C-B$ is also false. \hspace*{\fill} $\blacksquare $ 
        \item \textbf{Betweennes in $\mathbb{M}$}: Suppose $A-B-C$ is true in $\mathbb{E}$. Then, we have in the Minkowski plane
            \bigbreak \noindent 
            \begin{figure}[ht]
                \centering
                \incfig{bet}
                \label{fig:bet}
            \end{figure}
            \bigbreak \noindent 
            So we see
            \begin{align*}
                d_{\mathbb{M}}(AB) + d_{\mathbb{M}}(BC) &= \left\lvert x_{1} -x_{2} \right\rvert + \left\lvert y_{1} - y_{2} \right\rvert + \left\lvert x_{2} - x_{3} \right\rvert + \left\lvert y_{2} - y_{3} \right\rvert
            \end{align*}
            \bigbreak \noindent 
            We can then drop the absolute value bars by examining the configuration and determining which order the subtraction needs to happen to yield a positive result. We have
            \begin{align*}
                &(x_{2} - x_{1}) + (y_{2} - y_{1}) +(x_{3} -x_{2}) + (y_{3} - y_{2}) \\
                &=(x_{3}-x_{1}) + (y_{3} - y_{1}) = d_{\mathbb{M}}(AC)
            \end{align*}
            Thus, for $ A-B-C$ in $\mathbb{E}$, $A-B-C$ in $\mathbb{M}$ holds true. Similarly, $B-A-C$ in $\mathbb{E}$ implies $B-A-C$ in $\mathbb{M}$, and $A-C-B$ in $\mathbb{E}$ implies $A-C-B$ in $\mathbb{M}$
            \bigbreak \noindent 
            So for three collinear points $A,B,C$ in $\mathbb{E} $, exactly one (by the UMT) of $A-B-C$, $B-A-C$, $A-C-B$ occurs, and each relation implies the same relation happens in $\mathbb{M}$.
            \bigbreak \noindent 
            If $A-B-C$ happens in $\mathbb{M}$, then the other two do not by the UMT,  so only $A-B-C$ will then be true in $\mathbb{E}$. We state
            \begin{align*}
                A-B-C \text{ in } \mathbb{E} \iff A-B-C \text{ in } \mathbb{M}
            \end{align*}
        \item \textbf{Betweenness among the planes}: We have
            \begin{align*}
                A\text{-}B\text{-}C \text{ in } \mathbb{E} \iff A\text{-}B\text{-}C \text{ in } \mathbb{M} \\
                A\text{-}B\text{-}C \text{ in } \mathbb{E} \iff A\text{-}B\text{-}C \text{ in } \mathbb{G} \\
                A\text{-}B\text{-}C \text{ in } \mathbb{E} \iff A\text{-}B\text{-}C \text{ in } \mathbb{H} \\
            \end{align*}
        \item \textbf{Inside out}: Consider $\mathbb{P} = \{A,B,C,D,E,F\}$, $\mathbb{L}:\ \ell = \{A,B,C,D\}, m = \{A,E\}, n  = \{C,E\} , v = \{D,E\} $, and distance
            \begin{align*}
                \begin{array}{c|ccccc}
                   &A&B&C&D&E \\ 
                    A & 0 & 3 & 1 & 2 & 4\\
                    B &  3 & 0 & 2  & 1 & 4\\
                    C &  1 & 2 & 0 & 3 & 4\\
                    D & 2 &1 & 3 & 0 & 4\\
                    E & 4 & 4& 4 & 4 & 0\\
                \end{array}
            \end{align*}
            \bigbreak \noindent 
            \begin{figure}[ht]
                \centering
                \incfig{ic}
                \label{fig:ic}
            \end{figure}
            \bigbreak \noindent 
            The seven axioms hold, $\mathbb{D} = \{0,1,2,3,4\}, \omega = 4$, and all betweenness occurs for points on $\ell$
            \begin{align*}
                A-C-B \quad A-D-B \quad C-A-D \quad C-B-D
            \end{align*}
        \item \textbf{Segments and rays}: Let $A\ne B$ be points in $ \mathbb{P}$ with $AB < \omega $. Then, there is a unique line through $A,B$, call it $ \overleftrightarrow{AB}$ 
            \begin{itemize}
                \item \textbf{The segment} $\overline\{AB\} = \{A,B\} \cup \{X: A-X-B\}$
                \item \textbf{The ray} $\overrightarrow\{AB\} = \{A,B\} \cup \{X: A-X-B\} \cup \{X: A-B-X\}$
            \end{itemize}
            \textbf{Note:} $\{X: A-X-B\} \cup \{X: A-B-X\} = \varnothing$
            \bigbreak \noindent 
            \textbf{Notation}: $\overline{AB}, \overrightarrow{AB}, \overleftrightarrow{AB}$ denote sets of points, with $\{A,B\} \subseteq \overline{AB} \subseteq \overrightarrow{AB} \subseteq \overleftrightarrow{AB}$
        \item \textbf{Carriers}: We call the line $\overleftrightarrow{AB}$ the \textbf{carrier} of $\overrightarrow{AB} $
        \item \textbf{Segments and rays on $\mathbb{S}$}
            \bigbreak \noindent 
            \fig{.5}{./figures/24.png}
            \bigbreak \noindent 
            Ray $\overrightarrow{AB}$ goes from $A$, through $B$, around to $A^{*}$. Since $A-B-A^{*}$, $\overrightarrow{AB}$ includes $A^{*} $
            \bigbreak \noindent 
            We have 
            \begin{itemize}
                \item \textbf{Segment} $\overline{AB} = \{A,B\} \cup \{X: A-X-B\}$ as usual
                \item \textbf{Ray} $\overrightarrow{AB} = \{A,B\} \cup \{X: A-X-B\} \cup \{X: B-X-A^{*}\} \cup \{A^{*}\}$, where $A^{*}$ is the antipode of $A$
            \end{itemize}

        \item \textbf{Proving results about general (abstract) planes $\mathbb{P}$}: We only use the undefined terms point, line, distance, the definitions, the assumed axioms, previously proved results, arithmetic of $\mathbb{R}$, and logic.
            \bigbreak \noindent 
            Sketches from $\mathbb{E}$, while sometimes useful, are not valid for general proofs. General planes include many examples besides $\mathbb{E}$, and Euclidean pictures may not apply to them, and may be misleading.
            \bigbreak \noindent 
            We assume plane $\mathbb{P}$, in which we have points, lines, and the first seven axioms satisfied.
            \bigbreak \noindent 
            Recall, for points $A\ne B$, $AB < \omega$,
            \begin{itemize}
                \item \textbf{Betweenness}: $A-B-C$ if $A,B,C$ are different, collinear, and $AB + BC = AC $
                \item \textbf{Segment} $\overline{AB} = \{A,B\} \cup \{X: A-X-B\}$
                \item \textbf{Ray} $\overrightarrow{AB} = \{A,B\} \cup \{X: A-X-B\} \cup \{X: A-B-X\}  $
            \end{itemize}
        \item \textbf{Proposition 6.3: Segments and lines}:
            \bigbreak \noindent 
            \textbf{Proposition.}
            \begin{enumerate}[label=(\alph*)]
                \item $\overline{AB}$ lies in one line, the line $\overleftrightarrow{AB} $
                \item $\overline{AB} = \overline{BA} $
                \item If $x\in \overline{AB}$, with $X \ne B$, then $AX < AB $
            \end{enumerate}
            \bigbreak \noindent 
            \textbf{\textit{Proof}} a.) Since $\overline{AB}$ exists, we have $AB < \omega$. Thus, by incidence axioms three and four, there is exactly one line containing points $A$, and $B$. Namely, $\overleftrightarrow{AB} $. If $X$ is any other point in $\overline{AB}$, then $A-X-B$ by definition of $\overline{AB}$. Thus, $X$ is collinear with $A,B$ by definition of betweenness, and hence, $x \in \overleftrightarrow{AB}$
            \bigbreak \noindent 
            b.) We have
            \begin{align*}
                \overline{AB} &= \{A,B\} \cup \{X: A-X-B\} \tag{1}\\
                \overline{BA} &= \{B,A\} \cup \{X: B-X-A\} \tag{2}\\
            \end{align*}
            But, since ordering in sets doesn't matter, $\{B,A\} = \{A,B\}$, and we have seen previously that $B-X-A = A-X-B$. Thus, (2) is precisely (1). That is, $\{B,A\} \cup \{X: B-X-A\} = \{A,B\} \cup \{X: A-X-B\}$, and therefore $\overline{AB} = \overline{BA} $
            \bigbreak \noindent 
            c.) We have
            \begin{align*}
                \overline{AB} = \{A,B\} \cup \{X: A-X-B\}
            \end{align*}
            Let $x\in \overline{AB} $, with $X\ne B$. Then, we have $A-X-B$, and $AX + XB = AB$. This implies that $AB$ greater than both $AX, XB$, which means $AB > AX$.
            \bigbreak \noindent 
            \textbf{Note}: Ray $\overrightarrow{AB}$ is also contained in exactly one line, the line $\overleftrightarrow{AB}$
            \bigbreak \noindent 
            Also, $\overrightarrow{AB} = \overrightarrow{BA}$ mostly does not hold. We have
            \begin{align*}
                \overrightarrow{AB} &= \{A,B\} \cup \{X: A-X-B\} \cup \{X: A-B-X\} \\
                \overrightarrow{BA} &= \{A,B\} \cup \{X: A-X-B\} \cup \{X: B-A-X\} \\
            \end{align*}
            Since $\{X: A-B-X\} \ne \{X: B-A-X\}$, it  is not generally the case that $\overrightarrow{AB} = \overrightarrow{BA} $ (In general). The scenario where $\overrightarrow{AB} = \overrightarrow{BA} $ is when $\overline{AB}, \overline{BA}$ are exactly the line where they are contained. This fact is true in the IO example, since $\overline{AB} = \overline{BA } = \overline{CD} = \overline{DC} = \{A,B,C,D\} = \ell  $
        \item \textbf{Proposition 6.4}
            \bigbreak \noindent 
            \textbf{Proposition}: Let $A,B,C,D$ be collinear points with $0 < AB < \omega$, $0< CD<\omega$, and $\overline{AB} = \overline{CD}$, then
            \begin{enumerate}[label=(\alph*)]
                \item Either $\{A,B\} = \{C,D\}$ or $\{A,B\} \cap \{C,D\} = \varnothing$
                \item $AB = CD$
            \end{enumerate}
            \bigbreak \noindent 
            \textbf{\textit{Proof (a)}} Part a says that $\{A,B\}$ and $\{C,D\}$ can have two (all) elements in common, or no elements in common. Thus, we show that it cannot be the case that they have one element in common
            \bigbreak \noindent 
            Suppose for the sake of contradiction that $\{A,B\}$ and $\{C,D\}$ have exactly one element in common. Assume it is $A=C$. Then, $A \ne B$, $B\ne C$, and $B \ne D$. 
            \bigbreak \noindent 
            By definition of a segment, $D \in \overline{CD} = \{AB\}$, which implies $A\text{-}D\text{-}B$ since $D\ne A$ and $D \ne B$. Also, $B \in  \overline{AB} = \overline{CD}$ implies $C\text{-}B\text{-} D$ (since $B \ne C$ and $B \ne D$). But, since $A = C$, we have $C\text{-}B\text{-}D  = A\text{-}B\text{-}D$ which cannot happen by the UMT since we know we have $A\text{-}D\text{-}B $. Thus, our assumption that $A = C$ must be false. A similar argument for the other equality pairs shows that the two sets must not contain exactly one common element.
            \bigbreak \noindent 
            Therefore, $\{A,B\} = \{C,D\}$ or $\{A,B\} \cap \{C,D\} = \varnothing$
            \bigbreak \noindent 
            \textbf{\textit{Proof (b)}} If $\{A,B\} = \{C,D\} $ then $AB = CD$ by substitution. So, we may assume that $\{A,B\} \cap \{C,D\} = \varnothing$. 
            \bigbreak \noindent 
            We have $C,D \in \overline{CD} = \overline{AB}$, and $C,D \ne A \text{ or } B$, which implies $A\text{-}C\text{-} B$ and $A\text{-}D\text{-}B$. Similarly, $A,B \in \overline{AB} = \overline{CD} $ yields $C\text{-}A\text{-}D$ and $C\text{-}B\text{-}D$. Hence, we have $AC + AD = CD$ and $CB +BD = CD$. Adding these two equations and making suitable substitutions  yields $2CD = 2AB$, hence, $CD = AB$
            \endpf

    \end{itemize}

    \pagebreak 
    \subsection{Three axioms for the line}
    \begin{itemize}
        \item \textbf{Proposition 7.1}
            \bigbreak \noindent 
            \textbf{Proposition}. If $A\text{-}B\text{-}C$ and $A\text{-}C\text{-}D$, then $A,B,C,D$ are distinct and collinear 
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} Be the definition of betweenness, $A,C,D$ are distinct and collinear, and $AC + CD = AD$. Since $CD > 0$, $AC < AD$. But, since $AD \leq \omega$, it must be that $AC < \omega$. Thus, $A,C$ are together in a unique line  (the line $\overleftrightarrow{AC} $)
            \bigbreak \noindent 
            Also, $A,B,C$ are distinct and collinear. Thus, $B,D$ are both collinear with $A$ and $C$ which implies all four points must be in $\overleftrightarrow{AC}$, and hence they are all collinear.
            \bigbreak \noindent 
            The only way two of $A,B,C,D$ could be equal is if $B =D$. But then, substituting $B$ for $D$ in $A\text{-}C\text{-}D$, we get $A\text{-}C\text{-}B$. This contradicts $A\text{-}B\text{-}C$ and the UMT. Thus, all four points are different. \endpf

        \item \textbf{Definition:} Define $A\text{-}B\text{-}C\text{-}D$ to mean the following betweenness relations are all satisfied
            \begin{align*}
                A\text{-}B\text{-}C \quad A\text{-}B\text{-}D \quad A\text{-}C\text{-}D \quad B\text{-}C\text{-}D 
            \end{align*}
            \bigbreak \noindent 
            Also, for collinear points $A,B,C,D$
            \begin{align*}
                A\text{-}B\text{-}C\text{-}D \implies AB + BC + CD = AD
            \end{align*}
        \item \textbf{Proposition 7.2} If $A\text{-}B\text{-}C\text{-}D$, then $A,B,C,D$ are distinct and collinear, and $D\text{-}C\text{-}B\text{-}A $
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} $A\text{-}B\text{-}C\text{-}D$ implies $ A\text{-}B\text{-}C$, $ A\text{-}B\text{-}D, A\text{-}C\text{-}D, B\text{-}C\text{-}D$. Since $ A\text{-}B\text{-}C$ and $ A\text{-}C\text{-}D$ are true, then $A,B,C,D$ are distinct and collinear, if we switch the order on the four betweenness relations (first point and last for each of them), we get precisely 
            \begin{align*}
                D\text{-}C\text{-}B\text{-}A
            \end{align*}
            \endpf
        \item \textbf{Betweenness of points axiom (Ax. BP)}: If $A,B,C$ are distinct, collinear points, and if $AB + BC \leq \omega$, then there exists a betweenness relation among $A,B,C$
            \bigbreak \noindent 
            What this is really saying is that if \textbf{any} of $AB + BC$, $BA + AC$, $AC + CB$ is $ \leq \omega$, then there is a betweenness relation.
            \bigbreak \noindent 
            \textbf{Note:} If Ax.BP is true for a plane $\mathbb{P}$, and if $AB + BC \leq \omega$ for distinct collinear $A,B,C$, then there is a betweenness relation, but not necessarily $ A\text{-}B\text{-}C $
            \bigbreak \noindent 
            When $\omega = \infty$, then for any distinct collinear $A,B,C$, $AB +BC  < \infty = \omega $, so there will be a betweenness relation
        \item \textbf{What would make Ax.BP false?} Three collinear points $A,B,C$ so that at least one of $AB + BC \leq \omega, AC + CB \leq \omega, BA + AC \leq \omega$, and no betweenness relation for $A,B,C$ exists
            \bigbreak \noindent 
            \textbf{Note:} If there are no lines with three points, then the axiom is vacuously true.
        \item \textbf{Planes with first 8 axioms}: Consider a general plane $\mathbb{P}$ with points, lines, distance, and all 8 axioms true. We can establish some important properties of all these planes
        \item \textbf{Triangle inequality for the line}: If $A,B,C$ are any three distinct, collinear points, then 
            \begin{align*}
                AB + BC \geq AC 
            \end{align*}
            \bigbreak \noindent 
            \textbf{Note:} Don't worry about why the word triangle is in the name. Also, the triangle inequality is not necessarily true without Ax.BP
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} We examine two cases, either $ AB + BC > \omega$, or $AB + BC \leq \omega$. In this first case, $AB +BC > \omega$ implies $AB +BC > AC$, since by the definition of $\omega$, $AC \leq \omega$.
            \bigbreak \noindent 
            Next, we consider $AB + BC \leq \omega$. In this case, by Ax.BP, there must exist a betweenness relation among $A,B,C$. One of the following must be satisfied
            \begin{align*}
                A\text{-}B\text{-}C \\
                B\text{-}A\text{-}C \\
                A\text{-}C\text{-}B
            \end{align*}
            \bigbreak \noindent 
            Assume its $ A\text{-}B\text{-}C$, then by definition of $ A\text{-}B\text{-}C$, we have $AB + BC = AC$, which implies $AB + BC \geq AC$ is satisfied.
            \bigbreak \noindent 
            Next, assume its $ B\text{-}A\text{-}C$, then $BA + AC = BC$. We have
            \begin{align*}
                AC &= BC - AB \\
                AC + 2AB &= BC - AB + 2AB \\
                AC + 2AB &= BC + AB \\
                AC + 2AB &= AB + BC
            \end{align*}
            In this case, since $2AB > 0$ by distance axiom 2, we have $AC + 2AB \geq AC$. Thus, $AC + 2AB = AB + BC \geq AC $ 
            \bigbreak \noindent 
            Lastly, assume the relation we have is $ A\text{-}C\text{-}B$, then $AC + CB = AB$. We have
            \begin{align*}
                AC + CB &= AB \\
                AC &= AB - CB \\
                AC + 2CB &= AB - CB + 2CB \\
                AC + 2CB &= AB + CB \\
                AC + 2CB &= AB + BC \\
            \end{align*}
            Similar to the previous argument, since $AC + 2CB \geq AC$, we have $AC + 2CB = AB + BC \geq AC $
            \bigbreak \noindent 
            Therefore, $ AB + BC \geq AC$ \endpf
        \item \textbf{Rule of insertion}: 
            \begin{itemize}
                \item If $ A\text{-}B\text{-}C$ and $ A\text{-}X\text{-}B$, then $ A\text{-}X\text{-}B\text{-}C $
                \item If $ A\text{-}B\text{-}C$ and $ B\text{-}X\text{-}C$, then $ A\text{-}B\text{-}X\text{-}C $
            \end{itemize}
            \bigbreak \noindent 
            \textbf{\textit{Proof (a).}} Since $ A\text{-}B\text{-}C$, and $ A\text{-}X\text{-}B$, then we know that $A,X,B,C$ are distinct, collinear. 
            \bigbreak \noindent 
            By the definition of betweenness, we have
            \begin{align*}
                AB  + BC &= AC \\
                AX + XB &= AB
            \end{align*}
            Thus,
            \begin{align*}
                AX + XB + BC &= AC
            \end{align*}
            By the triangle inequality, we have $XB + BC \geq XC$. Thus,
            \begin{align*}
                AC & =AX + XB +  BC \geq AX + XC
            \end{align*}
            But the triangle inequality also implies that 
            \begin{align*}
                AX + XC \geq AC
            \end{align*}
            Thus, since $AX + XC \leq AC \leq AX + XC$. Thus, it must be that $AC = AX + XC$. Hence, $ A\text{-}X\text{-}C $. Next, plugging $AC = AX + XC$ into $AC = AX + XB + BC$ yields
            \begin{align*}
                AX + XC &= AX + XB + BC \\
                \implies XB + BC &= XC
            \end{align*}
            Thus, $ X\text{-}B\text{-}C $.
            \bigbreak \noindent 
            \textbf{\textit{Proof (b)}} If $ A\text{-}B\text{-}C$ and $ B\text{-}X\text{-}C$, then $ C\text{-}B\text{-}A$ and $ C\text{-}X\text{-}B $, so by part (a), we have
            \begin{align*}
                C\text{-}X\text{-}B\text{-}A
            \end{align*}
            Which means $ A\text{-}B\text{-}X\text{-}C$ \endpf
        \item \textbf{What does the betweenness of points axiom get us?} The triangle inequality and the insertion theorem 
        \item \textbf{Quadrichotomy Axiom for Points (Ax.QP)}: If $A,B,C,X$ are distinct, collinear points, and if $ A\text{-}B\text{-}C$. Then, at least one of the following must hold
            \begin{align*}
                X\text{-}A\text{-}B, \quad A\text{-}X\text{-}B, \quad B\text{-}X\text{-}C, \quad \text{or } \quad B\text{-}C\text{-}X
            \end{align*}
            \bigbreak \noindent 
            Thus, Ax.QP says that whenever $ A\text{-}B\text{-}C$ (say on line $\ell$), then any other point $X$ on line $\ell$ is in either $ \overrightarrow{BA} $ or $ \overrightarrow{BC} $. That is,
            \begin{align*}
                \ell = \overrightarrow{BA} \cup \overrightarrow{BC}
            \end{align*}
            \bigbreak \noindent 
            Ax.QP is true for
            \begin{itemize}
                \item $\mathbb{E}$
                \item $\mathbb{M}$
                \item $\mathbb{G}$
                \item $\mathbb{H}$
                \item $\mathbb{S}$
                \item $\mathbb{R}^{3}$
                \item $\hat{\mathbb{E}}$ (bumpy plane)
            \end{itemize}
            It is also true vacuously for the white stripes model, the TDM, and the Fano plane
            \bigbreak \noindent 
            Ax.QP is also true for the Inside Out (IO) example. It is vacuously two for the 2-point lines, but we can also check that it is satisfied for $\ell = \{A,B,C,D\} $
            \bigbreak \noindent 
            \textbf{Note:} If the first 8 axioms are true for a plane $\mathbb{P}$, and if $\omega = \infty$, then the statement of Ax.QP can be proved to hold true in $\mathbb{P}$. A key reason for this is that because $\omega = \infty$, any three collinear points must have a betweenness relation.
        \item \textbf{When is Ax.QP false?}: In a plane with at least four collinear points $A,B,C,X$ with $ A\text{-}B\text{-}C$, and none of 
            \bigbreak \noindent 
            \begin{align*}
                    X\text{-}A\text{-}B, \quad A\text{-}X\text{-}B, \quad B\text{-}X\text{-}C, \quad \text{or } \quad B\text{-}C\text{-}X
            \end{align*}
            Are true
        \item \textbf{Proposition 7.5}: If $X \ne Y$ are points distinct from $A$ on ray $\overrightarrow{AB}$, then at least one of $ A\text{-}X\text{-}Y$ or $ A\text{-}Y\text{-}X$ or $X,Y$ in $ \overline{AB}$ is true.
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} If $Y = B$, then either $ A\text{-}X\text{-}Y$ or $ A\text{-}Y\text{-}X$ by definition of $ \overrightarrow{AB}$, so we may assume that $Y \ne B$, and similarly that $X \ne B$.
            \bigbreak \noindent 
            Now either $ A\text{-}X\text{-}B$ or $ A\text{-}B\text{-}X$, and either $ A\text{-}Y\text{-}B$ or $ A\text{-}B\text{-}Y$ by definition of $ \overrightarrow{AB}$, we consider each of the four possibilities. 
            \bigbreak \noindent 
            Suppose that $ A\text{-}B\text{-}X$ and $ A\text{-}Y\text{-}B$ are true. Then, the rule of insertion says that $ A\text{-}Y\text{-}B\text{-}X$, which gets us $ A\text{-}Y\text{-}X$. Similarly, if $ A\text{-}B\text{-}Y$ and $ A\text{-}X\text{-}B$ are true, then we get $ A\text{-}X\text{-}Y$ by the rule of insertion
            \bigbreak \noindent 
            Suppose that $ A\text{-}B\text{-}X$ and $ A\text{-}B\text{-}Y$ are true. $ A\text{-}B\text{-}X $ and Ax.QP says that one of 
            \begin{align*}
                Y\text{-}A\text{-}B, \quad A\text{-}Y\text{-}B, \quad B\text{-}Y\text{-}X, \quad \text{ or } \quad B\text{-}X\text{-}Y 
            \end{align*}
            Is true. But, since $ A\text{-}B\text{-}Y$ is true, then we can't have either $ Y\text{-}A\text{-}B$ or $ A\text{-}Y\text{-}B$ because this would contradict the UMT. Thus, we know we have either $ B\text{-}Y\text{-}X$  or $ B\text{-}X\text{-}Y$. If $ B\text{-}Y\text{-}X$ then by the rule of insertion we get $ A\text{-}Y\text{-}X$. If $ B\text{-}X\text{-}Y $ then the rule of insertion gets us $ A\text{-}X\text{-}Y$
            \bigbreak \noindent 
            Finally, suppose that $ A\text{-}X\text{-}B$ and $ A\text{-}Y\text{-}B$. The best we can do is assert that $X,Y$ are in $ \overline{AB}$ by the definition  of a segment.
        \item \textbf{What we have so far}: Nine axioms for a general plane, which are satisfied in the following examples that we have seen
            \begin{itemize}
                \item $\mathbb{E} $
                \item $\mathbb{M} $
                \item $\mathbb{S} $
                \item $\mathbb{H} $
                \item $\mathbb{G} $
                \item $\hat{\mathbb{E}} $
                \item $ws$
                \item $\mathbb{R}^{3} $
                \item Fano
                \item TDM
                \item IO
            \end{itemize}
            None of the axioms so far says that rays have to exist on any particular line, or even in the plane overall. We need another axiom to guarantee that rays (and segments) exist on every line.
        \item \textbf{Nontriviality Axiom (Ax.N)}: For any point $A$ on a line $\ell$ there exists a point $B$ on $\ell$ with $0 < AB < \omega$
            \bigbreak \noindent 
            This axiom is true for the planes in which $\omega = \infty$ ($\mathbb{E}$, $\mathbb{M}$, $\mathbb{H}$, $\mathbb{G}$, $\mathbb{R}^{3}$, $\hat{\mathbb{E}} $, ws)
            \bigbreak \noindent 
            This axiom is also true for $\mathbb{S}$ and Fano, where $\omega < \infty $
            \bigbreak \noindent 
            Ax.N is false for TDM, and for the two point lines in IO.
            \bigbreak \noindent 
            We now assume the IO axioms for our general plane $\mathbb{P}$
        \item \textbf{Theorem 7.6}: This next theorem is the only one in chapters 6-9 that is about points on more than a single line
            \bigbreak \noindent 
            \textbf{Theorem.} For any point $A$ on a line $\ell$ there exists a point $C$ not on $\ell$ with $0 < AC <\omega$ 
            \bigbreak \noindent 
            To prove this, we first show that there is a point $C$ not on $\ell$, then we show that this point $C$ satisfies $0 < AC < \omega$
            \bigbreak \noindent 
            \textbf{\textit{Proof.}}
            \bigbreak \noindent 
            First, we get a point $X$ not on $\ell$. By incidence axiom 1, there is a line $m \ne \ell$, by incidence axiom 2, there is a point $X$ on $m$. By Ax.N, there is a point $y$ on $m$ with 
            \begin{align*}
                0 < XY < \omega
            \end{align*}
            Thus, $\overleftrightarrow{XY} $ is the \textbf{unique} line through $X$ and $Y$. If $X,Y$ were both on $\ell$, then $\ell = \overleftrightarrow{XY} = m$, which contradicts there being a unique line through $X,Y$. Thus, at least one of $X$ or $Y$, say $X$ is not on $\ell$.
            \bigbreak \noindent 
            \begin{figure}[ht]
                \centering
                \incfig{fig123}
                \label{fig:fig123}
            \end{figure}
            \bigbreak \noindent 
            Second, we need to get a point $C$ not on $\ell$, with $0 < AC < \omega$
            \bigbreak \noindent 
            There is a line $t$ through $A$ and $X$, with $t\ne \ell$, since $X$ is on $t$, but not on $\ell$. Ax.N says that there is a point $C$ on $t$ with $0 < AC < \omega$. By incidence axiom 4, $t$ is the only line through $A$ and $C$. If $C$ were on $\ell$, then both $A$ and $C$ on $\ell$ would imply $\ell = t$, which is a contradiction.
            \bigbreak \noindent 
            Therefore, $C$ is not on $\ell$, and $0 < AC < \omega$.
            \bigbreak \noindent 
\begin{figure}[ht]
    \centering
    \incfig{fig12}
    \label{fig:fig12}
\end{figure}
        \item \textbf{Note about Ax.N} This axiom stops us from construction examples of planes in which all points are collinear. (See proof above)
        \item \textbf{Important fact}:  Suppose $X$ is a point on a ray $\overrightarrow{AB}$ in a general plane.
            \begin{enumerate}
                \item If $ A\text{-}X\text{-}B$ then $AX < AB $
                \item If $ A\text{-}B\text{-}X$ then $AX > AB $
                \item IF $X = B$ then $AX = AB$
            \end{enumerate}


    \end{itemize}




    \pagebreak 
    \subsection{Exam 1 Axioms definitions and theorems}
    \begin{itemize}
    \item \textbf{Euclids fifth postulate}: If a straight line intersects two straight lines such that the interior angles on one side add up to less than two right angles, then the two straight lines, if extended indefinitely, will meet on the side where the angles are less than two right angles.
    \item \textbf{Playfairs postulate}: ”Through a given point not on a line, there is exactly one line parallel to the given line”
        \item \textbf{$\mathbb{E}$}
            \begin{itemize}
                \item \textbf{Points:} $(x,y)$
                \item \textbf{Lines:} Each \textit{nonvertical line} $\ell$ in $\mathbb{E}$ consists of all points $(x,y)$, where $y = mx + b$ for some fixed $m$ and $b$. Each \textit{vertical line} $\ell$ consists of all $(x,y)$, where $x=a$ for some fixed $a$
                \item \textbf{Distance}: $e(AB) = \sqrt{(x_{2} - x_{1})^{2} + (y_{2} - y_{1})^{2}} = \left\lvert x_{1}-x_{2} \right\rvert \sqrt{1+m^{2}}$
            \end{itemize}
        \item \textbf{$\mathbb{M}$}
            \begin{itemize}
                \item \textbf{Points:} $(x,y)$
                \item \textbf{Lines}: Same as in $\mathbb{E}$
                \item \textbf{Distance}: $d_{\mathbb{M}}(Ab) = \left\lvert x_{2} - x_{1} \right\rvert + \left\lvert y_{2} - y_{1} \right\rvert = \left\lvert x_{1} - x_{2} \right\rvert(1+\left\lvert m \right\rvert$) 
            \end{itemize}
        \item \textbf{$\mathbb{S}$}:
            \begin{itemize}
                \item \textbf{Points:} $(x,y)$
                \item \textbf{Lines:} Great circle through two points
                \item \textbf{Distance}:
                    \begin{align*}
                        d_{\mathbb{S}} &= r\theta = r\cos^{-1}{\left(\frac{ax+by+cz}{r^{2}}\right)}
                    \end{align*}
            \end{itemize}
        \item \textbf{$\mathbb{H}$}
            \begin{itemize}
                \item \textbf{Points}: $(x,y)$
                \item \textbf{Lines}: Chords on unit circle through two given points
                \item \textbf{Distance}: $d_{\mathbb{H}} = \ln{\left(\frac{e(AN)e(BM)}{e(AM)e(BM)}\right)}$
                    \bigbreak \noindent 
                    Use the following formulas if $M,N$ have the chance of being misplaced
                    \begin{align*}
                        \left\lvert \ln{\left(\frac{e(AN)e(BM)}{e(AM)e(BN)}\right)} \right\rvert = \left\lvert \ln{\left(\frac{e(AM)e(BN)}{e(AN)e(BM)}\right)} \right\rvert
                    \end{align*}
            \end{itemize}
        \item \textbf{$\mathbb{G}$}
            \begin{itemize}
                \item \textbf{Points:}   $(x,y)$, $x \leq 0$,or  $x > 1 $
                \item \textbf{Lines:} Lines in $\mathbb{G}$ are defined to be the same as in $\mathbb{E}$, except that for any nonvertical line $y=mx+b$, the part in the missing strip is deleted. So a typical nonvertical line $\ell$ consists of all $(x,y)$ with $y=mx+b$ ($m,b$ fixed) and with $x \leq 0$ or $x > 1$
                \item \textbf{Distance}: 
                    \begin{align*}
                        d_{\mathbb{G}}(AB) = \begin{cases}
                            e(AB) & \text{ for $A,B$ on the same side of the gap} \\     
                            e(AB) - e(CD) & \text{ for $A,B$ on the opposite sides of the gap} 
                        \end{cases}
                    \end{align*}
                \item \textbf{$\hat{\mathbb{E}}$}
                \item \textbf{Fano}
                \item \textbf{White stripes}
                \item \textbf{Trivial discrete}
                \item \textbf{Inside out}
            \end{itemize}
        \item \textbf{Interior and Exterior angles}: Interior angles are the angles inside the triangle. Each vertex of the triangle has one interior angle. The sum of the interior angles of a triangle is always $180^{\circ}$
            \bigbreak \noindent 
            Exterior angles are the angles formed outside the triangle when one side of the triangle is extended.
            At each vertex, an exterior angle is supplementary to the interior angle (they add up to $180^{\circ} $
            \bigbreak \noindent 
            If an interior angle at a vertex is $A$, the corresponding exterior angle $E$ is:
            \begin{align*}
                E = 180^{\circ} - A
            \end{align*}
            \bigbreak \noindent 
            The sum of the exterior angles of a triangle (one at each vertex) is always $360^{\circ}$ , regardless of the shape of the triangle.
        \item \textbf{Remote angles}: Remote angles refer to the interior angles of a triangle that are not adjacent to a given exterior angle
        \item \textbf{More on points}:
            \begin{itemize}
                \item \textbf{Collinear points}: Points that lie on the same straight line.
                \item \textbf{Noncollinear points}: Points that do not lie on the same straight line.
                \item \textbf{Coplanar points}: Points that lie on the same plane.
                \item \textbf{Concurrent Points}: Points where three or more lines intersect.
                \item \textbf{Equidistant Points}: Points that are all the same distance from a particular point or object.
                \item \textbf{Lattice Points}:  Points with integer coordinates.
                \item \textbf{Interior points:} Points that lie inside a given shape.
                \item \textbf{Exterior points:} Points that lie outside a given shape.
            \end{itemize}
        \item \textbf{Congruent triangles}: Congruent triangles are triangles that are exactly the same in shape and size. This means that all corresponding sides and angles of one triangle are equal to those of the other triangle.
        \item \textbf{Vertical (opposite) angles}: Vertical angles (also called opposite angles) are the angles that are formed by two intersecting lines and are opposite to each other
        \item \textbf{Theorem (\textit{Exterior angle inequality})}: An exterior angle of a triangle is greater than either remote interior angle. That is, if $\triangle ABC$ is a any triangle, and point $D$ is on the extension of segment $\overline{BC}$ through $C$, then
            \begin{align*}
                \angle ACD > \text{ both } \angle A \text{ and } \angle B
            \end{align*}
        \item \textbf{Relationship of angles}:
            \bigbreak \noindent 
            % \begin{figure}[ht]
            %     \centering
            %     \incfig{tc}
            %     \label{fig:tc}
            % \end{figure}
            \bigbreak \noindent 
            \begin{itemize}
                \item \textbf{Interior angles}: Interior angles are the angles that are inside the transversal configuration. Angles $a,b,c,d$ are interior
                \item \textbf{Exterior angles}: Exterior angles are the angles that are inside the transversal configuration. Angles $e,f,g,h$ are exterior
                \item \textbf{Consecutive interior angles}: Pairs of interior angles that are on the same side of the transversal. Angles $c,d$ are consecutive interior, and $a,b$ are consecutive interior
                \item \textbf{Consecutive exterior angles}: Pairs of exterior angles that are on the outside of the transversal configuration. Angles $e,g$ are consecutive exterior, angles $f,h$ are consecutive exterior
                \item \textbf{Alternate interior angles}: Pairs of interior angles that are on opposite sides but not complementary, angles $b,d$ and $a,c$ are alternate interior
                \item \textbf{Alternate exterior angles}: Pairs of exterior angles that are on opposite sides but not complementary, angles $e,h$, and $f,g$ are alternate exterior
                \item \textbf{Vertical angles}: Angles that are opposite each other, formed when two lines intersect. Vertical angles are of equal measure. Pairs $d,h$ - $a,g$ - $e,b$ - and $f,c$ are vertical
                \item \textbf{Supplementary angles}: Angle pairs that sum to 180, pairs $a,h$ - $d,g$ - $f,b$ - and $e,c$ are supplementary
                \item \textbf{Complementary angles}: Angle pairs that sum to 90, none in the transversal configuration
            \end{itemize}
        \item \textbf{Proposition (Equal alternate interior angles)}. Suppose $a + b = 180$, then $b = d$, and $c=a $.
        \item \textbf{Upper bounds}: Suppose $S$ is a set of real numbers, we define $b \in \mathbb{R}$ as an \textit{upper bound} for $S$ if for all $x\in S, x \leq b$
            \bigbreak \noindent 
            The negation of this definition is, there exists $x\in S$ such that $x \nleq b$, or $x > b$. Thus, to prove some $b$ is not an upper bound for $S$, we can show that some element of $S$ is greater than $b$
            \bigbreak \noindent 
            There are of course  sets that do not have any upper bounds. Consider the set $S = \{n:\ n\in \mathbb{N} \text{ and } n>0\} $. This set has no upper bound.
            \bigbreak \noindent 
            If $S = \varnothing$, then every $b \in \mathbb{R}$ is an upper bound for $S$. This statement is vacuously true.
        \item \textbf{Least upper bound (supremum)}: $c\in \mathbb{R}$ is a \textit{least upper bound} of a set $S$ of real numbers if 
            \begin{enumerate}
                \item $c$ is an upper bound for $S$
                \item $c \leq b$ for all upper bounds $b$ of $S$
            \end{enumerate}
            \bigbreak \noindent 
            \textbf{Note:} The supremum of a set $S$ is denoted $ b = \text{sup}(S)$, where $b$ is the supremum of the set
        \item \textbf{Least upper bound property of $\mathbb{R}$}: If $S$ is a nonempty set of real numbers that has an upper bound in $\mathbb{R}$, then $S$ has a least upper bound (l.u.b) in $\mathbb{R} $
            \bigbreak \noindent 
            This justifies, among other things, that infinite decimals exist as real numbers, since an infinite decimal can be defined as the least upper bound of the set of all its finite truncations. For example, suppose $S$ is the set of all finite decimal expansions of $\pi$.
            \begin{align*}
                S = \{3,3.1,3.14,3.141, 3.1415,...\}
            \end{align*}
            Then, $S$ as an $l.u.b$ $\pi$, and $\pi\not\in S $
        \item \textbf{Least upper bound proposition}
            \bigbreak \noindent 
            \textbf{Proposition.} Let $S$ be a nonempty set of real numbers that has a least upper bound $b \in \mathbb{R}$. Let $t \in \mathbb{R}$ such that $t < b$. Then, there exists some $s \in S$ such that $t < s \leq b$.
        \item \textbf{Lower bounds}: Let $S$ be a nonempty set of real numbers. Then $g\in \mathbb{R}$ is a \textit{lower bound} for $S$ if $g \leq x$ for all $x\in S$.
        \item \textbf{Greater lower bounds (Infimum)}: $h\in \mathbb{R}$ is a \textit{greatest lower bound}, also called the \textit{infimum}, or \textit{inf} for $S$ if $ h$ is a lower bound for $S$ and $h \geq g$ for all lower bounds $g$ of $S$
        \item \textbf{Infimum proposition}
            \bigbreak \noindent 
            \textbf{Proposition}. Let $S$ be a nonempty set or real numbers that has a lower bound in $\mathbb{R}$. Then $S$ has a infimum in $\mathbb{R}$





        \item \textbf{Undefined terms}: 
            \begin{itemize}
                \item \textbf{$\mathbb{P}$:} Set of elements, called \textbf{points.}
                \item \textbf{$\mathbb{L}$:} Collection of subsets of $\mathbb{P}$, called \textbf{lines}
                \item A function $d:\ \mathbb{P}\times \mathbb{P} \to \mathbb{R} $, called a \textbf{distance function}
            \end{itemize}
        \item \textbf{Notation, terminology}
        \begin{itemize}
            \item A line is a set of points
            \item If $P$ is on the line $m$ ($P\in m$), we say that "$P$ is on $m$", or "$m$ goes through $P$"
            \item If two or more points are on the same line, we say they are \textbf{collinear}
            \item Denote distance $d(P,Q)$, or $d(PQ)$, or just $PQ$
        \end{itemize}
    \item         \textbf{Axiom of distance}: For all points $P,Q$
        \begin{enumerate}
            \item $PQ \geq 0 $
            \item $PQ = 0 \iff P=Q $
            \item $PQ = QP $
        \end{enumerate}
    \item         \textbf{Axioms of incidence}
        \begin{enumerate}
            \item There are at least two different lines
            \item Each line contains at least two different points
            \item Each pair of points are together in at least one line
            \item Each pair of points $P,Q$, with $PQ < \omega$ are together in at most one line
        \end{enumerate}
    \item \textbf{Betweenness of points axiom (Ax. BP)}: If $A,B,C$ are distinct, collinear points, and if $AB + BC \leq \omega$, then there exists a betweenness relation among $A,B,C$
        \bigbreak \noindent 
        What this is really saying is that if \textbf{any} of $AB + BC$, $BA + AC$, $AC + CB$ is $ \leq \omega$, then there is a betweenness relation.
        \bigbreak \noindent 
        \textbf{Note:} If Ax.BP is true for a plane $\mathbb{P}$, and if $AB + BC \leq \omega$ for distinct collinear $A,B,C$, then there is a betweenness relation, but not necessarily $ A\text{-}B\text{-}C $
        \bigbreak \noindent 
        When $\omega = \infty$, then for any distinct collinear $A,B,C$, $AB +BC  < \infty = \omega $, so there will be a betweenness relation
    \item \textbf{Quadrichotomy Axiom for Points (Ax.QP)}: If $A,B,C,X$ are distinct, collinear points, and if $ A\text{-}B\text{-}C$. Then, at least one of the following must hold
        \begin{align*}
            X\text{-}A\text{-}B, \quad A\text{-}X\text{-}B, \quad B\text{-}X\text{-}C, \quad \text{or } \quad B\text{-}C\text{-}X
        \end{align*}
        \bigbreak \noindent 
        Thus, Ax.QP says that whenever $ A\text{-}B\text{-}C$ (say on line $\ell$), then any other point $X$ on line $\ell$ is in either $ \overrightarrow{BA} $ or $ \overrightarrow{BC} $. That is,
        \begin{align*}
            \ell = \overrightarrow{BA} \cup \overrightarrow{BC}
        \end{align*}

    \item \textbf{Nontriviality Axiom (Ax.N)}: For any point $A$ on a line $\ell$ there exists a point $B$ on $\ell$ with $0 < AB < \omega$
        \bigbreak \noindent 
        This axiom is true for the planes in which $\omega = \infty$ ($\mathbb{E}$, $\mathbb{M}$, $\mathbb{H}$, $\mathbb{G}$, $\mathbb{R}^{3}$, $\hat{\mathbb{E}} $, ws)
        \bigbreak \noindent 
        This axiom is also true for $\mathbb{S}$ and Fano, where $\omega < \infty $
    \item \textbf{Betweenness}: Let $\mathbb{P}$ be a plane with points, lines, distance, and satisfy the seven axioms (3 distance, 4 incidence). Define
        \bigbreak \noindent 
        \textbf{Definition.} Point $B$ lies \textbf{between} points $A$ and $C$, denoted $A-B-C$ provide that
        \begin{enumerate}
            \item $A,B$, and $C$ are different and collinear
            \item $AB + BC = AC $
        \end{enumerate}
    \item \textbf{Uniqueness Middle Theorem (UMT)}:
        \bigbreak \noindent 
        \textbf{Theorem}: If $A-B-C$ then $B-A-C$ and $A-C-B$ are false.
    \item \textbf{Segments and rays}: Let $A\ne B$ be points in $ \mathbb{P}$ with $AB < \omega $. Then, there is a unique line through $A,B$, call it $ \overleftrightarrow{AB}$ 
        \begin{itemize}
            \item \textbf{The segment} $\overline\{AB\} = \{A,B\} \cup \{X: A-X-B\}$
            \item \textbf{The ray} $\overrightarrow\{AB\} = \{A,B\} \cup \{X: A-X-B\} \cup \{X: A-B-X\}$
        \end{itemize}
        \textbf{Note:} $\{X: A-X-B\} \cup \{X: A-B-X\} = \varnothing$
        \bigbreak \noindent 
        \textbf{Notation}: $\overline{AB}, \overrightarrow{AB}, \overleftrightarrow{AB}$ denote sets of points, with $\{A,B\} \subseteq \overline{AB} \subseteq \overrightarrow{AB} \subseteq \overleftrightarrow{AB}$
    \item \textbf{Segments and rays on $\mathbb{S}$}
        \begin{itemize}
            \item \textbf{Segment} $\overline{AB} = \{A,B\} \cup \{X: A-X-B\}$ as usual
            \item \textbf{Ray} $\overrightarrow{AB} = \{A,B\} \cup \{X: A-X-B\} \cup \{X: B-X-A^{*}\} \cup \{A^{*}\}$, where $A^{*}$ is the antipode of $A$
        \end{itemize}
    \item \textbf{Proposition: Segments and lines}:
        \bigbreak \noindent 
        \textbf{Proposition.}
        \begin{enumerate}[label=(\alph*)]
            \item $\overline{AB}$ lies in one line, the line $\overleftrightarrow{AB} $
            \item $\overline{AB} = \overline{BA} $
            \item If $x\in \overline{AB}$, with $X \ne B$, then $AX < AB $
        \end{enumerate}
    \item \textbf{Proposition}
        \bigbreak \noindent 
        \textbf{Proposition}: Let $A,B,C,D$ be collinear points with $0 < AB < \omega$, $0< CD<\omega$, and $\overline{AB} = \overline{CD}$, then
        \begin{enumerate}[label=(\alph*)]
            \item Either $\{A,B\} = \{C,D\}$ or $\{A,B\} \cap \{C,D\} = \varnothing$
            \item $AB = CD$
        \end{enumerate}
    \item \textbf{Proposition}
        \bigbreak \noindent 
        \textbf{Proposition}. If $A\text{-}B\text{-}C$ and $A\text{-}C\text{-}D$, then $A,B,C,D$ are distinct and collinear 
        \bigbreak \noindent 
        \textbf{\textit{Proof.}} Be the definition of betweenness, $A,C,D$ are distinct and collinear, and $AC + CD = AD$. Since $CD > 0$, $AC < AD$. But, since $AD \leq \omega$, it must be that $AC < \omega$. Thus, $A,C$ are together in a unique line  (the line $\overleftrightarrow{AC} $)
        \bigbreak \noindent 
        Also, $A,B,C$ are distinct and collinear. Thus, $B,D$ are both collinear with $A$ and $C$ which implies all four points must be in $\overleftrightarrow{AC}$, and hence they are all collinear.
        \bigbreak \noindent 
        The only way two of $A,B,C,D$ could be equal is if $B =D$. But then, substituting $B$ for $D$ in $A\text{-}C\text{-}D$, we get $A\text{-}C\text{-}B$. This contradicts $A\text{-}B\text{-}C$ and the UMT. Thus, all four points are different. \endpf

    \item \textbf{Definition:} Define $A\text{-}B\text{-}C\text{-}D$ to mean the following betweenness relations are all satisfied
        \begin{align*}
            A\text{-}B\text{-}C \quad A\text{-}B\text{-}D \quad A\text{-}C\text{-}D \quad B\text{-}C\text{-}D 
        \end{align*}
        \bigbreak \noindent 
        Also, for collinear points $A,B,C,D$
        \begin{align*}
            A\text{-}B\text{-}C\text{-}D \implies AB + BC + CD = AD
        \end{align*}
    \item \textbf{Proposition.} If $A\text{-}B\text{-}C\text{-}D$, then $A,B,C,D$ are distinct and collinear, and $D\text{-}C\text{-}B\text{-}A $
        \bigbreak \noindent 

    \item \textbf{Triangle inequality for the line}: If $A,B,C$ are any three distinct, collinear points, then 
        \begin{align*}
            AB + BC \geq AC 
        \end{align*}
        \bigbreak \noindent 
        \textbf{Note:} Don't worry about why the word triangle is in the name. Also, the triangle inequality is not necessarily true without Ax.BP
        \bigbreak \noindent 

    \item \textbf{Rule of insertion}: 
        \begin{itemize}
            \item If $ A\text{-}B\text{-}C$ and $ A\text{-}X\text{-}B$, then $ A\text{-}X\text{-}B\text{-}C $
            \item If $ A\text{-}B\text{-}C$ and $ B\text{-}X\text{-}C$, then $ A\text{-}B\text{-}X\text{-}C $
        \end{itemize}

    \item \textbf{What does the betweenness of points axiom get us?} The triangle inequality and the insertion theorem 

    \item \textbf{Proposition 7.5}: If $X \ne Y$ are points distinct from $A$ or ray $\overrightarrow{AB}$, then at least one of $ A\text{-}X\text{-}Y$ or $ A\text{-}Y\text{-}X$ or $X,Y$ in $ \overline{AB}$ is true.
    \item \textbf{Theorem 7.6}: This next theorem is the only one in chapters 6-9 that is about points on more than a single line
        \bigbreak \noindent 
        \textbf{Theorem.} For any point $A$ on a line $\ell$ there exists a point $C$ not on $\ell$ with $0 < AC <\omega$ 
        \bigbreak \noindent 
        To prove this, we first show that there is a point $C$ not on $\ell$, then we show that this point $C$ satisfies $0 < AC < \omega$

    \item \textbf{Important fact}:  Suppose $X$ is a point on a ray $\overrightarrow{AB}$ in a general plane.
        \begin{enumerate}
            \item If $ A\text{-}X\text{-}B$ then $AX < AB $
            \item If $ A\text{-}B\text{-}X$ then $AX > AB $
            \item IF $X = B$ then $AX = AB$
        \end{enumerate}









    \end{itemize}

    \pagebreak 
    \subsection{The real ray axiom, Antipodes, and opposite rays}
    \begin{itemize}
                \item \textbf{Real ray Axiom (Ax.RR)}: For any ray $ \overrightarrow{AB}$, and for any real number $s $ with $0 \leq s \leq \omega$, there is a point $X$ in $\overrightarrow{AB}$ with $AX = s$
            % \begin{align*}
            %     \{s:\ s \in \mathbb{R}, 0 \leq s \leq \omega \} = \{AX:\ X \in \overrightarrow{AB}\}
            % \end{align*}
            This axiom says that every nonnegative real number not exceeding $\omega$ produces at least one point on the ray.
            \bigbreak \noindent 
            So by $Ax.RR$, the distances $AX$, for all points $X$, $ \overrightarrow{AB}$ covers all real numbers in $[0,\infty]$ if $\omega < \infty$, and all real numbers in $[0,\infty) $ if $\omega = \infty $
        \item \textbf{Theorem 8.1}: If $\omega = \infty$, then $\mathbb{D} = [0,\infty$); if $\omega < \infty$, then $\mathbb{D} = [0,\omega] $
            \bigbreak \noindent 
            Since there are infinitely many real numbers in the interval $[0,\omega]$, for $\omega < \infty$, as well as in $[0,\infty)$, there must be infinitely many points on ray $\overrightarrow{AB}$
            \bigbreak \noindent 
            By Ax.N, any line $\ell$ in $\mathbb{P}$ contains points $A \ne B$, with $AB < \omega$ contains points $A \ne B$, with $AB < \omega$, hence $\ell$ contains ray $\overrightarrow{AB}$. Since $\overrightarrow{AB}$ has infinitely many points, so does $\ell $
            \bigbreak \noindent 
            The points in $\overline{AB}$ are exactly the points $X$ in $\overrightarrow{AB}$ with $AX = s \leq AB$. Since $[0,AB]$ contains infinitely many real numbers, there are also infinitely many on $\overline{AB}$. These remark proves the next theorem
        \item \textbf{Theorem 8.2} Each segment, ray, and line has infinitely many points.
            \bigbreak \noindent 
            Note that Ax.RR is false for WS and Fano, so this theorem does not hold. Also, Ax.RR is vacuously true for TDM, since there are no rays.
        \item \textbf{Proposition (needs proof) 8.11} Let $A,B$ be any two points on line $m$, with $0 < AB <\omega$. Then, there exists a point $C$ on $m$ with $ C\text{-}A\text{-}B$ and $ CB < \omega$.
        \item \textbf{Theorem 8.3}
            \bigbreak \noindent 
            \textbf{Proposition}. If $X \ne Y$ are points different from $A$ on ray $\overrightarrow{AB}$, then one of $ A\text{-}X\text{-}Y$ or $ A\text{-}Y\text{-}X$ is true.
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} Suppose toward a contradiction that the conclusion is false. So not $ A\text{-}X\text{-}Y$ and not $ A\text{-}Y\text{-}X $
            \bigbreak \noindent 
            By prop 7.5, $X,Y$ are in $\overline{AB}$. If $Y = B$, then $X$ in $\overline{AB}$ implies $ A\text{-}X\text{-}B$, so $ A\text{-}X\text{-}Y$, which is a contradiction
            \bigbreak \noindent 
            Similarly, if $X = B$, then $Y$ in $\overline{AB}$ implies $ A\text{-}Y\text{-}B$, so $ A\text{-}Y\text{-}X$, contradiction.
            \bigbreak \noindent 
            So neither $X$ nor $Y = B$, hence $ A\text{-}X\text{-}B$, $ A\text{-}Y\text{-}B$
            \bigbreak \noindent 
            Now, we use Ax.RR just to produce one more point on $\overrightarrow{AB}$. We know $AB < \omega$ by definition of $ \overrightarrow{AB}$. So, we pick a point $E$ such that $AE = s$, with $AB < s \leq \omega$ (there are infinitely many). Since $AE > AB$, $E$ must be in the ray $\overrightarrow{AB}$ but not in the segment $\overline{AB}$. Thus, $ A\text{-}B\text{-}E$
            \bigbreak \noindent 
            We have $AB + BE = AE$. Also, $BE = EB < AE \leq \omega$. So, $EB < \omega$, and $ \overrightarrow{EB}$ is defined
            \bigbreak \noindent 
            We have $ A\text{-}X\text{-}B$ and $ A\text{-}B\text{-}E$, so by ROI, we have $ A\text{-}X\text{-}B\text{-}E$ implies $ E\text{-}B\text{-}X $ 
            \bigbreak \noindent 
            Also, $ A\text{-}Y\text{-}B$ and $ A\text{-}B\text{-}E$ by the ROI says we have $ A\text{-}Y\text{-}B\text{-}E$, which implies $ E\text{-}B\text{-}Y $
            \bigbreak \noindent 
            So, points $X,Y$ are in ray $ \overrightarrow{EB}$ but not in $ \overline{EB} $. Let's now shift our focus to the ray $ \overrightarrow{EB}$. We can apply prop 7.5 to points $X,Y$ on ray $ \overrightarrow{EB}$. Either $ E\text{-}X\text{-}Y$ or $ E\text{-}Y\text{-}X$. Equivalently, either $ Y\text{-}X\text{-}E$ or $ X\text{-}Y\text{-}E $
            \bigbreak \noindent 
            Recall that we have $ A\text{-}Y\text{-}B\text{-}E$, which gives us $ A\text{-}Y\text{-}E$, and $ A\text{-}X\text{-}B\text{-}E$ gives us $ A\text{-}X\text{-}E$
            \bigbreak \noindent 
            If $ Y\text{-}X\text{-}E$, then $ A\text{-}Y\text{-}E$ and ROI gives us $ A\text{-}Y\text{-}X\text{-}E$, which implies $ A\text{-}Y\text{-}X$
            \bigbreak \noindent 
            If $ X\text{-}Y\text{-}E$, then $ A\text{-}X\text{-}E $ and ROI gives us $ A\text{-}X\text{-}Y\text{-}E$, which implies $ A\text{-}X\text{-}Y $
            \bigbreak \noindent 
            So, $ A\text{-}Y\text{-}X$ or $ A\text{-}X\text{-}Y$ holds anyway, contradicting our initial supposition
            \bigbreak \noindent 
            Therefore, $ A\text{-}X\text{-}Y$ or $ A\text{-}Y\text{-}X$ is true \endpf
        \item \textbf{Theorem 8.4}
            \bigbreak \noindent 
            \textbf{Proposition}. If $C$ is any point on ray $ \overrightarrow{AB}$ with $ 0 < AC < \omega$, then $ \overrightarrow{AC} = \overrightarrow{AB} $
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} If $C = B$, then trivially $ \overrightarrow{AC}  = \overrightarrow{AB}$. So we may assume that $ C \ne B$, and we already know that $C \ne A$ since $ AC > 0$. Since $0 < AC < \omega$, ray $ \overrightarrow{AC} $ is defined.
            \bigbreak \noindent 
            Now, $C$ on $\overrightarrow{AB}$ with $ C \ne A$ or $B$ implies $ A\text{-}C\text{-}B$ or $ A\text{-}B\text{-}C$. These betweenness relations imply $ B \in \overrightarrow{AC}$ by definition of $\overrightarrow{AC} $
            \bigbreak \noindent 
            If $X\ne A$ or $C$ is any other point on $\overrightarrow{AB}$, theorem 8.3 applied to $ \overrightarrow{AB}$ implies $ A\text{-}X\text{-}C $ or $ A\text{-}C\text{-}X$, which implies $ x \in \overrightarrow{AC}$. Therefore, $\overrightarrow{AB} \subseteq \overrightarrow{AC}$
            \bigbreak \noindent 
            If $X\ne A$ or $B$ is any other point on $\overrightarrow{AC}$, theorem 8.3 applied to $\overrightarrow{AC}$ implies $ A\text{-}X\text{-}B$ or $ A\text{-}B\text{-}X$, which implies $x\in \overrightarrow{AB} $. Therefore, $\overrightarrow{AC} \subseteq \overrightarrow{AB}$
            \bigbreak \noindent 
            Hence, $\overrightarrow{AB} = \overrightarrow{AC} $ and there is nothing sacred about the point $B$ in the ray $\overrightarrow{AB}$ \endpf
        \item \textbf{Endpoints}: 
            \bigbreak \noindent 
            \textbf{Definition}. Point $A$ is called an endpoint of ray $\overrightarrow{AB} $
        \item \textbf{Proposition}
            \bigbreak \noindent 
            \textbf{Proposition 8.5}: A ray has at most two endpoints
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} Suppose toward a contradiction that ray $h$ has three different endpoints $A,C,E$. Thus,
            \begin{align*}
                \overrightarrow{AB} = \overrightarrow{CD} = \overrightarrow{EF}
            \end{align*}
            With $A,C,E$ distinct, we apply theorem 8.3 three times
            \bigbreak \noindent 
            \begin{enumerate}
                \item Points $A,C,E$ on $\overrightarrow{AB}$ implies $ A\text{-}C\text{-}E $ or $ A\text{-}E\text{-}C$, so not $ C\text{-}A\text{-}E$ by UMT
                \item Points $A,C,E$ on $ \overrightarrow{CD}$ implies $ C\text{-}A\text{-}E$ or $ C\text{-}E\text{-}A$, by (1) it cannot be $ C\text{-}A\text{-}E$, thus it must be $ C\text{-}E\text{-}A$
                \item Points $ A\text{-}C\text{-}E$ on $\overrightarrow{EF}$ implies $ E\text{-}A\text{-}C$ or $ E\text{-}C\text{-}A$. This contradicts $ C\text{-}E\text{-}A$ by UMT
            \end{enumerate}
            \bigbreak \noindent 
            \endpf
        \item \textbf{More on Ax.RR}: Given a real number $s$ with $ 0 \leq s \leq \omega$, there is nothing explicit in $Ax.RR$ about there being only one point $X$ in $\overrightarrow{AB}$ with $AX = s$, we now prove that this is indeed the case.

        \item \textbf{Theorem 8.6 (Unique distances for Rays (UDR))}
            \bigbreak \noindent 
            \textbf{Proposition} For any ray $ \overrightarrow{AB}$ and any real number $s$ with $0 \leq s \leq \omega$, there is a \textbf{unique} point $X$ on $\overrightarrow{AB}$ with $AX = s$. $X$ is in $\overline{AB}$ if and only if $s \leq   AB $
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} For $s=0$, $X = A$ is the unique point in $\overrightarrow{AB}$ with $AX = 0$, by Ax.D2
            \bigbreak \noindent 
            If $X \ne Y$ are points distinct from $A$ on $\overrightarrow{AB}$, theorem 8.3 implies either $ A\text{-}X\text{-}Y $ or $ A\text{-}Y\text{-}X$. So either $ AX < AY$ or $ AX > AY$. Either way,  $AX \ne AY$, so no two different points will be the same distance $s$ from $A$
        \item \textbf{Planes with the 11 axioms}: All theorems proved thus far hold for a plane that satisfies the 11 axioms. Major examples in which the 11 axioms are true as well as all theorems of propositions stated are $\mathbb{E}, \hat{\mathbb{E}}, \mathbb{M}, \mathbb{G}, \mathbb{S}, \mathbb{R}^{3}$, and $\mathbb{H}$
            \bigbreak \noindent 
            With these axioms and theorems stated thus far, we are soon arriving at a point in which all lines on any plane that satisfies the 11 axioms will either behave like a Euclidean  line if $\omega = \infty$, or a spherical line (great circle) if $\omega < \infty $
        \item \textbf{Proposition 8.7}: Let $\overline{AB}$ be a segment and $X,Y \in \overline{AB}$. Then, $XY \leq AB$, and if $XY = AB$, then $\{X,Y\} = \{A,B\}$
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} If $X$ or $Y$ equals $A$, say $X = A$ and $Y \ne B$, then $ A\text{-}Y\text{-}B$. So, $XY = AY < AB$ by proposition 6.3. Similarly, if $X$ or $Y$ equals $B$ and the other point is not $A$, then $XY < AB$
            \bigbreak \noindent 
            So, we may assume that $ A\text{-}X\text{-}B$ and $ A\text{-}Y\text{-}B$. 
            \bigbreak \noindent 
            Since $ \overline{AB} \subseteq \overrightarrow{AB} $, we apply theorem 8.3, which yields that either $ A\text{-}X\text{-}Y$ or $ A\text{-}Y\text{-}X $
            \bigbreak \noindent 
            Thus, $XY < (AY \text{ or } AX ) < AB $ \endpf
        \item \textbf{Proposition 8.8} If $\overline{AB} = \overline{CD}$, then $\{A,B\}  = \{C,D\}$
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} If $\overline{AB} = \overline{CD}$, then $ AB = CD$ by proposition 6.4.
            \bigbreak \noindent 
            Thus, by proposition 8.7, $\{C,D\} = \{A,B\} $ \endpf
        \item \textbf{More on endpoints}: We can now say that a segment uniquely determines two endpoints
        \item \textbf{Definition (Interior points and length for a segment):} Given a segment $ \overline{AB}$, $A$ and $B$ are called its endpoints. All other points of $\overline{AB}$ are called \textbf{Interior points} of $\overline{AB}$
            \bigbreak \noindent 
            Distance $AB$ is called the \textbf{length} of $\overline{AB} $
        \item \textbf{Definition}: The interior of $\overline{AB}$, denoted $\text{Int}\overline{AB}$ or $\overline{AB}^{0}$, means the set of all interior points of $\overline{AB}$. That is, $\text{Int}\overline{AB} = \overline{AB}^{0} = \{X: A\text{-}X\text{-}B\}$
        \item \textbf{Proposition 8.9}: In each segment $\overline{AB}$ there is a unique point $M$, called the \textbf{midpoint} of $\overline{AB} $, with the property that $AM = \frac{1}{2}AB$. Further, $AM = MB $
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} Let $s = \frac{1}{2}AB$. By UDR, there is a unique point $M$ in $\overrightarrow{AB}$ with $AM = s = \frac{1}{2}AB$. Since $s < AB$, $M$ is in $\overline{AB} $. Since $AM \ne 0$ and $AM \ne AB$, $M \ne A$ or $B$. Thus, $ A\text{-}M\text{-}B$, and
            \begin{align*}
                AM + MB &= AB \implies \frac{1}{2}AB + MB = AB \\
                \therefore MB &= \frac{1}{2}AB
            \end{align*}
            \endpf
        \item \textbf{Theorem 9.1 (Antipode on line theorem)}: Let $A$ be a point on a line $m$ (in a plane with the 11 axioms). Assume that $\omega < \infty$. Then, there exists a unique point $A^{*}_{m}$ on $m$ such that $AA_{m}^{*} = \omega$. Further, if $X$ is any other point on $m$, then $ A\text{-}X\text{-}A^{*}_{m} $
            \bigbreak \noindent 
            \textbf{\textit{Proof.}}
            Let \( X \) be any point on \( m \) with \( 0 < AX < \omega \).
            (Such points exist by Ax.~N). So ray \( \overrightarrow{AX} \) is defined.
            \bigbreak \noindent 
            Thm 8.6 (uDR) for \( A = \omega \) implies There is a unique point \( A_m^* \) on \( \overrightarrow{AX} \) with \( AA_m^* = \omega \).
            \( AA_m^* > AX \) and the Important Fact \(\Rightarrow A - X - A_m^* \).
            \bigbreak \noindent 
            Now, we show that there is no other point $P$ anywhere on $m$ with $AP = \omega$
            \bigbreak \noindent 
            Suppose toward a contradiction that $P$ is another point on $m$ with $AP = \omega$. Then, $P$ is not in $\overrightarrow{AX}$.
            \bigbreak \noindent 
            Ax.QP applied to $ A\text{-}X\text{-}A^{*}_{m}$ and point $P$ implies one of 
            \begin{align*}
                P\text{-}A\text{-}X, \quad A\text{-}P\text{-}X, \quad X\text{-}P\text{-}A_{m}^{*}, \quad X\text{-}A^{*}_{m}\text{-}P
            \end{align*}
            If $ P\text{-}A\text{-}X$, then $PX > PA = \omega$, a contradiction.
            \bigbreak \noindent 
            If $ A\text{-}P\text{-}X$, then $ AX > AP = \omega$, another contradiction
            \bigbreak \noindent 
            If $ X\text{-}P\text{-}A_{m}^{*} $, then $ A\text{-}X\text{-}A_{m}^{*}$ and ROI yields $ A\text{-}X\text{-}P\text{-}A_{m}^{*}$ yields $ A\text{-}X\text{-}P$, which implies $P \in \overrightarrow{AX}$, contradiction.
            \bigbreak \noindent 
            Thus, $ X\text{-}A_{m}^{*}\text{-}P$. Consequently, $A_{m}^{*}P < XP \leq \omega$, so $\overline{A_{m}^{*}P} $ is defined
            \bigbreak \noindent 
            UDR says there's a point $U$ with $A_{m}^{*}\text{-}U\text{-}P$. So, $UP < A_{m}^{*}P < \omega$, which implies $U \ne A$ as $AP = \omega$
            \bigbreak \noindent 
            Ax.QP applied to $ A_{m}^{*}\text{-}U\text{-}P$ and point $A$ yields one of 
            \begin{align*}
                A\text{-}A_{m}^{*}\text{-}U, \quad A_{m}^{*}\text{-}A\text{-}U, \quad U\text{-}A\text{-}P, \quad U\text{-}P\text{-}A
            \end{align*}
            Which implies one of 
            \begin{align*}
                AU>AA_{m}^{*} = \omega ,\quad A_{m}^{*}U > A_{m}^{*}A = \omega, \quad UP > AP = \omega, \quad UA > PA = \omega
            \end{align*}
            All contradictions. Thus, $A_{m}^{*}$ is the only point on $m$ with $AA_{m}^{*} = \omega$
            \bigbreak \noindent 
            So, if $X$ is any point on $m$ other than $A$ or $A_{m}^{*}$, then $0 < AX < \omega$. Hence, ray $\overrightarrow{AX} $ is defined.
            \bigbreak \noindent 
            Now the first part of this proof applies to $ \overrightarrow{AX}$, and so $ A\text{-}X\text{-}A_{m}^{*}$
            \bigbreak \noindent 
            \endpf
        \item \textbf{Definition}. Assume $\omega < \infty$. Let $A$ be a point on a line $m$. The unique point $A_{m}^{*}$ on $m$ such that $AA_{m}^{*} = \omega$ is called the \textbf{antipode} of $A$ on $m$. Thus,
            \begin{align*}
                \begin{cases}
                    A,A_{m}^{*} \text{ are on m, }  AA_{m}^{*} = \omega \\
                    \text{and } A\text{-}X\text{-}A_{m}^{*} \text{ for all other points $X$ on $m$}
                \end{cases}
            \end{align*}
        \item \textbf{Theorem 9.2 (Almost-uniqueness for Quadrichotomy)}:  
            Suppose that \( A, B, C, X \) are distinct points on a line \( m \),  
            and that \( A - B - C \). Then \textbf{\textit{exactly one}} of the following holds:  
            \[
                X - A - B, \quad A - X - B, \quad B - X - C, \quad B - C - X
            \]
            with the \textbf{\textit{only exception}} that both \( X - A - B \) and \( B - C - X \) are true  
            when \( \omega < \infty \) and \( X = B_m^* \).
            \bigbreak \noindent 
            (Note that \( B_m^* - A - B \) and \( B - C - B_m^* \) \textbf{\textit{are both true}} by Thm. 9.1)
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} 
            By Axiom QP, at least one of \( X - A - B \), \( A - X - B \), \( B - X - C \), \( B - C - X \) holds.
            \bigbreak \noindent 
            \textbf{Suppose \( A - X - B \).} Then \( A - B - C \) and the Rule of I. \( \Rightarrow A - X - B - C \).  
            \bigbreak \noindent 
            So \( A - X - B \) and \( X - B - C \) are true (by definition of \( A - X - B - C \)),  
            hence \( X - A - B \), \( B - X - C \), \( B - C - X \) are false by the UMT (Thm. 6.2).  
            Thus, \( A - X - B \Rightarrow \) none of the other three relations hold.
            \bigbreak \noindent 
            \textbf{Suppose \( B - X - C \).} Then \( A - B - C \) and the Rule of I. \( \Rightarrow A - B - X - C \).  
            \bigbreak \noindent 
            So \( A - B - X \) and \( B - X - C \) are true, hence the UMT \( \Rightarrow \)  
            \( X - A - B \), \( A - X - B \), \( B - C - X \) are false.  
            \bigbreak \noindent 
            Thus, \( B - X - C \Rightarrow \) none of the other three relations hold.
            \bigbreak \noindent 
            So if more than one of \( X - A - B \), \( A - X - B \), \( B - X - C \), \( B - C - X \) holds,  
            they must be exactly \( X - A - B \) and \( B - C - X \).
            \bigbreak \noindent 
            Now assume that \( X - A - B \) and \( B - C - X \) are true.
            \bigbreak \noindent 
            \textbf{Suppose (toward a contradiction)} that \( BX < \omega \).  
            \bigbreak \noindent 
            Then ray \( \overrightarrow{BX} \) is defined,  
            and \( X - A - B \), \( B - C - X \Rightarrow A, C \) are in \( \overrightarrow{BX} \).  
            \bigbreak \noindent 
            So Thm. 8.3 \( \Rightarrow \) one of \( B - A - C \) or \( B - C - A \) is true.  
            \bigbreak \noindent 
            This contradicts \( A - B - C \) and the UMT (Thm. 6.2).  
            \bigbreak \noindent 
            Therefore, \( BX = \omega \), hence \( X = B_m^* \).
            \bigbreak \noindent 
            Corollary 8.5 showed that any ray has at most two endpoints.  
            Prop. 9.3 will show that when \( \omega < \infty \), any ray \( \overrightarrow{AB} \) with carrier \( m \)  
            (\( m = \overleftrightarrow{AB} \)) has a second endpoint, namely \( A_m^* \).  
            This generalizes what happens on \( \mathbb{S} \), where \( \overrightarrow{AB} = A^*B \).
        \item \textbf{Proposition 9.3 (needs proof)}: Assume \( \omega < \infty \). Let \( A, B \) be points on line \( m \)  
            with \( 0 < AB < \omega \). Then  
            \begin{enumerate}
                \item[(a)] \( \overrightarrow{AB} = \overline{AB} \cup \overline{BA_m^*} \) and \( \overline{AB}^{\circ} \cap \overline{BA_m^*}^{\circ} = \varnothing \).
                \item[(b)] \( \overrightarrow{AB} = \overrightarrow{A_m^* B} \), so that if \( A \) is an endpoint of a ray  
                    with carrier \( m \), then so is \( A_m^* \).
            \end{enumerate}
        \item \textbf{Theorem 9.4}.
            If \( h \) is a ray with two endpoints \( A \) and \( P \),  
            then \( \omega < \infty \) and \( P = A_m^* \), where \( m \) is the carrier of \( h \) (\( h \subseteq m \)).
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} Suppose (toward a contradiction) that \( AP < \omega \).  
            \bigbreak \noindent 
            Since \( P \) is an endpoint of \( h \), and \( A \) is on \( h \) with \( 0 < AP < \omega \),  
            Thm. 8.4 \( \Rightarrow h = \overrightarrow{PA} \).  
            \bigbreak \noindent 
            But \( A \) is also an endpoint of \( h \), so Thm. 8.4 \( \Rightarrow h = \overrightarrow{AP} \).
            \bigbreak \noindent 
            Let \( a \) be any number with \( AP < a \leq \omega \).
            \bigbreak \noindent 
            Axiom RR or Thm. 8.6 \( \Rightarrow \) there is a point \( X \) on \( \overrightarrow{AP} \) with  
            \( AX > a > AP \). So the Important Fact \( \Rightarrow A - P - X \).
            \bigbreak \noindent 
            Since \( h = \overrightarrow{PA} \) and \( X \) is on \( h \),  
            one of \( X = A \), \( X = P \), \( P - X - A \), or \( P - A - X \) is true, by definition of \( \overrightarrow{PA} \).  
            This contradicts \( A - P - X \), by the UMT.
            \bigbreak \noindent 
            Therefore, \( AP = \omega \), which implies that  
            \( \omega < \infty \) and \( P = A_m^* \).
        \item \textbf{Definition (interior points of a ray)}: Let \( h = \overrightarrow{AB} \) be a ray.  
            All points of \( h \) that are not endpoints of \( h \) are called \textit{interior points} of \( h \).  
            \bigbreak \noindent 
            The \textit{interior} of \( h \) is the set of all interior points of \( h \),  
            and is denoted by \( h^\circ \), \( \overline{AB}^\circ \), or \( \text{Int } \overrightarrow{AB} \).
        \item \textbf{Definition (Opposite rays)}: Two rays with the same endpoint whose union is a line are called \textbf{opposite rays}
        \item \textbf{Theorem 9.6 (Opposite ray theorem)}: If $ B\text{-}A\text{-}C$, then $\overrightarrow{AB}$ and $\overrightarrow{AC}$ are opposite rays
            \bigbreak \noindent 
            Also, for $m = \overleftrightarrow{AB}$
            \begin{align*}
                \overrightarrow{AB} \cap \overrightarrow{AC} = 
                \begin{cases}
                    \{A\}     & \text{ if } \omega = \infty \\
                    \{A, A_{m}^{*}\}     & \text{ if } \omega<\infty
                \end{cases}
            \end{align*}
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} $ B\text{-}A\text{-}C$ implies both $AB,AC  < BC \leq \omega$. So, rays $\overrightarrow{AB}, \overrightarrow{AC}$ are defined, and $\overleftrightarrow{AB}$ is the unique line through $A,B$, and $\overleftrightarrow{AC}$ is the unique line through $A,C$
            \bigbreak \noindent 
            $ B\text{-}A\text{-}C$ implies $B,A,C$ collinear which implies $ \overleftrightarrow{AB} = \overleftrightarrow{AC}$. Call this line $m$.
            \bigbreak \noindent 
            We have $\overrightarrow{AB}, \overrightarrow{AC} \subseteq m$
            \bigbreak \noindent 
            If $X \ne A,B,C$ is on $m$, then Ax.QP say one of 
            \begin{align*}
                X\text{-}B\text{-}A \quad B\text{-}X\text{-}A \quad A\text{-}X\text{-}C \quad A\text{-}C\text{-}X
            \end{align*}
            Must be satisfied. In other words, $X$ is in $\overrightarrow{AB}$ or $\overrightarrow{AC}$. So, $m \subseteq \overrightarrow{AB}  \cup \overrightarrow{AC}$, hence $ m = \overrightarrow{AB} \cup \overrightarrow{AC} $
            \bigbreak \noindent 
            Since $\overrightarrow{AB}$ and $\overrightarrow{AC}$ have the same endpoint, and $\overrightarrow{AB} \cup \overrightarrow{AC}  = m $, $\overrightarrow{AB}$ and $\overrightarrow{AC} $ are opposite rays
            \bigbreak \noindent 
            What about $\overrightarrow{AB} \cap \overrightarrow{AC}$? $ B\text{-}A\text{-}C$ implies not $ A\text{-}B\text{-}C$ or $ A\text{-}C\text{-}B$, so $B \not\in \overrightarrow{AC}$, and $C \not\in \overrightarrow{AB}$
            \bigbreak \noindent 
            So neither $B$ nor $C$ is in $\overrightarrow{AB} \cap \overrightarrow{AC}$
            \bigbreak \noindent 
            Let $X$ be any point $\ne A,B,C$ in $m$. Suppose $X \in \overrightarrow{AB} \cap \overrightarrow{AC}$
            \begin{align*}
                &X \in \overrightarrow{AB} \implies X\text{-}B\text{-}A \text{ or } B\text{-}X\text{-}A \\
                &X \in \overrightarrow{AC} \implies A\text{-}X\text{-}C \text{ or } A\text{-}C\text{-}X
            \end{align*}
            So two of $ X\text{-}B\text{-}A, B\text{-}X\text{-}A, A\text{-}X\text{-}C, A\text{-}C\text{-}X$ are true
            \bigbreak \noindent 
            Theorem 9.2 applied to $ B\text{-}A\text{-}C$ and point $X$ implies it must be $ X\text{-}B\text{-}A$ and $ A\text{-}C\text{-}X$, with $\omega < \infty$ and $ X = A_{m}^{*}$
            \bigbreak \noindent 
            Therefore, $ \overrightarrow{AB} \cap \overrightarrow{AC} \subseteq \{A, A_{m}^{*}\} $
            \bigbreak \noindent 
            A is on both rays, by definition of a ray, and $ A\text{-}B\text{-} A_{m}^{*}, A\text{-}C\text{-}A_{m}^{*}$ with theorem 9.1 implies $A_{m}^{*}$ is on $\overrightarrow{AB}$ and $\overrightarrow{AC}$, so $\overrightarrow{AB} \cap \overrightarrow{AC} = \{A,A_{m}^{*}\}$ when $\omega < \infty $, and $\{A\}$ when $\omega = \infty $
        \item \textbf{Corollary 9.7}: Each ray has a unique opposite ray.
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} Let $\overrightarrow{AB}$ be a ray. Proposition 8.11 says there's a point $C$ on $\overrightarrow{AB}$ with $ C\text{-}A\text{-}B$. Then, $AC<BC \leq \omega$, and the opposite ray theorem implies $\overrightarrow{AC}$ is an opposite ray to $\overrightarrow{AB} $
            \bigbreak \noindent 
            Let $h$ be any ray opposite to $\overrightarrow{AB}$, so that $h$ has endpoint $A$, and $h \cup \overrightarrow{AB} = \overleftrightarrow{AB}$
            \bigbreak \noindent 
            C is not in $\overrightarrow{AB}$ (from $ C\text{-}A\text{-}B$ or from $\overrightarrow{AB} \cap \overrightarrow{AC} = \{A,A_{m}^{*}\}$), hence $C \in h $
            \bigbreak \noindent 
            $h$ has endpoint $A$, $C$ in $h$ with $0 < AC < \omega$ implies $h = \overrightarrow{AC} $ by theorem 8.4

        \item \textbf{Notation:} Denote the ray opposite to ray $h$ by $h^{\prime}$. So, $\overrightarrow{AB}^{\prime}$ means the ray opposite $\overrightarrow{AB} $
        \item \textbf{Corollary 9.8}: Let $A,B$ be points on line $m$ with $0 <AB<\omega <\infty$. Then $\overrightarrow{AB}^{\prime} = \overrightarrow{AB_{m}^{*}} $
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} Theorem 9.1 implies $ B\text{-}A\text{-}B_{m}^{*}$. Then, theorem 9.6 implies $\overrightarrow{AB_{m}^{*}}  = \overrightarrow{AB}^{\prime}$
        \item \textbf{Corollary 9.9 (needs proof)}: Let $A,B$ be points on line $m$ with $ 0 < AB < \omega < \infty$. Then, $ m = \overline{AB} \cup \overline{BA_{m}^{*}} \cup \overline{A_{m}^{*}B_{m}^{*}} \cup \overline{B_{m}^{*}A}$, with the interiors of these segments being disjoint.
        \item \textbf{Theorem 9.10 (Needs proof)}: Let $A,B$ be points on line $m$ with $0 < AB < \omega < \infty$ . Let $C \ne A,B,A_{m}^{*}, B_{m}^{*} $ be another point on $m$. Then there is no betweenness relation for $A,B,C$ if and only if $C \in \overline{A_{m}^{*}B_{m}^{*}}^{0}$
        \item \textbf{When $\omega < \infty$, any line $m$ is "like a circle"}: Suppose $ B\text{-}A\text{-}C$ on $m$
            \bigbreak \noindent 
            \begin{figure}[ht]
                \centering
                \incfig{lineah}
                \label{fig:lineah}
            \end{figure}
            \bigbreak \noindent 
            $\overrightarrow{AB}, \overrightarrow{AC}$ are opposite rays by theorem 9.6. $A_{m}^{*}$ is on both $ \overrightarrow{AB} = \overrightarrow{A_{m}^{*}B}$ and $\overrightarrow{AC} = \overrightarrow{A_{m}^{*}C} $ by prop 9.3
            \bigbreak \noindent 
            So is it really like a circle? Take point $X$ on $\overrightarrow{AB}$ near $A_{m}^{*}$, and point $Y$ on $\overrightarrow{AC}$ near $A_{m}^{*} $
            \bigbreak \noindent 
            \begin{figure}[ht]
                \centering
                \incfig{lineah2}
                \label{fig:lineah2}
            \end{figure}
            \bigbreak \noindent 
            Say distances $XA_{m}^{*}, YA_{m}^{*}$ are small enough so that $XA_{m}^{*} + YA_{m}^{*} \leq \omega$. To show that this line should be a circle, we need 
            \begin{align*}
                X\text{-}A_{m}^{*}\text{-}Y
            \end{align*}
            Because $XA_{m}^{*} + YA_{m}^{*} \leq \omega$, Ax.BP says there is a B.R among $X,Y,A_{m}^{*} $
            \bigbreak \noindent 
            Suppose $ A_{m}^{*}\text{-}X\text{-}Y$, can we get a contradiction?
            \bigbreak \noindent 
            $A_{m}^{*}X < \omega$, $X$ on $\overrightarrow{AB} = \overrightarrow{A_{m}^{*}B}$ implies $\overrightarrow{A_{m}^{*}B} = \overrightarrow{A_{m}^{*}X} $ (Thm 8.4)
            \bigbreak \noindent 
            So if $ A_{m}^{*}\text{-}X\text{-}Y$, then $Y$ is in $\overrightarrow{A_{m}^{*}X} = \overrightarrow{AB}$. But, $Y \in \overrightarrow{AC}$, and $\overrightarrow{AC}, \overrightarrow{AB}$ have only points $A,A_{m}^{*}$ in common. A similar argument reveals why it also cannot be  $A_{m}^{*}\text{-}Y\text{-}X$
            \bigbreak \noindent 
            Thus, it must be $ X\text{-}A_{m}^{*}\text{-}Y$, and the line is "like a circle"






    \end{itemize}

    \pagebreak 
    \subsection{Separation}
    \begin{itemize}
        \item \textbf{Proposition between} Let $\overrightarrow{AB}$ and $\overrightarrow{AC}$ be opposite rays, and points $X \in \text{Int}\overrightarrow{AB}$, $Y \in \text{Int}\overrightarrow{AC} $ with $AX + AY \leq \omega$, then $ X\text{-}A\text{-}Y$
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} $X$ in $\text{Int}\overrightarrow{AB}$ implies $0 < AX < \omega$. So, by theorem 8.4, $\overrightarrow{AB} = \overrightarrow{AX}$. Similarly, $\overrightarrow{AC} = \overrightarrow{AY}$. 
            \bigbreak \noindent 
            Ax.BP and $AX + AY \leq \omega$ implies there is B.R among $A,X,Y$
            \bigbreak \noindent 
            If $ A\text{-}X\text{-}Y$ then the definition of a ray implies $ Y \in \overrightarrow{AX} = \overrightarrow{AB}$. But, $Y \in \text{Int}\overrightarrow{AC}$, and $\overrightarrow{AB} \cap \text{Int}\overrightarrow{AC}  = \varnothing$, which by the opposite ray theorem (9.6) is a contradiction. So, not $ A\text{-}X\text{-}Y$, and similarly not $ A\text{-}Y\text{-}X $. Thus, we have $ X\text{-}A\text{-}Y $
        \item \textbf{Definition}. A subset $S$ of $\mathbb{P}$ is \textbf{convex} if for each pair of points $X \ne Y$ in $S$ with $XY < \omega$, $\overline{XY} \subseteq S$ holds.
            \bigbreak \noindent 
            \begin{figure}[ht]
                \centering
                \incfig{conex1}
                \label{fig:conex1}
            \end{figure}
            \bigbreak \noindent 
            Each of these are convex sets in $\mathbb{E}$, with or without the boundary
            \bigbreak \noindent 
            \begin{figure}[ht]
                \centering
                \incfig{nonconvex2}
                \label{fig:nonconvex2}
            \end{figure}
            \bigbreak \noindent 
            Each of these are non-convex in $\mathbb{E}$, with or without the boundary
        \item \textbf{Theorem 10.1}: If $S_{1}$ and $S_{2}$ are convex sets in $\mathbb{P}$, then so is $S_{1} \cap S_{2}$
        \item \textbf{Theorem 10.2}: Segments, rays, and lines are convex.
        \item \textbf{Definition}: A pair of sets $H,K$ in $\mathbb{P}$ is called \textbf{opposed around a line $m$} if 
            \begin{itemize}
                \item $H,K \ne \varnothing $
                \item $H,K$ are convex
                \item $H \cap K = \varnothing $
                \item $H \cup K = \mathbb{P} - m$
            \end{itemize}
            \bigbreak \noindent 
            \begin{figure}[ht]
                \centering
                \incfig{opposed}
                \label{fig:opposed}
            \end{figure}
            \bigbreak \noindent 
            So we can imagine this line $m$ that extends indefinitely, everything above the line is in $H$, below the line is in $K$, and the sets $H,K$ are said to be \textbf{opposed around the line $m$}
            \bigbreak \noindent 
            \begin{figure}[ht]
                \centering
                \incfig{ing}
                \label{fig:ing}
            \end{figure}
            \bigbreak \noindent 
            \begin{figure}[ht]
                \centering
                \incfig{insh}
                \label{fig:insh}
            \end{figure}
        \item \textbf{H and K in $\mathbb{R}^{3}$}: Imagine the wall in front of you as part of a plane in $3$-dim space that extends to infinity in every direction.
            \bigbreak \noindent 
            Picture $m$ as vertical line on the wall. Let $H$ be all the points on your side of the wall (plane), together with all points on the wall (plane) to the right of $m$. In particular, you are in $H$
            \bigbreak \noindent 
            Let $K$ be all the points on the other side of the wall (plane), together with all points on the wall (plane) to the left of $m$
        \item \textbf{Theorem 10.3} Let $H,K$ be sets opposed around a line $m$ in $\mathbb{P}$. Suppose that $A,C$ are points so that $C \in m$, $A \in H$, $AC < \omega$. Then, $\text{Int}\overrightarrow{CA} \subseteq H$, and $\text{Int}\overrightarrow{CA}^{\prime} \subseteq K $
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} Let $\ell = \overleftrightarrow{AC}$. $\ell \ne m$, since $A \not\in m$. $\ell, m$ meet only at $C$ (or also at $C_{m}^{*} = C_{\ell}^{*} $ if $\omega < \infty $ and if $C_{m}^{*} = C_{\ell}^{*} $). So, $\text{Int}\overrightarrow{CA} \subseteq H \cup K $
            \bigbreak \noindent 
            Let $CA = a < \omega$. Pick a number $b$ with $0 < b < \omega - a$. Ax.RR implies there is a point $B$ on $\overrightarrow{CA}^{\prime}$ with $CB = b$, so $\overrightarrow{CA}^{\prime} = \overrightarrow{CB} $.
            \bigbreak \noindent 
            For all points $Y$ with $ C\text{-}Y\text{-}B$ or $Y = B$, $CY \leq b < \omega - a$
            \bigbreak \noindent 
            Let $ y = CY$. Then, $y < \omega - a$, which implies $a+y < \omega$, which implies $CA + CY < \omega $
            \bigbreak \noindent 
            Proposition between implies $ A\text{-}C\text{-}Y$, so $ AY = AC + CY < \omega$. Thus, $\overline{AY}$ is defined, and $C \in \overline{AY} $
            \bigbreak \noindent 
            If $Y \in H$, then $A \in H$ and $H$ is convex (by the definition of opposed sets). This implies $\overline{AY} \subseteq H$, which implies $ C \in H $
            \bigbreak \noindent 
            But, $C \in m$, and $H \subseteq\mathbb{P}-m$, contradiction. Therefore, $Y \in K$ for all $Y \ne C$ in $\overline{CB} $
            \bigbreak \noindent 
            Now, let $X$ be any point in $\text{Int}\overrightarrow{CA}$, let $x = CX$, so $0 < x < \omega$. Pick a number $y$ with $0 < y < \begin{cases} b \\ w-x\end{cases} $
            \bigbreak \noindent 
            Ax.RR implies there is a point $Y$ on $\overrightarrow{CA}^{\prime} = \overrightarrow{CB}$ with $CY = y  < b$. So, $Y \in \overline{CB}$, hence $Y \in K $
            \bigbreak \noindent 
            Also, $XC  + CY = x + y  < x + \omega - x = \omega $
            \bigbreak \noindent 
            Prop between implies $ X\text{-}C\text{-}Y$, so $XY = XC  + CY < \omega $. Thus, $\overline{XY}$ is defined, and $C \in \overline{XY}$. 
            \bigbreak \noindent 
            If $X \in K$, then $Y \in K$ and $K$ is convex (defn of opposed sets). This implies $\overline{XY} \subseteq K$, which implies $C \in K$. But, $C \in m$, and $K \subseteq\mathbb{P} - m$ (defn. opposed sets). A contradiction.
            \bigbreak \noindent 
            Therefore, $X \in H$ \textbf{for all} $X \in \text{Int} \overrightarrow{CA} $, and $\text{Int}\overrightarrow{CA} \subseteq H$
            \bigbreak \noindent 
            A similar argument, starting with $B \in K$ reveals $\text{Int}\overrightarrow{CB}\subseteq K$; that is $\text{Int}\overrightarrow{CA}^{\prime} \subseteq K$
        \item \textbf{Corollary 10.4}: let $H,K$ be sets opposed around a line $m$, let $A,B$ be points not on $m$, with $ A\text{-}X\text{-}B$ for some point $X \in m$. Then, $A,B$ lie one in each of $H$ and $K$, in some order.
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} $A,B$ are in $\mathbb{P} - m = H \cup K$. We may assume $A \in H$. Thm 9.6 implies $ \overrightarrow{XB} = \overrightarrow{XA}^{\prime}$. So, Thm 10.3 implies $\text{Int}\overrightarrow{XA} \subseteq H$, $\text{Int}\overrightarrow{XB} \subseteq K$, hence $B \in K$
        \item \textbf{Definition}: Let $m$ be a line. Sets $H,K$ are called \textbf{opposite halfplanes with edge $m$} if:
            \bigbreak \noindent 
            \begin{align*}
                &H,K \text{ are opposed around $m$, and whenever } X \in H, Y \in K \text{ and } XY < \omega, \\ &\text{ then, } \overline{XY} \cap m \ne \varnothing
            \end{align*}
            Planes that satisfy this are
            \begin{align*}
                \mathbb{E}, \hat{\mathbb{E}}, \mathbb{M}, \mathbb{H}, \mathbb{S}
            \end{align*}
            This definition is not true for $\mathbb{G}$, or $\mathbb{R}^{3} $
            \bigbreak \noindent 
\begin{figure}[ht]
    \centering
    \incfig{somefig2}
    \label{fig:somefig2}
\end{figure}
            \bigbreak \noindent 
            The existence of opposite halfplanes yields a number of important properties. When $\omega < \infty$, they guarantee the uniqueness of an antipode throughout the plane, not just line-by-line
        \item \textbf{Theorem 10.5}: Suppose that $m$ is a line  so that there exists a pair $H,K$ of opposite half planes with edge $m$. Suppose also that $\omega < \infty$ and $A$ is a point on $m$. If $B$ is any point in $\mathbb{P}$ with $AB = \omega$, then $B \in m$ (so $B = A_{m}^{*}$, and there is only one point $B$ in all of $\mathbb{P}$ with $AB = \omega$)
            \bigbreak \noindent 
            This is what happens on $\mathbb{S}$, where we see that every line $m$ does have a pair of opposite halfplanes with edge $m$
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} Suppose toward a contradiction that $B \not\in m$. Axioms I3 and N imply there is a line $n$ through $A$ and $B$, and a point $P$ on $n$ with $0 < AP < \omega$
            \bigbreak \noindent 
            \begin{figure}[ht]
                \centering
                \incfig{stac}
                \label{fig:stac}
            \end{figure}
            \bigbreak \noindent 
            Antipode on line thm (9.1) implies $B = A_{m}^{*}$, and $ A\text{-}P\text{-}B$, so $BP < \omega$.
            \bigbreak \noindent 
            $B \not\in m$ and Ax.I4 implies $m\cap n = \{A\} $
            \bigbreak \noindent 
            Prop 9.3 implies $\overrightarrow{AP} = \overrightarrow{A_{m}^{*}P} = \overrightarrow{BP}$. Note that $n = \overleftrightarrow{BP} $
            \bigbreak \noindent 
            Prop 8.11 implies there's a point $Q$ with $ Q\text{-}B\text{-}P$ and $PQ < \omega$.
            \bigbreak \noindent 
            Thm 9.6 (opp. ray thm) implies $\overrightarrow{BQ} = \overrightarrow{BP}^{\prime} = \overrightarrow{AP^{\prime}}$, so $Q \in \text{Int}\overrightarrow{AP}^{\prime}$. We may assume that $P \in H$, so Thm 10.3 implies $Q \in K$. Then, $H,K$ are opposite halfplanes, and $PQ < \omega$, which implies $\overline{PQ} \cap m  = \varnothing $
            \bigbreak \noindent 
            But, $P,Q \in  n$ implies  $\overline{PQ} \subseteq n$, which implies $\overline{PQ} \cap m \subseteq n \cap m = \{A\} $. So, $A \in \overline{PQ}$, and $ Q\text{-}B\text{-}P$, which implies $ B \in \overline{PQ}$
            \bigbreak \noindent 
            Prop 8.7 implies $AB \leq PQ$, but $AB = \omega$, and $PQ < \omega$, a contradiction.
            \endpf
        \item \textbf{Theorem 10.6}: Suppose that there is a pair $H,K$ of opposite halfplanes with edge $m$. Let $A \ne B$ be points not on $m$. Then, 
            \begin{align*}
                A,B \text{ lie one in each of $H,K$ } \iff \text{ there is a point $X$ on $m$ such that $ A\text{-}X\text{-}B $}
            \end{align*}
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} If $ A\text{-}X\text{-}B$ for some point $X$ on $m$, then coroll. 10.4 (which only required that $H,K$ be opposed around $m$) implies $A,B$ lie one in each of $H,K$.
            \bigbreak \noindent 
            Suppose that $A,B$ lie one in each of $H,K$. If $AB < \omega$, then $\overline{AB} \cap m \ne \varnothing $ by the definition of opposite halfplanes, so $ A\text{-}X\text{-} B$ for some $X$ in $m$
            \bigbreak \noindent 
            If $AB = \omega$, then Ax.I3 and the antipode on line thm (9.1) implies $B$ is the antipode $P$ on $n$, and $ A\text{-}P\text{-}B$ for all other points $P$  on $n$. If some  such point $P$ is also on $m$, then we are done. Otherwise, since $A,B$ lie one in each of $H,K$, and $P$ is in $H$ or $K$. We may assume that $A,P$ are in $H$ and $B$ is not in $K$.
            \bigbreak \noindent 
            Now, $ A\text{-}P\text{-}B$ implies $PQ < \omega$. So the defn of opposite halfplanes implies $ P\text{-}X\text{-}B$ for some point $X \in m$. $ A\text{-}P\text{-}B$, $ P\text{-}X\text{-}B$, and the rule of insertion implies
            \begin{align*}
                A\text{-}P\text{-}X\text{-}B \implies A\text{-}X\text{-}B
            \end{align*}
        \item \textbf{Corollary 10.7 (Needs proof)}: Suppose that there is a pair $H,K$ of opposite halfplanes with edge a line $m$. Then, $H,K$ is the only pair of sets opposed around $m$.
            \bigbreak \noindent 
            \textbf{\textit{Proof.}}
            \bigbreak \noindent 
            \textbf{Note:} It follows from coroll 10.7 that since lines $m$ in $\mathbb{G}$ and in $\mathbb{R}^{3}$ have sets opposed around them that are not opposite halfplanes, then there are no other pairs of sets that are opposite halfplanes with edge $m$
        \item \textbf{A difference between $\mathbb{R}^{3}$ and $\mathbb{G}$}: No line $m$ in $\mathbb{R}^{3}$ has a pair of opposite halfplanes with edge $m$. Some lines $m$ in $\mathbb{G}$ have a pair of opposite halfplanes with edge $m$
        \item \textbf{Separation Axiom Ax.S}: for each line $m$, there exists a pair of opposite halfplanes with edge $m$. 
            \bigbreak \noindent 
            Ax.S is true for $\mathbb{E}, \hat{\mathbb{E}}, \mathbb{M},\mathbb{H}, \mathbb{S}$. But, is false for $\mathbb{G}, \mathbb{R}^{3}$
        \item \textbf{Definition}: Let $H,K$ be opposite halfplanes with edge $m$. Two points in the same halfplane are said to be on the \textbf{same side} of $m$. 
            \bigbreak \noindent 
            Two points in opposite halfplanes are said to be \textbf{opposite sides} of $m$
        \item \textbf{Theorem 10.8}: Suppose that $\omega < \infty$. For each point $ A$, there is exactly one point $A^{*}$ in $\mathbb{P}$ with $AA^{*} = \omega$. Also, every line through $A$ goes through $A^{*}$ as well.
        \item \textbf{Definition}: $A^{*} $ is called the \textbf{antipode} of $A$
            \bigbreak \noindent 
            Note that the antipode property of the sphere $\mathbb{S}$ is now true for every plane $\mathbb{P}$ with 12 axioms, when $\omega < \infty$
            \bigbreak \noindent 
            Whats also true when $\omega < \infty$ is that for all points $A\in \mathbb{P}$ and all points $X \in \mathbb{P}$ with $X \ne A$ or $A^{*}$, then
            \begin{align*}
                A\text{-}X\text{-}A^{*}
            \end{align*}
            \bigbreak \noindent 
            This is because there is a line $m$ through $A$ and $X$ (Ax.I3). In fact, it is the unique line $\overleftrightarrow{AX}$, since $X \ne A^{*}$ implies $AX < \omega $
            \bigbreak \noindent 
            Thm 10.8 implies $A^{*}$ is on $m$, and then the antipode on line theorem (9.1) implies $ A\text{-}X\text{-}A^{*} $
        \item \textbf{Corollary 10.9}: Suppose that $\omega < \infty$. For any line $m$ and point $P$, there are just two possibilities:
            \begin{align*}
               \begin{cases}
                   P,P^{*} &\text{ both on $m$}     \\
                   P, P^{*} &\text{on opposite sides of $m$}
               \end{cases}
            \end{align*}
            \bigbreak \noindent 
            \textbf{Proof.} If either $P$ or $P^{*}$ is on $m$, then so is the other, by Thm 10.8, so we need to only consider when neither $P$ nor $P^{*}$ is on $m$. Let $X$ be any point on $m$. By what we showed above, $ P\text{-}X\text{-}P^{*}$. Then, thm 10.6 implies $P,P^{*}$ are in opposite halfplanes with edge $m$
        \item \textbf{Proposition Noncollinear}: If $A,B,C$ are three noncollinear points (not all on the same line), then $AB, AC,BC$ all less than $\omega$.
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} Suppose (toward a contradiction) that $AB = \omega$. Then, $B = A^{*}$, so $ A\text{-}C\text{-}B$. In particular, $A,C,B$ are collinear, a contradiction.
            \bigbreak \noindent 
            The following theorem was given as an axiom, in place of axioms, by Moritz pasch in 1882. It is equivalent to the separation Axiom: We prove here that it holds as a consequence of Ax.S (along with the other axioms); and also one can assume the statement of the theorem, and prove that the statement of Ax.S must be true.
        \item \textbf{Theorem 10.10 (Pasch's Axioms) (needs proof)}: 
            Let $A,B,C$ be three noncollinear points. Let $X$ be a point with $ B\text{-}X\text{-}C $, and $m$ a line through $X$ but not through $A,B,$ or $C$. Then, exactly one of
            \begin{enumerate}
                \item $m$ contains a point $Y$ with $ A\text{-}Y\text{-}C$
                \item $m$ contains a point $Z$ with $ A\text{-}Z\text{-}B $
            \end{enumerate}
            \bigbreak \noindent 
            \begin{figure}[ht]
                \centering
                \incfig{pasch}
                \label{fig:pasch}
            \end{figure}
            \bigbreak \noindent 
            Note that there are two things to prove.
            \begin{itemize}
                \item $a$ or $b$ happens, and not both at once, for line $m$
            \end{itemize}
            Hint: Opposite halfplanes.
        \item \textbf{Theorem 10.11}: Assume that $\omega < \infty$. Then, any two distinct lines must have a point (in fact, a pair of antipodes) in common.
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} Suppose toward a contradiction that there are lines $m,n$ with $m \cap n = \varnothing $. Let $H,K$ be the opposite halfplanes with edge $m$, and let $P$ be a point on $n$. Since $P \not\in m$, we may assume that $P \in H $
            \bigbreak \noindent 
            \begin{figure}[ht]
                \centering
                \incfig{lines}
                \label{fig:lines}
            \end{figure}
            \bigbreak \noindent 
            Coroll. 10.9 implies $P^{*} \in K$. $P^{*} \in n$ by thm 10.8. Let $Q$ be any other point on $n$. Then, $QP$, $QP^{*} < \omega$. So, $\overline{QP}, \overline{QP^{*}}$ are defined, and are contained in $n$. 
            \bigbreak \noindent 
            If $Q \in H$, then Ax.S implies $\overline{QP^{*}} \cap m \ne \varnothing$, which implies $n\cap m \ne \varnothing $
            \bigbreak \noindent 
            If $ Q\in K$, then Ax.S implies $\overline{QP} \cap m \ne \varnothing$, which implies $n \cap m \ne \varnothing $,
            \bigbreak \noindent 
            Both of these a contradiction.



    \end{itemize}

    \pagebreak 
    \subsection{Pencils and Angles}
    \begin{itemize}
        \item \textbf{Definition: \textit{Coterminal rays}}: Rays with the same endpoint
            \bigbreak \noindent 
            \begin{figure}[ht]
                \centering
                \incfig{coterm}
                \label{fig:coterm}
            \end{figure}
            \bigbreak \noindent 
        \item \textbf{Definition: \textit{Angle}}: $\underline{ab} = a \cup b $, where $a,b$ are coterminal rays
            \bigbreak \noindent 
            \begin{figure}[ht]
                \centering
                \incfig{ang}
                \label{fig:ang}
            \end{figure}
            \bigbreak \noindent 
        \item \textbf{duality}: To develop properties of coterminal rays, our steps will be closely analogous to those we took to study collinear points. The analogy is called \textbf{duality}
            \bigbreak \noindent 
            The theory of coterminal rays is dual to the theory of collinear points when $\omega < \infty$
            \bigbreak \noindent 
            But, the theory of rays will be good whether $\omega < \infty$ or $\omega = \infty$
        \item \textbf{Definition: \textit{Pencil of rays at point $A$}}: The set of all rays with endpoint $A$: denote by $P_{A}$ or just $P$
            \bigbreak \noindent 
            \begin{figure}[ht]
                \centering
                \incfig{pencil}
                \label{fig:pencil}
            \end{figure}
            \bigbreak \noindent 
            When $\omega < \infty$, each ray $h = \overrightarrow{AB} = \overrightarrow{A^{*}B}$, so $P_{A} = P_{A^{*}} $. $h^{\prime} $ is the opposite ray to $h$, as before
            \bigbreak \noindent 
            \begin{figure}[ht]
                \centering
                \incfig{circ}
                \label{fig:circ}
            \end{figure}
            \bigbreak \noindent 

        \item \textbf{Undefined Term \textit{Angle distance function, or angle measure}}: A function $\mu$ from all pairs $(p,q) $ of coterminal rays to $\mathbb{R}$
            \bigbreak \noindent 
            We abbreviate the angular distance between rays $p,q$, or the angle measure of the angle $pq$, $\mu(p,q)$ as $pq$ 
            \bigbreak \noindent 
            \begin{figure}[ht]
                \centering
                \incfig{undef}
                \label{fig:undef}
            \end{figure}
            \bigbreak \noindent 
        \item \textbf{Angular distance in each of $\mathbb{E}$, $\hat{\mathbb{E}}$, $\mathbb{M}$, $\mathbb{S}$, $\mathbb{H}$}
            \begin{itemize}
                \item \textbf{$\mathbb{E}$, $\hat{\mathbb{E}}$, $\mathbb{M} $}: The usual measure in degrees (0 to 180)
                    \bigbreak \noindent 
                    \begin{figure}[ht]
                        \centering
                        \incfig{eangle}
                        \label{fig:eangle}
                    \end{figure}
                    \bigbreak \noindent 
                    \begin{align*}
                        pq = \cos^{-1}{\left(\frac{1+mn}{\sqrt{1+m^{2}}\sqrt{1+n^{2}}}\right)}
                    \end{align*}
                    From the law of cosines
                \item \textbf{$\mathbb{S}$}: 
                    \bigbreak \noindent 
                    \fig{.7}{./figures/s1.png}
                    \bigbreak \noindent 
                    Rays $\hat{p}, \hat{q}$ in plane tangent to $\mathbb{S}$ at $A$. $pq$ is defined as $\hat{p}\hat{q}$ in the tangent plane 
            \end{itemize}
                \item \textbf{Definition \textit{(Ordinary angle)}}: An ordinary angle is any angle that is less than 180 degrees.
                \item $\mathbb{H}$: Consider $\mathbb{H}$ in a horizontal plane, cover $\mathbb{H}$ with a hemispherical dome. Rays $p,q$ in $ \mathbb{H}$, coterminal at point $A$. Project vertically up to dome: $p,q$ project to circular arcs on the dome, $A$ to $\hat{A}$
                    \bigbreak \noindent 
                    $\mu_{\mathbb{H}}(p,q)$ is the measure of ordinary angle between the two lines in the tangent plane at $\hat{A} $
                    \bigbreak \noindent 
                    \fig{.7}{./figures/h1.png}
                    \bigbreak \noindent 
                    \begin{figure}[ht]
                        \centering
                        \incfig{diag2}
                        \label{fig:diag2}
                    \end{figure}
                    \bigbreak \noindent 
                    \begin{align*}
                       \mu_{\mathbb{H}}(p,q) = \cos^{-1}{\left(\frac{1+mn-bc}{\sqrt{1+m^{2}-b^{2}}\sqrt{1+n^{2}-c^{2}}}\right)} 
                    \end{align*}
                \item \textbf{Measure axioms}
                    \begin{enumerate}
                        \item [M1]: For all coterminal rays $p,q$, $0 \leq pq \leq 180$
                        \item [M2]: $pq = 0 \iff p=q$
                        \item [M3]: $pq = qp$
                        \item [M4]: $pq = 180 \iff q=p^{\prime} $
                    \end{enumerate}
                    \bigbreak \noindent 
                    Note that $M4$ implies that $180$ is the suppremum (and the max) for angular distance. Observe that this fact is analogous to distances (when $\omega < \infty$), and $PQ = \omega \iff Q = P^{*} $
                    \bigbreak \noindent 
                    Thus,
                    \begin{align*}
                        180 \leftrightarrow \omega \\
                        p^{\prime} \leftrightarrow P^{*}
                    \end{align*}
                \item \textbf{Definition \textit{(betweenness for rays)}}: Ray $b$ lies \textbf{between} rays $a$ and $c$ ($ a\text{-}b\text{-}c $) provided that
                    \begin{enumerate}[label=(\alph*)]
                        \item $a,b,c$ are different, coterminal
                        \item $ab + bc = ac $
                    \end{enumerate}
                \item \textbf{Theorem 11.1 \textit{(symmetry of betweenness)}}: $ a\text{-}b\text{-}c \iff c\text{-}b\text{-}a$
                \item \textbf{Theorem 11.3 \textit{UMT}}: If $ a\text{-}b\text{-}c$, then $ b\text{-}a\text{-}c$ and $ a\text{-}c\text{-}b$ are false.
                    \bigbreak \noindent 
                    \textbf{\textit{Proof.}} The definition of $ a\text{-}b\text{-}c$ implies $ a\text{-}b\text{-}c$ are different, coterminal, and $ab + bc = ac$. 
                    \bigbreak \noindent 
                    Observe that each of $ab,bc,ac >0$ by axioms M1,M2. Thus, $ab + bc =ac$ implies $ac$ is larger than each of $ab,bc$
                    \bigbreak \noindent 
                    Suppose toward a contradiction that $ b\text{-}a\text{-}c$ is also true. Then, $ bc$ larger than each of $ba = ab$, and $ac$. This contradicts $ac$ larger than $ab, bc $
                    \bigbreak \noindent 
                    Therefore, $ b\text{-}a\text{-}c$ is false. Similarly, $ a\text{-}c\text{-}b$ is also false \endpf
                \item \textbf{Note about Incidence axioms}: There are no duals for the four incidence axioms we studied previously for points
                    \bigbreak \noindent 
                    We needed those axioms to begin developing how the undefined terms point and line are related. For instance, to guarantee points $A,B$ are in a \textbf{unique} line together, we require $AB < \omega$ (Ax.I4). Point $A$ can be in many different lines
                    \bigbreak \noindent 
                    But, rays and pencils are defined concepts, and by the definitions, and by what we have proved about rays, ray $a = \overrightarrow{AB}$ is in \textbf{just one pencil $P_{A} $}
                    \bigbreak \noindent 
                    Coterminal rays $a = \overrightarrow{AB}, b = \overrightarrow{AC}$ are automatically in the unique pencil $P_{A}$, no proof needed.
                    \bigbreak \noindent 
                    The dual of axiom N is a true statement for rays, but we don't need to assume it; we can prove it.
                \item \textbf{Theorem 11.2 (non-triviality)}: For any ray $p$ there is a coterminal ray $q$ so that $0 < pq < 180$
                    \bigbreak \noindent 
                    \textbf{\textit{Proof.}} Let $p = \overrightarrow{AB}$ be on line $\ell$. Thm 7.6 implies there is a point $C$ not on $\ell$ with $0 < AC < \omega$. Then, ray $\overrightarrow{AC}$ is defined, and is not contained in $\ell$, since $C \not\in \ell$. So, $\overrightarrow{AC} \ne p$, and $\ne p^{\prime}$. 
                    \bigbreak \noindent 
                    Let $q = \overrightarrow{AC}$. Then, 
                    \begin{align*}
                        180 \geq &pq \geq 0 \quad (\text{Ax.M1})\\
                                 &pq \ne 0 \quad (\text{Ax.M2},\ q\ne p) \\
                                 &pq \ne 180 \quad (\text{Ax.m4},\ q \ne p^{\prime}) 
                    \end{align*}
                    Therefore, $0 < pq < 180$
                \item \textbf{Definition \textit{(Wedge, fan)}}: Let $p,q$ be coterminal rays with $0<pq<180$.
                    \begin{itemize}
                        \item \textbf{Wedge $\overline{pq} = \{p,q\} \cup \{r: p\text{-}r\text{-}q\}$}
                        \item \textbf{Fan $\overrightarrow{pq} = \{p,q\} \cup \{r: p\text{-}r\text{-}q\} \cup \{r: p\text{-}q\text{-}r\}$}
                    \end{itemize}
                    (The duals of segment and ray)
                \item \textbf{Betweenness of rays axiom (Ax.BR)}: If $a,b,c$ are distinct, coterminal rays, and if $ab+bc \leq 180$, then there exists a betweenness relation among $a,b,c$
                    \bigbreak \noindent 
                    Thus, if no betweenness relation exists, then
                    \begin{align*}
                        ab + bc > 180 \\
                        ac + cb > 180 \\
                        ba + ac > 180
                    \end{align*}
                \item \textbf{Definition \textit{(quad betweenness)}}: $ a\text{-}b\text{-}c\text{-}d $ means that all four of 
                    \begin{align*}
                        a\text{-}b\text{-}c \quad a\text{-}b\text{-}d \quad a\text{-}c\text{-}d \quad b\text{-}c\text{-}d
                    \end{align*}
                    are true
                \item \textbf{Theorem \textit{(Triangle inequality for rays)}}: If $a,b,c$ are three distinct, coterminal rays, then $ab + bc \geq ac$
                \item \textbf{Theorem 11.5 \textit{(Rule of insertion for rays)}}:
                    \begin{enumerate}[label=(\alph*)]
                        \item If $ a\text{-}b\text{-}c$ and $ a\text{-}r\text{-}b$, then $ a\text{-}r\text{-}b\text{-}c $
                        \item If $ a\text{-}b\text{-}c $ and $ b\text{-}r\text{-}c $, then $ a\text{-}b\text{-}r\text{-}c $
                    \end{enumerate}
                \item \textbf{Quadrichotomy of Rays Axiom (Ax.QR)}: If $a,b,c,x$ are distinct, coterminal rays, and if $ a\text{-}b\text{-}c$, then at least one of the following must hold
                    \begin{align*}
                        x\text{-}a\text{-}b \quad a\text{-}x\text{-}b \quad b\text{-}x\text{-}c \quad b\text{-}c\text{-}x
                    \end{align*}
                    \bigbreak \noindent 
                    So, Ax.QR says that whenever $ a\text{-}b\text{-}c$ (say in pencil $P$), then any other ray in $P$ is in either fan $\overrightarrow{ba}$ or fan $\overrightarrow{bc} $ (so $P = \overrightarrow{ba} \cup \overrightarrow{bc} $)
                \item \textbf{Real fan axiom (Ax.RF)}: For any fan $\overrightarrow{ab} $ and for any real number $t$ with $ 0 \leq t \leq 180$, there is a ray $r$ in $\overrightarrow{ab} $ with $ar = t $
                    \bigbreak \noindent 
                    Ax.RF says every real number from 0 to 180 produces at least one ray in the fan
                    \bigbreak \noindent 
                    \textbf{Note:} Ax.RF is one version of what is sometimes called the \textbf{Protractor Axiom}
                \item \textbf{Notes}: Axioms $M1-M4$, $BR,QR,RF$ are true for $\mathbb{E}, \hat{\mathbb{E}}, \mathbb{M}, \mathbb{H}, \mathbb{S}$
                    \bigbreak \noindent 
                    All the results of chapters 8,9 have dual results for rays in a pencil. We do not state them all. But some that we do not state are needed for the proofs of some that we do.
                \item \textbf{Theorem 11.6 (Unique angular distance for fans)}: For any fan $\overrightarrow{pq}$ and any real number $t$ with $0 \leq t \leq 180$, there is a unique ray $r$ in $\overrightarrow{pq}$ with $pr = t$. $r$ is in $\overline{pq} $ if and only if $t \leq  pq$
                \item \textbf{Theorem 11.8}: If ray $a$ lies in pencil $P$, then $ a\text{-}r\text{-}a^{\prime} $ for every other ray $r$ in $P$
                    \bigbreak \noindent 
                    \textbf{Note:} We assumed in Ax.M4 that $a^{\prime} $ is the unique ray in $P$ with angular distance 1800 from a, so most of the proof of thm 9.1 does not need to be dualized. Alternatively, we could omit the assumption that $pq =180 \implies q = p^{\prime} $ and prove that this must be so by writing the dual of the full proof of thm 9.1.
                \item \textbf{Theorem 11.9 (Almost uniqueness of quadrichotomy for rays)}: Suppose that $a,b,c,r$ are distinct rays in a pencil $P$, and that $ a\text{-}b\text{-}c$. Then, \textbf{exactly} one of 
                    \begin{align*}
                        r\text{-}a\text{-}b \quad a\text{-}r\text{-}b \quad b\text{-}r\text{-}c \quad b\text{-}c\text{-}r
                    \end{align*}
                    With the exception that both $ r\text{-}a\text{-}b $ and $ b\text{-}c\text{-}r$ are true when $r = b^{\prime} $
                    \bigbreak \noindent 
                    \textbf{\textit{Proof}}: We proceed by dualizing the proof of theorem 9.2.
                    \bigbreak \noindent 
                    By Axiom.QR, at least one of 
                    \begin{align*}
                        r\text{-}a\text{-}b \quad a\text{-}r\text{-}b \quad b\text{-}r\text{-}c \quad b\text{-}c\text{-}r
                    .\end{align*}
                    Suppose we have $ a\text{-}r\text{-}b$. Then, $ a\text{-}b\text{-}c$ and the rule of insertion yields $ a\text{-}r\text{-}b\text{-}c $
                    \bigbreak \noindent 
                    So, $ a\text{-}r\text{-}b $ and $ r\text{-}b\text{-}c$ are true. Which, by the UMT guarantees that both $ b\text{-}r\text{-}c $ and $ b\text{-}c\text{-}r$ are false. 
                    \bigbreak \noindent 
                    Next, suppose that $ b\text{-}r\text{-}c$ is true. Then, $ a\text{-}b\text{-}c$ and the rule of insertion yields $ a\text{-}b\text{-}r\text{-}c$. So, $ a\text{-}b\text{-}r$ and $ b\text{-}r\text{-}c$ are true, and by the UMT, all three of  $ r\text{-}a\text{-}b$, $ a\text{-}r\text{-}b$, $ b\text{-}c\text{-}r$ are false. Thus, none of the other three relations hold.
                    \bigbreak \noindent 
                    So, if more than one of $ r\text{-}a\text{-}b, a\text{-}r\text{-}b, b\text{-}r\text{-}c, b\text{-}c\text{-}r$ holds, they must be exactly $ r\text{-}a\text{-}b$ and $ b\text{-}c\text{-}r$
                    \bigbreak \noindent 
                    Assume that $ r\text{-}a\text{-}b$ and $ b\text{-}c\text{-}r$ are true. Suppose toward a contradiction that $br < 180$. Then, fan $\overrightarrow{br}$ is defined, and $ r\text{-}a\text{-}b, b\text{-}c\text{-}r$ implies $a,c$ are in $ \overrightarrow{br}$. By the dual of theorem 8.3 (stated above), one of 
                    \begin{align*}
                        b\text{-}a\text{-}c \quad \text{or} \quad b\text{-}c\text{-}a
                    \end{align*}
                    is true. But, this contradicts $ a\text{-}b\text{-}c$ by the UMT. 
                    \bigbreak \noindent 
                    Therefore, $br = 180$, hence $ r = b^{\prime}$. \endpf

                \item \textbf{Theorem 11.10 (Opposite fan theorem)}: Let $p,q,r$ be rays in pencil $P$ such that $ q\text{-}p\text{-}r$. Then, $ \overrightarrow{pq} \cup \overrightarrow{pr} = P$, and $ \overrightarrow{pq} \cap \overrightarrow{pr} = \{p,p^{\prime}\} $
                \item \textbf{Corollary 11.11}: If $p,q$ are rays in pencil $P$ with $0 < pq < 180$, then $P = \overrightarrow{pq} \cup \overrightarrow{pq^{\prime}} $ and $\overrightarrow{pq} \cap \overrightarrow{pq^{\prime}} = \{p,p^{\prime}\}$
                \item \textbf{Compatibility Axiom (Ax.C)}: Let $A,B,C$ be points on line $m$, and $X$ a point not on $m$. If $ A\text{-}B\text{-}C$, then $ \overrightarrow{XA}\text{-}\overrightarrow{XB}\text{-}\overrightarrow{XC} $
                    \bigbreak \noindent 
                    \begin{figure}[ht]
                        \centering
                        \incfig{compat}
                        \label{fig:compat}
                    \end{figure}
                    \bigbreak \noindent 
                    Notice $AB + BC = AC \implies ab + bc = ac $
                \item \textbf{Notation and terminology}: Recall that $\hcancel{pq}$ means $p \cup q$, then union of the rays. Measure of $\hcancel{pq} $ means the angular distance $pq$
                \bigbreak \noindent 
                Suppose $p = \overrightarrow{BA}$, $ q = \overrightarrow{BC}$. Then, write
                \begin{align*}
                    \hcancel{pq} = \underline{\angle ABC} = \underline{\angle CBA}
                \end{align*}
                Or just $\underline{\angle B}$ when clear, and
                \begin{align*}
                    pq = \angle ABC = \angle CBA
                \end{align*}
                or just $\angle B$.
                \bigbreak \noindent 
                \begin{figure}[ht]
                    \centering
                    \incfig{im}
                    \label{fig:im}
                \end{figure}
            \item \textbf{Definition}: 
                \begin{itemize}
                    \item \textbf{Zero angle:} $\underline{pq}$ is a \textbf{zero angle} if $pq = 0 $ ($\iff p = q$)
                    \item \textbf{Straight angle}: If $pq = 180 (\iff p = q^{\prime}) $
                    \item \textbf{Proper angle:} if $0 < pq < 180 $
                    \item \textbf{acute angle}: if $ 0 < pq < 90$
                    \item \textbf{right angle}: if $ pq = 90$
                    \item \textbf{obtuse angle}: if $ 90 < pq < 180$
                \end{itemize}
            \item \textbf{Proposition 11.14}
                \begin{enumerate}[label=(\alph*)]
                    \item If $\omega < \infty$, then $\angle ABC = \angle AB^{*}C$
                    \item If $P \in \overrightarrow{BA}^{0} $ and $Q \in \overrightarrow{BC}^{0} $, then $\angle ABC = \angle PBQ$
                        \bigbreak \noindent 
                \end{enumerate}
                \bigbreak \noindent 
                \textbf{\textit{Proof}} (a) If $\omega < \infty$, then thm 10.8 and prop 9.3 implies $\overrightarrow{BA} = \overrightarrow{B^{*}A} $, $\overrightarrow{BC} = \overrightarrow{B^{*}C} $. So, $\underline{\angle ABC} = \overrightarrow{BA} \cup \overrightarrow{BC} = \overrightarrow{B^{*}A} \cup \overrightarrow{B^{*}C} = \angle AB^{*}C$ 
                \bigbreak \noindent 
                (b) If $P \in \overrightarrow{BA}^{0}$ and $Q \in \overrightarrow{BC}^{0}$, then $0 < BP < \omega$ and $0 < BQ < \omega$ by thm 9.4. Then, $ \overrightarrow{BA} = \overrightarrow{BP} $ and $\overrightarrow{BC} = \overrightarrow{BQ} $ by thm 8.4, so $\angle ABC = \overrightarrow{BA} \cup \overrightarrow{BC}  = \overrightarrow{BP} \cup \overrightarrow{BQ} = \angle PBQ$
            \item \textbf{Proposition 11.15 (Midpoint)}: If $\underline{pq}$ is a proper angle, then there is exactly one ray $b$ in the wedge $\overline{pq}$ so that $pb  = \frac{1}{2}pq $
            \item \textbf{Definition}: The ray $b$ is called the \textbf{bisector} of angle $\underline{pq}$ 
            \item \textbf{Theorem 12.2 (Fan: halfplane)}: Let $H,K$ be opposite halfplanes with edge line $\ell$, point $B \in H$. Let $X,A$ be points on $\ell$ with $0 < AX < \omega$. Let $h = \overrightarrow{XA}$, $k = \overrightarrow{XB}$. Then, $H $ consists of all points on all rays of the fan $\overrightarrow{hk}$, except for the points of $\ell$
                \bigbreak \noindent 
                That is, $P \in H \iff P \in j^{0}$, where $j^{0}$ is the interior of some ray $j \in \overrightarrow{hk}$, $j \ne h$ or $h^{\prime}$
                \bigbreak \noindent 
                \begin{figure}[ht]
                    \centering
                    \incfig{figher}
                    \label{fig:figher}
                \end{figure}
                \bigbreak \noindent 
                \textbf{\textit{Proof.}} Since $B \in H$ and $k = \overrightarrow{XB}$, by theorem 10.3 $k^{0} \subseteq H$. Suppose that $j$ is a ray in $\overrightarrow{hk}$ with $j\ne k,h,h^{\prime}$. So, either $ h\text{-}j\text{-}k$ or $ h\text{-}k\text{-}j$ by the definition of Fan.
                \bigbreak \noindent 
                Suppose toward a contradiction that for some point $C \in j^{0}$, then $C \in K$. Theorem 8.4 implies $j = \overrightarrow{XC}$, and theorem 10.3 implies $j^{0} \in K$
                \bigbreak \noindent 
                \begin{figure}[ht]
                    \centering
                    \incfig{ok}
                    \label{fig:ok}
                \end{figure}
                \bigbreak \noindent 
                $B \in H$, $C \in K$, so by Theorem 10.6, $ B\text{-}P\text{-}C$ for some $p \in \ell$. If $P =X$, then $ B\text{-}X\text{-}C$, which implies $ \overrightarrow{XC} = \overrightarrow{XB}^{\prime}$ by the opposite ray theorem (9.6)
                \bigbreak \noindent 
                If $P=X^{*}$ then $ B\text{-}X^{*}\text{-}C$, which implies $ \overrightarrow{X^{*}C} = \overrightarrow{X^{*}B^{\prime}}$ by Theorem 9.6
                \bigbreak \noindent 
                Then, $ B,X,C,X^{*}$ collinear (theorem 10.8), so proposition 9.3 implies $ \overrightarrow{X^{*}C} = \overrightarrow{XC} = j$, $ \overrightarrow{X^{*}B} = \overrightarrow{XB} = k$
                \bigbreak \noindent 
                So, if $P = X$ or $X^{*} $, then $j=k^{\prime}$. Then,  theorem 11.8 implies $ k\text{-}h\text{-}j$. But, this contradicts $ h\text{-}j\text{-}k$ or $ h\text{-}k\text{-}j $ by Theorem 11.3 (UMT for rays). Therefore, $P \ne X$ or $X^{*}$, so $X$ is not collinear with $B,P,C$. Also, $P \in \ell$ so either $P \in h^{0}$ or $P \in \text{Int}h^{\prime}$. By theorem 8.4, $ \overrightarrow{XP} = h$ or $h^{\prime}$
                \bigbreak \noindent 
                By Ax.C and $ B\text{-}P\text{-}C$, $ \overrightarrow{XB}\text{-}\overrightarrow{XP}\text{-}\overrightarrow{XC}$, which implies either $ k\text{-}h\text{-}j$ or $ k\text{-}h^{\prime}\text{-}j$
                \bigbreak \noindent 
                Again, $j \in \overrightarrow{hk}$, which implies $ h\text{-}j\text{-}k$ or $ h\text{-}k\text{-}j$, so by UMT, $ k\text{-}h\text{-}j$ is false. Thus, $ k\text{-}h^{\prime}\text{-}j = j\text{-}h^{\prime}\text{-}k$
                \bigbreak \noindent 
                If $ h\text{-}j\text{-}k$ then ROI yields $ h\text{-}j\text{-}h^{\prime}\text{-}k$, which gives $ h\text{-}h^{\prime}\text{-}j$, contradicting $ h\text{-}j\text{-}h^{\prime}$.
                \bigbreak \noindent 
                So, for all rays $j \in \overrightarrow{hk}$ with $j\ne h$ or $h^{\prime}$, then $ j^{0} \subseteq H$. 
                \bigbreak \noindent 
                Now, $\text{Int}k^{\prime} \subseteq K$ by thm 10.3, and the proof thus far shows: for all $j \in \overrightarrow{hk^{\prime}} $ with $j \ne h$ or $h^{\prime}$, then $j^{0}  \subseteq K$
                \bigbreak \noindent 
                For any point $D \in H$, $ \overrightarrow{XD} \in P_{X}$, but $\overrightarrow{XD}\not\in \overrightarrow{hk^{\prime}} $ since $D \not\in K$. 
                \bigbreak \noindent 
                By coroll. 11.11, $\overrightarrow{XD} \in \overrightarrow{hk}$. So, points of $H$ equals points on all $j^{0}, j\in \overrightarrow{hk}, j\ne h,h^{\prime}$
                \endpf


            \item \textbf{Corollary 12.3}: Let $z$ by any number with $0 < z < 180$. For any ray $\overrightarrow{AB}$ there are exactly two rays $h,k$ in $P_{A}$ such that $\overrightarrow{AB}h = z = \overrightarrow{AB}k$. Furthermore, $h^{0}$ and $k^{0}$ lie in opposite halfplanes with edge $\overleftrightarrow{AB} $
                \bigbreak \noindent 
                \textbf{\textit{Proof.}} There is a ray $q$ with $0 < \overrightarrow{AB}q < 180 $ (Thm. 11.2). Thus, fan $\overrightarrow{\overrightarrow{AB}q} $ is defined.
                \bigbreak \noindent 
                Coroll 11.11 implies $P_{A}  = \overrightarrow{\overrightarrow{AB}q} \cup \overrightarrow{\overrightarrow{AB}q^{\prime}}$, the union of two fans that have only $\overrightarrow{AB}, \overrightarrow{AB}^{\prime}$ in common.
                \bigbreak \noindent 
                \begin{figure}[ht]
                    \centering
                    \incfig{threefans}
                    \label{fig:threefans}
                \end{figure}
                \bigbreak \noindent 
                Thm 11.6 implies there's a unique ray $h$ in $\overrightarrow{\overrightarrow{AB}q}$ with $\overrightarrow{AB}h = z$, and a unique ray $k$ in $\overrightarrow{\overrightarrow{AB}q^{\prime}}$ with $\overrightarrow{AB}k  = z$. So, there are only two such rays in $P_{A} = \overrightarrow{\overrightarrow{AB}q} \cup \overrightarrow{\overrightarrow{AB}q^{\prime}}$
                \bigbreak \noindent 
                \begin{figure}[ht]
                    \centering
                    \incfig{fourfans}
                    \label{fig:fourfans}
                \end{figure}
                \bigbreak \noindent 
                Thm 12.2 implies $h^{0}, k^{0}$ lie in opposite halfplanes with edge $\overleftrightarrow{AB}$

    \end{itemize}

    \pagebreak 
    \subsection{The Crossbar Theorem}
    \begin{itemize}
        \item \textbf{Intro:} This gives us a sufficient condition (a guarantee) that a ray will meet a line (in fact, a line segment)
            \bigbreak \noindent 
            Axiom C does not do this. It deals with rays that are already assumed to meet a line.
            \bigbreak \noindent 
            The context for the crossbar theorem is any general plane with the 20 axioms.
        \item \textbf{Theorem 12.4 (The Crossbar Theorem)}: If $\underline{hk}$ is a proper angle with vertex (common endpoint) $X$, if $A \in h^{0}$ (so $h = \overrightarrow{XA}$), $C \in k^{0}$ (so $k = \overrightarrow{XC}$), and $ h\text{-}j\text{-}k$, then there is an interior point $B$ of $j$ with $ A\text{-}B\text{-}C$
            \bigbreak \noindent 
            \begin{figure}[ht]
                \centering
                \incfig{cbt}
                \label{fig:cbt}
            \end{figure}
            \bigbreak \noindent 
            \textbf{Note:} $\underline{hk}$ proper means that $h \ne k$ or $k^{\prime}$, $ k \ne h$ or $h^{\prime}$. Hence, $C \not\in h \cup h^{\prime} = \overleftrightarrow{XA}$, $A \not\in k\cup k^{\prime} = \overleftrightarrow{XC} $. Then, $X,A,C$ noncollinear, so $AC<\omega$. 
            \bigbreak \noindent 
            Thus, segment $\overline{AC}$ is defined, and the crossbar theorem says that $j^{0}$ meets $\overline{AC}^{0} $. Note that
            \begin{align*}
                j^{0} \cap \overline{AC}^{0} \ne \varnothing
            .\end{align*}
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} Assume $h = \overrightarrow{XA}, k = \overrightarrow{XC}$, with $\underline{hk}$ a proper angle. Assume $ h\text{-}j\text{-}k$
            \bigbreak \noindent 
            \begin{figure}[ht]
                \centering
                \incfig{cbt2}
                \label{fig:cbt2}
            \end{figure}
            \bigbreak \noindent 
            $ h\text{-}j\text{-}k$ implies $h,j,k$ distinct, which implies $0 < hj < hk < jk $
            \bigbreak \noindent 
            $ h\text{-}j\text{-}k $ implies $hj + jk = hk < 180$, so $0 < hj < 180, 0 < jk < 180 $, which implies fans $\overrightarrow{jh}, \overrightarrow{jk}$ defined. $ h\text{-}j\text{-}k$ also implies $P_{X} = \overrightarrow{jh} \cup \overrightarrow{jk}$, with $\overrightarrow{jh} \cap \overrightarrow{jk} = \{j,j^{\prime}\} $ (thm 11.10)
            \bigbreak \noindent 
            Let $m =$ line $j \cup j^{\prime}$; we have $H,K$ opposite halfplanes with edge $m$ (Ax.S), we may assume that $A \in H$
            \bigbreak \noindent 
            Theorem 12.2 implies $H$ is the set of all points of all $r^{0}$, for $r \in \overrightarrow{jh}$, $r \ne j$ or $j^{\prime} $
            \bigbreak \noindent 
            $\overrightarrow{jk}$ is the "opposite fan" to $\overrightarrow{jh}$, so theorem 12.2 implies $k^{0} \subseteq K$, so $C \in K$
            \bigbreak \noindent 
            Now, theorem 10.6 implies there is a point $Y$ on $m$ with $ A\text{-}Y\text{-}C$
            \bigbreak \noindent 
            \begin{figure}[ht]
                \centering
                \incfig{ict23}
                \label{fig:ict23}
            \end{figure}
            \bigbreak \noindent 
            If $Y = X$ or $X^{*}$, then $ A\text{-}X\text{-}C$ or $ A\text{-}X^{*}\text{-}C$, which implies $A,X,C$ collinear. By theorem 10.8, if $ A,X^{*}, C$ collinear, then $A,X^{*}, C,X$ collinear, which contradicts $\underline{\angle AXC} = \underline{hk}$ proper.
            \bigbreak \noindent 
            So, $Y$ on $m$ with $Y \ne X, X^{*}$, which implies $Y \in j^{0}$ or $\text{Int}j^{\prime}$, which implies $\overrightarrow{XY} = j$ or $j^{\prime} $
            \bigbreak \noindent 
            $A,Y,C$ on line $\overleftrightarrow{AY}$, and $X$ not on $\overleftrightarrow{AY}$, so by Ax.C and $ A\text{-}Y\text{-}C$, $ \overrightarrow{XA}\text{-}\overrightarrow{XY}\text{-}\overrightarrow{XC} $, which implies $ h\text{-}\overrightarrow{XY}\text{-}k$
            \bigbreak \noindent 
            Suppose $ \overrightarrow{XY} = j^{\prime}$, then $ h\text{-}j^{\prime}\text{-}k$. By hypothesis, $ h\text{-}j\text{-}k$. So, $k$ in fans $\overrightarrow{hj^{\prime}} $ and $ \overrightarrow{hj} $. But, coroll 11.11 implies $ \overrightarrow{hj^{\prime}} \cap \overrightarrow{hj} = \{h, h^{\prime}\}$, and $ k \ne h$ or $h^{\prime}$ ($\underline{hk} $ proper). This is a contradiction, which implies $ \overrightarrow{XY} = j$, hence $Y \in j^{0}$, and $ A\text{-}Y\text{-}C$ \endpf



        \item \textbf{Definition}: Let $A,B,C$ be three noncollinear points (So $AB$, $BC$, $AC$ all $< \omega$ by prop noncollinear). 
            \bigbreak \noindent 
            \begin{itemize}
                \item The \textbf{triangle} $\triangle ABC$ is $\overline{AB} \cup \overline{BC} \cup \overline{CA}$
                \item The \textbf{sides} of $\triangle ABC$ are $\overline{AB}, \overline{BC}, \overline{CA}$
                \item The \textbf{vertices} of $\triangle ABC$ are $A,B,C$
                \item The \textbf{angles} of $\triangle ABC$ are
                    \begin{align*}
                        \underline{\angle CAB} &= \underline{\overrightarrow{AC}\overrightarrow{AB}} = \angle A = \angle A \\
                        \underline{\angle ABC} &= \underline{\overrightarrow{BA}\overrightarrow{BC}} = \angle B \\
                        \underline{\angle BCA} &= \underline{\overrightarrow{CB}\overrightarrow{CA}} = \angle C
                    .\end{align*}
                \item $\angle CAB$ are vertex $A$ are called \textbf{opposite} $\overline{BC}$, etc...
                \item The \textbf{angle sum} $\sigma(ABC) = \angle A + \angle B + \angle C$ (This is not necessarily 180)
            \end{itemize}
        \item \textbf{Note:} Say we have $\triangle ABC$ and ray $j$ with $ \overrightarrow{BA}\text{-}j\text{-}\overrightarrow{BC} $
            \begin{figure}[ht]
                \centering
                \incfig{mytri}
                \label{fig:mytri}
            \end{figure}
            $\angle ABC$ is proper, since $A,B,C$ are noncollinear. So, the crossbar theorem implies  $j^{0}$ meets $\overline{AC}^{0}$. Thus, there is a point $D$ on $j^{0}$ with $ A\text{-}D\text{-}C$
            \bigbreak \noindent 
            \begin{figure}[ht]
                \centering
                \incfig{mytri2}
                \label{fig:mytri2}
            \end{figure}
            \bigbreak \noindent 
            \textbf{Note:} Euclid does this a lot, with no explicit justification.
        \item \textbf{Note about the crossbar theorem}: The proof of the crossbar theorem depends on the fan: halfplane theorem (12.2), which in turn depends on the separation axiom (Ax.S)
            \bigbreak \noindent 
            In $\mathbb{G}$, where $Ax.S$ is false, the crossbar theorem is also false.




    \end{itemize}

    \pagebreak 
    \subsection{Duals of results from chapters 8 and 9}
    \bigbreak \noindent 
    \subsubsection{Theorems (14)}
    \begin{itemize}
        \item \textbf{Theorem 8.1D}: The set of angle measures $\mathbb{D} = [0,180]$
        \item \textbf{Theorem 8.2D}: All wedges, fans, pencils have infinitely many rays
        \item \textbf{Theorem 8.3D}: Let $x\ne y$ be distinct from $a$ on fan $\overrightarrow{ab}$. Then, exactly one of 
            \begin{align*}
                a\text{-}x\text{-}y \quad \text{ or } \quad a\text{-}y\text{-}x
            .\end{align*}
        \item \textbf{Theorem 8.4D}: Let $\overrightarrow{ab}$ be a fan. If $c \in \overrightarrow{ab}$, $0 < c < 180$, then $\overrightarrow{ab} = \overrightarrow{ac}$
        \item \textbf{Theorem 8.6D}: Stated in theorem 11.6
        \item \textbf{Theorem 9.1D}: Let ray $a$ be in pencil $P$, there exists a unique fan $a^{\prime} \in P$ such that $aa^{\prime} = 180$. For all other rays $x\in P$, $ a\text{-}x\text{-}a^{\prime} $
        \item \textbf{Theorem 9.2D}: Stated in theorem 11.8
        \item \textbf{Theorem 9.4D}: If $ap = 180$ in some fan $h$, then $p = a^{\prime}$.
        \item \textbf{Theorem 9.6D}: Stated in theorem 11.9
        \item \textbf{Theorem 9.7D}: Each fan has a unique opposite fan.
        \item \textbf{Theorem 9.8D}: Let rays $a,b \in P$, if $0 < ab < 180$, then fan $\overrightarrow{ab}^{\prime} = \overrightarrow{ab^{\prime}} $
        \item \textbf{Theorem 9.9D}: Let rays $a,b \in P$, if $0 < ab < 180$, then $P = \overline{ab} \cup \overline{ab^{\prime}} \cup \overline{ba^{\prime}} \cup \overline{b^{\prime}a^{\prime}}$, where the interiors of these wedges are disjoint.
        \item \textbf{Theorem 9.10D}: Let rays $a,b \in P$, if $0 < ab < 180$, and $c$ is some other ray in $P$, then there exists no betweenness relation among $a,b,c$ if and only if $c \in \overline{a^{\prime}b^{\prime}} $
    \end{itemize}

    \bigbreak \noindent 
    \subsubsection{Propositions}
    \begin{itemize}
        \item \textbf{Proposition 8.11D}: Let $a,b \in P$, $0 < ab < 180$, there exists $c\in P$ such that $ c\text{-}a\text{-}b$, $cb < 180$
        \item \textbf{Proposition 8.5D}: A fan has at most two terminal rays 
        \item \textbf{Proposition 8.7D}: Let $\overline{ab}$ be a wedge, for all $x,y \in \overline{ab}$, $xy \leq ab$, if $xy = ab$, then $\{x,y\} = \{a,b\}$
        \item \textbf{Proposition 8.8D}: If $\overline{ab}  = \overline{cd}$, then $\{a,b\} = \{c,d\}$
        \item \textbf{Proposition 8.9D}: Stated in proposition 11.15
        \item \textbf{Proposition 9.3D}: Let $a,b \in P$ such that $0 < ab < 180$. Then,
            \begin{itemize}
                \item Fan $\overrightarrow{ab} = \overline{ab} \cup \overline{ba^{\prime}}$, with $\overline{ab} \cap \overline{ba^{\prime}}  = \varnothing$
                \item Fan $\overrightarrow{ab} = \overrightarrow{a^{\prime}b} $
            \end{itemize}
    \end{itemize}

    \pagebreak 
    \subsection{Side angle side congruence}
    \begin{itemize}
        \item \textbf{Intro}: We know present the 21 axiom for a general plane, in fact the last axiom that will be covered here. Euclid called it a theorem, but we'll show that it is not a consequence of the first 20 axioms, and so cannot be proved from them.
            \bigbreak \noindent 
            Our context is a general plane with the 20 axioms. These include $\mathbb{E}, \hat{\mathbb{E}}, \mathbb{M}, \mathbb{H}, \mathbb{S} $
        \item \textbf{Definition: Congruence}: Two segments $\overline{AB}$ and $ \overline{XY}$ are \textbf{congruent} $(\cong)$ if they have the same length: $\overline{AB} \cong \overline{XY} $ means $AB = XY$
            \bigbreak \noindent 
            Two angles $\angle CAB$ and $\angle ZXY$ are congruent if they have the same angle measure
            \bigbreak \noindent 
            Two triangles $\triangle ABC$ and $\triangle ZXY$ are congruent under the correspondence $A\leftrightarrow X$, $B \leftrightarrow Y, C\leftrightarrow Z$ (Write as $ABC \leftrightarrow XYZ $) if 
            \begin{align*}
                \overline{AB} \cong \overline{XY},\quad \overline{BC} \cong\overline{YZ} ,\quad \overline{AC} \cong \overline{XZ}
            .\end{align*}
            and 
            \begin{align*}
                \angle ABC \cong \angle XYZ, \quad \angle CAB \cong \angle ZXY,\quad \angle BCA \cong \angle YZX
            .\end{align*}
            denote this by $\triangle ABC \cong \triangle XYZ $
            \bigbreak \noindent 
            \textbf{Note:} The correspondence of vertices is an essential part of the congruence of triangles.
        \item \textbf{Side-angle-side axiom (Ax.SAS)}: If under the correspondence $ABC \leftrightarrow XYZ$ between the vertices of $ \triangle ABC$ and those of $ \triangle XYZ$, two sides of $ \triangle ABC$ are congruent to the corresponding two sides of $\triangle XYZ$, and the angle included between these two sides of $ \triangle ABC$ is congruent to the corresponding angle of $\triangle XYZ$, then $\triangle ABC \cong \triangle XYZ$
        \item \textbf{The bumpy plane $\hat{\mathbb{E}}$}: Observe that Ax.SAS is false for the bumpy plane
            \bigbreak \noindent 
            \begin{figure}[ht]
                \centering
                \incfig{bp}
                \label{fig:bp}
            \end{figure}
        \item \textbf{The taxicab plane}: Observe that Ax.SAS is false for the taxicab plane
            \bigbreak \noindent 
            \begin{figure}[ht]
                \centering
                \incfig{taxi}
                \label{fig:taxi}
            \end{figure}
            \bigbreak \noindent 
            \textbf{Note}: Since the first 20 axioms are true for $\hat{\mathbb{E}}, \mathbb{M} $, but the 21st is false, is is therefore not possible that the 21st is a consequence of the first 20.
            \bigbreak \noindent 
            Ax.SAS is true for $\mathbb{E}, \mathbb{S}, \mathbb{H}$
        \item \textbf{Definition: Absolute plane}: An \textbf{absolute plane} $\mathbb{P}$ is a set of points $\mathbb{P}$ with lines, distance, and angular distance (all undefined terms), such that all 21 axioms are true. The three planes above are absolute planes
        \item \textbf{Theorem 13.1 (ASA)}: If under the correspondence $ABC \leftrightarrow XYZ$, two angles and the included side of $\triangle ABC$ are congruent, respectively, to the corresponding two angles and included side of $\triangle XYZ$, then $\triangle ABC \cong \triangle XYZ $
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} If we can show that $BC = YZ$, then we can apply Ax.SAS: $ \overline{BA} \cong \overline{YX}$, $\overline{BC} \cong \overline{YZ}, \angle ABC \cong \angle XYZ$ implies $\triangle ABC \cong \triangle XYZ$
            \bigbreak \noindent 
            Suppose toward a contradiction that $ BC \ne YZ$. Since we are given exactly the same information about the two triangles, we may choose notation so that $BC > YZ$
            \bigbreak \noindent 
            Thm 8.6 implies there is a point $W$ on $\overrightarrow{BC}$ with $BW = YZ$, then $BW < BC$, which implies $ B\text{-}W\text{-}C$
            \bigbreak \noindent 
            \begin{figure}[ht]
                \centering
                \incfig{thefig}
                \label{fig:thefig}
            \end{figure}
            \bigbreak \noindent 
            $\overrightarrow{BC} = \overrightarrow{BW}$ (Thm 8.4), and Thm 10.3 implies $C,W$ are in the same halfplane with edge $\overleftrightarrow{AB}$. IN particular, $A,B,W$ are noncollinear, and we have $\triangle ABW$ with
            \begin{align*}
                \overline{BA} \cong \overline{YX},\ \overline{BW} \cong \overline{YZ},\ \angle ABW = \angle ABC \cong \angle XYZ
            .\end{align*}
            Note that $\angle ABW = \angle ABC$ by prop 11.14. With these facts, and by Ax.SAS, $ \triangle ABW \cong \triangle XYZ$, which implies $ \angle BAW \cong \angle YXZ$.
            \bigbreak \noindent 
            $\angle YXZ = \angle X \cong \angle A = \angle BAC$, so $\angle BAW = \angle BAC$
            \bigbreak \noindent 
            $ B\text{-}W\text{-}C$ and Ax.C implies $ \overrightarrow{AB}\text{-}\overrightarrow{AW}\text{-}\overrightarrow{AC} $, which implies $ \overrightarrow{AB}\overrightarrow{AW}  + \overrightarrow{AW}\overrightarrow{AC} = \overrightarrow{AB}\overrightarrow{AC}$, which implies $ \angle BAW + \angle WAC = \angle BAC $
            \bigbreak \noindent 
            Thm 10.3 implies $W,B$ in a halfplane with edge $\overleftrightarrow{AC}$, which implies $W,A,C$ noncollinear, which implies $ \angle WAC$ is proper. Thus, $ \angle WAC > 0$, and $ \angle BAW < \angle BAC$, which is a contradiction \endpf
        \item \textbf{Definition: types of triangles}
            \begin{itemize}
                \item A triangle is \textbf{isosceles} if two sides have the same length
                \item \textbf{Equilateral} if all three sides have the same length
                \item \textbf{Equiangular} if all three angles have the same measure  
            \end{itemize}
            \textbf{Note:} A triangle can be called \textbf{scalene} if all all three sides have different lengths and all three angles have different measures
        \item \textbf{Theorem 13.2 (pons asinorum ("Bride of asses"))} In any $\triangle ABC$, 
            $$ AB = AC \iff \angle ACB = \angle ABC $$
            \bigbreak \noindent 
            \begin{figure}[ht]
                \centering
                \incfig{assse}
                \label{fig:assse}
            \end{figure}
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} Consider the correspondence $ABC \leftrightarrow ACB$ between the vertices of $\triangle ABC$ and itself
            \bigbreak \noindent 
            \begin{figure}[ht]
                \centering
                \incfig{mig}
                \label{fig:mig}
            \end{figure}
            \bigbreak \noindent 
            Suppose that $AB = AC$, then 
            \begin{align*}
                \overline{AB} \cong \overline{AC},\quad \overline{AC} \cong \overline{AB}, \quad \angle BAC \cong \angle CAB
            .\end{align*}
            So, Ax.SAS implies $\triangle ABC \cong \triangle ACB $, which implies $ \angle ACB = \angle ABC $
            \bigbreak \noindent 
            Suppose that $\angle ACB = \angle ABC$. Then, $\angle ACB \cong \angle ABC$, $\angle ABC \cong \angle ACB$, $\overline{CB} = \overline{BC} $
            \bigbreak \noindent 
            So, Thm 13.1 (ASA) implies $ \triangle ABC \cong \triangle ACB $, which implies $ AB = AC$ by congruence
        \item \textbf{Corollary 13.3}: A triangle is equilateral if and only if it is equiangular
        \item \textbf{Theorem 13.4 (SSS)}: If in $\triangle ABC$ and $\triangle XYZ$, $\overline{AB} \cong \overline{XY}$, $ \overline{BC} \cong \overline{YZ}$ and $\overline{CA} \cong \overline{ZX}$, then 
            \begin{align*}
                \triangle ABC \cong \triangle XYZ
            .\end{align*}
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} We'll show that $\angle BAC = \angle YXZ$. Then, $Ax.SAS$ will imply $\triangle ABC \cong \triangle XYZ$.
            \bigbreak \noindent 
            So, assume for the sake of contradiction that $ \angle A \ne \angle X$. We may assume that $\angle A > \angle X$ (otherwise, just switch the notation for vertices from one triangle to the other). So, $\angle BAC > \angle YXZ$, which means that $\overrightarrow{AB}\overrightarrow{AC} > \angle X$. We apply Ax.RF (or thm 11.6) to the fan $ \overrightarrow{\overrightarrow{AB}\overrightarrow{AC}} $. Thus, there is a ray $h$ in $\overrightarrow{\overrightarrow{AB}\overrightarrow{AC}} $ with $\overrightarrow{AB}h  = \angle X$
            \bigbreak \noindent 
            \begin{figure}[ht]
                \centering
                \incfig{anglex}
                \label{fig:anglex}
            \end{figure}
            \bigbreak \noindent 
            Because $\overrightarrow{AB}h = \angle X < \overrightarrow{AB}\overrightarrow{AC}$, we have $ \overrightarrow{AB}\text{-}h\text{-}\overrightarrow{AC} $. $\underline{\overrightarrow{AB}\overrightarrow{AC}}$ is a proper angle, since $A,B,C$ noncollinear. So, we may apply the Crossbar Theorem
            \bigbreak \noindent 
            Therefore, there is a point $D \in h^{0}$ with $ B\text{-}D\text{-}C$. So, $ h = \overrightarrow{AD} $ (thm 8.4)
            \bigbreak \noindent 
            \begin{figure}[ht]
                \centering
                \incfig{helloworld}
                \label{fig:helloworld}
            \end{figure}
            \bigbreak \noindent 
            Ax.RR implies there is a point $P$ on $h$ with $AP = XZ$. So, $h = \overrightarrow{AP}$. $\overline{AB} \cong \overline{XY}$, $\overline{AP} \cong \overline{XZ}, \underline{\angle BAP} = \underline{\overrightarrow{AB}h} \cong \underline{\angle YXZ}$
            \bigbreak \noindent 
            Ax.SAS implies $ \triangle ABP \cong \triangle XYZ $, which implies $BP = YZ = BC$ (So $P \ne D$, since $BD< BC$), and $AP = XZ = AC $
            \bigbreak \noindent 
            \begin{figure}[ht]
                \centering
                \incfig{newtri}
                \label{fig:newtri}
            \end{figure}
            \bigbreak \noindent 
            Pons Asinorum (13.2) for $ \triangle BCP$ implies $ \angle BCP  = \angle BPC$, and for $ \triangle ACP $, $\angle ACP = \angle APC $
            \bigbreak \noindent 
            $ B\text{-}D\text{-}C$ and Ax.C implies $ \overrightarrow{PB}\text{-}\overrightarrow{PD}\text{-}\overrightarrow{PC} $, which implies $\angle BPD + \angle DPC = \angle BPC $, implies $ \angle BPC > \angle DPC $
            \bigbreak \noindent 
            Now, we consider separately two cases for $P$ on $\overrightarrow{AD}$
            \bigbreak \noindent 
            1.) $ A\text{-}P\text{-}D$. Then, $ \overrightarrow{PD} = \overrightarrow{PA}^{\prime}$ (Thm 9.6), so Thm 11.8 implies $ \overrightarrow{PA}\text{-}\overrightarrow{PC}\text{-}\overrightarrow{PD}$
            \bigbreak \noindent 
            Thus, $ \angle APC + \angle DPC = \angle APD = 180 $ (Ax.M4)
            \bigbreak \noindent 
            $ A\text{-}P\text{-}D $ and Ax.C implies $ \overrightarrow{CA}\text{-}\overrightarrow{CP}\text{-}\left(\overrightarrow{CD} =  \overrightarrow{CB}\right)$, which implies $ \angle ACP + \angle BCP = \angle ACB$. 
            \bigbreak \noindent 
            Since $\angle ACP = \angle APC$ and $\angle BCP = \angle BPC > \angle DPC$, $ \angle ACB = \angle ACP + \angle BCP = \angle APC + \angle BPC > \angle APC + \angle DPC = 180 $, which contradicts Ax.M1
            \bigbreak \noindent 
            2.) $ A\text{-}D\text{-}P$ (needs proof)







    \end{itemize}

    \pagebreak 
    \subsection{Perpendiculars}
    \begin{itemize}
        \item \textbf{Intro:} We've introduced 21 axioms, and defined an \textbf{absolute plane}, which hase
            \begin{itemize}
                \item \textbf{Undefined terms}: Point, line, distance, angle measure
                \item \textbf{21 Axioms}
            \end{itemize}
            Examples are $\mathbb{E}, \mathbb{H},\mathbb{S} $
            \bigbreak \noindent 
            Next, we introduce and study \textbf{perpendicular lines}.
        \item \textbf{Definition: Supplementary angles}: Two angles are \textbf{supplementary} if their measures sum to 180.
        \item \textbf{Theorem 14.1 (Supplementary angles theorem)}: If $h,j$ are coterminal rays, then $\underline{hj}$ and $\underline{jh^{\prime}} $ are supplementary
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} If $j = h$, then $hj = 0$ by Ax.M2, and $jh^{\prime} = 180$ by Ax.M4. So, $hj + jh^{\prime} = 180$.
            \bigbreak \noindent 
            If $j \ne h$ or $h^{\prime}$, thm 11.8 implies $ h\text{-}j\text{-}h^{\prime}$, which means $hj + jh^{\prime} = hh^{\prime}$, and $hh^{\prime} =180$ by Ax.M4
        \item \textbf{Definition}: Angles $\underline{hk}, \underline{rs}$ are \textbf{vertical} if $\{r,s\}  = \{h^{\prime}, k^{\prime}\}$
        \item \textbf{Theorem 14.2 (Vertical angles theorem)}: Vertical angles are congruent
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} Given vertical angles $\underline{hk}, \underline{h^{\prime}k^{\prime}}$, Thm 14.1 implies $\underline{hk}$ and $\underline{kh^{\prime} }$ are supplementary, and $ \underline{kh^{\prime}}$ and $h^{\prime}k^{\prime} $ are supplementary, which implies $kh + kh^{\prime} = 180 = kh^{\prime} + h^{\prime}k^{\prime}$, which implies $hk = h^{\prime}k^{\prime} $
            \bigbreak \noindent 
            \textbf{Note:} When two lines intersect, four angles are formed, angles $\underline{hk}, \underline{kh^{\prime}}, \underline{h^{\prime}k^{\prime}},$ and $\underline{k^{\prime}h} $, the measure of any one determines the measure of the others, by thms 14.1, 14.2. In particular, if $hk = 90$, then all four angle measures are $90$
        \item \textbf{Definition: Perpendicular}: Two intersecting lines $m,n$ are \textbf{perpendicular} (at point of intersection $B$) if the four angles they determine at $B$ are right angles, we write $m\perp n$ (at $B$)
            \bigbreak \noindent 
            \textbf{Note:} Prop 11.14 implies if $m \perp n$ at $B$, then $m \perp n$ at $B^{*} $ (when $\omega < \infty $)
        \item \textbf{Theorem 14.3}: Through any point $A$ on a line $m$, there is exactly one line $n$ perpendicular to $m$
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} Let $h$ be a ray on $m$ with endpoint $A$ (so $m = h \cup h^{\prime} $)
            \bigbreak \noindent 
            Since $0 < 90 < 180$, coroll 12.3 implies there are exactly two rays $k,j$ in $P_{A}$ with $hk = hj = 90 $
            \bigbreak \noindent 
            Let $n = k \cup k^{\prime}$. $hk = 90$ implies $ hk^{\prime} = 90$ by thm 14.1, so $j = k^{\prime}$. That is, $k, k^{\prime}$ are the only rays in $P_{A}$ that form a right angle with $h$. So, $n$ is the only line through $A$ with $m \perp n$
        \item \textbf{Definition: The perpendicular bisector}: The \textbf{perpendicular bisector} of a segment $\overline{AB}$ is the line perpendicular to $\overleftrightarrow{AB}$ at the midpoint $M$ of $\overline{AB}$
        \item \textbf{Theorem 14.9 (needs proof)}: Every point of the perpendicular bisector of a segment is equidistant from the endpoints of the segment: $AX = BX$ for all $X$ on the perpendicular bisector
        \item \textbf{Theorem 14.10 (converse of 14.9)}: Let $m = \overleftrightarrow{AB}$, suppose that line $n\ne m$ meets $m$ at the midpoint $M$ of $\overline{AB}$. Suppose that there is some point $X$ on $n$, not on $m$, so that $AX = BX$. Then, $n \perp n$ at $M$
        \item \textbf{Note about 14.9 and 14.10}: Theorems 14.9 and 14.10 say that the perpendicular bisector of $\overline{AB}$ consists exactly of the points $X$ in $\mathbb{P}$ such that $AX = BX $
        \item \textbf{Theorem 14.4}: Through a point $A$ not on a given line $m$ there is at least one line $n$ perpendicular to $m$
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} Choose any point $B$ on $m$. Since $A$ is not on $m$, $AB < \omega$. So, there is a unique line $\overleftrightarrow{AB}$ through $A$ and $B$, and ray $\overrightarrow{BA}$ is defined. Let $h$ be a ray in $m$ with endpoint $B$, so that $m = h \cup h^{\prime}$. Let $\theta  = \overrightarrow{BA}h $
            \bigbreak \noindent 
            \fig{.6}{./figures/g1.png}
            \bigbreak \noindent 
            $A$ is in a halfplane $H$ with edge $m$. Coroll. 12.3 implies there is a second ray $k$ with endpoint $B$ so that $k^{0}$ is in the opposite halfplane $K$ and $hk= \theta$
            \bigbreak \noindent 
            \fig{.5}{./figures/g2.png}
            \bigbreak \noindent 
            Ax.RR implies there's a point $C$ on $k$ with $BC = BA$. Thm 10.6 implies there's a point $X$ on $m$ with $ A\text{-}X\text{-}C$. Then, $ \overrightarrow{XC} =  \overrightarrow{XA}^{\prime}$
            \bigbreak \noindent 
            We consider some cases
            \bigbreak \noindent 
            1.) If $X = B,$ then $\overrightarrow{BC} = \overrightarrow{BA}^{\prime}$, so $ \overrightarrow{BA}h + \overrightarrow{BC}h = 180$ (Thm 14.1), this implies $ \theta  + \theta  = 180$, thus $\theta  = 90$. So, $\overleftrightarrow{AB} \perp m$. Note that this is the lucky case. Our random choice of $B$ gave us a perpendicular line.
            \bigbreak \noindent 
            2.) If $X = B^{*}$, then $\overrightarrow{B^{*}C} = \overrightarrow{B^{*}A}^{\prime}$. Thus, $h$ has endpoint $B^{*}$ (Prop 9.3). So, Thm 14.1 implies $ \overrightarrow{B^{*}A}h + \overrightarrow{B^{*}C}h = 180$
            \bigbreak \noindent 
            In this case, prop 11.14 implies $\overrightarrow{B^{*}A}h = \overrightarrow{BA}h$, and $\overrightarrow{B^{*}C}h = \overrightarrow{BC}h$ $\overrightarrow{BA}h  + \overrightarrow{BC}h$, and thus $\theta  + \theta  = 180$, so $\theta  = 90$. Thus, we have $\overleftrightarrow{AB} \perp m$
            \bigbreak \noindent 
            3.) If $X$ is in $h^{0}$, then $ h = \overrightarrow{BX}$.
            \bigbreak \noindent 
            \fig{.6}{./figures/g3.png}
            \bigbreak \noindent 
            $\theta  = \angle ABX = \angle CBX$, so $\underline{\angle ABX} \cong \underline{\angle CBX} $, $ \overline{BA} \cong \overline{BC}, \overline{BX} \cong \overline{BX}$, and thus Ax.SAS implies $\triangle ABX \cong \triangle CBX $, which implies $\angle AXB = \angle CXB$ by definition of congruent triangles
            \bigbreak \noindent 
            From here, thm 14.1 implies $180 = \overrightarrow{XA}\overrightarrow{XB} + \overrightarrow{XC}\overrightarrow{XB} = \angle AXB + \angle CXB$. 
            \bigbreak \noindent 
            Therefore, $\angle AXB = \angle CXB = 90$. So, $\overleftrightarrow{AX} \perp m$ at $X$, $\overleftrightarrow{AX}$ goes through $A$.
            \bigbreak \noindent 
            If $X$ is on $\text{Int}h^{\prime}$, thm 14.1 implies $\angle ABX = 180 - \theta = \angle CBX$. So again, $\overline{BA} \cong \overline{BC}$, $ \overline{BX} \cong \overline{BX}, \underline{\angle ABX} \cong \underline{\angle CBX}$ and Ax.SAS implies $ \triangle ABX  \cong \triangle CBX$, which implies $\angle AXB = \angle CXB$.
            \bigbreak \noindent 
            \fig{.6}{./figures/g4.png}
            \bigbreak \noindent 
            By thm 14.1, $180 = \overrightarrow{XA}\overrightarrow{XB} + \overrightarrow{XC}\overrightarrow{XB} = \angle AXB + \angle CXB$, so $\angle AXB = \angle CXB = 90$, hence $\overleftrightarrow{AX} \perp m$, and $\overleftrightarrow{AX}$ goes through $A$. \endpf
        \item \textbf{Definition: Pole}: Point $A$ is a \textbf{Pole} of line $m$ if there exists a point $X$ on $m$ such that 
            \begin{align*}
                \overleftrightarrow{AX} \perp m \text{ and } AX = \frac{\omega}{2}
            .\end{align*}
            \bigbreak \noindent 
            \fig{.6}{./figures/g5.png}
            \bigbreak \noindent 
            So, poles will exist only when $\omega < \infty$. Think $m=$ equator, $A=$ north pole.
            \bigbreak \noindent 
            \fig{.6}{./figures/g6.png}
        \item \textbf{Theorem 14.5}: If there are two different lines through a point $A$ and perpendicular to a line $m$, then $A$ is a pole of $m$.
            \bigbreak \noindent 
            \begin{figure}[ht]
                \centering
                \incfig{picture}
                \label{fig:picture}
            \end{figure}
            \bigbreak \noindent 
            \textbf{\textit{Proof.}}  We'll show that $AX_{1} = \frac{\omega}{2}$. Note that $X_{1}X_{2} < \omega$, since $n_{1} \ne n_{2}$
            \bigbreak \noindent 
            $AX_{1} < \omega$ (Since $A$ is not on $m$), so $\overrightarrow{X_{1}A}$, and $ \overrightarrow{X_{1}A}^{\prime} $ exist. Ax.RR implies there is a point $B$ on $\overrightarrow{X_{1}A}^{\prime}$ with $X_{1}B = X_{1}A$
            \bigbreak \noindent 
            \begin{figure}[ht]
                \centering
                \incfig{pic2}
                \label{fig:pic2}
            \end{figure}
            \bigbreak \noindent 
            $\overline{X_{1}A} \cong \overline{X_{1}B}, \overline{X_{1}X_{2}} \cong X_{1}X_{2}, \angle AX_{1}X_{2} = 90 = \angle BX_{1}X_{2}$, thus $\underline{\angle AX_{1}X_{2}} \cong\underline{\angle BX_{1}X_{2}}$. So, by Ax.SAS, $\triangle AX_{1}X_{2} \cong \triangle BX_{1}X_{2}$, which implies $ \angle BX_{2}X_{1} = \angle AX_{2}X_{1} = 90$
            \bigbreak \noindent 
            Thus, $\overrightarrow{X_{2}X_{1}}\overrightarrow{X_{2}B} = \overrightarrow{X_{2}X_{1}}\overrightarrow{X_{2}A}^{\prime}$ with $ \overrightarrow{X_{2}B}^{0}, \overrightarrow{X_{2}A^{\prime}}$ in same halfplane with edge $m$, which implies $\overrightarrow{X_{2}B} = \overrightarrow{X_{2}A}^{\prime} $ (Coroll. 12.3)
            \bigbreak \noindent 
            This implies $\overrightarrow{X_{2}B} \subseteq n_{2}$, $n_{1}, n_{2}$ meet in $A$ and $B $
            \bigbreak \noindent 
            Ax.I4 implies $AB = \omega$. Then, $ A\text{-}X_{1}\text{-}B$ and $AX_{1} = BX_{1}$, which implies $ AX_{1} = BX_{1} = \frac{\omega}{2} $ \endpf
        \item \textbf{Theorem 14.6}: If $A$ is a pole of line $m$, then every line through $A$ is perpendicular to $m$, and meets $m$ at a point distance $\frac{\omega}{2} $ from $A$. Also, every line perpendicular to $m$ goes through $A$
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} Let $X$ be a point on $m$ given by the definition of pole: $\overleftrightarrow{AX} \perp m$ (at $X$), and $AX =\frac{\omega}{2}$
            \bigbreak \noindent 
            Then, $\overleftrightarrow{AX^{*}} \perp m$ at $X^{*}$ by prop 11.14, and $AX^{*} = \omega - AX = \omega - \frac{\omega}{2} = \frac{\omega}{2}$
            \bigbreak \noindent 
            Let $Y$ be any point on $m$, $Y \ne X$ or $X^{*}$. Then, $X,Y,A$ are noncollinear, so $\triangle AXY$ is defined.
            \bigbreak \noindent 
            \fig{.6}{./figures/g7.png}
            \bigbreak \noindent 
            $A^{*}$ is on the opposite side of $m$ from $A$ (Coroll 10.9) and $ A\text{-}X\text{-}A^{*}$ (thm 10.8, 9.1), which implies $\frac{\omega}{2} + XA^{*} = AX + XA^{*} = AA^{*} = \omega$, implies $XA^{*} = \frac{\omega}{2} = XA$; along with $XY = XY$, $ \angle AXY = 90 = \angle A^{*}XY $
            \bigbreak \noindent 
            So, Ax.SAS implies $\triangle AXY \cong  \triangle A^{*}XY$, which implies $\angle AYX = \angle A^{*}YX $
            \bigbreak \noindent 
            Since $\overrightarrow{YA^{*}}  = \overrightarrow{YA}^{\prime}$ (coroll 9.8).
            \bigbreak \noindent 
            Thm 14.1 implies $\overrightarrow{YA}\overrightarrow{YX}$ and $\overrightarrow{YA^{*}YX} $ are supplementary, which implies $\angle AYX + \angle A^{*}YX = 180 $
            \bigbreak \noindent 
            Therefore, $\angle AYX = \angle A^{*}YX = 90$, hence $\overleftrightarrow{AY} \perp m$ at $Y$
            \bigbreak \noindent 
            Also, $ A\text{-}Y\text{-}A^{*}$ implies $ AY + YA^{*} = AA^{*} = \omega$. $\triangle AXY \cong \triangle A^{*}XY$ implies $AY = A^{*}Y$, which implies $AY = A^{*}Y = \frac{\omega}{2} $
            \bigbreak \noindent 
            If $\ell$ is any line perpendicular to $m$, say at point $Z$, we just proved that $ \overleftrightarrow{AZ} \perp m$. By Thm 14.3, $\overleftrightarrow{AZ} = \ell$, so $\ell$ goes through $A$ \endpf
        \item \textbf{Corollary 14.7}: Suppose $\omega < \infty$, each line $m$ has exactly two poles, $A$ and $A^{*}$
        \item \textbf{Definition: \textit{Right triangle}}: A \textbf{right triangle} is a triangle with exactly \textbf{one} right angle.
        \item \textbf{Definition: \textit{Hypotenuse}}: In a right triangle, the \textbf{hypotenuse} is the side opposite the right angle. The \textbf{legs} are the other two sides
        \item \textbf{Definition: \textit{Birectangular triangle}}: A triangle with exactly \textbf{two} right angles is a \textbf{birectangular} (e.g $\triangle ABC$ on $\mathbb{S}$ with $B,C$ on equator, $A = $ north pole). 
        \item \textbf{Definition: \textit{Trirectangular triangle}}: A triangle with three right angles is \textbf{trirectangular}
        \item \textbf{Definition: \textit{small triangle}}: A triangle is \textbf{small} if all sides have length $< \frac{\omega}{2}$. (So when $\omega = \infty$, every triangle is small).
            \bigbreak \noindent 
            If $\triangle ABC$ has more than one right angle (say $\angle B = \angle C = 90$), then $ \overleftrightarrow{AB}, \overleftrightarrow{AC}$ both perpendicular to $ \overleftrightarrow{BC}$, so thm 14.5 implies $A$ is a pole for $\overleftrightarrow{BC}$. Then, Thm 14.6 implies $AB = AC = \frac{\omega}{2}$, which implies $\triangle ABC$ is \textbf{not} small.
    \end{itemize}

    \pagebreak 
    \subsection{The Exterior Angle Inequality and the Triangle Inequality}
    \begin{itemize}
        \item \textbf{Definition: \textit{Cevian}:} A \textbf{Cevian} is a segment from a vertex of a triangle to a point on the opposite side.
            \bigbreak \noindent 
            \fig{.6}{./figures/g8.png}
        \item \textbf{Theorem 15.1 (Cevian theorem)}: Suppose $\omega < \infty$, if $AB < \frac{\omega}{2}$, and $AC \leq \frac{\omega}{2}$ in $\triangle ABC$, and if $ B\text{-}D\text{-}C $ (so $\overline{AD} $ is a cevian of $\triangle ABC$), then $AD < \frac{\omega}{2}$
            \bigbreak \noindent 
            \fig{.6}{./figures/g9.png}
            \bigbreak \noindent 
            \fig{.6}{./figures/g10.png}
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} Ax.RR implies there's a point $X$ on $\overrightarrow{AB}$ with $AX  = \frac{\omega}{2}$. $AX = \frac{\omega}{2} > AB$ implies $ A\text{-}B\text{-}X$. So, $ \overleftrightarrow{AB} = \overleftrightarrow{AX} , \overrightarrow{AB} = \overrightarrow{AX}$
            \bigbreak \noindent 
            Thm. 14.3 implies there's a line $ n \perp \overleftrightarrow{AB} $ at $X$
            \bigbreak \noindent 
            \fig{.6}{./figures/g11.png}
            \bigbreak \noindent 
            $AX =\frac{\omega}{2}$ and $ \overleftrightarrow{AX} \perp n$ implies $A$ is a pole of $n$. $\overleftrightarrow{AC}$ meets $n$ twice, at a pair of antipodes (Thm. 10.11), one of which will be on $\overleftrightarrow{AC}$, the other on $ \overrightarrow{AC}^{\prime}$. So, $\overrightarrow{AC}$ meets $n$ at a point $E$. 
            \bigbreak \noindent 
            Thm 14.6 implies $AE = \frac{\omega}{2}$, so either $ A\text{-}C\text{-}E $ (if $AC < \frac{\omega}{2}$), or $C = E$ (if $AC = \frac{\omega}{2}$).
            \bigbreak \noindent 
            Let $H$ be the halfplane with edge $n$ that contains $A$. Thm 10.3 and $ X\text{-}B\text{-}A $ implies $ B \in H$
            \bigbreak \noindent 
            If $ A\text{-}C\text{-}E $
            \bigbreak \noindent 
            \fig{.6}{./figures/g12.png}
            \bigbreak \noindent 
            Then Thm 10.3 implies $ C \in H$. $H$ convex implies $ \overline{BC} \subseteq H $, so $ B\text{-}D\text{-}C$ implies $ D \in H$.
            \bigbreak \noindent 
            If $C = E$, 
            \bigbreak \noindent 
            \fig{.6}{./figures/g13.png}
            \bigbreak \noindent 
            Then $ B\text{-}D\text{-}E$ and Thm 10.3 implies $ D \in H$ in this case also
            \bigbreak \noindent 
            $ \overleftrightarrow{AD}$ meets $n$ in a pair of antipodes, so $\overleftrightarrow{AD}$ meets $n$ in a point $Y$, and $AY = \frac{\omega}{2}$ by Thm 14.6
            \bigbreak \noindent 
            $D \in H$, $Y \in n$ implies $D \ne Y$. If $ A\text{-}Y\text{-}D$ then Thm 10.6 implies $A,D$ are in opposite halfplanes, which is false.
            \bigbreak \noindent 
            So, $Y \in \overrightarrow{AD}$, which implies we must have $ A\text{-}D\text{-}Y$. Therefore $AD < AY = \frac{\omega}{2} $
            \bigbreak \noindent 
            \fig{.6}{./figures/g14.png}
            \bigbreak \noindent 
            \endpf
        \item \textbf{Definition: \textit{exterior and remote interior angles}:} Given $ \triangle ABC$, and $D$ a point with $ B\text{-}C\text{-}D$, then $ \underline{\angle ACD} $ is called an \textbf{exterior angle} of $\triangle ABC$, and $ \underline{ \angle A}, \underline{ \angle B}$ are called the \textbf{remote interior angles} (relative to $ \underline{\angle ACD} $)
            \bigbreak \noindent 
            \fig{.6}{./figures/g15.png}
        \item \textbf{Theorem 15.3 (EAI)}: An exterior angle of a small triangle has larger measure than either remote interior angle
            \bigbreak \noindent 
            \fig{.6}{./figures/g15.png}
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} We aim to show $ \angle ACD > \angle A,\ \angle ACD > \angle B $
            \bigbreak \noindent 
            $ \overline{AC}$ has midpoint $M$, Ceviain Thm and $BA,BC < \frac{\omega}{2}$ (definition of small triangle) implies $ BM < \frac{\omega}{2} $
            \bigbreak \noindent 
            Then, $2BM < \omega$, so Ax.RR implies there is a point $E$ on ray $\overrightarrow{BM}$ with $BE =  2BM > BM$. So, defn of ray implies $ B\text{-}M\text{-}E$, hence $ME = BM $
            \bigbreak \noindent 
            \fig{.6}{./figures/g16.png}
            \bigbreak \noindent 
            \fig{.6}{./figures/g17.png}
            \bigbreak \noindent 
            $ A\text{-}M\text{-}C, B\text{-}M\text{-}E$ and thm 9.6 implies $ \overrightarrow{MC} = \overrightarrow{MA}^{\prime}, \overrightarrow{ME} = \overrightarrow{MB}^{\prime}$, so $ \underline{\angle AMB}, \underline{\angle CME}$ are vertical. Then, Thm 14.2 implies $ \underline{\angle AMB} \cong \underline{\angle CME} $
            \bigbreak \noindent 
            So, $\overline{MA} \cong \overline{MC}, \ \overline{MB} \cong \overline{ME}$, and AX.SAS implies $ \triangle AMB  \cong \triangle CME$.
            \bigbreak \noindent 
            Hence, $\angle MCE  = \angle MAB = \angle CAB$. Ax.C and $ B\text{-}M\text{-}E$ implies $ \overrightarrow{CB}\text{-}\overrightarrow{CM}\text{-}\overrightarrow{CE} $. $ B\text{-}C\text{-}D $ implies $ \overrightarrow{CD} = \overrightarrow{CB}^{\prime} $, which implies $ \overrightarrow{CB}\text{-}\overrightarrow{CE}\text{-}\overrightarrow{CD} $ (Thm 11.8)
            \bigbreak \noindent 
            Then, Thm 11.5 (ROI for rays) implies $ \overrightarrow{CB}\text{-}\overrightarrow{CM}\text{-}\overrightarrow{CE}\text{-}\overrightarrow{CD}$, which implies $ \overrightarrow{CM}\text{-}\overrightarrow{CE}\text{-} \overrightarrow{CD}$. So, $ \angle ACD = \angle MCD = \angle MCE + \angle ECD > \angle MCE = \angle CAB = \angle A$
            \bigbreak \noindent 
            From point $F$ with $ A\text{-}C\text{-}F$, exterior angle $ \underline{\angle BCF}$ vertical to $ \underline{\angle ACD} $. $N = $ midpoint of $\overline{BC}$, $G \in \overrightarrow{AN}$, $NA = NG $ implies $ \triangle BNA \cong\triangle CNG$, which implies $\angle NCG = \angle NBA = \angle B$. Then, $\angle ACD = \angle BCF = \angle NCG + \angle GCF > \angle NCG = \angle B $
            \bigbreak \noindent 
            \fig{.6}{./figures/g18.png}
        \item \textbf{Corollary 15.4 (needs proof)}: The nonright angles of a small right triangle are accute
        \item \textbf{Corollary 15.5 (needs proof)}: The base angles of an isosceles triangle whose congruent sides are $< \frac{\omega}{2}$ are acute.
        \item \textbf{Triangle inequality informal proof sketch}: In any $ \triangle ABC$ (in any absolute plane),
            \begin{align*}
                AB + BC > AC
            .\end{align*}
            Suppose toward a contradiction that in $ \triangle ABC $, $ AB + BC < AC$ (we'll worry later about contradicting $ AB + BC = AC $)
            \bigbreak \noindent 
            Then, there's a point $X$ in $ \overline{AC}$ with $AX = AB$, and a point $ Y$ in $ \overline{XC}$ with $YC = BC$
            \bigbreak \noindent 
            \fig{.6}{./figures/g19.png}
            \bigbreak \noindent 
            Pons asinorum for $ \triangle ABX$ implies $ \angle ABX = \angle BXA (\alpha)$, and $ \triangle BYC$ implies $ \angle CBY = \angle CYB (\beta) $
            \bigbreak \noindent 
            \fig{.6}{./figures/g20.png}
            \bigbreak \noindent 
            If the EAI applies to $ \triangle BXY$, then 
            \begin{align*}
                \beta = \angle BYC > \angle BXY = 180 - \alpha
            .\end{align*}
            Which, implies $ \alpha + \beta > 180$. But, at $B$, $ \alpha + \beta < \angle ABC < 180$, a contradiction
        \item \textbf{Proposition 15.6}: If $AB < \frac{\omega}{2}$, and $ BC \leq \frac{\omega}{2}$ in $ \triangle ABC$, then $AB + BC > AC$
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} Suppose toward a contradiction that $AB + BC \leq AC$. Then, $0 < BC \leq AC - AB$. So, $AB < AC$
            \bigbreak \noindent 
            By Ax.RR, there is a point $X$ on $\overrightarrow{AC}$ with $AX = AB$, hence $ A\text{-}X\text{-}C $
            \bigbreak \noindent 
            \fig{.6}{./figures/g21.png}
            \bigbreak \noindent 
            Pons asinorum for $ \triangle ABX $ implies $ \angle ABX = \angle BXA$ ($\alpha $).
            \bigbreak \noindent 
            $AX + BC = AC$ implies $ XC = AC - AX = AC - AB \geq BC$ 
            \bigbreak \noindent 
            \textbf{Case 1)} Suppose that $XC = BC$. Pons asinorum for $ \triangle BCX$ implies $ \angle XBC  = \angle BXC$ ($\beta$)
            \bigbreak \noindent 
            \fig{.6}{./figures/g22.png}
            \bigbreak \noindent 
            $ A\text{-}X\text{-}C$ and Ax.C implies $ \overrightarrow{BA}\text{-}\overrightarrow{BX}\text{-}\overrightarrow{BC} $, which implies $ \angle ABC = \alpha + \beta = \angle AXB + \angle BXC = 180$ (Thm 14.1)
            \bigbreak \noindent 
            But, $A,B,C$ not collinear, implies $ \underline{\angle ABC}$ is proper, which implies $ \angle ABC \ne 180$ by Ax. M4, a contradiction.
            \bigbreak \noindent 
            \textbf{Case 2)} Suppose that $ XC > BC$. Ax.RR for $\overrightarrow{CX}$ implies there is a point $Y$ on $\overrightarrow{CX}$ with $CY = BC$. Then, $CY < CX$ implies $ C\text{-}Y\text{-} X$ by definition of $\overrightarrow{CX}$, which implies $  X\text{-}Y\text{-}C$. 
            \bigbreak \noindent 
            Let $M$ be the midpoint of $\overline{XY}$. $BA < \frac{\omega}{2}, BC \leq \frac{\omega}{2}$ (by hypothesis) so Theorem 15.1 implies $ BX,BM,BY < \frac{\omega}{2}$
            \bigbreak \noindent 
            \fig{.6}{./figures/g23.png}
            \bigbreak \noindent 
            $XM = MY = \frac{1}{2}XY < \frac{\omega}{2}$, so $ \triangle BMX, \triangle BMY$ are small.
            \bigbreak \noindent 
            Pons asinorum for $\triangle BYC$ implies $ \angle CBY  = \angle CYB$ ($\beta$). EAI for $\triangle BMY$ implies $ \beta = \angle BYC > \angle BMY$. EAI for $ \triangle BMX \implies \alpha = \angle BXA > \angle BMX $
            \bigbreak \noindent 
            So, $ \alpha + \beta > \angle BMX + \angle BMY = 180 $ (Thm 14.1), but $180 > \angle ABC$ (Ax.M1, M4). We have
            \begin{align*}
                180 &> \angle ABC \\
                    &= \angle ABX + \angle XBC \\
                    &= \angle ABX + \angle XBY + \angle YBC \\
                    &= \alpha + \angle XBY + \beta > \alpha +\beta
            .\end{align*}
            A contradiction, so $ AB  + BC \leq AC$ is false, therefore $AB + BC > AC $
        \item \textbf{Theorem 15.7 (The triangle inequality)}: In any $\triangle ABC$, 
            \begin{align*}
                AB + BC > AC
            .\end{align*}
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} If $AB \geq \frac{\omega}{2}$ and $ BC   \geq \frac{\omega}{2}$, then $AB + BC \geq \omega > AC$. So, we may assume that one of $AB,BC$, say $AB$ is less than $\frac{\omega}{2}$.
            \bigbreak \noindent 
            If $ BC \leq \frac{\omega}{2}$, then $ AB < \frac{omega}{2} $ and prop 15.6 implies $ AB + BC  > AC $. So, we may assume that $BC > \frac{\omega}{2}$and $ AB < \frac{\omega}{2}$. In particular, $\omega < \infty$
            \bigbreak \noindent 
            If $AC \leq \frac{\omega}{2}$, then $AB + BC > BC > \frac{omega}{2} \geq AC$ and we're done. So, we may also assume that $AC > \frac{\omega}{2}$. We now have $AB < \frac{omega}{2} $, $BC > \frac{omega}{2}, AC > \frac{\omega}{2}$
            \bigbreak \noindent 
            $A,B,C^{*}$ are not collinear, since if $ C^{*}$ is on line $ \overleftrightarrow{AB} $, then so is $C$ by Thm. 10.8, a contradiction. So, $\triangle ABC^{*} $ is defined
            \bigbreak \noindent 
            Now, $BC^{*} = \omega - BC$ and $AC^{*} = \omega - AC$. $AC > \frac{\omega}{2} $ implies $AC^{*} < \omega - \frac{\omega}{2} = \frac{\omega}{2} $
            \bigbreak \noindent 
            So, in $\triangle ABC^{*}$, we have $BA < \frac{\omega}{2}$, and $AC^{*} < \frac{\omega}{2}$. Then, prop 15.6 implies that $BA + AC^{*} > BC^{*}$
            \bigbreak \noindent 
            \fig{.6}{./figures/g24.png}
            \bigbreak \noindent 
            So, $BA + (\omega - AC) > (\omega -BC) $ implies $AB + BC > AC$ \endpf
        \item \textbf{Corollary 15.8}: For any points $A,B,C$, $AB + BC \geq AC$. 
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} If $A,B,C $ are not collinear, Thm 15.7 implies $AB + BC > AC$. If $A,B,C$ are collinear and distinct, Thm 7.3 implies $AB + BC \geq AC$
            \bigbreak \noindent 
            If $B = A$ or $C$, then $ AB + BC = AC$. If $A = C$, then $AB + BC \geq 0 = AC$. So, the inequality holds in every case.
        \item \textbf{Theorem 16.1 (Comparison theorem)}: If one angle of a triangle is larger than a second, then the side opposite the lager angle is longer than the side opposite the smaller angle; and conversely. 
            \bigbreak \noindent 
            That is, in $\triangle ABC$, 
            \begin{align*}
                \angle B > \angle C \iff AC > AB
            .\end{align*}
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} Assume $\angle B > \angle C$. Ax.RF implies there is a ray $j$ in Fan $\overrightarrow{\overrightarrow{BC}\overrightarrow{BA}} $ so that $\overrightarrow{BC}j = \angle C$. Then, $\overrightarrow{BC}j < \angle B = \angle CBA = \overrightarrow{BC}\overrightarrow{BA}$, which implies $ \overrightarrow{BC}\text{-}j\text{-}\overrightarrow{BA}$
            \bigbreak \noindent 
            $\underline{\overrightarrow{BC}\overrightarrow{BA}}$ is proper, since $A,B,C$ are noncollinear, so the crossbar theorem may be applied.
            \bigbreak \noindent 
            $j^{0}$ meets $\overline{AC}^{0}$ at a point $D$, so $ A\text{-}D\text{-}C$ and $ j = \overrightarrow{BD} $
            \bigbreak \noindent 
            Now, $\angle CBD = \overrightarrow{BC}j = \angle ACB = \angle DCB$ (prop 11.14), so pons asinorum for $ \triangle DBC $ implies $ CD = BD$. Triangle inequality for $ \triangle ADB $ implies 
            \begin{align*}
                AB &< AD + BD \\
                   &= AD + CD = AC
            .\end{align*}
            Therefore, $ AC > AB$
            \bigbreak \noindent 
            If $ \angle C  > \angle B$, then the same argument, reversing the notation $ B \leftrightarrow C$ implies $ AB > AC$.
            \bigbreak \noindent 
            If $ \angle B = \angle C$, then pons asinorum implies $ AC = AB $. Thus,
            \begin{align*}
                \angle B > \angle C \iff AC > AB
            .\end{align*}
        \item \textbf{Corollary 16.2 (Needs proof)}: The hypotenuse of a small right triangle is its longest side
        \item \textbf{Theorem 16.3}: Suppose that in $\triangle ABC$, $ \angle C = 90$ and $AC  < \frac{\omega}{2} $. Then, $\underline{\angle B}$ is acute and $AB  > AC$.
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} If we can show that $ AB > AC$, then the Comparison Theorem will imply that $\angle C > \angle B$, so $90 = \angle C > \angle B$, hence $\underline{\angle B} $ will be acute. Thus, we aim to show that $ AB > AC $
            \bigbreak \noindent 
            By hypothesis, $ AC < \frac{\omega}{2}$. So, if $ AB \geq \frac{\omega}{2}$, then $ AB > AC$ and we're done. So, we may assume that $AB < \frac{\omega}{2} $
            \bigbreak \noindent 
            Let $M$ be the midpoint of $\overline{BC}$. Then, $BM = CM = \frac{1}{2}BC < \frac{\omega}{2} $. $AM < \frac{\omega}{2}$ by the Cevian Theorem (15.1), so $\triangle ABM$ and $ \triangle ACM$ are both small.
            \bigbreak \noindent 
            In particular, $ \triangle ACM$ is a small right triangle. Coroll. 16.2 implies $AM > AC$ and $ 90 = \angle ACM > \angle AMC$
            \bigbreak \noindent 
            EAI (15.3) for $\triangle AMB$ implies $ \angle AMC > \angle ABM$, so 
            \begin{align*}
                \angle ACM &> \angle ABM \\
                \implies \angle ACB &> \angle ABC
            .\end{align*}
            So, the Comparison theorem 16.1 implies $AB > AC $
        \item \textbf{Definition:} for any line $m$ and point $A$, the \textbf{distance between $A$ and $m$}, denoted $d(A,m)$, is the minimum distance $AX$ for all points $X$ on $m$.
            \bigbreak \noindent 
            \textbf{Note:} If $A$ is on $m$, then $d(A,m) = AA = 0$
        \item \textbf{Theorem 16.8}: Let $m$ be a line, $C \in m$, $A \not\in m$, $\overleftrightarrow{AC} \perp m$
            \begin{enumerate}[label=(\alph*)]
                \item If $AC < \frac{\omega}{2}$ then $d(A,m) = AC$; and $AC < AX$, all $X \ne C$ on $m$
                \item If $AC = \frac{\omega}{2}$ (so $\omega<\infty$), then $d(A,m) = \frac{\omega}{2} = AX$, all $X \in m$
                \item If $AC > \frac{\omega}{2}$ (so $\omega<\infty$), then $d(A,m)  = \omega - AC = AC^{*}$; and $AC^{*} < AX$, all $X \ne C^{*}$ on $m$
            \end{enumerate}
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} 
            \bigbreak \noindent 
            (a) Suppose $\overleftrightarrow{AC} \perp m$ and $AC < \frac{\omega}{2}$. If $ X \in m$ with $X \ne C$ or $C^{*}$, then $A,C,X$ are not collinear, so $\triangle ACX$ exists. Then, Thm. 16.3 implies $AC < AX$. If $X = C^{*}$, then $AX = AC^{*} = \omega - AC > \frac{\omega}{2} $ , since $AC < \frac{\omega}{2} $ so again $AC < AX$
            \bigbreak \noindent 
            (b) Suppose $\overleftrightarrow{AC} \perp m$ and $ AC = \frac{\omega}{2} $. Then $A$ is a pole of $m$, so Thm 14.6 implies $AX = \frac{\omega}{2}$ for all $X$ on $m$
            \bigbreak \noindent 
            (c) Suppose $\overleftrightarrow{AC} \perp m$ and $AC > \frac{\omega}{2}$. Then, $AC^{*} = \omega - AC < \frac{\omega}{2}$. $C^{*}$ is on $m$ (Thm 10.8), and $\overleftrightarrow{AC^{*}} \perp m$ at $C^{*}$ (prop 11.14), so part (a) implies $AC^{*} < AX$ for all $X \ne C^{*}$ on $m$
    \end{itemize}

    \pagebreak 
    \subsection{Extra}
    \begin{itemize}
        \item \textbf{Definition: \textit{parallel lines}}: Two lines $m\ne n$ are called \textbf{parallel} if $m \cap n = \varnothing$. If so, we write $m \parallel n $
            \bigbreak \noindent 
            Suppose a point $P$ is not on a line $m$. There are exactly three mutually exclusive possibilities
            \begin{enumerate}[label=(\roman*)]
                \item There is no line through $P$ parallel to $m$ 
                \item There is exactly one line through $P$ parallel to $m$
                \item There are at least two lines through $P$ parallel to $m$
            \end{enumerate}
        \item \textbf{Definition} An absolute plane in which $(i)$ holds for every line $m$ and point $P$ not on $m$ (ie no parallel lines) is called \textbf{spherical}
            \bigbreak \noindent 
            An absolute plane in which (ii) holds for every line $m$ and point $P$ not on $m$ is called \textbf{Euclidean}
            \bigbreak \noindent 
            An absolute plane in which (iii) holds for every line $m$ and point $P$ not on $m$ is called \textbf{Hyperbolic}
            \bigbreak \noindent 
            These properties do not mix, only one must hold per absolute plane. 
        \item \textbf{Theorem}: In any absolute plane $\mathbb{P}$, exactly one of the following must occur.
            \begin{enumerate}[label=(\roman*)]
                \item $\mathbb{P}$ is Spherical; $\sigma(ABC) = \angle A + \angle B + \angle C > 180$ for all $\triangle ABC$, and $\omega < \infty $ 
                \item $\mathbb{P}$ is Euclidean; $\sigma(ABC) = \angle A + \angle B + \angle C = 180$ for all $\triangle ABC$, and $\omega = \infty $ 
                \item $\mathbb{P}$ is Hyperbolic; $\sigma(ABC) = \angle A + \angle B + \angle C < 180$ for all $\triangle ABC$, and $\omega = \infty $ 
            \end{enumerate}
            \bigbreak \noindent 
            $\mathbb{S},\ \mathbb{E}$, and $\mathbb{H} $ are essentially the only examples, up to isometry.
        \item \textbf{Theorem (\textit{AAA})}: Suppose that $\triangle ABC$ and $\triangle XYZ$ are triangles in a non-Euclidean absolute plane with $\angle A = \angle X$, $ \angle B = \angle Y$, and $ \angle C = \angle Z$. Then, $\triangle ABC \cong \triangle XYZ$
    \end{itemize}


    \pagebreak 
    \unsect{Numerical Linear Algebra}
    \bigbreak \noindent 
    \subsection{Introduction}
    \bigbreak \noindent 
    \begin{itemize}
        \item \textbf{Matrix Notation}: For a matrix $A \in \mathbb{R}^{m\times n} $, we say
            \begin{align*}
                A = (a_{ij}) = \begin{bmatrix}
                    a_{11} & a_{12} & \cdots & a_{1n} \\
                    a_{21} & a_{22} & \cdots & a_{2n} \\
                    \vdots & \vdots & \ddots & \vdots \\
                    a_{m1} & a_{m2} & \cdots & a_{mn}
                \end{bmatrix}
            \end{align*}
            with $a_{ij} \in \mathbb{R} $.
        \item \textbf{Vector notation}: For a vector $x \in \mathbb{R}^{n} $ (or $\mathbb{R}^{n\times 1} $), we have
            \begin{align*}
                x = \begin{bmatrix}
                    x_{1} \\ x_{2} \\ \vdots \\ x_{n}
                \end{bmatrix}
            \end{align*}
            for $x_{i} \in \mathbb{R} $.

        \item \textbf{Submatrix notation (rows)}: 
            \[
                A(i,:) \in \mathbb{R}^{1 \times n} \;\;\Longleftrightarrow\;\;
                A(i,:) = \bigl[\, a_{i1} \; a_{i2} \; \cdots \; a_{in} \,\bigr].
            \]
        \item \textbf{Submatrix notation (columns)}: 
            \[
                A(:,j) \in \mathbb{R}^{m \times 1} \;\;\Longleftrightarrow\;\;
                A(:,j) =
                \begin{bmatrix}
                    a_{1j} \\
                    a_{2j} \\
                    \vdots \\
                    a_{mj}
                \end{bmatrix}.
            \]
        \item \textbf{Sparse Matrix}: A sparse matrix or sparse array is a matrix in which most of the elements are zero. There is no strict definition regarding the proportion of zero-value elements for a matrix to qualify as sparse but a common criterion is that the number of non-zero elements is roughly equal to the number of rows or columns.
        \item \textbf{Dense Matrix}: if most of the elements are non-zero, the matrix is considered dense
        \item \textbf{Sparsity}: The number of zero-valued elements divided by the total number of elements is sometimes referred to as the sparsity of the matrix.
        \item \textbf{Band Matrix}: a band matrix or banded matrix is a sparse matrix whose non-zero entries are confined to a diagonal band, comprising the main diagonal and zero or more diagonals on either side.
            \bigbreak \noindent 
            \[
                A(i_{1}:i_{2},:) \in \mathbb{R}^{(i_{2}-i_{1}+1) \times n}
                \;\;\Longleftrightarrow\;\;
                A(i_{1}:i_{2},:) =
                \begin{bmatrix}
                    a_{i_{1}1} & a_{i_{1}2} & \cdots & a_{i_{1}n} \\
                    a_{i_{1}+1,1} & a_{i_{1}+1,2} & \cdots & a_{i_{1}+1,n} \\
                    \vdots & \vdots & \ddots & \vdots \\
                    a_{i_{2}1} & a_{i_{2}2} & \cdots & a_{i_{2}n}
                \end{bmatrix}.
            \]

            \[
                A(:,j_{1}:j_{2}) \in \mathbb{R}^{m \times (j_{2}-j_{1}+1)}
                \;\;\Longleftrightarrow\;\;
                A(:,j_{1}:j_{2}) =
                \begin{bmatrix}
                    a_{1j_{1}} & a_{1,j_{1}+1} & \cdots & a_{1j_{2}} \\
                    a_{2j_{1}} & a_{2,j_{1}+1} & \cdots & a_{2j_{2}} \\
                    \vdots & \vdots & \ddots & \vdots \\
                    a_{mj_{1}} & a_{m,j_{1}+1} & \cdots & a_{mj_{2}}
                \end{bmatrix}.
            \]
            Where
            \[
                \begin{aligned}
                    A(i_{1}:i_{2},:) &:\;\; \text{all rows between } i_{1} \text{ and } i_{2}, \;\text{across all columns}, \\[6pt]
                    A(:,j_{1}:j_{2}) &:\;\; \text{all columns between } j_{1} \text{ and } j_{2}, \;\text{across all rows}.
                \end{aligned}
            \]
        \item \textbf{Transposition}: $\mathbb{R}^{m\times n} \to \mathbb{R}^{n\times m} $
            \begin{align*}
                C = A^{\top} \iff c_{ij} = a_{ji}
            .\end{align*}
        \item \textbf{Addition} $\quad (\mathbb{R}^{m \times n} \times \mathbb{R}^{m \times n} \;\to\; \mathbb{R}^{m \times n})$
            \[
                C = A + B 
                \;\;\Longrightarrow\;\; 
                c_{ij} = a_{ij} + b_{ij}.
            \]

        \item \textbf{Scalar-matrix Multiplication: } $\quad (\mathbb{R} \times \mathbb{R}^{m \times n} \;\to\; \mathbb{R}^{m \times n})$
            \[
                C = \alpha A 
                \;\;\Longrightarrow\;\; 
                c_{ij} = \alpha a_{ij}.
            \]

        \item \textbf{Matrix-matrix Multiplication: } $\quad (\mathbb{R}^{m \times p} \times \mathbb{R}^{p \times n} \;\to\; \mathbb{R}^{m \times n})$
            \[
                C = AB 
                \;\;\Longrightarrow\;\; 
                c_{ij} = \sum_{k=1}^{p} a_{ik} b_{kj}.
            \]
        \item \textbf{Matrix-vector Multiplication: } $\quad (\mathbb{R}^{m \times n} \times \mathbb{R}^n \;\to\; \mathbb{R}^m)$
            \[
                y = Ax 
                \;\;\Longrightarrow\;\; 
                y_i = \sum_{j=1}^{n} a_{ij} x_j.
            \]

        \item \textbf{Inner product (or dot product): } $\quad (\mathbb{R}^n \times \mathbb{R}^n \;\to\; \mathbb{R})$
            \[
                c = x^T y 
                \;\;\Longrightarrow\;\; 
                c = \sum_{i=1}^{n} x_i y_i.
            \]

            \item \textbf{Outer product: } $\quad (\mathbb{R}^m \times \mathbb{R}^n \;\to\; \mathbb{R}^{m \times n})$
            \[
                C = x y^T 
                \;\;\Longrightarrow\;\; 
                c_{ij} = x_i y_j.
            \]
        \item \textbf{Flops}: A flop is a floating-point operation between numbers stored in a floating-point format on a computer.
            \bigbreak \noindent 
            If $x$ and $y$ are numbers stored in a floating point format, then the following operations are each one flop
            \begin{align*}
                x + y \quad x - y \quad xy \quad x / y
            .\end{align*}

        \item \textbf{Empty sum}: In standard mathematical convention, if the lower bound exceeds the upper bound, the sum is defined to be zero
            \begin{align*}
                \sum_{i=k}^{j} f(k) = 0 \quad \text{if } i > j
            \end{align*}

        \item \textbf{Conformable matrix}: Simply a matrix that has the right dimensions to partcipate
        \item \textbf{Relationship between the sign of a 2-degree polynomial and its discriminant}: Recall that for a degree two polynomial, $p(x) \in P_{2}$, $p(x) = ax^{2} + bx +c$, the discriminant is given by
            \begin{align*}
                D &= b^{2} - 4ac
            .\end{align*}
            \begin{itemize}
                \item $p(x) \geq 0$ for all $x$ if and only if $a > 0 $ $D \leq 0$
                \item $p(x) \leq 0$ for all $x$ if and only if $a < 0 $ $D \leq 0$
            \end{itemize}





    \end{itemize}



    \pagebreak \bigbreak \noindent 
    \subsection{Gaussian Elimination and its variants (1)}
    \bigbreak \noindent 
    \subsubsection{Matrix Multiplication}
    \bigbreak \noindent 
    \begin{itemize}
        \item \textbf{Matrix Multiplication}:        
            In general, if $A$ is a real matrix with $m$ rows and $n$ columns, and $x$ is a real vector with $n$ entries, then
            \[
                A = 
                \begin{bmatrix}
                    a_{11} & \cdots & a_{1n} \\
                    \vdots & \ddots & \vdots \\
                    a_{m1} & \cdots & a_{mn}
                \end{bmatrix}
                \in \mathbb{R}^{m \times n}
                \quad \text{and} \quad
                x =
                \begin{bmatrix}
                    x_{1} \\
                    \vdots \\
                    x_{n}
                \end{bmatrix}
                \in \mathbb{R}^n.
            \]
            If $b = Ax$, then $b \in \mathbb{R}^m$ and
            \[
                b_i = \sum_{j=1}^n a_{ij} x_j
                = a_{i1}x_1 + \cdots + a_{in}x_n, 
                \quad i = 1, \ldots, m.
            \]
            Thus, $b_i$ is the \textbf{inner-product} between the $i$-row of $A$, 
            \[
                A(i,:) = [a_{i1} \;\; \cdots \;\; a_{in}], \quad (i = 1, \ldots, m)
            \]
            and the vector $x$.
            \bigbreak \noindent 
            Also,
            \[
                b = A(:,1) x_1 + \cdots + A(:,n)x_n,
            \]
            so $b$ is a \textbf{linear combination} of the columns of $A$, i.e.,
            \[
                A(:,j) = 
                \begin{bmatrix}
                    a_{1j} \\
                    a_{2j} \\
                    \vdots \\
                    a_{mj}
                \end{bmatrix},
                \quad j = 1, \ldots, n.
            \]
        \item \textbf{Matrix-Matrix Multiplication}:
            Let $A \in \mathbb{R}^{m \times n}$ and $X \in \mathbb{R}^{n \times p}$.
            \bigbreak \noindent 
            If $B = AX$ then $B \in \mathbb{R}^{m \times p}$ and
            \[
                b_{ij} = \sum_{k=1}^n a_{ik} x_{kj}, \quad i = 1, \ldots, m, \;\; j = 1, \ldots, p.
            \]
            That is, $b_{ij}$ is the inner-product between row $i$ of $A$ and column $j$ of $X$.
            \bigbreak \noindent 
            Also, each column of $B$ is a linear combination of the columns of $A$.
            \bigbreak \noindent 
            Total flops required for matrix multiplication is
            \[
                \sum_{i=1}^m \sum_{j=1}^p \sum_{k=1}^n 2 = 2mnp.
            \]
            If $A, X \in \mathbb{R}^{n \times n}$, then computing $B = AX$ requires $2n^3 = O(n^3)$ flops.
            \bigbreak \noindent 
            We can see this by describing the algorithm for Matrix-Matrix multiplication
            \bigbreak \noindent 
            \begin{jlcode}
            for i = 1:m
                for j = 1:n
                    for k = 1:p
                        C[i,j] += A[i,k]B[k,j]
                    end
                end
            end
            \end{jlcode}
            \bigbreak \noindent 
            The multiplication $A[i,j]B[k,j]$ is one flop, followed by the addition. Therefore, two flops per iteration of the innermost loop.
        \item \textbf{Block Matrices}:
            Partition $A \in \mathbb{R}^{m \times n}$ and $X \in \mathbb{R}^{n \times p}$ into blocks:
            \[
                A =
                \begin{blockarray}{ccc}
   & n_1 & n_2 \\
   \begin{block}{c[cc]}
       m_1 & A_{11} & A_{12} \\
       m_2 & A_{21} & A_{22} \\
   \end{block}
                \end{blockarray}
                \quad,\qquad
                X =
                \begin{blockarray}{ccc}
   & p_1 & p_2 \\
   \begin{block}{c[cc]}
       n_1 & X_{11} & X_{12} \\
       n_2 & X_{21} & X_{22} \\
   \end{block}
                \end{blockarray}
            \]
                    where $n = n_1 + n_2$, $m = m_1 + m_2$, and $p = p_1 + p_2$.
                    \bigbreak \noindent 
                    If $B = AX$, and 
                    \begin{align*}
                        B = 
                        \begin{blockarray}{ccc}
                            & p_{1} & p_{2} \\
                            \begin{block}{c[cc]}
                                m_{1} & B_{11} & B_{12} \\
                                m_{2} & B_{21} & B_{22} \\
                            \end{block}
                        \end{blockarray}
                    ,\end{align*}
                    then
                    \[
                        \begin{blockarray}{cc}
  &  \\
  \begin{block}{[cc]}
      B_{11} & B_{12} \\
      B_{21} & B_{22} \\
  \end{block}
                        \end{blockarray}
                        =
                        B = AX =
                        \begin{blockarray}{cc}
  &  \\
  \begin{block}{[cc]}
      A_{11} & A_{12} \\
      A_{21} & A_{22} \\
  \end{block}
                        \end{blockarray}
                        \begin{blockarray}{cc}
  &  \\
  \begin{block}{[cc]}
      X_{11} & X_{12} \\
      X_{21} & X_{22} \\
  \end{block}
                        \end{blockarray}
                    \]

                    \[
                        =
                        \begin{blockarray}{cc}
  &  \\
  \begin{block}{[cc]}
      A_{11}X_{11} + A_{12}X_{21} & A_{11}X_{12} + A_{12}X_{22} \\
      A_{21}X_{11} + A_{22}X_{21} & A_{21}X_{12} + A_{22}X_{22} \\
  \end{block}
                        \end{blockarray}
                    \]

                    That is,
                    \[
                        B_{ij} = \sum_{k=1}^2 A_{ik} X_{kj}, 
                        \qquad i,j = 1,2.
                    \]

                \item \textbf{Transpose of block matrices}: Let $A \in \mathbb{R}^{n\times n}$, with
                    \begin{align*}
                        A  = \begin{bmatrix} A_{11} & a_{12} \\ A_{21} & a_{22} \end{bmatrix}
                    .\end{align*}
                    Then, 
                    \begin{align*}
                        A^{\top} = \begin{bmatrix}
                            A_{11}^{\top} & A_{21}^{\top} \\ A_{12}^{\top} & A_{22}^{\top}
                        \end{bmatrix}
                    \end{align*}
                \item \textbf{Transpose of a block vector}: Similarly, if $ x \in \mathbb{R}^{n}$ is decomposed into blocks
                    \begin{align*}
                        \begin{pmatrix} x_{1} \\ x_{2} \end{pmatrix}
                    ,\end{align*}
                    with $x_{1} \in \mathbb{R}^{n_{1}} ,\; x_{2} \in \mathbb{R}^{n_{2}},\; n = n_{1} + n_{2}$, then
                    \begin{align*}
                        x^{\top} = \begin{pmatrix} x_{1}^{\top} & x_{2}^{\top} \end{pmatrix}
                    \end{align*}



    \end{itemize}

    \pagebreak 
    \subsubsection{Systems of Linear Equations}
    \begin{itemize}
        \item \textbf{Systems of linear equations}: Let $A \in \mathbb{R}^{n\times n},\ b \in \mathbb{R}^{m}$, our goal is to find $x \in \mathbb{R}^{m}$ such that $Ax = b$
        \item \textbf{Singularity}: A \textbf{singular matrix} is a square matrix that does not have an inverse.
            \bigbreak \noindent 
            A \textbf{nonsingular} matrix is a square matrix that does have an inverse.
            \bigbreak \noindent 
            The following are equivalent, if any one holds, they all hold
            \begin{itemize}
                \item $Ax = b$ has a unique solution
                \item $\det(A)\ne 0$
                \item $A^{-1}$ exists
                \item There is no nonzero vector $y \in \mathbb{R}^{m}$ such that $Ay=0 $
                \item The columns of $A$ are linearly independent
                \item The rows of $A$ are linearly independent
                \item Given any vector $b$, there is exactly one vector $x$ such that $Ax=b$
            \end{itemize}
            If any one of the following are true, they all are true, and $A$ is non-singular
        \item \textbf{Solution to $Ax = b$}: If $A$ is nonsingular, then $A^{-1}$ exists, and
            \begin{align*}
                x = A^{-1}b
            .\end{align*}
            Which is the unique solution to $Ax=b$
            \bigbreak \noindent 
            \textbf{Note:} Practically, it is not wise to compute $A^{-1}$, as this can be expensive.
    \end{itemize}

    \pagebreak 
    \subsubsection{Triangular systems}
    \begin{itemize}
        \item \textbf{Upper triangular}: A square matrix $A = U$ of the form 
            \begin{align*}
                A = U = \begin{bmatrix}
                    u_{11} & u_{12} & \cdots & u_{1n} \\
                    0 & u_{22} & \cdots & u_{2n} \\
                    \vdots & \vdots & \ddots & \vdots \\
                    0 & 0 & \cdots & u_{nn}
                \end{bmatrix}
            \end{align*}
            is called \textbf{upper triangular}.
            \bigbreak \noindent 
            Formally, a matrix $A$ is upper triangular if $a_{ij} = 0 $ whenever $i > j $
        \item \textbf{Lower triangular}: A square matrix $A = L$
            \begin{align*}
                A = L = \begin{bmatrix}
                    \ell_{11} & 0 & \cdots & 0 \\
                    \ell_{21} & \ell_{22} & \cdots & 0 \\
                    \vdots & \vdots & \ddots & \vdots \\
                    \ell_{n1} & \ell_{n2} & \cdots & \ell_{nn}
                \end{bmatrix}
            \end{align*}
            is called \textbf{lower triangular}
            \bigbreak \noindent 
            Formally, a matrix $A$ is lower triangular if $a_{ij} = 0 $ whenever $i < j $
        \item \textbf{Theorem 1.3.1}: Let $A$ be a triangular matrix. Then, $A$ is nonsingular if and only if $g_{ii} \ne 0$ for $i=1,2,...,n $
        \item \textbf{Solutions to triangular systems}: 
            Consider the system
            \begin{align*}
               \begin{bmatrix}
                    \ell_{11} & 0 & \cdots & 0 \\
                    \ell_{21} & \ell_{22} & \cdots & 0 \\
                    \vdots & \vdots & \ddots & \vdots \\
                    \ell_{n1} & \ell_{n2} & \cdots & \ell_{nn}
                \end{bmatrix} \begin{bmatrix}
                    x_{1} \\ x_{2} \\ \vdots \\ x_{n}
                \end{bmatrix}
                =
                \begin{bmatrix}
                    b_{1} \\ b_{2} \\ \vdots \\ b_{n}
                \end{bmatrix}
            .\end{align*}
            So,
            \begin{align*}
                \ell_{11}x_{1} &= b_{1} \\
                \ell_{21}x_{1} + \ell_{22}x_{2} &= b_{2} \\
                \vdots \\
                \ell_{n1}x_{1} + \ell_{n2}x_{2} + \cdots + \ell_{nn}x_{n} &= b_{n}
            .\end{align*}
            Then,
            \begin{align*}
                x_{1} &= \frac{b_{1}}{\ell_{11}}
            \end{align*}
            and,
            \begin{align*}
                \ell_{22}x_{2} &= b_{2} - \ell_{21}x_{1} \\
                \implies x_{2} &= \frac{b_{2}-\ell_{21}x_{1}}{\ell_{22}}
            .\end{align*}
            In general, we have
            \begin{align*}
                x_{i} = \frac{b_{i} - \sum_{j=1}^{i-1}\ell_{ij}x_{j}}{\ell_{ii}}
            \end{align*}
            for $i =1,2,...,n $. This method is called \textbf{Forward Substitution}.
            \bigbreak \noindent 
            A similar process is used on upper triangular matrices and is called \textbf{Backward Substitution}.
        \item \textbf{Counting flops for the forward substitution method}: We have 
                \begin{jlcode}
                for i = 1:n
                    for j=1:i-1
                        b[i] = b[i] - ell[i,j]b[j]
                    end
                    b[i] = b[i] / ell[i,i]
                end
                \end{jlcode}
            Thus, the count of flops is
            \begin{align*}
                n+\sum_{i=1}^{n}2(i-1) &= n+2 \sum_{i=1}^{n}(i-1) = n+2 \left( \sum_{i=1}^{n}i - \sum_{i=1}^{n} 1\right)  \\
                   &= n + 2 \left(\sum_{i=1}^{n}i - n\right) = n+ 2 \left(\frac{n(n+1)}{2}-n\right) \\
                   &=n + n^{2} - n = n^{2}
            \end{align*}
            So, forward substitution is $\mathcal{O}(n^{2})$
        \item \textbf{Column oriented forward substitution}: Suppose we have $Lx = b$ when $L$ is lower triangular, we split the matrix into the following blocks
            \begin{align*}
                \begin{bmatrix}
                    \ell_{11} & 0 \\
                    \hat{\ell} & \hat{L}
                \end{bmatrix}
                \begin{bmatrix}
                    x_{1} \\ \hat{x} 
                \end{bmatrix}
                = \begin{bmatrix}
                    b_{1} \\ \hat{b}
                \end{bmatrix}
            .\end{align*}
            With $\hat{\ell} \in \mathbb{R}^{n-1}$, $\hat{L} \in \mathbb{R}^{n-1 \times n-1} $, $\hat{x} \in \mathbb{R}^{n-1}$, $\ell_{11}, x_{1}, b_{1} \in \mathbb{R}$. Note that $\hat{L}$ is also lower triangular.
            \bigbreak \noindent 
            We have
            \begin{align*}
                \ell_{11}x_{1} &= b_{1} \implies x_{1} = \frac{b_{1}}{\ell_{11}} \\
                \hat{\ell}x_{1} + \hat{L}\hat{x} &= \hat{b} \implies \hat{L}\hat{x} = \hat{b} - \hat{\ell}x_{1}
            \end{align*}
            Thus, we reduced the dimension by one. We repeat this process for the remaining $x_{i}$. The process is 
            \begin{enumerate}
                \item Compute $x_{1} = \frac{b_{1}}{\ell_{11}} $
                \item Compute $\hat{b} - \hat{\ell}x_{1} = \tilde{b} \in \mathbb{R}^{n-1} $
                \item Find $\hat{L}x = \tilde{b} $
            \end{enumerate}
        \item \textbf{Counting flops for column oriented forward substitution}: Let $f_{n}$ be the flop count, we have
            \begin{align*}
                f_{n} = 1 + 2(n-1) + f_{n-1}
            .\end{align*}
            With
            \begin{align*}
                f_{n-1} = 1 + 2(n-2) + f_{n-2}, \\
                f_{n-2} = 1 + 2(n-3) + f_{n-3}, \\
            .\end{align*}
            Until
            \begin{align*}
                f_{n-(n-1)} = f_{1} = 1 + 2((n-(n-1))-1) + f_{0}  = 1 + 2(0) + f_{0} = 1
            \end{align*}
            with $f_{0} = 0 $
            \bigbreak \noindent 
            So, 
            \begin{align*}
                f_{n} &= 1 + 2(n-1) + 1 + 2(n-2) + ... + 1 + 2((n-(n-1))-1) \\
                &= \sum_{i=1}^{n} 1 + 2(n-1) = n + 2n^{2} - 2\sum_{i=1}^{n}i \\
                &=...=n^{2}
            .\end{align*}
            Thus, column oriented forward substitution is also $\mathcal{O}(n^{2})$
        \item \textbf{Solutions of an upper triangular system (Backward substitution)}: 
            Let $A \in \mathbb{R}^{n\times n},\ x \in \mathbb{R}^{n},\ b \in \mathbb{R}^{n}$, with
            \begin{align*}
                A = \begin{bmatrix}
                    a_{11} & a_{12} & \cdots & a_{1n} \\
                    0 & a_{22} & \cdots & a_{2n} \\
                    \vdots & \vdots & \ddots & \vdots \\
                    0 & 0 & \cdots & a_{nn}
                \end{bmatrix},  \quad \;
                x = \begin{bmatrix}
                    x_{1} \\ x_{2} \\ \vdots \\ x_{n}
                \end{bmatrix}, \quad \;
                b = \begin{bmatrix}
                    b_{1} \\ b_{2} \\ \vdots \\ b_{n}
                \end{bmatrix}
            .\end{align*}
            Then, 
            \begin{align*}
                 \begin{bmatrix}
                    a_{11} & a_{12} & \cdots & a_{1n} \\
                    0 & a_{22} & \cdots & a_{2n} \\
                    \vdots & \vdots & \ddots & \vdots \\
                    0 & 0 & \cdots & a_{nn}
                \end{bmatrix}
                \begin{bmatrix}
                    x_{1} \\ x_{2} \\ \vdots \\ x_{n}
                \end{bmatrix} 
                = \begin{bmatrix}
                    b_{1} \\ b_{2} \\ \vdots \\ b_{n}
                \end{bmatrix}
            \end{align*}
            implies
            \begin{align*}
                a_{11}x_{1} + a_{12}x_{2} + \cdots + a_{1n}x_{n} &= b_{1} \\
                a_{22}x_{2} + a_{23}x_{3} + \cdots + a_{2n}x_{n} &= b_{2} \\
                                                                 &\vdots \\
                a_{nn} x_{n} &= b_{n}
            .\end{align*}
            So, 
            \begin{align*}
                x_{1} &= \frac{b_{1} - (a_{12}x_{2} + a_{13}x_{3} + \cdots + a_{1n}x_{n})}{a_{11}} ,\\
                x_{2} &= \frac{b_{2} - (a_{23}x_{3} + a_{24}x_{4} + \cdots + a_{2n}x_{n})}{a_{22}} ,\\
                x_{n} &= \frac{b_{n}}{a_{nn}}
            .\end{align*}
            In general, we have that
            \begin{align*}
                x_{i} &= \frac{b_{i} - \sum_{j=i+1}^{n}a_{ij}x_{j}}{a_{ii}}, \quad i=n,n-1,...,1
            \end{align*}
        \item \textbf{Column-oriented backward substitution}:
            Let $U \in \mathbb{R}^{n\times n}$ be upper triangular, $x \in \mathbb{R}^{n}$, and $ b \in \mathbb{R}^{n}$ which gives the system
            \begin{align*}
                \begin{bmatrix}
                    u_{11} & u_{12} & \cdots & u_{1n} \\
                    0 & u_{22} & \cdots & u_{2n} \\
                    \vdots & \vdots & \ddots & \vdots \\
                    0 & 0 & \cdots & u_{nn}
                \end{bmatrix} 
                \begin{bmatrix}
                    x_{1} \\ x_{2} \\ \vdots \\ x_{n}
                \end{bmatrix}
                = 
                \begin{bmatrix}
                    b_{1} \\ b_{2} \\ \vdots \\ b_{n}
                \end{bmatrix}
            \end{align*}
            Split the system into the following block decomposition
            \begin{align*}
                \begin{bmatrix}
                    \hat{U} & u \\
                    0^{\top} & u_{nn}
                \end{bmatrix}
                \begin{bmatrix}
                    \hat{x} \\ x_{n}
                \end{bmatrix}
                = 
                \begin{bmatrix}
                    \hat{b} \\ b_{n}
                \end{bmatrix}
            \end{align*}
            Then,
            \begin{align*}
                \hat{U}\hat{x} + ux_{n} &= \hat{b} \implies \hat{U}\hat{x} = \hat{b} - ux_{n} = \tilde{b}, \\
                u_{nn}x_{n} &= b_{n} \implies x_{n} = \frac{b_{n}}{u_{nn}}
            \end{align*}
            Thus, the column-oriented backward substitution algorithm is defined by the following steps
            \begin{enumerate}
                \item Compute $x_{n} = \frac{b_{n}}{u_{nn}} $
                \item Compute $\tilde{b} = \hat{b} - ux_{n} $
                \item Run the algorithm on $\hat{U}, \tilde{b}$. That is, $\text{Alg}(\hat{U}, \tilde{b})$
            \end{enumerate}
            \bigbreak \noindent 
            The non-recursive pseudocode in the spirit of 1.3.5 and 1.3.13 is
            \bigbreak \noindent 
            \begin{jlcode}
                for |$i=n,...,1$|
                if |$U[i,i] = 0$|, set error flag, exit

                |$b[i] = b[i] / U[i,i]$|

                for |$j = i-1,...,1$|
                |$b[j] = b[j] - U[j,i] \cdot b[i]$|
                end
                end
            \end{jlcode}

        \item \textbf{Counting flops for the above backward substitution algorithm}: The general expression for $x_{i}$ can be expressed as
            \bigbreak \noindent 
            \begin{jlcode}
            for i = n:-1:1
                for j = i+1:n
                    B[i] = B[i] - A[i,j] * B[j]
                end
                B[i] = B[i]/A[i,i]
            end
            \end{jlcode}
            \bigbreak \noindent 
            So, the flop count is
            \begin{align*}
                f(n) &= n+\sum_{i=1}^{n}2(n-(i+1) + 1) \\
                     &=n+\sum_{i=1}^{n}2(n-i) \\
                     &=n+2\sum_{i=1}^{n}n - 2\sum_{i=1}^{n}i \\
                     &= n + 2n^{2} - 2 \cdot \frac{n(n+1)}{2} \\
                     &= n + 2n^{2} - (n^{2} +n) \\
                     &= n + 2n^{2} -n^{2}-n \\
                     &=n^{2}
            \end{align*}
            So, the backward substitution algorithm is $\mathcal{O}(n^{2}) $
        \item \textbf{Triangular matrices after multiplication}:
            triangular matrices stay triangular after multiplication, provided you multiply matrices of the same triangular type:
            \begin{itemize}
                \item Upper triangular $\times$ upper triangular = upper triangular
                \item Lower triangular $\times$ lower triangular = lower triangular
            \end{itemize}
        \item \textbf{Triangular matrices after transpose}
            \begin{itemize}
                \item The transpose of an upper triangular matrix is a lower triangular matrix
                \item The transpose of an lower triangular matrix is a upper triangular matrix
            \end{itemize}
        \item \textbf{Triangular matrices after inversion}: The inverse of a lower triangular matrix is lower triangular, and the inverse of an upper triangular matrix is upper triangular
        \item \textbf{Diagonal matrices}: Are both upper triangular and lower triangular (at the same time).

            
    \end{itemize}

    \pagebreak 
    \subsubsection{Positive Definite Systems}
    \begin{itemize}
            \item \textbf{Positive definite matrix}: A matrix $A$ is \textbf{positive definite} provided that the following two conditions are satisfied
            \begin{enumerate}
                \item $A$ is symmetric. That is, $A = A^{\top} $
                \item $x^{\top}Ax > 0 $ for all $x\ne 0$
            \end{enumerate}
        \item \textbf{Positive Definiteness Characterizations}:  
            Let $A \in \mathbb{R}^{n \times n}$ be a symmetric matrix. Then the following are equivalent:  
            \begin{enumerate}
                \item $A$ is positive definite.  
                \item All eigenvalues of $A$ are positive.  
                \item All \emph{leading principal minors} of $A$ are positive (Sylvester’s criterion).  
            \end{enumerate}  
            Moreover:  
            \begin{itemize}
                \item (1) $\implies$ $a_{ii} > 0$ for all $i=1,2,\dots,n$.  
                \item (1) $\implies$ every principal submatrix of $A$ is positive definite.  
            \end{itemize}

        \item \textbf{Properties of positive definite (p.d) matrices}:
            \begin{enumerate}
                \item If $A$ is p.d then $A$ is \textit{non-singular}
                    \bigbreak \noindent 
                    \textbf{Note:} Since $A$ is non-singular there is no $y \in \mathbb{R}^{n}$, $y\ne 0$ such that $Ay = 0$
                \item If $A = M^{\top}M$ for some $M$ non-singular than $A$ is p.d
                \item $A$ is p.d if and only if all eigenvalues of $A$ are positive
                    \bigbreak \noindent 
                    Recall that $\lambda$ is an eigenvalue of $A$ if there exists $x_{\lambda} \ne 0$ such that $Ax_{\lambda}  = \lambda x_{\lambda}$
                \item If $A$ is p.d then all principal submatrices are p.d
                \item $A$ is p.d if and only if all leading principal minors are positive
                \item If $A$ is p.d than $\det(A) > 0$
                \item $A$ is p.d if and only if there exists a unique upper triangular matrix $R$ such that $A = R^{\top}R$ (Cholesky factorization described below)
            \end{enumerate}
            \textbf{Note:} Property two is a key property.
            \bigbreak \noindent 
            \textbf{\textit{Proof of (1)}}: Assume $A$ is a p.d matrix. Further assume (for the sake of contradiction) that $A$ is singular. Then, there exits $y \in \mathbb{R}^{n},\; y \ne 0 $ with $Ay=0$
            \bigbreak \noindent 
            Since $Ay = 0$, $y\ne 0$, then $y^{\top}Ay=0$ when $y\ne 0$, so $A$ is not p.d.
            \bigbreak \noindent 
            Therefore if $A$ is p.d, then $A$ is non-singular $\endpf$
            \bigbreak \noindent 
            \textbf{\textit{Proof of (2)}}: Assume that $A  = M^{\top}M$ for some $M$ non-singular. Then,
            \begin{align*}
                A^{\top} = (M^{\top}M)^{\top} = M^{\top}(M^{\top})^{\top} = M^{\top}M = A
            \end{align*}
            So $A$ is symmetric ($A = A^{\top}$)
            \bigbreak \noindent 
            Next, let $x \ne 0$
            \begin{align*}
                x^{\top}Ax &= x^{\top}(M^{\top}M)x \\
               &=(x^{\top}M^{\top})(Mx) \\
               &=(Mx)^{\top}(Mx)
            \end{align*}
            let $y = Mx $, then
            \begin{align*}
                y^{\top}y &= \begin{pmatrix} y_{1} & y_{2} & \cdots&y_{n} \end{pmatrix} \begin{pmatrix} y_{1} \\ y_{2} \\ \vdots & y_{n} \end{pmatrix} \\
                          &=y_{1}^{2} + y_{2}^{2} + ... + y_{n}^{2} > 0
            \end{align*}
            So, $A$ is p.d $\endpf$
            \bigbreak \noindent 
            Note that since $x \ne 0$, $y\ne 0$. 
            \bigbreak \noindent 
            Recall that $y^{\top}y = \norm{y}^{2}$
            \bigbreak \noindent 
            \textbf{\textit{Proof of (3)}}: Assume that $A$ is p.d. Let $\lambda$ be an eigenvalue for $A$. Then,
            \begin{align*}
                Ax_{\lambda} &= \lambda x_{\lambda} \\
                \implies x^{\top}_{\lambda}Ax_{\lambda} &= x^{\top}_{\lambda}\lambda x \\
                                                        &= \lambda x^{\top}_{\lambda} x_{\lambda}
            \end{align*}
            First, observe that $x^{\top}Ax >0$ since $A$ p.d. Thus,
            \begin{align*}
               \lambda x_{\lambda}^{\top} x > 0 
            \end{align*}
            and since $x_{\lambda}^{\top} x = \norm{x_{\lambda}}^{2} $, we have
            \begin{align*}
                \lambda \norm{x_{\lambda}}^{2} > 0 
            \end{align*}
            since $ \norm{x_{\lambda}}^{2} > 0$, and $ \lambda \norm{x_{\lambda}} > 0 $, it follows that $\lambda > 0$

        
        \item \textbf{principal submatrices}: 
            A \textbf{principal submatrix} of a square matrix 
            $A \in \mathbb{R}^{n \times n}$ is obtained by selecting a subset of indices 
            $I \subseteq \{1, \ldots, n\}$, and then keeping only the rows and the columns of $A$ with those same indices.
            \bigbreak \noindent 
            \textbf{Examples}
            \[
                A =
                \begin{bmatrix}
                    a_{11} & a_{12} & a_{13} \\
                    a_{21} & a_{22} & a_{23} \\
                    a_{31} & a_{32} & a_{33}
                \end{bmatrix},
            \]
            \begin{itemize}
                \item Choosing $I = \{1,2\}$ gives the principal submatrix
                    \[
                        \begin{bmatrix}
                            a_{11} & a_{12} \\
                            a_{21} & a_{22}
                        \end{bmatrix}.
                    \]

                \item Choosing $I = \{2,3\}$ gives
                    \[
                        \begin{bmatrix}
                            a_{22} & a_{23} \\
                            a_{32} & a_{33}
                        \end{bmatrix}.
                    \]
                \item Choosing $I = \{1,3\} $ gives
                    \begin{align*}
                        \begin{bmatrix}
                            a_{11} & a_{13} \\
                            a_{31} & a_{33}
                        \end{bmatrix}
                    \end{align*}
                \item Choosing $I = \varnothing$ gives the empty matrix, usually denoted $0_{M_{0,0}}$. This empty matrix is in fact a principal submatrix of $A$
            \end{itemize}
            \bigbreak \noindent 
            Note that if $I$ is the subset of indices, then the submatrix is denoted $A[I]$, or $A[I,I]$
            \bigbreak \noindent 
            If $A\in\mathbb{R}^{n\times n} $
            \begin{align*}
                A = \begin{bmatrix}
                    a_{11} & a_{12} & \cdots &a_{1n} \\
                    a_{21} & a_{22} & \cdots &a_{2n} \\
                    \vdots & \vdots & \ddots & \vdots \\
                    a_{n1} & a_{n2} & \cdots &a_{nn}
                \end{bmatrix}
            \end{align*}
            Then the principal submatrices are
            \begin{align*}
                A_{1} &= \begin{bmatrix}
                    a_{11}
                \end{bmatrix} \\
                    A_{2} &= \begin{bmatrix}
                    a_{11} & a_{12} \\
                    a_{21} & a_{22}
                \end{bmatrix} \\
                        A_{3} &= \begin{bmatrix}
                    a_{11} & a_{12} & \cdots &a_{1n} \\
                    a_{21} & a_{22} & \cdots &a_{2n} \\
                    \vdots & \vdots & \ddots & \vdots \\
                    a_{n1} & a_{n2} & \cdots &a_{nn}
                \end{bmatrix}
            \end{align*}
        \item \textbf{Leading principal minors}: Let $A\in \mathbb{R}^{n\times n}$, with
            \begin{align*}
                A = \begin{bmatrix}
                    a_{11} & a_{12} & \cdots & a_{1n} \\
                    a_{21} & a_{22} & \cdots & a_{2n} \\
                    \vdots & \vdots & \ddots & \vdots \\
                    a_{n1} & a_{n2} & \cdots & a_{nn} 
                \end{bmatrix}
            \end{align*}
            \begin{itemize}
                \item Take $I_{1} = \{1\} $
                    \begin{align*}
                       A_{I_{1}} = \begin{bmatrix} a_{11} \end{bmatrix}
                    \end{align*}
                \item Take $I_{2} = \{1,2\} $
                    \begin{align*}
                        A_{I_{2}} = \begin{bmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{bmatrix}
                    \end{align*}
                \item Take $I_{k} = \{1,2,...,k\} $ for $k < n$
                    \begin{align*}
                        A_{I_{k}} = \begin{bmatrix}
                            a_{11} & a_{12} & \cdots & a_{1k} \\
                            a_{21} & a_{22} & \cdots & a_{2k} \\
                            \vdots & \vdots & \ddots & \vdots \\
                            a_{k1} & a_{k2} & \cdots & a_{kk} \\
                        \end{bmatrix}
                    \end{align*}
                \item Take $I_{n} = \{1,2,...,n\} $ (The whole matrix)
                    \begin{align*}
                        A = \begin{bmatrix}
                            a_{11} & a_{12} & \cdots & a_{1n} \\
                            a_{21} & a_{22} & \cdots & a_{2n} \\
                            \vdots & \vdots & \ddots & \vdots \\
                            a_{n1} & a_{n2} & \cdots & a_{nn} 
                        \end{bmatrix}
                    \end{align*}
            \end{itemize}
            \bigbreak \noindent 
            These matrices are a special chain of principal submatrices called \textit{leading principal submatrices}
            \bigbreak \noindent 
            This family of principal submatrices are the ones most often used in certain matrix theory results.
        \item \textbf{Determinant of the empty matrix}: We define
            \begin{align*}
                \det(\varnothing) = 1
            \end{align*}
        \item \textbf{principal minors}: A principal minor is simply the determinant of a principal submatrix.
        \item \textbf{Cholesky decomposition and the Cholesky Factor}: Let $A \in \mathbb{R}^{n\times n}$ be p.d, then $A = R^{\top}R$ where $R$ is upper triangular with $r_{ii} > 0$. The matrix $R$ is called the \textbf{Cholesky factor}.
            \bigbreak \noindent 
            If $A = R^{\top}R$, then $Ax = b$ can be written as 
            \begin{align*}
                R^{\top}Rx = b
            \end{align*}
            where
            \begin{align*}
                \begin{cases}
                        Rx &= y \quad \text{(Lower triangular)}              \\
                        R^{\top}y &= b \quad \text{(Upper triangular)}
                \end{cases}
            \end{align*}
            and since these new systems are triangular, they can be solved quickly with forward or backward substitution.
        \item \textbf{Inner product formulas to compute $R$ (Cholesky factor)}: We have the formulas
            \begin{align*}
                r_{ii} &= \sqrt{a_{ii} - \sum_{k=1}^{i-1}r_{ki}^{2}} \quad i = 1,2,...,n \\
                r_{ij} &= \frac{a_{ij} - \sum_{k=1}^{i-1}r_{ki}r_{kj}}{r_{ii}} \quad j = i+1,...,n
            \end{align*}
        \item \textbf{Recursive column oriented method to find the Cholesky factor $R$ (Outer product method)}: Let $A \in \mathbb{R}^{n\times n}$. Assume that $A$ is positive definite, so $A = A^{\top}$, and $A = R^{\top}R$ for a unique upper triangular matrix $R$. We have,
            \begin{align*}
                A &= \begin{bmatrix} a_{11} & a_{12} & \cdots & a_{1n} \\ a_{21} & a_{22} & \cdots & a_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ a_{n1} & a_{n2} & \cdots & a_{nn} \end{bmatrix} = 
                \begin{bmatrix}
                    r_{11} & 0  & \cdots & 0\\
                    r_{12} & r_{22}  &  \cdots & 0 \\
                    \vdots & \vdots & \ddots & \vdots\\
                    r_{1n} & r_{2n} & \cdots & r_{nn}
                \end{bmatrix}
                \begin{bmatrix}
                    r_{11} & r_{12} & \cdots & r_{1n} \\
                    0 & r_{22} & \cdots & r_{2n} \\
                    0 & 0 & \ddots & \vdots \\
                    0 & 0  & \cdots & r_{nn}
                \end{bmatrix}
            \end{align*}
            We then perform a matrix decomposition 
            \begin{align*}
                \begin{bmatrix}
                    a_{11} & a^{\top} \\
                    a & \hat{A}
                \end{bmatrix}
                =
                \begin{bmatrix}
                    r_{11} & 0^{\top} \\
                    r & \hat{R}^{\top}
                \end{bmatrix}
                \begin{bmatrix}
                    r_{11} & r^{\top} \\
                    0 & \hat{R}
                \end{bmatrix}
            .\end{align*}
            Where $\hat{A} = \hat{A}^{\top} \in \mathbb{R}^{n-1 \times n-1}$, $a \in \mathbb{R}^{n-1}$, $\hat{R}^{\top} \in \mathbb{R}^{n-1\times n-1}$ lower triangular, and $\hat{R} \in \mathbb{R}^{n-1\times n-1}$ upper triangular. Further,
            \begin{align*}
                \hat{A} &= \begin{bmatrix} a_{22} & \cdots & a_{2n} \\ \vdots & \ddots & \vdots \\ a_{n2} & \cdots & a_{nn} \end{bmatrix},\; a = \begin{pmatrix} a_{21} \\ a_{31} \\ \vdots \\ a_{n1} \end{pmatrix}, \\
                \hat{R} &= \begin{bmatrix} r_{22} &  \cdots & r_{2n} \\ 0 & \ddots & \vdots \\ 0  & \cdots & r_{nn} \end{bmatrix},\; r = \begin{pmatrix} r_{12} \\ r_{13} \\ \vdots \\ r_{1n} \end{pmatrix} 
            .\end{align*}
            So, given this decomposition, we see that 
            \begin{align*}
                a_{11} &= r_{11}^{2} \implies r_{11} = \sqrt{a_{11}}
            .\end{align*}
            Recall that in the definition of the Cholesky factor $R$, $r_{ii} > 0$ for $i = 1,2,...,n  $
            \bigbreak \noindent 
            Continuing the matrix multiplication, we have that
            \begin{align*}
                a &= r_{11}r \implies r = \frac{a}{r_{11}}
            \end{align*}
            and, 
            \begin{align*}
                \hat{A} &=rr^{\top} + \hat{R}\hat{R}^{\top} \implies \hat{R}^{\top}\hat{R} = \hat{A} - rr^{\top} = \tilde{A}
            \end{align*}
            \bigbreak \noindent 
            Thus, 
            the recursive column oriented algorithm to compute the Cholesky factor $R$ is given by the following steps
            \begin{enumerate}
                \item $r_{11} = \sqrt{a_{11}}$
                \item $r = \frac{a}{r_{11}} $
                \item $\tilde{A} = \hat{A} - rr^{\top} $
                \item $\text{Alg}(\tilde{A}) = \hat{R} $
            \end{enumerate}
        \item \textbf{Counting flops for the recursive algorithm above}:
            \begin{enumerate}
                \item (Step 1): One flop
                \item (Step 2): $n-1$ flops
                \item (Step 3): $(n-1)^{2}$ flops. Notice that $r \in \mathbb{R}^{n-1},\; r^{\top} \in \mathbb{R}^{n-1}$, and the outer product
                    \begin{align*}
                        rr^{\top} \in \mathbb{R}^{n-1\times n-1}
                    \end{align*}
                    and requires $(n-1)^{2}$ flops.
                    \bigbreak \noindent 
                    Since $\hat{A} \in \mathbb{R}^{n-1\times n-1}$, $ \hat{A} - rr^{\top}$ requires an addition $(n-1)^{2}$ flops. So, step 3 requires $2(n-1)^{2}$ flops
            \end{enumerate}
            \bigbreak \noindent 
            Thus, 
            \begin{align*}
                f_{n} &= 1 + (n-1) + 2(n-1)^{2} + f_{n-1} \\
                &=n + 2(n-1)^{2} + f_{n-1}
            .\end{align*}
            Where
            \begin{align*}
                f_{n-1} &= 1 + (n-2) + 2(n-2)^{2} + f_{n-2} = n-1 + 2(n-2)^{2} + f_{n-2} \\
                        &\vdots \\
                f_{n-(n-1)}  &= f_{1} = 1 + 0 + 0 + f_{0} 
            .\end{align*}
            Note that $f_{0} = 0$. So, $f_{1} = 1$. In total, we have
            \begin{align*}
                f_{n} &= n + 2(n-1)^{2} + n-1 + 2(n-2)^{2} + n-2 + 2(n-3)^{2} + ... + 1 \\
                      &= \sum_{k=1}^{n} k + 2(k-1)^{2} \\
                      &= \sum_{k=1}^{n}k + 2\sum_{k=1}^{n} k^{2} -2k + 1 \\
                      &= \sum_{k=1}^{n}k + 2\sum_{k=1}^{n}k^{2} - 4\sum_{k=1}^{n}k + 2\sum_{k=1}^{n}1 \\
                      &= 2 \sum_{k=1}^{n}k^{2} - 3\sum_{k=1}^{n}k + 2k \\
                      &= 2 \left(\frac{n(n+1)(2n+1)}{6}\right) - 3 \left(\frac{n(n+1)}{2}\right) + 2k = \mathcal{O}(n^{3})
            \end{align*}
            \bigbreak \noindent 
            So, the recursive algorithm is $\mathcal{O}(n^{3})$
        \item \textbf{Consequence of proof n.1.2:} If $A$ has any $a_{ii} \leq 0$, $A$ is not positive definite.
        \item \textbf{Bordered form of Choleskys method}: Suppose $A \in \mathbb{R}^{n\times n}$ is positive definite. Then, $A$ admits a decomposition $A = R^{\top}R$, for a unique upper triangular matrix $R$ called the Cholesky factor, with $r_{ii} > 0$ for $i =1,2,...,n$. So,
            \begin{align*}
                        A &= \begin{bmatrix} a_{11} & a_{12} & \cdots & a_{1n} \\ a_{21} & a_{22} & \cdots & a_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ a_{n1} & a_{n2} & \cdots & a_{nn} \end{bmatrix} = 
                \begin{bmatrix}
                    r_{11} & 0  & \cdots & 0\\
                    r_{12} & r_{22}  &  \cdots & 0 \\
                    \vdots & \vdots & \ddots & \vdots\\
                    r_{1n} & r_{2n} & \cdots & r_{nn}
                \end{bmatrix}
                \begin{bmatrix}
                    r_{11} & r_{12} & \cdots & r_{1n} \\
                    0 & r_{22} & \cdots & r_{2n} \\
                    0 & 0 & \ddots & \vdots \\
                    0 & 0  & \cdots & r_{nn}
                \end{bmatrix}
            \end{align*}
        We then perform a matrix decomposition 
        \begin{align*}
            \begin{bmatrix}
                \hat{A} & a \\
                a^{\top} & a_{nn}
            \end{bmatrix}
             =
             \begin{bmatrix}
                 \hat{R}^{\top} & 0 \\
                 r^{\top} & r_{nn}
             \end{bmatrix}
             \begin{bmatrix}
                 \hat{R} & r \\
                 0 & r_{nn}
             \end{bmatrix}
        .\end{align*}
        So, 
        \begin{align*}
            \hat{A} &= \hat{R}^{\top}\hat{R}, \\
            a &= \hat{R}^{\top}r, \\
            a_{nn} &= r^{\top}r + r_{nn}^{2} \implies r_{nn} = \sqrt{a_{nn} - r^{\top}r}
        .\end{align*}
        So, the steps for the algorithm are 
        \begin{enumerate}
            \item Recurse $\hat{A}$ until $A \in \mathbb{R}^{1\times 1}$
            \item Solve the lower triangular system $\hat{R}^{\top}r = a$ by forward substitution
            \item Compute $r_{nn} = \sqrt{a_{nn} - r^{\top}r} $
            \item Return the step two on the previous call
        \end{enumerate}
        The above algorithm is postorder recursion and requires $\mathcal{O}(n^{3})$ flops.



    \end{itemize}

    \pagebreak 
    \subsubsection{Banded Matrices}
    \begin{itemize}
        \item \textbf{Cholesky factor $R$ in a diagonal matrix}: If $A = D = \begin{bmatrix}
                a_{11} & 0 & \cdots & 0 \\
                0 & a_{22} & \cdots & 0 \\
                \vdots & \vdots & \ddots & \vdots \\
                0 & 0 & \cdots & a_{nn} 
        \end{bmatrix}$ then
        \begin{align*}
            R = \begin{bmatrix}
                \sqrt{a_{11}} & 0 & \cdots & 0 \\
                0 & \sqrt{a_{22}} & \cdots & 0 \\
                \vdots & \vdots & \ddots & \vdots \\
                0 & 0 & \cdots & \sqrt{a_{nn}}
            \end{bmatrix}
        \end{align*}
        \item \textbf{Banded matrix}: A banded matrix is a sparse matrix whose nonzero entries are confined to a diagonal band, consisting of the main diagonal and a fixed number of diagonals on either side of it.
            \bigbreak \noindent 
                Let $A \in \mathbb{R}^{m \times n}$.  
                Then $A$ is called a \textbf{banded matrix} if there exist nonnegative integers $p, q$ (called the \emph{lower} and \emph{upper bandwidths}) such that
                \[
                    a_{ij} = 0 \quad \text{whenever } i - j > p \text{ or } j - i > q.
                \]
                \begin{itemize}
                    \item The \emph{lower bandwidth} $p$ is the number of subdiagonals (below the main diagonal) that may contain nonzero entries.
                    \item The \emph{upper bandwidth} $q$ is the number of superdiagonals (above the main diagonal) that may contain nonzero entries.
                \end{itemize}
                The \emph{total bandwidth} is sometimes defined as $p + q + 1$, counting the main diagonal as well.
                \bigbreak \noindent 
                A tridiagonal matrix has lower bandwidth $p=1$ and upper bandwidth $q=1$:
                \[
                    A =
                    \begin{bmatrix}
                        a_{11} & a_{12} & 0      & 0      \\
                        a_{21} & a_{22} & a_{23} & 0      \\
                        0      & a_{32} & a_{33} & a_{34} \\
                        0      & 0      & a_{43} & a_{44}
                    \end{bmatrix}.
                \]
                The total bandwidth here is $1 + 1 + 1 = 3$.

        \item \textbf{Column envelope}: The column envelope of $A $ is the set of indices $(i,j)$ in the upper triangular part of $A$ (including the main diagonal). Define
            \begin{align*}
            \text{colenv}\{A\} = \{(i,j)\} :\; i \leq j \text{ and } a_{kj} \ne 0 \text{ for } k \leq i
            \end{align*}
        \item \textbf{Theorem}: Let $A$ be p.d, if $R$ is the Cholesky factor of $A$, then
            \begin{align*}
                \text{colenv}\{R\}  = \text{colenv}\{A\}
            \end{align*}
    \end{itemize}

    \pagebreak 
    \subsubsection{Gaussian Elimination and LU Decompositions}
    \begin{itemize}
        \item \textbf{Intro to $LU$ decomposition}: Consider a matrix $A \in \mathbb{R}^{n\times n} $. If we can factor $A$ as $A = LU$, for $L$ lower triangular, $U$ upper triangular, then the system $Ax = b$, for vectors $x,b \in \mathbb{R}^{n}$ turns into
            \begin{align*}
                LUx = b
            .\end{align*}
            We can then split this system as follows
            \begin{align*}
                \begin{cases}
                    Ly &= b \\
                    Ux &= y
                \end{cases}
            \end{align*}
            First, we solve $Ly = b$ with forward substitution to find $y$. We can then solve $Ux = y$ with backward substitution to find the target $x$.
            \bigbreak \noindent 
            Recall that the forward and backward substitution methods for solving linear systems requires $\mathcal{O}(n^{2})$ flops.
        \item \textbf{Elementary operations on systems that do not change the solution set}. We have the operations
            \begin{enumerate}
                \item Interchange rows.
                \item Multiply an equation by a nonzero constant.
                \item Add a multiple of one equation to another equation.
            \end{enumerate}
            We show that these elementary operations (E.O) leave the solution set unchanged. Let the original system be $S$ and the modified system be $S^{\prime}$. To show that the solution set is unchanged is to show that if a vector $x$ is a solution to $S$ then it is also a solution to $S^{\prime}$, and vice versa.
            \bigbreak \noindent 
            Consider the system $S$, $Ax = b$ for $A \in \mathbb{R}^{n\times n}$, $x,b \in \mathbb{R}^{n}$. So, the system is
            \begin{align*}
                a_{11}x_{1} + a_{12}x_{2} + ... + a_{1n}x_{n} &= b_{1},\\
                a_{21}x_{1} + a_{22}x_{2} + ... + a_{2n}x_{n} &= b_{2},\\
                                                              &\vdots\\
                a_{n1}x_{1} + a_{n2}x_{2} +  ... + a_{nn}x_{n} &= b_{n}
            .\end{align*}
            \bigbreak \noindent 
            \textbf{\textit{Proof (2).}} Multiply an arbitrary equation by a nonzero scalar $k$, suppose we choose the second equation. $S^{\prime}$ is then
             \begin{align*}
                a_{11}x_{1} + a_{12}x_{2} + ... + a_{1n}x_{n} &= b_{1},\\
                k(a_{21}x_{1} + a_{22}x_{2} + ... + a_{2n}x_{n}) &= k(b_{2}),\\
                                                              &\vdots\\
                a_{n1}x_{1} + a_{n2}x_{2} +  ... + a_{nn}x_{n} &= b_{n}
            .\end{align*}
            Let $(c_{1}, c_{2},...,c_{n})$ be a solution to the original system $S$. That is, it satisfies all equations. Let's look at the second equation
            \begin{align*}
                a_{21}c_{1} + a_{22}c_{2} + ... + a_{2n}c_{n} &= b_{2}\\
            \end{align*}
            If we multiply by $k$, we get
            \begin{align*}
                k(a_{21}c_{1} + a_{22}c_{2} + ... + a_{2n}c_{n}) &= k(b_{2})
            .\end{align*}
            Which means $(c_{1}, c_{2}, ..., c_{n}) $ also satisfies the second equation in $S^{\prime}$. Since all other equations were left unchanged, $(c_{1}, c_{2},...,c_{n}) $ satisfies those equations as well. So, the solution set is the same for both systems.
            \bigbreak \noindent 
            \textbf{Note}: If $k=0$, the second equation would collapse to $0=0$, which would enlarge the solution set. In this case, a constraint would be removed from the  system $S^{\prime}$. The second equation is now tautological, it imposes no restriction. The solution set would be
            \begin{align*}
                \{x\in \mathbb{R}^{n}:\ Ax = b \text{ for all rows except row two}\}
            \end{align*}
            \bigbreak \noindent 
            Every solution of $S$ is also a solution of $S^{\prime}$, but the converse need not hold: $S^{\prime}$ could have solutions that don’t satisfy the second original equation. 
            \bigbreak \noindent 
            So $S^{\prime}$ has at least as many solutions, and possibly more. If the second equation was independent of the others, then yes - you've enlarged the solution set.
            \bigbreak \noindent 
            \textbf{\textit{Proof (3).}} Add an arbitrary equation to a different equation. Suppose we add the first equation to the second equation. Note that we leave the first unchanged. $S^{\prime}$ is then
             \begin{align*}
                a_{11}x_{1} + a_{12}x_{2} + ... + a_{1n}x_{n} &= b_{1},\\
                (a_{21}+a_{11})x_{1} + (a_{22}+a_{12})x_{2} + ... + (a_{2n} + a_{1n})x_{n}) &= b_{2} + b_{1},\\
                                                                                            &\vdots\\
                a_{n1}x_{1} + a_{n2}x_{2} +  ... + a_{nn}x_{n} &= b_{n}
            .\end{align*}
            Let $c = (c_{1}, c_{2}, ..., c_{n}) $ be a solution to $S$, so $c$ is a solution to the first, second, and the remaining equations. That is, 
            \begin{align*}
                a_{11}c_{1} + a_{12}c_{2} + ... + a_{1n}c_{n} &= b_{1},\\
                a_{21}c_{1} + a_{22}c_{2} + ... + a_{2n}c_{n} &= b_{2},\\
                                                              &\vdots\\
                a_{n1}c_{1} + a_{n2}c_{2} +  ... + a_{nn}c_{n} &= b_{n}
            .\end{align*}
            Add the first equation to the second, we get
             \begin{align*}
                a_{11}c_{1} + a_{12}c_{2} + ... + a_{1n}c_{n} &= b_{1},\\
                (a_{21}+a_{11})c_{1} + (a_{22}+a_{12})c_{2} + ... + (a_{2n} + a_{1n})c_{n}) &= b_{2} + b_{1},\\
                                                                                            &\vdots\\
                a_{n1}c_{1} + a_{n2}c_{2} +  ... + a_{nn}c_{n} &= b_{n}
            \end{align*}
            which is precisely $S^{\prime}$, so $c$ satisfies $ S^{\prime}$. Next, let $c = (c_{1}, c_{2}, ..., c_{n}) $ be a solution to $S^{\prime}$. So,
             \begin{align*}
                a_{11}c_{1} + a_{12}c_{2} + ... + a_{1n}c_{n} &= b_{1},\\
                (a_{21}+a_{11})c_{1} + (a_{22}+a_{12})c_{2} + ... + (a_{2n} + a_{1n})c_{n}) &= b_{2} + b_{1},\\
                                                                                            &\vdots\\
                a_{n1}c_{1} + a_{n2}c_{2} +  ... + a_{nn}c_{n} &= b_{n}
            \end{align*}
            Subtract the first equation from the second, and we get back $S$. So, the solution set remains unchanged.
        \item \textbf{Elimination matrix $E$}: An elimination matrix is just a special matrix that performs a single step of Gaussian elimination when you multiply it by another matrix.
            \begin{itemize}
                \item Suppose you want to eliminate the entry in row $i$, column $j$ of $A$.
                \item In elimination, you would replace row $i$ by
                    \[
                        \text{row}_{i} - m \cdot \text{row}_{j},
                    \]
                    where $m = \tfrac{a_{ij}}{a_{jj}}$.
            \end{itemize}

            The \textbf{elimination matrix} $E$ is the identity matrix, except in position $(i,j)$, where it has $-m$.

            So:
            \[
                E = I - m e_i e_j^{\top},
            \]
            where $e_i$ and $e_j$ are standard basis vectors (all zeros except a one at the $i^{\text{th}}$ or $j^{\text{th}} $ position).
            \bigbreak \noindent 
            Multiplying $E$ by $A$ from the left actually performs that row operation:
            \[
                EA = A \quad \text{with entry $(i,j)$ zeroed out.}
            \]
            Let
            \[
                A = \begin{bmatrix}
                    2 & 1 \\
                    4 & 3
                \end{bmatrix}.
            \]
            We want to eliminate the entry in the bottom-left.  
            \bigbreak \noindent 
            The multiplier is
            \[
                m = \frac{4}{2} = 2.
            \]
            The elimination matrix is
            \[
                E = \begin{bmatrix}
                    1 & 0 \\
                    -2 & 1
                \end{bmatrix}.
            \]
            Now check:
            \[
                EA =
                \begin{bmatrix}
                    1 & 0 \\
                    -2 & 1
                \end{bmatrix}
                \begin{bmatrix}
                    2 & 1 \\
                    4 & 3
                \end{bmatrix}
                =
                \begin{bmatrix}
                    2 & 1 \\
                    0 & 1
                \end{bmatrix},
            \]

        \item \textbf{LU Factorization without E.O (1)}: We perform Gaussian Elimination on the augmented system $[A|b] $ to yield a new system $[U|y]$.
            \bigbreak \noindent 
        We move down the main diagonal selecting $a_{ii}$ as the \textbf{pivot element}, and row $i$ as the \textbf{pivot row}. We do this for $i = 1,2,...,n$. For each pivot element, we get the elements $a_{ki} = 0$ for $k=i+1,i+2,...,n$ we can accomplish this without interchanging rows so long as the pivot elements are nonzero.
        \bigbreak \noindent 
        We perform elementary operations of the form
        \begin{align*}
            -m_{ki}r_{i} + r_{k} \to r_{k}^{\prime} \quad \text{ for } k = i+1,i+2,...,n
        \end{align*}
        where $r_{i}$ is the $i^{\text{th}}$ row, $r_{k}$ is the $k^{\text{th}}$ row, and $m_{ki}$ is the \textbf{multiplier} $m_{ki} = \frac{a_{ki}}{a_{ii}} $
        \bigbreak \noindent 
        Upon completion of the Gaussian elimination, the collected multipliers together with ones in the main diagonal and zeros in the entries above the main diagonal form the matrix $L$.
        \bigbreak \noindent 
        \textbf{Additional information:} We perform Gaussian elimination on the augmented matrix $[A|b] \to [U|y]$. Note that after we achieve $U$, we have the system $Ux = y$ with the same solution set as $Ax = b$. 
        \bigbreak \noindent 
        Notice that each step has its own elimination matrix $E_{1},E_{2},... $. If we apply them in sequence,
        \begin{align*}
            E_{k} \cdots E_{2}E_{1}A = U
        \end{align*}
        Where $U$ is the final upper triangular matrix. It follows that 
        \begin{align*}
            A = (E_{k} \cdots E_{2}E_{1})^{-1}U
        \end{align*}
        Define 
        \begin{align*}
            (E_{k} \cdots E_{2}E_{1})^{-1} = L
        \end{align*}
        It’s lower triangular because each elimination matrix is lower triangular, and the inverse of a lower triangular matrix is also lower triangular.
        \bigbreak \noindent 

        \bigbreak \noindent 
        For example, consider the system
        \begin{align*}
            \begin{bmatrix}
                \begin{array}{ccc|c}
                    2 & 1 & 1 & 7 \\
                    2 & 2 & -1 & 3 \\
                    4 & -1 & 6 & 20
               \end{array}
            \end{bmatrix}
        \end{align*}
        We start with $a_{11} = 2$ as the pivot element, and row one as the pivot row. We perform elementary operations of the form above to get $a_{21} = a_{31} = 0$. To get $a_{21} = 0$, we have the operation
        \begin{align*}
            -1r_{1} + r_{2} \to r_{2}^{\prime}, \quad m_{21} = 1
        \end{align*}
        To get $a_{31} = 0$, we perform the operation
        \begin{align*}
            -2r_{1} + r_{3} \to r_{3}^{\prime}, \quad m_{31} = 2
        \end{align*}
        After the two operations, we have
        \begin{align*}
            \begin{bmatrix}
                \begin{array}{ccc|c}
                    2 & 1 & 1 & 7 \\
                    0 & 1 & -2 & -4 \\
                    0 & -3 & 4 & 6
                \end{array}
            \end{bmatrix}
        \end{align*}
        We move to the next pivot element $a_{22} = 1$. To get $a_{32} = 0$, we perform the operation
        \begin{align*}
            -(-3)r_{2} + r_{3} \to r_{3}^{\prime},\quad m_{32} = -3
        \end{align*}
        So after the last operation we have
        \begin{align*}
            \begin{bmatrix}
                \begin{array}{ccc|c}
                    2 & 1 & 1 & 7 \\
                    0 & 1 & -2 & -4 \\
                    0 & 0 & -2 & -6
                \end{array}
            \end{bmatrix}
        \end{align*}
        % \item \textbf{Theorem}: Let $A \in \mathbb{R}^{n\times n}$, $A$ can be factored as $A = LU$, for $L \in \mathbb{R}^{n\times n}$ unit lower triangular, $U \in \mathbb{R}^{n\times n}$ upper triangular if and only if all leading principal submatrices of $A$ are nonsingular.
    \item \textbf{Theorem}: Let $A \in \mathbb{R}^{n\times n}$ be non-singular. Then, we can solve the system $Ax = b$, $b \in \mathbb{R}^{n}$ using Gaussian Elimination without row interchanges if and only if all landing principal sub-matrices of $A$ are non-singular.
    \item \textbf{Theorem}: Let $A \in \mathbb{R}^{n \times n}$. Then $A$ admits an LU factorization
            \[
                A = LU,
            \]
            where $L \in \mathbb{R}^{n \times n}$ is unit lower triangular and 
            $U \in \mathbb{R}^{n \times n}$ is upper triangular, 
            \textbf{without row interchanges}, if and only if all leading principal 
            submatrices of $A$ are nonsingular.

    \item \textbf{Row oriented algorithm to find $LU$ factorization}: 
        Let $A \in \mathbb{R}^{n\times n}$. If $A$ can be factored into products $LU$, for $U \in \mathbb{R}^{n\times n}$ upper triangular, $L \in \mathbb{R}^{n\times n}$ unit lower triangular, then
        \begin{align*}
            A &= LU  
        \end{align*}
        implies
        \begin{align*}
              \begin{bmatrix}
                a_{11} & a_{12} & a_{13} & \cdots & a_{1n} \\
                a_{21} & a_{22} & a_{23} & \cdots & a_{2n} \\
                a_{31} & a_{32} & a_{33} & \cdots & a_{3n} \\
                \vdots & \vdots & \vdots & \ddots & \vdots \\
                a_{n1} & a_{n2} & a_{n3} & \cdots & a_{nn} \\
            \end{bmatrix}
             = \begin{bmatrix}
                 1 & 0 & 0 & \cdots & 0 \\
                 \ell_{21} & 1 & 0 & \cdots & 0 \\
                 \ell_{31} & \ell_{32} & 1 & \cdots & 0 \\
                 \vdots & \vdots & \vdots & \ddots & \vdots \\
                 \ell_{n1} & \ell_{n2} & \ell_{n3} & \cdots & 1
             \end{bmatrix}
             \begin{bmatrix}
                 u_{11} & u_{12} & u_{13} & \cdots & u_{1n} \\
                 0  & u_{22} & u_{23} & \cdots & u_{1n} \\
                 0& 0& u_{33} & \cdots & u_{3n} \\
                 \vdots & \vdots & \vdots & \ddots & \vdots \\
                 0 & 0 & 0 & \cdots & u_{nn}
             \end{bmatrix}
        .\end{align*}
        Let's first examine the formula for $u_{ij}$ by solving for each row in $U$
        \begin{enumerate}
            \item \textbf{Row 1}: for $j=1,\dots,n$
                \[
                    u_{1j} = a_{1j}.
                \]

            \item \textbf{Row 2}: for $j=2,\dots,n$
                \[
                    u_{2j} = a_{2j} - \ell_{21}\,u_{1j}.
                \]

            \item \textbf{Row 3}: for $j=3,\dots,n$
                \[
                    u_{3j} = a_{3j} - \ell_{31}\,u_{1j} - \ell_{32}\,u_{2j}.
                \]

            \item \textbf{Row $i$}: for $j=i,\dots,n$
                \[
                    u_{ij} = a_{ij} - \sum_{k=1}^{i-1} \ell_{ik}\,u_{kj}.
                \]

            \item \textbf{Row $n$}: (just the diagonal entry)
                \[
                    u_{nn} = a_{nn} - \sum_{k=1}^{n-1} \ell_{nk}\,u_{kn}.
                \]
        \end{enumerate}
        Next, we look at the formula for $\ell_{ij}$ by solving for each column in $L$
        \begin{enumerate}
            \item \textbf{Column 1}: for $i=2,\dots,n$
                \[
                    \ell_{i1} \;=\; \frac{a_{i1}}{u_{11}}.
                \]
            \item \textbf{Column 2}: for $i=3,\dots,n$
                \[
                    \ell_{i2} \;=\; \frac{a_{i2} - \ell_{i1}u_{12}}{u_{22}}.
                \]
            \item \textbf{Column 3}: for $i=4,\dots,n$
                \[
                    \ell_{i3} \;=\; \frac{a_{i3} - \ell_{i1}u_{13} - \ell_{i2}u_{23}}{u_{33}}.
                \]
            \item \textbf{Column $j$}: for $i=j+1,\dots,n$
                \[
                    \ell_{ij} \;=\; \frac{a_{ij} - \sum_{k=1}^{j-1}\ell_{ik}u_{kj}}{u_{jj}}.
                \]
            \item \textbf{Column $n$}: (no entries below the diagonal to compute if $j=n$)
                \[
                    \text{Only } \ell_{nn}=1 \text{ (by unit lower convention).}
                \]
        \end{enumerate}
        So, we see that
        \begin{align*}
            u_{ij} &= a_{ij} - \sum_{k=1}^{i-1}\ell_{ik}u_{kj} \quad j=i,i+1,...,n \tag{1} \\
            \ell_{ij} &= \frac{a_{ij} - \sum_{k=1}^{j-1}\ell_{ik}u_{kj}}{u_{jj}} \quad i=j+1,j+2,...,n \tag{2}
        \end{align*}
        \bigbreak \noindent 
        To use these formulas to find each $u_{ij}$ we first need to plug $i=1$ into $(1)$, then after we get the first row of $U$, we can plug in $j=1$ into $(2)$ to get the first column of $L$, and so on.

        \item \textbf{Column oriented recursive algorithm to find the $LU$ factorization}: 
            Assume $A \in \mathbb{R}^{n\times n }$ admits an $LU$ factorization for $L\in \mathbb{R}^{n\times n}$ unit lower triangular, $U$ upper triangular. Then,
            \begin{align*}
                A = LU
            \end{align*}
            implies
            \begin{align*}
                \begin{bmatrix} a_{11} & a_{12} & \cdots & a_{1n} \\ a_{21} & a_{22} & \cdots & a_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ a_{n1} & a_{n2} & \cdots & a_{nn} \end{bmatrix} = 
                \begin{bmatrix} 1 & 0 & \cdots & 0 \\ \ell_{21} & \ell_{22} & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ \ell_{n1} & \ell_{n2} & \cdots & 1 \end{bmatrix}
                \begin{bmatrix} u_{11} & u_{12} & \cdots & u_{1n} \\ 0 & u_{22} & \cdots & u_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & u_{nn} \end{bmatrix}
            .\end{align*}
            Decompose $A = LU$ into the blocks
            \begin{align*}
                \begin{bmatrix}
                    a_{11} & b^{\top} \\
                    a & \hat{A}
                \end{bmatrix}
                =
                \begin{bmatrix}
                    1 & 0^{\top} \\
                    \ell & \hat{L}
                \end{bmatrix}
                \begin{bmatrix}
                    u_{11} & u^{\top} \\
                    0 & \hat{U}
                \end{bmatrix}
            .\end{align*}
            We see that
            \begin{align*}
                a_{11} &= u_{11}, \\
                b^{\top} &= u^{\top}, \\
                \ell u_{11} &= a \implies \ell = \frac{a}{u_{11}}, \\
                \hat{A} &= \ell u^{\top} + \hat{L}\hat{U} \implies \hat{L}\hat{U} = \hat{A} - \ell u^{\top}
            .\end{align*}
            Define $\tilde{A} = \hat{A} - \ell u^{\top}$. 
            The recursive algorithm is then defined by the following steps.
            \begin{enumerate}
                \item $u_{11} = a_{11} $ (zero flops)
                \item $u^{\top} = b^{\top} $ (zero flops)
                \item $\ell = \frac{a}{u_{11}} $ ($n-1$ flops) 
                \item $\tilde{A} = \hat{L}\hat{U} = \hat{A} - \ell u^{\top} $ ($2(n-1)^{2}$ flops)
                \item $\text{Alg}(\tilde{A})$
            \end{enumerate}
            Let $f(n)$ be the flop count for the above algorithm. We have
            \begin{align*}
                f(n) &= 0 + 0  + (n-1) + 2(n-1)^{2} + f_{n-1} = (n-1) + 2(n-1)^{2} + f_{n-1}\\
                f(n-1) &= ((n-1) - 1) + 2((n-1)-1)^{2} + f_{n-2} \\
                       &\vdots \\
                f(n-(n-2)) &= f_{2} = ((n-(n-2))-1) + 2((n-(n-2))-1)^{2} + f_{n-(n-1)} \\
                f_{n-(n-1)} &= f_{1} = 0 
            \end{align*}
            So, the total number of flops is given by the sum
            \begin{align*}
                \sum_{k=2}^{n} (k-1) + 2(k-1)^{2}
            .\end{align*}
            Let $ i = k-1 $. When  $k=2$, $i = 1$. When $k=n$, $i = n-1$. So, the sum becomes
            \begin{align*}
                \sum_{i=1}^{n-1} 2i^{2} + i
            \end{align*}
            \begin{remark}
                We have the summation rules
                \begin{align*}
                    \sum_{i=1}^{n} i = \frac{n(n+1)}{2}, \quad \sum_{i=1}^{n} i^{2} = \frac{n(n+1)(2n+1)}{6}
                \end{align*}
                Plug in $n-1$ for each,
                \begin{align*}
                   \sum_{i=1}^{n} i = \frac{(n-1)n}{2}, \quad \sum_{i=1}^{n} i^{2} = \frac{(n-1)n(2n-1)}{6} 
                \end{align*}
            \end{remark}
            So,
            \begin{align*}
                \sum_{i=1}^{n-1} 2i^{2} + i &= 2\left(\frac{(n-1)n(2n-1)}{6}\right) + \frac{(n-1)n}{2} \\
                                            &= \frac{2}{3}n^{2} -\frac{1}{2}n^{2} - \frac{1}{6}n = \frac{2}{3}n^{3} + \mathcal{O}(n^{2})
            \end{align*}
            Therefore, the number of flops required for the recursive outer product method to find the $LU$ factorization is $\frac{2}{3}n^{3} + \mathcal{O}(n^{2}) $
        \item \textbf{Bordered form $LU$ decomposition algorithm}:
            Assume $A \in \mathbb{R}^{n\times n }$ admits an $LU$ factorization for $L\in \mathbb{R}^{n\times n}$ unit lower triangular, $U$ upper triangular. Then,
            \begin{align*}
                A = LU
            \end{align*}
            implies
            \begin{align*}
                \begin{bmatrix} a_{11} & a_{12} & \cdots & a_{1n} \\ a_{21} & a_{22} & \cdots & a_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ a_{n1} & a_{n2} & \cdots & a_{nn} \end{bmatrix} = 
                \begin{bmatrix} 1 & 0 & \cdots & 0 \\ \ell_{21} & \ell_{22} & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ \ell_{n1} & \ell_{n2} & \cdots & 1 \end{bmatrix}
                \begin{bmatrix} u_{11} & u_{12} & \cdots & u_{1n} \\ 0 & u_{22} & \cdots & u_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & u_{nn} \end{bmatrix}
            .\end{align*}
            Decompose $A = LU$ into the blocks
            \begin{align*}
                \begin{bmatrix}
                    \hat{A} & b \\
                    a^{\top} & a_{nn}
                \end{bmatrix}
                = 
                \begin{bmatrix}
                    \hat{L} & 0 \\
                    \ell^{\top} & 1
                \end{bmatrix}
                \begin{bmatrix}
                    \hat{U} & u \\
                    0^{\top} & u_{nn}
                \end{bmatrix}
            \end{align*}
            Where $\hat{A}, \hat{L}, \hat{U}$ are the $(n-1)^{\text{th}}$ leading principal submatrices of $A,L,U$.
            \bigbreak \noindent 
            Then, 
            \begin{align*}
                \hat{A} &= \hat{L}\hat{U} \\
                \hat{L}U &= b  \\
                \ell^{\top} \hat{U} &= a^{\top} \implies \hat{U}^{\top}\ell = a \\
                u_{nn} &= a_{nn} - \ell^{\top}u^{\top}
            \end{align*}
        % \item \textbf{$LU $ factorization of symmetric, non positive definite systems}: Let $A \in \mathbb{R}^{n\times n}$ be symmetric, but not positive definite. So, $A = A^{\top}$. Assume that $A$ admits an $LU$ factorization $A = LU$.  
        %     \bigbreak \noindent 
        %     In this case, we require $L$ to not be unit lower triangular, just lower triangular.
        %     \begin{align*}
        %         A^{\top} &= A = LU = (LU)^{\top} = U^{\top}L^{\top}
        %     \end{align*}
        %     So,
        %     \begin{align*}
        %         U^{\top} &= L, \\
        %         L^{\top} &= U
        %     \end{align*}
        %     Thus,
        %     \begin{align*}
        %         A &= LL^{\top} \quad \text{ or } \quad A = U^{\top}U
        %     \end{align*}
        %     Which would halve the flops of the normal $A = LU$ factorization. 
        %     \bigbreak \noindent 
        %     If $L$ were unit lower triangular, then
        %     \begin{align*}
        %         A &= LDL^{\top}
        %     \end{align*}
        %
        \item \textbf{Intro to row interchanges (pivoting)}: Without pivoting, Gaussian elimination can behave as if the problem were ill-conditioned even when it is not, because tiny pivots can amplify rounding errors. This is why we use partial pivoting.
            \bigbreak \noindent 
            Consider the system
            \begin{align*}
                \begin{bmatrix}
                    0.0003 & 1.566 \\
                    0.3454 & -2.436
                \end{bmatrix}
                \begin{bmatrix}
                    x_{1} \\ x_{2}
                \end{bmatrix}
                =
                \begin{bmatrix}
                    1.569 \\ 1.018
                \end{bmatrix}
            .\end{align*}
            Solving the system by Gaussian Elimination without pivoting, we get $m_{21} = \frac{0.3454}{0.0003} = 1151.3333$. After Gaussian Elimination, we get
            \begin{align*}
                x_{1} = 3.333, \quad x_{2} = 1.001
            .\end{align*}
            We note that the exact solution to the system is $x_{1} = 10, x_{2} = 1$. So what happened? We see that $x_{2}$ is far from the true solution.
            \bigbreak \noindent 
            If we instead swap the rows to use the second row as the pivot row, we get
            \begin{align*}
                m_{21} = \frac{0.0003}{0.3454} = 0.0008686
            .\end{align*}
            Then, $x_{1} = 10.01,\; x_{2} = 1$.
            \bigbreak \noindent 
            We we round off a number $\alpha$, we get $\alpha \to \bar{\alpha}$, where $\alpha = \bar{\alpha} + \epsilon $, for some small $\epsilon$. If this number is then multiplied by a scalar $m$, we get
            \begin{align*}
                m\alpha = m\bar{\alpha} + m\epsilon     
            .\end{align*}
            So, the error grows as $m$ grows. Our goal is to select the pivot such that $m$ is minimized.
            \bigbreak \noindent 
            If we select the largest element in the $k^{\text{th}}$ column (at step $k$), then we can guarantee $m \leq 1$.
        \item \textbf{Partial pivoting}: At iteration $k$ of Gaussian Elimination, we swap row $k$ with some row below so that the new $a_{kk}$ has the largest absolute value compared to all entries below in column $k$.
        \item \textbf{Permutation matrix}: A permutation matrix is a special kind of square matrix that represents a permutation of elements. Formally:
            It is obtained from the identity matrix by rearranging its rows (or equivalently, its columns).
            \bigbreak \noindent 
            Each row and each column has exactly one entry equal to 1, and all other entries are 0.
            \bigbreak \noindent 
            Multiplying a vector (or another matrix) by a permutation matrix reorders its entries.
            \bigbreak \noindent 
            Suppose $P$ is formed by taking $I$ and interchanging rows one and two. Then,
            \begin{align*}
                P = \begin{pmatrix} 0 & 1 & 0 \\ 1 & 0 & 0 \\ 0 & 0 & 1 \end{pmatrix}
            .\end{align*}
            Then, 
            \begin{align*}
                P\begin{pmatrix} a \\ b \\ c \end{pmatrix} = \begin{pmatrix} b \\ a \\ c \end{pmatrix}
            .\end{align*}
            So, it swaps the first two entries.
    \item \textbf{Gaussian elimination with partial pivoting}:
        Eliminates entries below the pivots to produce an upper triangular system.
        \bigbreak \noindent 
        At each step, you swap the current row with one below it to bring the largest (by absolute value) element in the pivot column into the pivot position. This improves numerical stability.
    \item \textbf{$LU$ factorization with partial pivoting}: We we partial pivot rows in Gaussian Elimination (while building $L,U$), we make the same swap in $P$ (and in $L$ unless $A$ is used to store $L$ during Gaussian Elimination).
        \bigbreak \noindent 
        At the end, we get
        \begin{align*}
            PA = LU
        .\end{align*}
        Then, we also see that
        \begin{align*}
            A = P^{-1}LU = P^{T}LU
        .\end{align*}
        We can use this fact to solve systems $Ax = b$. We have
        \begin{align*}
            Ax &= b \\
            \implies PAx &= Pb \\
            \implies LUx &= Pb
        .\end{align*}
        Just like in standard $LU$ decomposition, we split the system in two triangular systems that can both be solved by substitution in $n^{2}$ flops. We have
        \begin{align*}
            \begin{cases}
                Ly &= Pb  \quad \text{(Lower triangular)}\\
                Ux &= y \quad \text{(Upper triangular)}
            \end{cases}
        .\end{align*}


\end{itemize}

    \pagebreak 
    \subsection{Chapter 1 tangents}
    \bigbreak \noindent 
    \subsubsection{Numerical error}
    \begin{itemize}
        \item \textbf{Numerical error when solving systems}: 
            Suppose we want to solve a linear system $Ax = b$. 
            In practice, due to floating-point roundoff and the large number of flops required for a big system, 
            the computed solution $\bar{x}$ will generally not satisfy $Ax = b$ exactly. 
            \bigbreak \noindent 
            We define the \textbf{residual} as
            \[
                r = b - A\bar{x},
            \]
            which measures how far $\bar{x}$ is from being an exact solution. 
            If $\bar{x}$ is a good approximation, then $r \approx 0$. 
            \bigbreak \noindent 
            If $\bar{x}$ is not sufficiently accurate, we can attempt to improve it. 
            Notice that
            \[
                A\bar{x} + r = b,
            \]
            so the residual can be used to correct the solution. 
            If $\bar{\bar{x}}$ denotes an improved approximation, then we want
            \[
                A\bar{\bar{x}} = b = A\bar{x} + r.
            \]
            \bigbreak \noindent 
            This motivates the following iterative refinement process:
            \begin{enumerate}
                \item Compute an approximate solution $\bar{x}$ to $Ax = b$.
                \item Compute the residual $r = b - A\bar{x}$.
                \item Solve the \emph{correction system} $A\delta = r$ for $\delta$.
                \item Update the solution: $\bar{x} \leftarrow \bar{x} + \delta$.
                \item Repeat until the residual is sufficiently small.
            \end{enumerate}
            \bigbreak \noindent 
            If we define a better solution $\bar{\bar{x}}$ as our computed solution plus $\delta$,
            \begin{align*}
                \bar{\bar{x}} = \bar{x} + \delta
            .\end{align*}
            We want
            \begin{align*}
                A\bar{\bar{x}} &= b, \\
                \implies A(\bar{x} + \delta ) &= b, \\
                \implies A\bar{x} + A\delta &= b
            .\end{align*}
            But, the residual is defined as $r = b - A\bar{x}$, so $A\bar{x} + r =b$. Thus, we have
            \begin{align*}
                A\bar{x} + A\delta &= A\bar{x} + r
            .\end{align*}
            Which we see implies that $A\delta = r $. So, we solve the new system $A\delta = r$ for delta, then update
            \begin{align*}
                \bar{x} \leftarrow \bar{x}  + \delta
            \end{align*}
            and repeat if necessary. 
            \bigbreak \noindent 
            \textbf{Note}: If we instead solve
            \begin{align*}
                A\bar{\bar{x}} = A\bar{x} + r
            \end{align*}
            directly, we will find that our computed $\bar{\bar{x}}$ is equal to $\bar{x} + \delta$.
        % \item \textbf{Numerical error when solving systems}: Suppose we have a system $Ax = b$, if the matrix is large and many flops occur, then the solution to the system will not be the solution. In general, we have if that $Ax = b$ produces $\bar{x}$, then the \textbf{residual} is defined as
        %     \begin{align*}
        %         r = b-A\bar{x},
        %     \end{align*}
        %     which is likely nonzero. If $\bar{x}$ is a good approximation, then $r \approx 0$
        %     \bigbreak \noindent 
        %     Suppose $\bar{x}$ is not a good solution, how could we find a better solution $\bar{\bar{x}} $ using $\bar{x}$?
        %     \bigbreak \noindent 
        %     We have that $r = b - A\bar{x} $, so $A\bar{x} + r = b$. If $\bar{\bar{x}}$ is another solution for $Ax = b$, then 
        %     \begin{align*}
        %         A\bar{\bar{x}} = b = A\bar{x} + r
        %     .\end{align*}
        %     The general process is 
        %     \begin{enumerate}
        %         \item Solve $Ax = b$ to get $\bar{x}$ and $r$
        %         \item Solve $Ax = A\bar{x} + r$ to get a new $\bar{x}$ and $r$
        %         \item Repeat the process and hope the error shrinks.
        %     \end{enumerate}



    \end{itemize}


    \pagebreak 
    \subsubsection{Outer Products and transposition tricks}
    \begin{itemize}
        \item \textbf{Build an $m\times n$ matrix from $n$ vectors in $\mathbb{R}^{m}$}: Suppose we have vectors $x_{1}, x_{2}, x_{3}, ..., x_{n} \in \mathbb{R}^{m}$, and we wish to construct the matrix formed by combining each vector $x_{k}$ for $k=1,2,3,...,n$. Algebraically, we have
            \begin{align*}
                X &= \begin{bmatrix} x_{1} & x_{2} & x_{3} & \cdots & x_{n} \end{bmatrix} = x_{1}e_{1}^{\top} + x_{2}e_{2}^{\top} + x_{3}e_{3}^{\top} + ... + x_{n}e_{n}^{\top} \\
                  &= \sum_{k=1}^{n}x_{k}e_{k}^{\top}
            \end{align*}
            where $e_{\ell}$ for $\ell = 1,2,3,...,n$ are the standard basis vectors in $\mathbb{R}^{n} $.
            \bigbreak \noindent 
            For example, if $x = \begin{pmatrix} x_{1} \\ x_{2} \end{pmatrix} \in \mathbb{R}^{2}$, and $y = \begin{pmatrix} y_{1} \\ y_{2} \end{pmatrix} \in \mathbb{R}^{2}$ then
            \begin{align*}
                X &= xe_{1}^{\top} + ye_{2}^{\top} = \begin{pmatrix} x_{1} \\ x_{2} \end{pmatrix} \begin{pmatrix} 1  & 0 \end{pmatrix} + \begin{pmatrix} y_{1} \\ y_{2} \end{pmatrix} \begin{pmatrix} 0 & 1 \end{pmatrix} \\
                  &= \begin{pmatrix} x_{1} & 0 \\ x_{2} & 0 \end{pmatrix} + \begin{pmatrix} 0& y_{1} \\ 0 & y_{2} \end{pmatrix} = \begin{pmatrix} x_{1} & y_{1} \\ x_{2} & y_{2} \end{pmatrix}
            \end{align*}
        \item \textbf{Construct a $m\times n$ matrix with a single element in some position}: Suppose we want an $m\times n$ matrix with $k$ in position $a_{ij}$ We take the outer product
            \begin{align*}
               A = me_{i}e_{j}^{\top}
            \end{align*}
            Where $e_{i}$ is the $i^{\text{th}} $ standard basis vector in $\mathbb{R}^{m}$, and $e_{j}$ is the $j^{\text{th}}$ standard basis vector in $\mathbb{R}^{n}$.

    \end{itemize}

    \pagebreak 
    \subsection{Sensitivity of linear systems (2)}
    \bigbreak \noindent 
    \subsubsection{Vector and matrix norms}
    \begin{itemize}
        \item \textbf{Norm:} A norm is an operation $\norm{\cdot }:\; \mathbb{R}^{n} \to \mathbb{R}_{+}:\; x \to \norm{x} \geq 0$  that satisfies
            \begin{enumerate}
                \item $\norm{x} = 0 \iff x = 0$
                \item $\norm{\alpha x} = \left\lvert \alpha \right\rvert \norm{x} $
                \item $\norm{x+y} \leq \norm{x} + \norm{y} $ (triangle inequality)
            \end{enumerate}
        \item \textbf{Euclidean norm (2-norm)}: The standard Euclidean distance. For $x \in \mathbb{R}^{n}$,
            \begin{align*}
                \norm{x}_{2} = \sqrt{x_{1}^{2} + x_{2}^{2} + ... + x_{n}^{2}}
            .\end{align*}
        \item \textbf{Manhattan norm (1-norm)}: Denoted $L^{1} $, and also called \textbf{Taxicab norm}. For $x \in \mathbb{R}^{n} $,
            \begin{align*}
                \norm{x}_{1} = \left\lvert x_{1} \right\rvert + \left\lvert x_{2} \right\rvert + ... + \left\lvert x_{n} \right\rvert
            .\end{align*}
        \item \textbf{$L$-Infinity (max) norm ($\infty$-norm)}: Denoted $L^{\infty}$. for $x\in \mathbb{R}^{n}$,
            \begin{align*}
                \norm{x}_{\infty} = \max_{1 \leq i \leq n} \left\lvert x_{i} \right\rvert = \max\{\left\lvert x_{1} \right\rvert, \left\lvert x_{2} \right\rvert, ..., \left\lvert x_{n} \right\rvert\} 
            .\end{align*}
        \item \textbf{Unit balls of norms in $\mathbb{R}^{2}$}:
                \begin{figure}[h]
                    \centering
                    \incfig{2norm}
                    \label{fig:2norm}
                \end{figure}
            \begin{itemize}
                \item \textbf{2-norm}: $B_{\norm{x}_{2}}(0,1) = \{x\in \mathbb{R}^{n}:\; x_{1}^{2} + x_{2}^{2} \leq 1\}$
                \item \textbf{1-norm}: $B_{\norm{x}_{1}}(0,1) = \{x\in \mathbb{R}^{n}:\; \left\lvert x_{1} \right\rvert + \left\lvert x_{2} \right\rvert \leq 1\}$
                \item \textbf{$\infty$-norm}: $B_{\norm{x}_{\infty}}(0,1) = \{x\in \mathbb{R}^{n}:\; \max_{1 \leq i \leq 2}\left\lvert x_{i} \right\rvert \leq 1\}$
            \end{itemize}
        \item \textbf{$p$-norm}: In $\mathbb{R}^{n}$, A more general norm is
            \begin{align*}
                \norm{x}_{p} = \left(\sum_{i=1}^{n} \left\lvert x_{i} \right\rvert^{p}\right)^{\frac{1}{p}} = \left(\left\lvert x_{1}^{p} \right\rvert + \left\lvert x_{2} \right\rvert^{p} + ... + \left\lvert x_{n} \right\rvert^{p}\right)^{\frac{1}{p}}
            \end{align*}
            for $1 \leq p < \infty $. The general $p$-norm satisfies all three properties of a norm only when $p \geq 1$. For smaller $p$, the triangle inequality does not hold.
        \item \textbf{Entrywise (Bad) Matrix-norms}: Consider the isomorphism $\phi:\; \mathbb{R}^{n\times n} \to \mathbb{R}^{n\cdot n} $. For example, 
            \begin{align*}
                \begin{pmatrix} a & b \\ c & d \end{pmatrix} \mapsto \begin{pmatrix} a \\ b \\c \\d \end{pmatrix}
            .\end{align*}
            This way, we can use our vector norms defined above on matrices. The norms we have seen so far would be
            \begin{align*}
                \norm{A}_{p} &= \left(\sum_{i=1}^{n}\sum_{j=1}^{n}\left\lvert a_{ij} \right\rvert^{p}\right)^{\frac{1}{p}}, \\
                \norm{A}_{1} &= \sum_{i=1}^{n}\sum_{j=1}^{n} \left\lvert a_{ij} \right\rvert, \\
                \norm{A}_{2} &= \left(\sum_{i=1}^{n}\sum_{j=1}^{n}a_{ij}^{2}\right)^{\frac{1}{2}}
            .\end{align*}
            \textbf{Note:} The matrix 2-norm $\norm{A}_{2}$ is also called the \textit{Frobenius} norm, denoted $\norm{A}_{F}$.
            \bigbreak \noindent 
            We see that in the Frobenius norm, $\norm{I}_{F} = \sqrt{n} \ne 1$. In general, we would like our matrix norms to have $\norm{I} = 1$ for all dimensions, and to not grow as the dimension increases.
            \bigbreak \noindent 
            These entrywise norms treat the matrix as a big vector and ignore its action on other vectors.
        \item \textbf{Properties of matrix norms}: Matrix norms satisfy the three required properties of norms.
            \begin{enumerate}
                \item $\norm{A} = 0 \iff A = 0 $
                \item $\norm{\alpha A} = \left\lvert \alpha \right\rvert \norm{A} $
                \item $\norm{A + B} \leq \norm{A} + \norm{B} $ (Triangle inequality)
            \end{enumerate}
        \item \textbf{Induced (operator) matrix norms}: For all $A\in \mathbb{R}^{n\times m}$, we define
            \begin{align*}
                \norm{A}_{p} := \max_{x\in\mathbb{R}^{n} \setminus \{0\}} \frac{\norm{Ax}_{p}}{\norm{x}_{p}}
            .\end{align*}

        \item \textbf{Properties of induced matrix norms}
            \begin{itemize}
                \item \textbf{Sub-multiplicativity}: $\norm{AB}_{p} \leq \norm{A}_{p}\norm{B}_{p} $
                \item \textbf{Consistency}: $\norm{Ax}_{p} \leq \norm{A}_{p}\norm{x}_{p} $
                \item \textbf{Normalization}: $\norm{I}_{p} = 1 $
            \end{itemize}
            These are what entrywise ("flattened") norms lack.
        \item \textbf{Induced matrix norms special cases}:
            \begin{center}
                \begin{tabular}{p{1cm}|p{5cm}|p{5cm}}
                    \toprule
                    $p$ & \textbf{Name} & \textbf{Explicit formula} \\
                    \midrule
                    $1$ & Maximum column sum &
                    $\displaystyle \|A\|_{1} = \max_{1 \leq j \leq n} \sum_{i=1}^{m} |a_{ij}|$ \\[3ex]
                    $2$ & Spectral norm &
                    $\displaystyle \|A\|_{2} = \sqrt{\lambda_{\max}(A^{T}A)}$ \\[3ex]
                    $\infty$ & Maximum row sum &
                    $\displaystyle \|A\|_{\infty} = \max_{1 \leq i \leq m} \sum_{j=1}^{n} |a_{ij}|$ \\
                    \bottomrule
                \end{tabular}
            \end{center}
        \item \textbf{Derived property of matrix norm}: For $A \in \mathbb{R}^{n\times n}$, we have
            \begin{align*}
                \norm{A} &= \max_{x\ne 0} \frac{\norm{Ax}}{\norm{x}} \geq \frac{\norm{Ax}}{\norm{x}} \\
                         &\implies \norm{Ax} \leq \norm{A}\norm{x}
            .\end{align*}
        \item \textbf{Cauchy Schwarz inequality for 2-norm (vector norm)}: states
            \begin{align*}
                \left\lvert x^{T}y \right\rvert \leq \norm{x}_{2}\norm{y}_{2} 
            .\end{align*}
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} Let $t \in \mathbb{R}$. We know that $0 \leq \norm{x + ty}_{2}^{2} $. Recall that $x^{T}x = \norm{x}_{2}^{2} = \norm{x}_{2}\norm{x}_{2}$.
            \bigbreak \noindent 
            We have
            \begin{align*}
                0 \leq (x + ty)^{T}(x+ty) &= x^{T}x + x^{T}ty + ty^{T}x + t^{2}y^{t}y \\
                                          &= \norm{x}_{2}^{2} + 2t(x^{T}y) + t^{2}\norm{y}_{2}^{2}
            .\end{align*}
            Observe that this is a 2-degree polynomial in $t$, call it $p_{2}(t)$.
            \begin{align*}
                p_{2}(t) &= \norm{y}_{2}^{2} t^{2} + 2(x^{t}y)t + \norm{x}_{2}^{2} \geq 0 
            .\end{align*}
            Since $p_{2}(t)$ is greater than or equal to zero, we know that the discriminant is less than or equal to zero. That is, $p_{2} (t) \geq 0 $ implies $D \leq 0$, where $D = (2(x^{T}y))^{2} -4(\norm{y}_{2}^{2})(\norm{x}_{2}^{2})$. Thus,
            \begin{align*}
                (2(x^{T}y))^{2} -4(\norm{y}_{2}^{2})(\norm{x}_{2}^{2}) &\leq 0 \\
                \implies 4(x^{T}y)^{2} -4(\norm{y}_{2}^{2})(\norm{x}_{2}^{2}) &\leq 0  \\
                \implies (x^{T}y)^{2} -(\norm{y}_{2}^{2})(\norm{x}_{2}^{2}) &\leq 0  \\
                \implies (x^{T}y)^{2}  &\leq \norm{y}_{2}^{2}\norm{x}_{2}^{2} \\
                \implies \left\lvert x^{T}y \right\rvert &\leq \norm{x}_{2}\norm{y}_{2}
            .\end{align*}

    \end{itemize}

    \pagebreak 
    \subsubsection{Condition number}
    \begin{itemize}
        \item \textbf{Intro to measuring solutions}: Consider a problem $(P)$, where
            \begin{align*}
                (P):\; Ax = b
            .\end{align*}
            Numerical techniques yields a solution $\hat{x}$, which may or may not be the true solution to $(P)$. Let $x$ be the true solution to the system. So, $x$ solves $Ax = b$. 
            \bigbreak \noindent 
            We want to measure the distance between the numerical solution $\hat{x} $ and the true solution $x$, we hope that the numerical solution $\hat{x}$ is close to $x$. If the distance is small, then $\hat{x}$ is a good solution.
        \item \textbf{Relative error}: The relative error in $\hat{x}$ is given by
            \begin{align*}
                \frac{\norm{\hat{x} - x}}{\norm{x}} = \frac{\norm{\delta x}}{\norm{x}}
            \end{align*}
            where $\hat{x} = x + \delta  x $, which implies $x = \hat{x} - \delta  x $.
        \item \textbf{Perturbation}: If numerical methods to solve a linear system $Ax = b$ yields $\hat{x}$, then $\hat{x}$ solves $\hat{A}\hat{x} = \hat{b}$. Note that it is possible for $\hat{A} = A$ or $\hat{b} = b $. If both $\hat{A} = A$ and $\hat{b} = b$, then $\hat{x} = x$.
            \bigbreak \noindent 
            $\hat{A}$ and $\hat{b}$ are called perturbed if they are modified versions of the original. If $\hat{A}$ is a perturbed matrix $A$, and $\hat{b}$ is a perturbed vector $b$, then
            \begin{align*}
                \hat{A} &= A + \delta A, \\
                \hat{b} &= b + \delta  b
            .\end{align*}
        \item \textbf{Perturbing $b$}: We can perturb $b$, but not $A$ such that $\hat{x}$ solves $A\hat{x} = \hat{b}$.
            \bigbreak \noindent 
            Recall that the residual vector is $\hat{r} = b - A\hat{x} $. If $\hat{b} = A\hat{x}$, then
            \begin{align*}
                \hat{b} = A\hat{x} = A\hat{x} - b + b = b - b + A\hat{x} = b - (b - A\hat{x}) = b - \hat{r}
            .\end{align*}
            Note that in this cases, $\hat{A} = A$. We can quantify the change in $b$ by observing that since $\hat{b} = b + \delta b$, and $\hat{b} = b - \hat{r}$, we have
            \begin{align*}
                b - \hat{r} &= b + \delta b\\
                \implies -\hat{r} &= \delta b 
            .\end{align*}
            \begin{align*}
                \frac{\norm{\delta x}}{\norm{x}} \leq \kappa \frac{\norm{\delta  b}}{\norm{b}}
            .\end{align*}
        \item \textbf{Condition number}: We wish to find an upper bound for the relative error in $x$, $\frac{\norm{\delta  x}}{\norm{ x}} $. We have the two systems,
            \begin{align*}
                Ax = b, \quad A\hat{x} = \hat{b}
            .\end{align*}
            Thus, we have
            \begin{align*}
                Ax &= b\; \tag{1}, \\
                A(x + \delta  x) &= b + \delta  b \; \tag{2}
            .\end{align*}
            Looking at $(1)$, we see
            \begin{align*}
                Ax &= b \implies \norm{b} = \norm{Ax}
            .\end{align*}
            But, by the Cauchy Schwarz inequality, $\norm{b} \leq \norm{A}\norm{x}$. So,
            \begin{align*}
                \norm{b} \leq \norm{A}\norm{x} \tag{1}
            .\end{align*}
            Looking at $(2) $, we see
            \begin{align*}
                A(x + \delta  x) &= b + \delta  b \\
                \implies Ax + A \delta x &= b + \delta  b \\
                \implies A \delta  x &= \delta  b  \\
                \implies \delta x &= A^{-1} \delta  b  \\
                \implies \norm{\delta  x} &= \norm{A^{-1} \delta  b} \\
                \implies \norm{ \delta  x} &\leq \norm{A^{-1}} \norm{\delta  b} \tag{2}
            .\end{align*}
            Notice that we can setup $(1)$ so that dividing $(2)$ by $(1)$ gives the relative error in $x$ on the left, and relative error of $b$ on the right. So,
            \begin{align*}
                \frac{1}{\norm{x}} \leq \frac{\norm{A}}{\norm{b}}
            .\end{align*}
            Now, we divide $(2)$ by $(1)$, we have
            \begin{align*}
                \frac{\norm{\delta x}}{\norm{x}} \leq \norm{A^{-1}}\norm{A} \frac{\norm{ \delta  b}}{\norm{b}}
            .\end{align*}
            We know have the relative error in the numerical solution bounded above by the relative error of $b$ times some constant $\norm{A^{-1}}\norm{A}$, we call this constant the condition number $\kappa(A)$. That is,
            \begin{align*}
                \kappa(A) = \norm{A^{-1}}\norm{A}
            .\end{align*}
            The condition number of a matrix $A$ measures how sensitive the solution of a linear system $A x = b$ is to small changes in $b$ (or in $A$).
            \bigbreak \noindent 
            We see that as $\kappa(A) \to \infty$, the relative error in $x$ grows without bound.
        \item \textbf{Properties of the condition number}: Let $A$ be a matrix, and $\kappa(A)$ be the condition number that measures the system $Ax = b$. The following two properties hold
            \begin{enumerate}
                \item $\kappa(A) \geq 1$
                \item $\kappa(I) = 1$
                \item $\kappa(A) = \kappa(A^{-1}) $
            \end{enumerate}
            \bigbreak \noindent 
            \textbf{\textit{Proof (1)}}: $\kappa(A) = \norm{A^{-1}}\norm{A}$. By Cauchy Schwarz,
            \begin{align*}
                \norm{A^{-1}A} \leq \norm{A^{-1}} \norm{A} \\
                \implies \norm{I} \leq \norm{A^{-1}}\norm{A} \\
                \implies 1 \leq \norm{A^{-1}}\norm{A} = \kappa(A) 
            .\end{align*}
            $\endpf $
            \bigbreak \noindent 
            \textbf{\textit{Proof (2)}}:
            \begin{align*}
                \kappa(I) = \norm{I^{-1}}\norm{I} = \norm{I} \norm{I} = 1 \cdot  1 = 1
            .\end{align*}
            $\endpf$
            \bigbreak \noindent 
            \textbf{\textit{Proof (3)}}:
            \begin{align*}
                \kappa(A^{-1}) = \norm{\left(A^{-1}\right)^{-1}}\norm{A^{-1}} = \norm{A^{-1}} \norm{A} = \kappa(A)
            .\end{align*}
            $\endpf$
        \item \textbf{Theorem \textit{(Relative Error Bound I)}}: Let $A$ be non-singular, $b \ne 0$, and $Ax = b$. If $A(x + \delta  x) = b + \delta  b$, then
            \begin{align*}
                \frac{\norm{ \delta  x}}{\norm{ x}} \leq \kappa(A) \frac{\norm{\delta  b }}{\norm{ b}}
            \end{align*}
            where $\kappa(A) = \norm{A^{-1}}\norm{A}$.
            \bigbreak \noindent 
            \textbf{Consequences.}
            \begin{enumerate}
                \item If $\kappa(A)$ small, the relative error in $x$ is small.
                \item If $\kappa(A)$ is large, then it is possible to have the relative error in $b$ small, but the relative error in $x$ large.
            \end{enumerate}
        \item \textbf{Well-conditioned and ill-conditioned in terms of $\kappa(A)$}: If $\kappa(A)$ is large, then $(P)$ is ill-conditioned. If $\kappa(A)$ is small (close to one), then $(P)$ is well-conditioned.
        \item \textbf{Pre-conditioned system (preconditioner)}: If the system
            \begin{align*}
                Ax =b
            .\end{align*}
            has a large condition number $\kappa(A)$, the system is ill-conditioned, meaning small perturbations in $b$ cause large changes in $x$.
            \bigbreak \noindent 
            To improve this, we introduce a preconditioner $B$ (sometimes written $M^{-1}$) such that $B \approx A^{-1}$ but is much easier to compute or apply.
            \bigbreak \noindent 
            We then multiply the equation by $B$ on the left
            \begin{align*}
                BAx = Bb
            .\end{align*}
            Define
            \begin{align*}
                \tilde{A} = BA, \quad \tilde{b} = Bb
            .\end{align*}
            This gives the \textbf{preconditioned system}
            \begin{align*}
                \tilde{A}x = \tilde{b}
            .\end{align*}
            We want to choose $B$ so that
            \begin{enumerate}
                \item $\kappa(\tilde{A}) = \kappa(BA) \ll \kappa(A) $, i.e., the new system is better conditioned,
                \item Solving $Bz = y $ is cheap 
            \end{enumerate}
            We don't usually form $BA$ explicitly, it usually destroys the structure and sparsity that make the original system efficient to handle.
            \bigbreak \noindent 
            If $A$ and $B$ are large sparse matrices (which they almost always are in practice), explicitly multiplying them gives a dense matrix $BA$
            \bigbreak \noindent 
            That means:
            \begin{enumerate}
                \item much higher memory usage,
                \item much slower matrix–vector products,
                \item and loss of efficiency in iterative methods.
            \end{enumerate}
            Forming $BA$ explicitly also introduces round-off errors, especially when $B$ approximates $A^{-1}$ You end up multiplying two ill-conditioned matrices, potentially worsening accuracy before solving anything.
            \bigbreak \noindent 
            If:
            \begin{itemize}
                \item $A$ is \textbf{moderate in size} (not huge, maybe $n \lesssim 10^{3}$),
                \item $A$ and $B$ are \textbf{dense} anyway (so sparsity isn’t being destroyed), and
                \item you can \textbf{compute or approximate $B$} reliably and cheaply,
            \end{itemize}
            then \textbf{explicitly forming} $BA$ may sometimes be beneficial.
            \bigbreak \noindent 
            For example:
            \begin{itemize}
                \item in small to medium dense problems (common in computational linear algebra, not large PDEs),
                \item or when using \textbf{direct solvers} (like LU or QR), where forming $BA$ once is acceptable,
                \item or when $B$ comes from a \textbf{stabilizing transformation} (e.g., scaling rows or columns).
            \end{itemize}
            In such cases, if $BA$ has a much smaller condition number than $A$,
            \[
                \kappa(BA) \ll \kappa(A),
            \]
            then solving 
            \[
                BAx = Bb
            \]
            can indeed give a \textbf{more accurate and stable solution} than directly solving 
            \[
                Ax = b.
            \]
            Usually, in \textbf{large or iterative} problems:
            \begin{itemize}
                \item Forming $BA$ destroys \textbf{structure} (sparsity, bandedness, symmetry).
                \item You have to store an entire new matrix (cost $O(n^2)$ memory).
                \item The cost of computing $BA$ can exceed the cost of solving $Ax = b$ itself.
                \item Rounding errors during multiplication can \textbf{degrade} the benefits of preconditioning.
            \end{itemize}
            In these cases, applying $B$ as an \textbf{operator} (by solving $Bz = y$ inside each iteration)
            gives the same effect with \textbf{less cost and better numerical behavior}.
            \bigbreak \noindent 
            If we let $B = A^{-1}$, then the system becomes
            \begin{align*}
                Ix = A^{-1}b
            \end{align*}
            where  $\tilde{A} = A^{-1}A = I$, $\tilde{b} = A^{-1}b = x$, and $\kappa(\tilde{A}) = \kappa(I) = 1$. If $\tilde{A} = I$, then
            \begin{enumerate}
                \item \textbf{The condition number is ideal:}
                    \[
                        \kappa(\tilde{A}) = \kappa(I) = 1.
                    \]
                    This is the \textit{best possible conditioning}—there is no amplification of errors at all.
                \item \textbf{The system becomes trivial:}
                    \[
                        I x = x = \tilde{b}.
                    \]
                    You have effectively solved the problem in one step.
                    \bigbreak \noindent 
                \item \textbf{Interpretation:}
                    \begin{itemize}
                        \item $B = A^{-1}$ is the \textit{perfect preconditioner}.
                        \item In practice, we cannot use it, because computing $A^{-1}$ explicitly 
                            is just as expensive (and less stable) than solving $A x = b$ directly.
                    \end{itemize}
                \item \textbf{Practical takeaway:} 
                    Preconditioning aims to \textbf{approximate this ideal case}:
                    \[
                        B \approx A^{-1},
                    \]
                    such that
                    \[
                        B A \approx I, \qquad \text{and hence} \qquad \kappa(B A) \approx 1,
                    \]
                    but without the full cost of inverting $A$.
            \end{enumerate}
            When we say
            \[
                \kappa(A) \gg 1,
            \]
            we mean the \textbf{mathematical problem} $A x = b$ is \textit{ill-conditioned}—small perturbations in $b$ cause large changes in $x$.
            \bigbreak \noindent 
            If we form
            \[
                \tilde{A} = A^{-1} A = I,
            \]
            then the \textbf{preconditioned system} has
            \[
                \kappa(\tilde{A}) = 1,
            \]
            so that transformed system is \textit{perfectly conditioned}. However, to achieve this we would have to 
            \textbf{compute $A^{-1}$ numerically}, and that step is where the \textit{instability} arises.
        \item \textbf{Numerical stability vs conditioning}
            \begin{itemize}
                \item \textbf{Conditioning}
                \begin{itemize}
                    \item A property of the \textbf{mathematical problem} itself.
                    \item Measures the \textbf{sensitivity} of the true solution to small input changes.
                    \item Example: if $A x = b$, then
                        \[
                            \frac{\|\delta x\|}{\|x\|} \le \kappa(A) \frac{\|\delta b\|}{\|b\|}.
                        \]
                        A large $\kappa(A)$ indicates an \textbf{ill-conditioned problem}.
                \end{itemize}
            \item \textbf{Numerical Stability}
                \begin{itemize}
                    \item A property of the \textbf{algorithm} used to solve the problem.
                    \item Measures how much \textbf{round-off and truncation errors} the algorithm introduces or amplifies.
                    \item A \textbf{stable algorithm} gives the exact solution to a \textit{nearby problem}:
                        \[
                            (A + \delta A)\hat{x} = b + \delta  b , \quad \|\delta A\|,\; \| \delta b\| \text{ small.}
                        \]
                \end{itemize}
        \end{itemize}
        \textbf{Note:} If an algorithm is \textbf{numerically unstable}, then the perturbations 
        $\delta A$ and/or $\delta b$ required to explain its result might be \textbf{large}:
        \[
            \|(A + \delta A) - A\| \text{ is not small.}
        \]
        Hence, the computed $\hat{x}$ is the exact solution to a \textit{far-away problem}:
        \[
            (A + \delta A)\hat{x} = b + \delta b,
        \]
        where $\|\delta A\|$ or $\|\delta b\|$ are no longer small compared to 
        $\|A\|$ or $\|b\|$.
        \bigbreak \noindent 
        This means the algorithm’s result may not correspond meaningfully to the 
        original system at all.


    \end{itemize}


    \pagebreak 
    \subsection{Chapter 1 Proofs}
    \bigbreak \noindent 
    \subsubsection{Positive definite (1)}
    \begin{itemize}
        \item \textbf{Theorem n.1.1}: Let $A\in \mathbb{R}^{n\times n}$, $ A = \begin{bmatrix} A_{11} & A_{12} \\ A_{21} & A_{22} \end{bmatrix} $ be a positive definite matrix. Then $A_{11}, \; A_{22}$ are positive definite.
            \bigbreak \noindent 
            Note that $A_{11} \in \mathbb{R}^{n_{1} \times n_{1}}$, $A_{12} \in \mathbb{R}^{n_{1} \times n_{2}}$, $A_{21} \in \mathbb{R}^{n_{2} \times n_{1}}$, and $A_{22} \in \mathbb{R}^{n_{2} \times n_{2}}$. So, $n = n_{1} + n_{2} $
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} Assume $A \in \mathbb{R}^{n\times n}$, $A = \begin{bmatrix} A_{11} & A_{12} \\ A_{21} & A_{22} \end{bmatrix} $, $A$ positive definite.
            \bigbreak \noindent
            First, we show that $A_{11} = A_{11}^{\top}$, and $A_{22} = A_{22}^{\top}$. Since $A$ p.d, 
            \begin{align*}
                A = A^{\top} \implies \begin{bmatrix} A_{11} & A_{12} \\ A_{21} & A_{22} \end{bmatrix} = \begin{bmatrix} A_{11}^{\top} & A_{21}^{\top} \\ A_{12}^{\top} & A_{22}^{\top} \end{bmatrix}
            .\end{align*}
            So, we see that
            \begin{align*}
                A_{11} &= A_{11}^{\top}, \\
                A_{22} &= A_{22}^{\top}
            .\end{align*}
            \bigbreak \noindent 
            Next, we show that $x^{\top}A_{11}x >0$, for $x \in \mathbb{R}^{n_{1}}$, $x\ne 0$ and $x^{\top}A_{22} x > 0$ for $x \in \mathbb{R}^{n_{2}} $, $x \ne 0 $
            \bigbreak \noindent 
            Let $ \bar{x} \in \mathbb{R}^{n_{1}}$, $ \bar{x} = \begin{pmatrix} x_{1} \\ 0 \end{pmatrix} $, $x_{1} \ne 0 $. Note that $ \bar{x}^{\top} = \begin{pmatrix} x_{1}^{\top} & 0\end{pmatrix} $. We observe
            \begin{align*}
                0 < \bar{x}^{\top} A x &= \begin{pmatrix} x_{1}^{\top} & 0 \end{pmatrix} \begin{bmatrix} A_{11} & A_{12} \\ A_{21} & A_{22} \end{bmatrix} \begin{pmatrix} x_{1} \\ 0 \end{pmatrix} \\
                    &= x_{1}^{\top}A_{11}x_{1} > 0
            .\end{align*}
            Similarly, $ \bar{x} \in \mathbb{R}^{n}$, $ \bar{x} = \begin{pmatrix} 0 \\ x_{2} \end{pmatrix} $, $ x_{2} \ne 0 \in \mathbb{R}^{n_{2}}$ reveals $x_{2}^{\top}A_{22}x_{2} > 0$.
            \bigbreak \noindent 
            Therefore, $A_{11},\; A_{22}$ are positive definite. $\endpf$
        \item \textbf{Theorem n.1.2}: Let $A \in \mathbb{R}^{n\times n}$ be a positive definite matrix, then $a_{ii} > 0$ for $i = 1,2,...,n $
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} Assume that $A \in \mathbb{R}^{n\times n}$ is a positive definite matrix. Define $e_{i}$ as the set of vectors with all zeros except for a one at the $i^{\text{th}}$ position. Since $A$ is positive definite,
            \begin{align}
                e_{i}^{\top} A e_{i} = a_{ii} > 0
            \end{align}
            for $i = 1,2,...,n$ $ \endpf $
            \bigbreak \noindent 
        \item \textbf{Theorem n.1.3}: Let $A \in \mathbb{R}^{n \times n}$ be a positive definite matrix and $X \in \mathbb{R}^{n\times n}$ be non-singular. Then, $B = X^{\top}AX$ is positive definite.
            \bigbreak \noindent 
            \begin{remark}
                If $A,B,C$ are matrices, then     
                \begin{align*}
                    (ABC)^{\top} = C^{\top}B^{\top}A^{\top}
                \end{align*}
            \end{remark}
            \textbf{\textit{Proof.}} Assume that $A \in \mathbb{R}^{n\times n}$ is positive definite, $X\in \mathbb{R}^{n\times n}$ non-singular, and $B = X^{\top}AX$. 
            \bigbreak \noindent 
            First, we show that $B = B^{\top}$. We have
            \begin{align*}
                B^{\top} &= (X^{\top}AX)^{\top} = X^{\top}A^{\top}(X^{\top})^{\top} \\
                &= X^{\top}A^{\top}X
            .\end{align*}
            But, $A$ is positive definite, and is therefore symmetric. So,
            \begin{align*}
                B^{\top} &= X^{\top}AX = B
            .\end{align*}
            Thus, $B$ is symmetric.
            \bigbreak \noindent 
            Next, we show that $x^{\top}Bx >0$, for $x \in \mathbb{R}^{n}$, $x\ne 0$. We have
            \begin{align*}
                x^{\top}Bx &= x^{\top}(X^{\top}AX)x = (x^{\top}X^{\top})A(Xx) \\
                &= (Xx)^{\top}A(Xx)
            \end{align*}
            Let $y = Xx$. Thus,
            \begin{align*}
                (Xx)^{\top}A(Xx) = y^{\top}Ay
            \end{align*}
            \bigbreak \noindent 
            Note that since $ x \ne 0 $, and $X$ non-singular, $y = Xx \ne 0$. Since $y \ne 0$, and $A$ p.d, $y^{\top}Ay >0$. 
            \bigbreak \noindent 
            Therefore, $B$ is positive definite. $\endpf $
        \item \textbf{Theorem n.1.4}: Let $A$ be positive definite. Then, $\det(A) > 0$
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} Assume $A$ is a positive definite matrix.
            \bigbreak \noindent 
            Since $A$ p.d,  $A = R^{\top}R$ for a unique upper triangular matrix $R$. Further, $r_{ii} >0$. We have
            \begin{align*}
                A &= R^{\top}R \\
                \implies \det(A) &= \det(R^{\top}R) \\
                                 &= \det(R^{\top})\det(R) \\
                                 &= \det(R)\det(R) \\
                                 &= \det(R)^{2} \\
                                 &= (r_{11}\cdot r_{12}\cdot \cdots \cdot r_{1n})^{2} \\
                                 &= r_{11}^{2}\cdot r_{12}^{2}\cdot \cdots \cdot r_{1n}^{2} > 0 \\
            .\end{align*}
            Therefore $\det(A)  > 0$ $\endpf $




    \end{itemize}


    \pagebreak 
    \subsection{Chapter 1}
    \bigbreak \noindent 
    \subsubsection{Definitions}
    \begin{itemize}
        \item \textbf{Matrix multiplication}: If $A$ is an $n \times m$ matrix, and $X$ is $m \times p$, we can form the product $B = AX$, which is $n \times p$. The $(i,j)$ entry of $B$ is
        \[
            b_{ij} = \sum_{k=1}^{m} a_{ik} x_{kj}.
        \]
    \item \textbf{Triangular matrix}:
        A matrix $G = (g_{ij})$ is \textit{lower triangular} if $g_{ij} = 0$ whenever $i < j$. 
        Thus a lower-triangular matrix has the form
        \[
            G =
            \begin{bmatrix}
                g_{11} & 0      & 0      & \cdots & 0 \\
                g_{21} & g_{22} & 0      & \cdots & 0 \\
                g_{31} & g_{32} & g_{33} & \cdots & \vdots \\
                \vdots & \vdots & \vdots & \ddots & 0 \\
                g_{n1} & g_{n2} & g_{n3} & \cdots & g_{nn}
            \end{bmatrix}.
        \]
        Similarly, an \textit{upper triangular} matrix is one for which $g_{ij} = 0$ whenever $i > j$. 
        A \textit{triangular} matrix is one that is either upper or lower triangular.
    \item \textbf{Positive definite matrix}: A square matrix $A$ is positive definite provided it satisfies
        \begin{enumerate}
            \item $A = A^{\top} $
            \item $x^{\top}Ax > 0$ for all $x\in \mathbb{R}^{n}$, $x\ne 0$
        \end{enumerate}
    \item \textbf{Column envelope}: The envelope of a sparse matrix is the set of positions around the diagonal that “must be included” when storing or working with the matrix in a compressed way.
        \bigbreak \noindent 
        It essentially captures the profile of where the nonzeros start (or stop) in each column.
        \bigbreak \noindent 
        Let $A  = (a_{ij})$. Define for each column $j$
        \begin{align*}
            q(j) = \text{min}\{i:\ a_{ij} \ne 0\}
        \end{align*}
        I.e the first nonzero entry in column $j$. Then, the column envelope of column $j$ is 
        \begin{align*}
            \{(i,j):\ q(j) \leq i \leq j\}
        \end{align*}
        So, for column $j$, you start at the first nonzero entry $q(j)$ and include all positions down to the diagonal ( $i=j$), whether or not some of them are explicitly zero.
        \bigbreak \noindent 
        The column envelope of the matrix $A$ is the union over all columns:
        \begin{align*}
            \text{colenv}\{A\} = \bigcup_{j=1}^{n}\{(i,j):\ q(j) \leq i \leq j\}
        \end{align*}
        with 
        \begin{align*}
            q(j) = \text{min}\{i:\ a_{ij} \ne 0\}
        \end{align*}
    \item \textbf{Row envelope}: For each row $i$ find the first nonzero entry
        \begin{align*}
            p(i) = \text{min}\{j:\ a_{ij} \ne 0\}.
        \end{align*}
        Then, the row envelope of row $i$ is 
        \begin{align*}
            \{(i,j):\ p(i) \leq j \leq i\}
        \end{align*}
        So, the row envelope is in the lower triangular part ($j \leq i $)
        \bigbreak \noindent 
        The row envelope of $A$ is then
        \begin{align*}
            \text{rowenv}\{A\} = \bigcup_{i=1}^{m}\{(i,j):\ p(i) \leq j \leq i\}
        \end{align*}
    \item \textbf{Envelope}: The envelope is the union of both envelopes. That is,
        \begin{align*}
            \text{env}\{A\} = &\text{rowenv}\{A\} = \bigcup_{i=1}^{m}\{(i,j):\ p(i) \leq j \leq i\}  \\
            \cup &\text{colenv}\{A\} = \bigcup_{j=1}^{n}\{(i,j):\ q(j) \leq i \leq j\}
        \end{align*}
    \item \textbf{Elementary operations on systems}:
        \begin{enumerate}
            \item Interchange rows.
            \item Multiply an equation by a nonzero constant.
            \item Add a multiple of one equation to another equation.
        \end{enumerate}

    \item \textbf{Transpose of block matrices}: Let $A \in \mathbb{R}^{n\times n}$, with
        \begin{align*}
            A  = \begin{bmatrix} A_{11} & a_{12} \\ A_{21} & a_{22} \end{bmatrix}
        .\end{align*}
        Then, 
        \begin{align*}
            A^{\top} = \begin{bmatrix}
                A_{11}^{\top} & A_{21}^{\top} \\ A_{12}^{\top} & A_{22}^{\top}
            \end{bmatrix}
        \end{align*}
    \item \textbf{Transpose of a block vector}: Similarly, if $ x \in \mathbb{R}^{n}$ is decomposed into blocks
        \begin{align*}
            \begin{pmatrix} x_{1} \\ x_{2} \end{pmatrix}
        ,\end{align*}
        with $x_{1} \in \mathbb{R}^{n_{1}} ,\; x_{2} \in \mathbb{R}^{n_{2}},\; n = n_{1} + n_{2}$, then
        \begin{align*}
            x^{\top} = \begin{pmatrix} x_{1}^{\top} & x_{2}^{\top} \end{pmatrix}
        \end{align*}

    \item \textbf{Nonsingular matrix}: A nonsingular matrix is a matrix that has an inverse
    \item \textbf{Singular matrix}: A singular matrix is a matrix that does not have an inverse
            \item \textbf{Positive definite matrix}: A matrix $A$ is \textbf{positive definite} provided that the following two conditions are satisfied
            \begin{enumerate}
                \item $A$ is symmetric. That is, $A = A^{\top} $
                \item $x^{\top}Ax > 0 $ for all $x\ne 0$
            \end{enumerate}

        \item \textbf{Cholesky decomposition and the Cholesky Factor}: Let $A \in \mathbb{R}^{n\times n}$ be p.d, then $A = R^{\top}R$ where $R$ is upper triangular with $r_{ii} > 0$. The matrix $R$ is called the \textbf{Cholesky factor}.
            \bigbreak \noindent 
            If $A = R^{\top}R$, then $Ax = b$ can be written as 
            \begin{align*}
                R^{\top}Rx = b
            \end{align*}
            where
            \begin{align*}
                \begin{cases}
                        Rx &= y \quad \text{(Lower triangular)}              \\
                        R^{\top}y &= b \quad \text{(Upper triangular)}
                \end{cases}
            \end{align*}
            and since these new systems are triangular, they can be solved quickly with forward or backward substitution.
        \item \textbf{Banded matrix}: A banded matrix is a sparse matrix whose nonzero entries are confined to a diagonal band, consisting of the main diagonal and a fixed number of diagonals on either side of it.
            \bigbreak \noindent 
                Let $A \in \mathbb{R}^{m \times n}$.  
                Then $A$ is called a \textbf{banded matrix} if there exist nonnegative integers $p, q$ (called the \emph{lower} and \emph{upper bandwidths}) such that
                \[
                    a_{ij} = 0 \quad \text{whenever } i - j > p \text{ or } j - i > q.
                \]
                \begin{itemize}
                    \item The \emph{lower bandwidth} $p$ is the number of subdiagonals (below the main diagonal) that may contain nonzero entries.
                    \item The \emph{upper bandwidth} $q$ is the number of superdiagonals (above the main diagonal) that may contain nonzero entries.
                \end{itemize}
                The \emph{total bandwidth} is sometimes defined as $p + q + 1$, counting the main diagonal as well.
        \item \textbf{$LU$ decomposition}: Consider a matrix $A \in \mathbb{R}^{n\times n} $. If we can factor $A$ as $A = LU$, for $L$ lower triangular, $U$ upper triangular, then the system $Ax = b$, for vectors $x,b \in \mathbb{R}^{n}$ turns into
            \begin{align*}
                LUx = b
            .\end{align*}
            We can then split this system as follows
            \begin{align*}
                \begin{cases}
                    Ly &= b \\
                    Ux &= y
                \end{cases}
            \end{align*}
            First, we solve $Ly = b$ with forward substitution to find $y$. We can then solve $Ux = y$ with backward substitution to find the target $x$.
        \item \textbf{More definiteness}: Let $A \in \mathbb{R}^{n\times n}$ be symmetric. Then,
            \begin{itemize}
                \item $A$ is \textbf{positive semidefinite} if $x^{\top}Ax \geq 0 $ for all $x\in \mathbb{R}^{n} $
                \item $A$ is negative definite if $x^{\top}Ax < 0 $ for all nonzero $x\in \mathbb{R}^{n}$
                \item $A$ is negative semidefinite if $x^{\top}Ax \leq 0$ for all $x\in \mathbb{R}^{n} $
            \end{itemize}
        \item \textbf{Definiteness notation}: Let $A\in \mathbb{R}^{n\times n}$ by symmetric.
            \begin{itemize}
                \item $A \succ 0$ means $A$ is positive definite.        
                \item $A \succeq 0$ means $A$ is positive semidefinite
                \item $A \prec 0$ means $A$ is negative definite
                \item $A \preceq 0$ means $A$ is negative semidefinite
            \end{itemize}

        \item \textbf{Ill-conditioned problem}: A problem $P$ is \textbf{ill-conditioned} if tiny variations in the information of $P$ leads to large variations of the solution of $P$
            \bigbreak \noindent 
            Consider a linear system. If $A,b$ is the information of the problem, then for a slightly perturbed $A$ and $b$, call them $\bar{A},\bar{b}$, where $\bar{A} - A$ and $\bar{b} - b$ tiny, an ill-conditioned problem will have 
            \begin{align*}
                \bar{x} - x
            \end{align*}
            large, if $x$ is the solution to $Ax = b$, and $\bar{x}$ is the solution to the system with $\bar{A}$ and $\bar{b}$.
            \bigbreak \noindent 
            If the problem is \textbf{well-conditioned}, $\bar{x} -x$ is also tiny.
        \item \textbf{Permutation matrix}: A permutation matrix is a special kind of square matrix that represents a permutation of elements. Formally:
            It is obtained from the identity matrix by rearranging its rows (or equivalently, its columns).
            \bigbreak \noindent 
            Each row and each column has exactly one entry equal to 1, and all other entries are 0.
            \bigbreak \noindent 
            Multiplying a vector (or another matrix) by a permutation matrix reorders its entries.
            \bigbreak \noindent 
            Suppose $P$ is formed by taking $I$ and interchanging rows one and two. Then,
            \begin{align*}
                P = \begin{pmatrix} 0 & 1 & 0 \\ 1 & 0 & 0 \\ 0 & 0 & 1 \end{pmatrix}
            .\end{align*}
            Then, 
            \begin{align*}
                P\begin{pmatrix} a \\ b \\ c \end{pmatrix} = \begin{pmatrix} b \\ a \\ c \end{pmatrix}
            .\end{align*}
            So, it swaps the first two entries.


    \end{itemize}

    \pagebreak 
    \bigbreak \noindent 
    \subsubsection{Properties}
    \begin{itemize}
        \item \textbf{Properties of a nonsingular matrix}:
            The following are equivalent, if any one holds, they all hold
            \begin{itemize}
                \item $Ax = b$ has a unique solution
                \item $\det(A)\ne 0$
                \item $A^{-1}$ exists
                \item There is no nonzero vector $y \in \mathbb{R}^{m}$ such that $Ay=0 $
                \item The columns of $A$ are linearly independent
                \item The rows of $A$ are linearly independent
                \item Given any vector $b$, there is exactly one vector $x$ such that $Ax=b$
            \end{itemize}
            If any one of the following are true, they all are true, and $A$ is non-singular
        \item \textbf{More properties of non-singular matrices}:
            \begin{enumerate}
                \item The product of non-singular matrices is non-singular.
                \item The inverse of a non-singular matrix is non-singular (obvious)
                \item The sum of two non-singular matrices \textbf{may not be} non-singular
                \item A non-singular matrix scaled by a nonzero scalar is non-singular
            \end{enumerate}
        \item \textbf{Triangular matrices}: 
            Triangular matrices are invariant under multiplication, transposition, and inversion
            \begin{itemize}
                \item Upper triangular $\times$ upper triangular = upper triangular
                \item Lower triangular $\times$ lower triangular = lower triangular
                \item The transpose of an upper triangular matrix is a lower triangular matrix
                \item The transpose of an lower triangular matrix is a upper triangular matrix
                \item The inverse of a lower triangular matrix is lower triangular, and the inverse of an upper triangular matrix is upper triangular
            \end{itemize}
        \item \textbf{Properties of positive definite (p.d) matrices}:
            \begin{enumerate}
                \item If $A$ is p.d then $A$ is \textit{non-singular}
                    \bigbreak \noindent 
                    \textbf{Note:} Since $A$ is non-singular there is no $y \in \mathbb{R}^{n}$, $y\ne 0$ such that $Ay = 0$
                \item If $A = M^{\top}M$ for some $M$ non-singular than $A$ is p.d
                \item If $A$ is p.d than $\det(A) > 0$
                \item If $A$ is p.d then all principal submatrices are p.d
                \item If $A$ is p.d then $a_{ii}>0$ for $i=1,2,...,n$. So, if any $a_{ii} \leq 0$, $A$ is not p.d.
                \item $A$ is p.d if and only if all leading principal minors are positive
                \item $A$ is p.d if and only if there exists a unique upper triangular matrix $R$ such that $A = R^{\top}R$ (Cholesky factorization described below)
                \item $A$ is p.d if and only if all eigenvalues of $A$ are positive
                    \bigbreak \noindent 
                    Recall that $\lambda$ is an eigenvalue of $A$ if there exists $x_{\lambda} \ne 0$ such that $Ax_{\lambda}  = \lambda x_{\lambda}$
            \end{enumerate}
            \textbf{Note:} Property two is a key property.
        \item \textbf{Cholesky factor $R$ in a diagonal matrix}: If $A = D = \begin{bmatrix}
                a_{11} & 0 & \cdots & 0 \\
                0 & a_{22} & \cdots & 0 \\
                \vdots & \vdots & \ddots & \vdots \\
                0 & 0 & \cdots & a_{nn} 
        \end{bmatrix}$ then
        \begin{align*}
            R = \begin{bmatrix}
                \sqrt{a_{11}} & 0 & \cdots & 0 \\
                0 & \sqrt{a_{22}} & \cdots & 0 \\
                \vdots & \vdots & \ddots & \vdots \\
                0 & 0 & \cdots & \sqrt{a_{nn}}
            \end{bmatrix}
        \end{align*}
    \item \textbf{$LU$ factorization in a symmetric matrix}: 
    \item \textbf{Properties of a permutation matrix}:
        \begin{enumerate}
            \item \textbf{Orthogonal:} $P^{T} = P^{-1} $
            \item \textbf{Determinant:}: $\det(P)=\pm 1$, depending on whether the permutation is even or odd.
            \item \textbf{Action on vectors:}: $Px$ permutes the coordinates of $x$
            \item \textbf{Action on matrices:} Left multiplication permutes rows; right multiplication permutes columns.
        \end{enumerate}



    \end{itemize}

    \pagebreak \bigbreak \noindent 
    \subsubsection{Theorems}
    \begin{itemize}
        \item \textbf{Theorem 1.1.19}: 
            Let $A \in \mathbb{R}^{m\times n}$ ,$X\in \mathbb{n\times p}$ and $B \in \mathbb{R}^{m \times p}$ be partitioned as follows
            \begin{align*}
                A &= 
                \begin{blockarray}{ccc} & m_{1} & m_{2} \\ 
                    \begin{block}{c[cc]} 
                        n_{1} & A_{11} & A_{12} \\ 
                        n_{2} & A_{21} & A_{22} 
                    \end{block} 
                \end{blockarray},\quad 
                \begin{cases}
                    m_{1} + m_{2} = m \\
                    n_{1} + n_{2} = n
                \end{cases} \\
                    X &= 
                \begin{blockarray}{ccc} 
                    & p_{1} & p_{2} \\ 
                    \begin{block}{c[cc]} 
                        n_{1} & X_{11} & X_{12} \\ 
                        n_{2} & X_{21} & X_{22} 
                    \end{block} 
                \end{blockarray}, 
                \quad \begin{cases} 
                    p_{1} + p_{2} = p \\ 
                    n_{1} + n_{2} = n 
                \end{cases} \\
                    B &= 
                \begin{blockarray}{ccc} & p_{1} & p_{2} \\ 
                    \begin{block}{c[cc]} m_{1} & B_{11} & B_{12} \\ 
                        m_{2} & B_{21} & B_{22} 
                    \end{block} 
                \end{blockarray}, 
                \quad \begin{cases} 
                        p_{1} + p_{2} = p \\ 
                        m_{1} + m_{2} = m 
                    \end{cases} 
            \end{align*}
            Then $AX = B$ if and only if 
            \begin{align*}
                A_{i1}X_{1j} = B_{ij} \quad \text{ for } i,j=1,2
            \end{align*}
        \item \textbf{Theorem 1.1.24}:  
            Make a finer partition of $A$ into $r$ block rows and $s$ block columns.
            \[
                A =
                \begin{bmatrix}
                    A_{11} & \cdots & A_{1s} \\
                    \vdots & \ddots & \vdots \\
                    A_{r1} & \cdots & A_{rs}
                \end{bmatrix}, \quad
                \begin{array}{l}
                    n_1 + \cdots + n_r = n \\
                    m_1 + \cdots + m_s = m
                \end{array}
            \]
            Then partition $X$ \emph{conformably} with $A$; that is, make the block row structure of
            $X$ identical to the block column structure of $A$.

            \[
                X =
                \begin{bmatrix}
                    X_{11} & \cdots & X_{1t} \\
                    \vdots & \ddots & \vdots \\
                    X_{s1} & \cdots & X_{st}
                \end{bmatrix}, \quad
                \begin{array}{l}
                    m_1 + \cdots + m_s = m \\
                    p_1 + \cdots + p_t = p
                \end{array}
            \]
            Finally, partition the product $B$ conformably with both $A$ and $X$.
            \[
                B =
                \begin{bmatrix}
                    B_{11} & \cdots & B_{1t} \\
                    \vdots & \ddots & \vdots \\
                    B_{r1} & \cdots & B_{rt}
                \end{bmatrix}, \quad
                \begin{array}{l}
                    n_1 + \cdots + n_r = n \\
                    p_1 + \cdots + p_t = p
                \end{array}
            \]
            \bigbreak \noindent 
            \textbf{Theorem}: Let $A,X$ and $B$ be partitioned like they are above. Then, $AX = B$ if and only if 
            \begin{align*}
                B_{ij} = \sum_{k=1}^{s}A_{ik}X_{kj} \quad i=1,...,r,\; j=1,...,t
            \end{align*}
        \item \textbf{Theorem 1.2.3}: Let $A$ be a square matrix. The following six conditions are equivalent; 
            that is, if any one holds, they all hold.
            \begin{enumerate}[label=(\alph*)]
                \item $A^{-1}$ exists.
                \item There is no nonzero $y$ such that $Ay = 0$.
                \item The columns of $A$ are linearly independent.
                \item The rows of $A$ are linearly independent.
                \item $\det(A) \neq 0$.
                \item Given any vector $b$, there is exactly one vector $x$ such that $Ax = b$.
            \end{enumerate}
        \item \textbf{Theorem 1.3.1}: Let $G$ be a triangular matrix. Then $G$ is \textbf{nonsingular} if and only if
            $g_{ij} \ne 0$ for $i=1,...,n $
        \item \textbf{Theorem 1.4.2}: If $A$ is positive definite, then $A$ is nonsingular
        \item \textbf{Corollary 1.4.3}: If $A$ is positive definite, the linear system $Ax = b$ has exactly one solution.
        \item \textbf{Theorem 1.4.4}: Let $M$ be any $n \times n$ nonsingular matrix, and let $A = M^{\top}M$. Then $A$ is positive definite.
        \item \textbf{Theorem 1.4.7 (\textit{Cholesky Decomposition Theorem})}: Let $A$ be positive definite. Then $A$ can be decomposed in exactly one way into a product
            \begin{align*}
                A = R^{\top}R
            \end{align*}
            such that $R$ is upper triangular and has all main diagonal entries $r_{ii}$ positive. $R$ is called the Cholesky factor of $A$.
            \bigbreak \noindent 
            We have
            \begin{align*}
                r_{ii} &= +\sqrt{a_{ii} - \sum_{k=1}^{i-1}r_{ki}^{2}}, \\
                r_{ij} &= \frac{\left(a_{ij} - \sum_{k=1}^{i-1}r_{ki}r_{kj}\right)}{r_{ii}} \quad j=i+1,...,n
            \end{align*}
            \textbf{Note:} $R$ is upper triangular, so we do not have to calculate entries $r_{ij}$ for $i > j $
        \item \textbf{Theorem 1.5.7}: Let $A$ be positive definite, and let $R$ be the Cholesky factor of A. Then $R$ and $A$ have the same envelope.
        \item \textbf{Theorem}: Let $A$ be p.d, if $R$ is the Cholesky factor of $A$, then
            \begin{align*}
                \text{colenv}\{R\}  = \text{colenv}\{A\}
            \end{align*}
    \item \textbf{Theorem}: Let $A \in \mathbb{R}^{n\times n}$ be non-singular. Then, we can solve the system $Ax = b$, $b \in \mathbb{R}^{n}$ using Gaussian Elimination without row interchanges if and only if all landing principal sub-matrices of $A$ are non-singular.
    \item \textbf{Theorem}: Let $A \in \mathbb{R}^{n \times n}$. Then $A$ admits an LU factorization
            \[
                A = LU,
            \]
            where $L \in \mathbb{R}^{n \times n}$ is unit lower triangular and 
            $U \in \mathbb{R}^{n \times n}$ is upper triangular, 
            \textbf{without row interchanges}, if and only if all leading principal 
            submatrices of $A$ are nonsingular.
    \item \textbf{Theorem 1.7.19 \textit{($LU$ Decomposition Theorem)}}: Let $A$ be an $n\times n$ matrix whose leading principal submatrices are all nonsingular. Then, $A$ can be decomposed in exactly one way into a product $A = LU$ such that $L$ is unit lower triangular and $U$ is upper triangular.


    \end{itemize}

    \pagebreak \bigbreak \noindent 
    \subsubsection{Propositions}
    \begin{itemize}
        \item \textbf{Proposition 1.1.6}: If $b = Ax$, then $b$ is a linear combination of the columns of $A$.
            \bigbreak \noindent 
            If we let $A_{j}$ denote the $j$th column of $A$, we have
            \[
                b = \sum_{j=1}^{m} A_{j} x_{j}.
            \]
        \item \textbf{Proposition 1.4.24}: Cholesky's algorithm (Row-oriented inner product formals) applied to an $n \times n$ matrix performs about $\frac{n^{3}}{3} $ flops.
        \item \textbf{Proposition 1.4.51}: If $A$ is positive definite, then $a_{ii} > 0$ for $i = 1,2,..,n$
        \item  \textbf{Proposition 1.4.53}: Let $A$ be positive definite, and consider a partition
            \begin{align*}
                A = \begin{bmatrix} A_{11} & A_{12} \\ A_{21} & A_{22} \end{bmatrix}
            \end{align*}
            in which $A_{11}$ and $A_{22} $ are square. Then $A_{11}$ and $A_{22}$ are positive definite.
        \item \textbf{Proposition 1.4.55} If $A$ and $X$ are $n\times n$, $A$ is positive definite, and $X$ is nonsingular then the matrix $B = X^{\top}AX$ is also positive definite.
        \item \textbf{Proposition 1.7.1}: If $\hat{A}x = \hat{b}$ is obtained from $Ax = b$ by an elementary operation of type 1,2, or 3, then the systems $Ax = b$ and $\hat{A}x = \hat{b}$ are equivalent.
        \item \textbf{Proposition 1.7.3}:  Suppose $\hat{A}$ is obtained from $A$ by an elementary row operation of
            type 1, 2, or 3. Then $\hat{A}$ is nonsingular if and only if $A$ is.
    \end{itemize}

    \pagebreak \bigbreak \noindent 
    \subsubsection{Algorithms and complexities}
    \begin{itemize}
        \item \textbf{Matrix multiplication}: $\mathcal{O}(n^{3}) $
        \item \textbf{Row oriented forward substitution of a lower triangular matrix}:
            Consider the system
            \begin{align*}
                \begin{bmatrix}
                    \ell_{11} & 0 & \cdots & 0 \\
                    \ell_{21} & \ell_{22} & \cdots & 0 \\
                    \vdots & \vdots & \ddots & \vdots \\
                    \ell_{n1} & \ell_{n2} & \cdots & \ell_{nn}
                    \end{bmatrix} \begin{bmatrix}
                    x_{1} \\ x_{2} \\ \vdots \\ x_{n}
                \end{bmatrix}
                =
                \begin{bmatrix}
                    b_{1} \\ b_{2} \\ \vdots \\ b_{n}
                \end{bmatrix}
            .\end{align*}
            In general, we have
            \begin{align*}
                x_{i} = \frac{b_{i} - \sum_{j=1}^{i-1}\ell_{ij}x_{j}}{\ell_{ii}}
            \end{align*}
            for $i =1,2,...,n $. This method is called \textbf{Forward Substitution}.
            \bigbreak \noindent 
            The row oriented forward substitution algorithm requires $\mathcal{O}(n^{2})$ flops.
        \item \textbf{Column oriented forward substitution (recursive algorithm)}: 
            Suppose we have $Lx = b$ when $L$ is lower triangular, we split the matrix into the following blocks
            \begin{align*}
                \begin{bmatrix}
                    \ell_{11} & 0 \\
                    \hat{\ell} & \hat{L}
                \end{bmatrix}
                \begin{bmatrix}
                    x_{1} \\ \hat{x} 
                \end{bmatrix}
                = \begin{bmatrix}
                    b_{1} \\ \hat{b}
                \end{bmatrix}
            .\end{align*}
            With $\hat{\ell} \in \mathbb{R}^{n-1}$, $\hat{L} \in \mathbb{R}^{n-1 \times n-1} $, $\hat{x} \in \mathbb{R}^{n-1}$, $\ell_{11}, x_{1}, b_{1} \in \mathbb{R}$. Note that $\hat{L}$ is also lower triangular.
            \begin{enumerate}
                \item Compute $x_{1} = \frac{b_{1}}{\ell_{11}} $
                \item Compute $\hat{b} - \hat{\ell}x_{1} = \tilde{b} \in \mathbb{R}^{n-1} $
                \item Find $\hat{L}x = \tilde{b} $
                \item Run the algorithm on $\hat{L}$, $\tilde{b}$. That is, $\text{Alg}(\hat{L}, \tilde{b}) $
            \end{enumerate}
            The recursive column oriented forward substitution algorithm requires $\mathcal{O}(n^{2})$ flops.
        \item \textbf{Row oriented backward substitution of an upper triangular matrix}:
            Let $A \in \mathbb{R}^{n\times n},\ x \in \mathbb{R}^{n},\ b \in \mathbb{R}^{n}$, with
            \begin{align*}
                A = \begin{bmatrix}
                    a_{11} & a_{12} & \cdots & a_{1n} \\
                    0 & a_{22} & \cdots & a_{2n} \\
                    \vdots & \vdots & \ddots & \vdots \\
                    0 & 0 & \cdots & a_{nn}
                \end{bmatrix},  \quad \;
                x = \begin{bmatrix}
                    x_{1} \\ x_{2} \\ \vdots \\ x_{n}
                \end{bmatrix}, \quad \;
                b = \begin{bmatrix}
                    b_{1} \\ b_{2} \\ \vdots \\ b_{n}
                \end{bmatrix}
            .\end{align*}
            In general, we have that
            \begin{align*}
                x_{i} &= \frac{b_{i} - \sum_{j=i+1}^{n}a_{ij}x_{j}}{a_{ii}}, \quad i=n,n-1,...,1
            \end{align*}
            The row oriented backward substitution algorithm requires $\mathcal{O}(n^{2})$ flops.
        \item \textbf{Column-oriented backward substitution}:
            Let $U \in \mathbb{R}^{n\times n}$ be upper triangular, $x \in \mathbb{R}^{n}$, and $ b \in \mathbb{R}^{n}$ which gives the system
            \begin{align*}
                \begin{bmatrix}
                    u_{11} & u_{12} & \cdots & u_{1n} \\
                    0 & u_{22} & \cdots & u_{2n} \\
                    \vdots & \vdots & \ddots & \vdots \\
                    0 & 0 & \cdots & u_{nn}
                \end{bmatrix} 
                \begin{bmatrix}
                    x_{1} \\ x_{2} \\ \vdots \\ x_{n}
                \end{bmatrix}
                = 
                \begin{bmatrix}
                    b_{1} \\ b_{2} \\ \vdots \\ b_{n}
                \end{bmatrix}
            \end{align*}
            Split the system into the following block decomposition
            \begin{align*}
                \begin{bmatrix}
                    \hat{U} & u \\
                    0^{\top} & u_{nn}
                \end{bmatrix}
                \begin{bmatrix}
                    \hat{x} \\ x_{n}
                \end{bmatrix}
                = 
                \begin{bmatrix}
                    \hat{b} \\ b_{n}
                \end{bmatrix}
            \end{align*}
            Then,
            \begin{align*}
                \hat{U}\hat{x} + ux_{n} &= \hat{b} \implies \hat{U}\hat{x} = \hat{b} - ux_{n} = \tilde{b}, \\
                u_{nn}x_{n} &= b_{n} \implies x_{n} = \frac{b_{n}}{u_{nn}}
            \end{align*}
            Thus, the column-oriented backward substitution algorithm is defined by the following steps
            \begin{enumerate}
                \item Compute $x_{n} = \frac{b_{n}}{u_{nn}} $
                \item Compute $\tilde{b} = \hat{b} - ux_{n} $
                \item Run the algorithm on $\hat{U}, \tilde{b}$. That is, $\text{Alg}(\hat{U}, \tilde{b})$
            \end{enumerate}
            \bigbreak \noindent 
            The non-recursive pseudocode in the spirit of 1.3.5 and 1.3.13 is
            \bigbreak \noindent 
            \begin{jlcode}
                for |$i=n,...,1$|
                if |$U[i,i] = 0$|, set error flag, exit

                |$b[i] = b[i] / U[i,i]$|

                for |$j = i-1,...,1$|
                |$b[j] = b[j] - U[j,i] \cdot b[i]$|
                end
                end
            \end{jlcode}
            \bigbreak \noindent 
            Requires $\mathcal{O}(n^{2})$ flops.

        \item \textbf{Flops required to solve triangular systems}: Let $Ax = b$ be a system of linear equations where $A$ is nonsingular. If $A$ is upper triangular or lower triangular we can solve the system in roughly $n^{2}$ flops.
        \item \textbf{Inner product formulas to compute $R$ (Cholesky factor)}: We have the formulas
            \begin{align*}
                r_{ii} &= \sqrt{a_{ii} - \sum_{k=1}^{i-1}r_{ki}^{2}} \quad i = 1,2,...,n \\
                r_{ij} &= \frac{a_{ij} - \sum_{k=1}^{i-1}r_{ki}r_{kj}}{r_{ii}} \quad j = i+1,...,n
            \end{align*}
            \bigbreak \noindent 
            The inner product formulas to compute $R$ requires $\mathcal{O}(n^{3})$ flops.
        \item \textbf{Recursive column oriented method to find the Cholesky factor $R$ (Outer product method)}: Let $A \in \mathbb{R}^{n\times n}$. Assume that $A$ is positive definite, so $A = A^{\top}$, and $A = R^{\top}R$ for a unique upper triangular matrix $R$. We have,
            \begin{align*}
                A &= \begin{bmatrix} a_{11} & a_{12} & \cdots & a_{1n} \\ a_{21} & a_{22} & \cdots & a_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ a_{n1} & a_{n2} & \cdots & a_{nn} \end{bmatrix} = 
                \begin{bmatrix}
                    r_{11} & 0  & \cdots & 0\\
                    r_{12} & r_{22}  &  \cdots & 0 \\
                    \vdots & \vdots & \ddots & \vdots\\
                    r_{1n} & r_{2n} & \cdots & r_{nn}
                \end{bmatrix}
                \begin{bmatrix}
                    r_{11} & r_{12} & \cdots & r_{1n} \\
                    0 & r_{22} & \cdots & r_{2n} \\
                    0 & 0 & \ddots & \vdots \\
                    0 & 0  & \cdots & r_{nn}
                \end{bmatrix}
            \end{align*}
            We then perform a matrix decomposition 
            \begin{align*}
                \begin{bmatrix}
                    a_{11} & a^{\top} \\
                    a & \hat{A}
                \end{bmatrix}
                =
                \begin{bmatrix}
                    r_{11} & 0^{\top} \\
                    r & \hat{R}^{\top}
                \end{bmatrix}
                \begin{bmatrix}
                    r_{11} & r^{\top} \\
                    0 & \hat{R}
                \end{bmatrix}
            .\end{align*}
            Where $\hat{A} = \hat{A}^{\top} \in \mathbb{R}^{n-1 \times n-1}$, $a \in \mathbb{R}^{n-1}$, $\hat{R}^{\top} \in \mathbb{R}^{n-1\times n-1}$ lower triangular, and $\hat{R} \in \mathbb{R}^{n-1\times n-1}$ upper triangular. Further,
            \bigbreak \noindent 
            The recursive column oriented algorithm to compute the Cholesky factor $R$ is given by the following steps
            \begin{enumerate}
                \item $r_{11} = \sqrt{a_{11}}$
                \item $r = \frac{a}{r_{11}} $
                \item $\tilde{A} = \hat{A} - rr^{\top} $
                \item $\text{Alg}(\tilde{A}) = \hat{R} $
            \end{enumerate}
            \bigbreak \noindent 
            The recursive column oriented algorithm to compute the Cholesky factor $R$ requires $\mathcal{O}(n^{3})$ flops.
        \item \textbf{Cholesky's algorithm}: Cholesky's algorithm  applied to an $n \times n$ matrix performs about $\frac{n^{3}}{3} $
        \item \textbf{Bordered form of Choleskys method}: Suppose $A \in \mathbb{R}^{n\times n}$ is positive definite. Then, $A$ admits a decomposition $A = R^{\top}R$, for a unique upper triangular matrix $R$ called the Cholesky factor, with $r_{ii} > 0$ for $i =1,2,...,n$. So,
            \begin{align*}
                        A &= \begin{bmatrix} a_{11} & a_{12} & \cdots & a_{1n} \\ a_{21} & a_{22} & \cdots & a_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ a_{n1} & a_{n2} & \cdots & a_{nn} \end{bmatrix} = 
                \begin{bmatrix}
                    r_{11} & 0  & \cdots & 0\\
                    r_{12} & r_{22}  &  \cdots & 0 \\
                    \vdots & \vdots & \ddots & \vdots\\
                    r_{1n} & r_{2n} & \cdots & r_{nn}
                \end{bmatrix}
                \begin{bmatrix}
                    r_{11} & r_{12} & \cdots & r_{1n} \\
                    0 & r_{22} & \cdots & r_{2n} \\
                    0 & 0 & \ddots & \vdots \\
                    0 & 0  & \cdots & r_{nn}
                \end{bmatrix}
            \end{align*}
        We then perform a matrix decomposition 
        \begin{align*}
            \begin{bmatrix}
                \hat{A} & a \\
                a^{\top} & a_{nn}
            \end{bmatrix}
             =
             \begin{bmatrix}
                 \hat{R}^{\top} & 0 \\
                 r^{\top} & r_{nn}
             \end{bmatrix}
             \begin{bmatrix}
                 \hat{R} & r \\
                 0 & r_{nn}
             \end{bmatrix}
        .\end{align*}
        So, 
        \begin{align*}
            \hat{A} &= \hat{R}^{\top}\hat{R}, \\
            a &= \hat{R}^{\top}r, \\
            a_{nn} &= r^{\top}r + r_{nn}^{2} \implies r_{nn} = \sqrt{a_{nn} - r^{\top}r}
        .\end{align*}
        So, the steps for the algorithm are 
        \begin{enumerate}
            \item Recurse $\hat{A}$ until $A \in \mathbb{R}^{1\times 1}$
            \item Solve the lower triangular system $\hat{R}^{\top}r = a$ by forward substitution
            \item Compute $r_{nn} = \sqrt{a_{nn} - r^{\top}r} $
            \item Return the step two on the previous call
        \end{enumerate}
        The above algorithm is postorder recursion and requires $\mathcal{O}(n^{3})$ flops.

        \item \textbf{Row oriented algorithm to compute $LU $ factorization}:
            Let $A \in \mathbb{R}^{n\times n}$. If $A$ can be factored into products $LU$, for $U \in \mathbb{R}^{n\times n}$ upper triangular, $L \in \mathbb{R}^{n\times n}$ unit lower triangular, then
            \begin{align*}
                A &= LU  
            \end{align*}
            implies
            \begin{align*}
                \begin{bmatrix}
                    a_{11} & a_{12} & a_{13} & \cdots & a_{1n} \\
                    a_{21} & a_{22} & a_{23} & \cdots & a_{2n} \\
                    a_{31} & a_{32} & a_{33} & \cdots & a_{3n} \\
                    \vdots & \vdots & \vdots & \ddots & \vdots \\
                    a_{n1} & a_{n2} & a_{n3} & \cdots & a_{nn} \\
                \end{bmatrix}
                = \begin{bmatrix}
                    1 & 0 & 0 & \cdots & 0 \\
                    \ell_{21} & 1 & 0 & \cdots & 0 \\
                    \ell_{31} & \ell_{32} & 1 & \cdots & 0 \\
                    \vdots & \vdots & \vdots & \ddots & \vdots \\
                    \ell_{n1} & \ell_{n2} & \ell_{n3} & \cdots & 1
                \end{bmatrix}
                \begin{bmatrix}
                    u_{11} & u_{12} & u_{13} & \cdots & u_{1n} \\
                    0  & u_{22} & u_{23} & \cdots & u_{1n} \\
                    0& 0& u_{33} & \cdots & u_{3n} \\
                    \vdots & \vdots & \vdots & \ddots & \vdots \\
                    0 & 0 & 0 & \cdots & u_{nn}
                \end{bmatrix}
            .\end{align*}
            The formulas are
        \begin{align*}
            u_{ij} &= a_{ij} - \sum_{k=1}^{i-1}\ell_{ik}u_{kj} \quad j=i,i+1,...,n \tag{1}, \\
            \ell_{ij} &= \frac{a_{ij} - \sum_{k=1}^{j-1}\ell_{ik}u_{kj}}{u_{jj}} \quad i=j+1,j+2,...,n \tag{2}
        .\end{align*}
        \bigbreak \noindent 
        To use these formulas to find each $u_{ij}$ we first need to plug $i=1$ into $(1)$, then after we get the first row of $U$, we can plug in $j=1$ into $(2)$ to get the first column of $L$, and so on.
        \item \textbf{Column oriented recursive algorithm to find the $LU$ factorization}: Assume $A \in \mathbb{R}^{n\times n }$ admits an $LU$ factorization for $L\in \mathbb{R}^{n\times n}$ unit lower triangular, $U$ upper triangular. Then,
            \begin{align*}
                A = LU
            \end{align*}
            implies
            \begin{align*}
                \begin{bmatrix} a_{11} & a_{12} & \cdots & a_{1n} \\ a_{21} & a_{22} & \cdots & a_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ a_{n1} & a_{n2} & \cdots & a_{nn} \end{bmatrix} = 
                \begin{bmatrix} 1 & 0 & \cdots & 0 \\ \ell_{21} & \ell_{22} & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ \ell_{n1} & \ell_{n2} & \cdots & 1 \end{bmatrix}
                \begin{bmatrix} u_{11} & u_{12} & \cdots & u_{1n} \\ 0 & u_{22} & \cdots & u_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & u_{nn} \end{bmatrix}
            .\end{align*}
            Decompose $A = LU$ into the blocks
            \begin{align*}
                \begin{bmatrix}
                    a_{11} & b^{\top} \\
                    a & \hat{A}
                \end{bmatrix}
                =
                \begin{bmatrix}
                    1 & 0^{\top} \\
                    \ell & \hat{L}
                \end{bmatrix}
                \begin{bmatrix}
                    u_{11} & u^{\top} \\
                    0 & \hat{U}
                \end{bmatrix}
            .\end{align*}
            The recursive algorithm is defined by the following steps.
            \begin{enumerate}
                \item $u_{11} = a_{11} $ (zero flops)
                \item $u^{\top} = b^{\top} $ (zero flops)
                \item $\ell = \frac{a}{u_{11}} $ ($n-1$ flops) 
                \item $\tilde{A} = \hat{L}\hat{U} = \hat{A} - \ell u^{\top} $ ($2(n-1)^{2}$ flops)
                \item $\text{Alg}(\tilde{A})$
            \end{enumerate}
            The number of flops required for the recursive outer product method to find the $LU$ factorization is $\frac{2}{3}n^{3} + \mathcal{O}(n^{2}) $
        \item \textbf{Bordered form $LU$ decomposition algorithm}:
        \item \textbf{Flops required to find $LU$ decomposition}: Suppose $A$ is a matrix that admits an $LU$ decomposition $A = LU$ for $L$ unit lower triangular, and $U$ upper triangular. The flops required to find this decomposition is roughly $\frac{2}{3}n^{3}$.
            \bigbreak \noindent 
            Thus, to solve the system $Ax = b$, we have
            \begin{align*}
                Ax = b \iff LUx = b
            .\end{align*}
            Let 
            \begin{align*}
                \begin{cases}
                    Ly &= b \quad \text{($n^{2}$ flops)} \\
                    Ux &=y \quad \text{($n^{2}$ flops)}
                \end{cases}
            .\end{align*}
            So, in total, we have $\frac{2}{3}n^{3} + n^{2} + n^{2} = \frac{2}{3}n^{3} + 2n^{2} = \mathcal{O}(n^{3})$ flops to solve the system $Ax = b$ with an $LU$ decomposition.
        \item \textbf{Flops required to perform Gaussian Elimination without row interchanges (pivoting)}: For a system $Ax = b$, reducing the system to $Ux = b^{\prime}$, where $U$ is upper triangular requires roughly $\frac{2}{3}n^{3} + n^{2} + 2n = \mathcal{O}(n^{3})$ flops
            \bigbreak \noindent 
            Consider step $k$ of the process. We need to eliminate all entries below the pivot in column $k$, there are $(n-k)$ of them. Then, we need to update the $(n-k) \times (n-k)$ submatrix that doesn't include column $k$ and row $k$. The operation on each element $a_{ij}$ in the $(n-k) \times (n-k)$ submatrix is
            \begin{align*}
                a_{ij} \to a_{ij} - m_{ik}a_{kj}
            \end{align*}
            where $m_{ik} = a_{ik} / a_{kk}$ is the multiplier. Thus, updating the $(n-k) \times (n-k)$ submatrix requires $2(n-k)^{2}$ flops.
            \bigbreak \noindent 
            We require an additional $(n-k)$ flops to each multiplier, and an additional $2(n-k)$ flops to eliminate each entry below the pivot. 
            \bigbreak \noindent 
            In total, we have
            \begin{align*}
                \sum_{k=1}^{n-1}2(n-k)^{2} + (n-k) + 2(n-k) = \sum_{k=1}^{n-1}2(n-k)^{2} + 3(n-k)
            .\end{align*}
            Let $j = n-k$. When $k=1$, $j=n-1$, and when $k=n-1$, $j = 1$. So, we have
            \begin{align*}
                \sum_{j=1}^{n-1}2j^{2} + 3j &= 2 \cdot \frac{(n-1)n(2n-1)}{6} + 3 \cdot \frac{(n-1)n}{2} \\
                                            &=\frac{2}{3}n^{3} + \frac{1}{2}n^{2} - \frac{7}{6}n \approx \frac{2}{3}n^{3} + \mathcal{O}(n^{2})= \mathcal{O}(n^{3})
            .\end{align*}
        \item \textbf{Flops required to solve $Ax =b$ with $A^{-1}$}: Suppose we have a system $Ax = b$, where $A$ is nonsingular. How many flops would it take to find $A^{-1}$, and then solve $x = A^{-1}b$.
            \bigbreak \noindent 
            If $A^{-1}$ exists, then it is a matrix $X$ such that $AX = I$. So, 
            \begin{align*}
                A\begin{bmatrix} x_{11} & x_{12} & \cdots & x_{1n} \\ x_{21} & x_{22} & \cdots & x_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ x_{n1} & x_{n2} & \cdots & x_{nn} \end{bmatrix}
                &= \begin{bmatrix} 1 & 0 & \cdots & 0 \\ 0 & 1 & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & 1 \end{bmatrix}
            .\end{align*}
            Let $\text{col}_{j}(X)$ denote the $j^{\text{th}}$ column of $X$, and $e_{j}$ denote the $j^{\text{th}} $ column of $I$. Then,
            \begin{align*}
               A\text{col}_{1}(X) = e_{1}, \quad A\text{col}_{2}(X) = e_{2}, \quad ..., \quad A\text{col}_{n}(X) = e_{n} 
            .\end{align*}
            In total we need to solve $n$ systems. If we solved all $n$ systems with Gaussian Elimination, it would require
            \begin{align*}
                n\left(\frac{2}{3}n^{3} + \mathcal{O}(n^{2})\right) = \frac{2}{3}n^{4} + \mathcal{O}(n^{3})
            \end{align*}
            flops. But, we can do better. Instead if we find the $LU$ decomposition for $A$, which would require $\frac{2}{3}n^{3}$ flops, we would have the $n$ systems
            \begin{align*}
                Ly_{1} &= e_{1} \quad Ly_{2} = e_{2} \quad \cdots \quad Ly_{n} = e_{n}\\
                U\text{col}_{1}(X) &= y_{1} \quad U\text{col}_{2}(X) = y_{2} \quad \cdots \quad U\text{col}_{n}(X) = y_{n}
            .\end{align*}
            We see that each system would require $2n^{2}$ flops. So, in total $2n^{3}$ flops to solve all systems. For the whole process,
            \begin{align*}
                \frac{2}{3}n^{3} + 2n^{3} = \frac{8}{3}n^{3}
            \end{align*}
            flops are required. So, $4$ times the flops of $LU$ decomposition and $8$ times the flops of Choleksy decomposition. 
    \end{itemize}

    \pagebreak 
    \subsection{Chapter 2 Proofs}
    \begin{itemize}
        \item \textbf{The vector 2-norm is a norm}: To prove that this is true, we need to show that $\norm{x}_{2}$ satisfies the following properties
            \begin{enumerate}
                \item $\norm{x}_{2} = 0 \iff x = 0$
                \item $\norm{\alpha x}_{2} = \left\lvert \alpha \right\rvert \norm{x}_{2} $
                \item $\norm{x + y}_{2} \leq \norm{x}_{2}  + \norm{y}_{2} $
            \end{enumerate}
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} (1) If $\norm{x}_{2} = 0$, then
            \begin{align*}
                \norm{x} = 0 &\implies \left(x_{1}^{2} + x_{2}^{2} + ... + x_{n}^{2}\right)^{\frac{1}{2}} = 0 \\
                             &\iff x_{1}^{2} + x_{2}^{2} + ... + x_{n}^{2} = 0 \\
                             &\iff x_{1} = x_{2} =  ... = x_{n} = 0
            .\end{align*}
            Conversely, if $x=0$, then
            \begin{align*}
                \norm{0}_{2} &= \left(0^{2} + 0^{2} + ... + 0^{2}\right)^{\frac{1}{2}} = 0^{\frac{1}{2}} = 0
            .\end{align*}
            (2)
            \begin{align*}
                \norm{\alpha x}_{2} &= \left((\alpha x_{1})^{2} + (\alpha x_{2})^{2} + ... + (\alpha x_{n})^{2}\right)^{\frac{1}{2}} \\
                                &= \left(\alpha^{2} x_{1}^{2} + \alpha^{2} x_{2}^{2} + ... + \alpha^{2} x_{n}^{2}\right)^{\frac{1}{2}} \\
                                &= \left(\alpha^{2}(x_{1}^{2} + x_{2}^{2} + ... + x_{n}^{2})\right)^{\frac{1}{2}} \\
                                &= (\alpha^{2})^{\frac{1}{2}}  (x_{1}^{2} + x_{2}^{2} + ... + x_{n}^{2})^{\frac{1}{2}} \\
                                &= \left\lvert \alpha \right\rvert \norm{x}_{2}
            .\end{align*}
            As desired.
            \bigbreak \noindent 
            (3)
            \begin{align*}
                \norm{x + y}^{2}_{2} &= (x+y)^{T}(x+y) = x^{T}x + x^{T}y + y^{T}x + y^{T}y \\
                                 &= x^{T}x + 2x^{T}y + y^{T}y = \norm{x}_{2}^{2} + 2x^{T}y + \norm{y}_{2}^{2}
            .\end{align*}
            By Cauchy Schwarz, $x^{T}y \leq \norm{x}_{2}\norm{y}_{2}$. So,
            \begin{align*}
                \norm{x}_{2}^{2} + 2x^{T}y + \norm{y}_{2}^{2} \leq \norm{x}_{2}^{2} + 2\norm{x}_{2}\norm{y}_{2} + \norm{y}_{2}^{2} = \left(\norm{x}_{2} + \norm{y}_{2}\right)^{2}
            .\end{align*}
            Since $\norm{x}_{2}^{2} + 2x^{T}y + \norm{y}_{2}^{2} = \norm{x+y}^{2}$, we have
            \begin{align*}
                \norm{x+y}_{2}^{2} \leq \left(\norm{x}_{2} + \norm{y}_{2}\right)^{2},
            \end{align*}
            which implies $\norm{x+y}_{2} \leq \norm{x}_{2} + \norm{y}_{2}$ $\endpf$



    \end{itemize}


    \pagebreak 
    \subsection{Chapter 2}
    \bigbreak \noindent 
    \subsubsection{Definitions}
    \begin{itemize}
        \item \textbf{Unit ball}: A unit ball is the set of all points whose distance from the origin is less than or equal to 1
            \bigbreak \noindent 
            Given a norm $\norm{\cdot}$ on a vector space (say $\mathbb{R}^{n}$), the unit ball is the set of all points that are within distance 1 of the origin under that norm:
            \begin{align*}
                B_{\norm{\cdot }}(0,1) = \{x \in \mathbb{R}^{n}:\; \norm{x} \leq 1\}
            .\end{align*}
            Where
            \begin{itemize}
                \item \textbf{B	"Ball"}: the set of all points within a certain distance (radius)
                \item \textbf{$\mathbf{0}$}: The center of the ball (here, the origin)
                \item $\mathbf{1}$:	The radius of the ball
                \item $\norm{\cdot}$: The norm used to measure distance
            \end{itemize}
            \bigbreak \noindent 
            It’s the region that's "1 unit away" from the origin according to the norm.
            \bigbreak \noindent 
            The boundary of this set, where $\norm{x} = 1$, is called the \textbf{unit sphere} (even though it might not look like a sphere geometrically).
            \bigbreak \noindent 
            It’s the set of all vectors you can "reach" from the origin if your allowed length is 1 (according to your chosen norm).
        \item \textbf{A more general ball}: In general, a ball with center at $a$, and radius $r$ is defined as 
            \begin{align*}
                B_{\norm{\cdot }}(a,r) = \{x \in \mathbb{R}^{n}:\; \norm{x-a} \leq r\}
            .\end{align*}
        \item \textbf{Vector norms}:
            \begin{itemize}
                \item \textbf{Euclidean norm (2-norm)}: The standard Euclidean distance. For $x \in \mathbb{R}^{n}$,
                    \begin{align*}
                        \norm{x}_{2} = \sqrt{x_{1}^{2} + x_{2}^{2} + ... + x_{n}^{2}}
                    .\end{align*}
                \item \textbf{Manhattan norm (1-norm)}: Denoted $L^{1} $, and also called \textbf{Taxicab norm}. For $x \in \mathbb{R}^{n} $,
                    \begin{align*}
                        \norm{x}_{1} = \left\lvert x_{1} \right\rvert + \left\lvert x_{2} \right\rvert + ... + \left\lvert x_{n} \right\rvert
                    .\end{align*}
                \item \textbf{$L$-Infinity (max) norm ($\infty$-norm)}: Denoted $L^{\infty}$. for $x\in \mathbb{R}^{n}$,
                    \begin{align*}
                        \norm{x}_{\infty} = \max_{1 \leq i \leq n} \left\lvert x_{i} \right\rvert = \max\{\left\lvert x_{1} \right\rvert, x_{2}, ..., x_{n}\} 
                    .\end{align*}
                \item \textbf{$p$-norm}: In $\mathbb{R}^{n}$, A more general norm is
                    \begin{align*}
                        \norm{x}_{p} = \left(\sum_{i=1}^{n} \left\lvert x_{i} \right\rvert^{p}\right)^{\frac{1}{p}} = \left(\left\lvert x_{1}^{p} \right\rvert + \left\lvert x_{2} \right\rvert^{p} + ... + \left\lvert x_{n} \right\rvert^{p}\right)^{\frac{1}{p}}
                    \end{align*}
                    for $1 \leq p < \infty $. The general $p$-norm satisfies all three properties of a norm only when $p \geq 1$. For smaller $p$, the triangle inequality does not hold.
            \end{itemize}
        \item \textbf{Frobenius norm}
            \begin{align*}
                \norm{A}_{2} = \norm{A}_{F} = \left(\sum_{i=1}^{n}\sum_{j=1}^{n}\left\lvert a_{ij} \right\rvert^{2}\right)^{\frac{1}{2}}
            .\end{align*}
            \bigbreak \noindent 
            \textbf{Note:} The Frobenius norm is not an induced norm. We have for the identity matrix $I$,
            \begin{align*}
                \norm{I}_{F} = \sqrt{n} \ne 1    
            .\end{align*}
        \item \textbf{Induced (operator) matrix norms}: For all $A\in \mathbb{R}^{n\times m}$, we define
            \begin{align*}
                \norm{A}_{p} := \max_{x\in\mathbb{R}^{n} \setminus \{0\}} \frac{\norm{Ax}_{p}}{\norm{x}_{p}}
            .\end{align*}
        \item \textbf{Induced matrix norms special cases}:
            \begin{center}
                \begin{tabular}{p{1cm}|p{5cm}|p{5cm}}
                    \toprule
                    $p$ & \textbf{Name} & \textbf{Explicit formula} \\
                    \midrule
                    $1$ & Maximum column sum &
                    $\displaystyle \|A\|_{1} = \max_{1 \leq j \leq n} \sum_{i=1}^{m} |a_{ij}|$ \\[3ex]
                    $2$ & Spectral norm &
                    $\displaystyle \|A\|_{2} = \sqrt{\lambda_{\max}(A^{T}A)}$ \\[3ex]
                    $\infty$ & Maximum row sum &
                    $\displaystyle \|A\|_{\infty} = \max_{1 \leq i \leq m} \sum_{j=1}^{n} |a_{ij}|$ \\
                    \bottomrule
                \end{tabular}
            \end{center}
        \item \textbf{Relative error}: The relative error in $\hat{x}$ is given by
            \begin{align*}
                \frac{\norm{\hat{x} - x}}{\norm{x}} = \frac{\norm{\delta x}}{\norm{x}}
            \end{align*}
            where $\hat{x} = x + \delta  x $, which implies $x = \hat{x} - \delta  x $.
        \item \textbf{Perturbation}: $\hat{A}$ and $\hat{b}$ are called perturbed if they are modified versions of the original. If $\hat{A}$ is a perturbed matrix $A$, and $\hat{b}$ is a perturbed vector $b$, then
            \begin{align*}
                \hat{A} &= A + \delta A, \\
                \hat{b} &= b + \delta  b
            .\end{align*}
        \item \textbf{Condition number $\kappa(A)$}: The condition number of a matrix $A$ measures how sensitive the solution of a linear system $A x = b$ is to small changes in $b$ (or in $A$).
            \begin{align*}
                \kappa(A) = \norm{A^{-1}}\norm{A}
            .\end{align*}
            \bigbreak \noindent 
            We see that as $\kappa(A) \to \infty$, the relative error in $x$ grows without bound.
        \item \textbf{Numerical stability vs conditioning}
            \begin{itemize}
                \item \textbf{Conditioning}
                \begin{itemize}
                    \item A property of the \textbf{mathematical problem} itself.
                    \item Measures the \textbf{sensitivity} of the true solution to small input changes.
                    \item Example: if $A x = b$, then
                        \[
                            \frac{\|\delta x\|}{\|x\|} \le \kappa(A) \frac{\|\delta b\|}{\|b\|}.
                        \]
                        A large $\kappa(A)$ indicates an \textbf{ill-conditioned problem}.
                \end{itemize}
            \item \textbf{Numerical Stability}
                \begin{itemize}
                    \item A property of the \textbf{algorithm} used to solve the problem.
                    \item Measures how much \textbf{round-off and truncation errors} the algorithm introduces or amplifies.
                    \item A \textbf{stable algorithm} gives the exact solution to a \textit{nearby problem}:
                        \[
                            (A + \delta A)\hat{x} = b + \delta  b , \quad \|\delta A\|,\; \| \delta b\| \text{ small.}
                        \]
                \end{itemize}
        \end{itemize}
        \textbf{Note:} If an algorithm is \textbf{numerically unstable}, then the perturbations 
        $\delta A$ and/or $\delta b$ required to explain its result might be \textbf{large}:
        \[
            \|(A + \delta A) - A\| \text{ is not small.}
        \]
        Hence, the computed $\hat{x}$ is the exact solution to a \textit{far-away problem}:
        \[
            (A + \delta A)\hat{x} = b + \delta b,
        \]
        where $\|\delta A\|$ or $\|\delta b\|$ are no longer small compared to 
        $\|A\|$ or $\|b\|$.
        \bigbreak \noindent 
        This means the algorithm’s result may not correspond meaningfully to the 
        original system at all.




    \end{itemize}

    \pagebreak 
    \subsubsection{Properties}
    \begin{itemize}
        \item \textbf{Norms}: $\norm{\cdot}$ is a norm if and only if the following properties are satisfied
            \begin{enumerate}
                \item $\norm{0} = 0 $ 
                \item $ \norm{\alpha x} = \left\lvert \alpha \right\rvert \norm{x}$
                \item $\norm{x+y} \leq \norm{x} + \norm{y} $ (triangle inequality)
            \end{enumerate}
        \item \textbf{Matrix norms}: A matrix norm is a function 
            \begin{align*}
                \norm{\cdot }:\ \mathbb{R}^{n\times n} \to \mathbb{R}_{+}:\ A \mapsto  \norm{A}
            .\end{align*}
        \item \textbf{Properties of matrix norms}: Matrix norms satisfy the three required properties of norms.
            \begin{enumerate}
                \item $\norm{A} = 0 \iff A = 0 $
                \item $\norm{\alpha A} = \left\lvert \alpha \right\rvert \norm{A} $
                \item $\norm{A + B} \leq \norm{A} + \norm{B} $ (Triangle inequality)
            \end{enumerate}

        \item \textbf{Properties of induced matrix norms}
            \begin{itemize}
                \item \textbf{Sub-multiplicativity}: $\norm{AB}_{p} \leq \norm{A}_{p}\norm{B}_{p} $
                \item \textbf{Consistency}: $\norm{Ax}_{p} \leq \norm{A}_{p}\norm{x}_{p} $
                \item \textbf{Normalization}: $\norm{I}_{p} = 1 $
            \end{itemize}
        \item \textbf{Additional property of matrix norms}: For $A \in \mathbb{R}^{n\times n}$, we have
            \begin{align*}
                \norm{A} &= \max_{x\ne 0} \frac{\norm{Ax}}{\norm{x}} \geq \frac{\norm{Ax}}{\norm{x}} \\
                         &\implies \norm{Ax} \leq \norm{A}\norm{x}
            .\end{align*}
        \item \textbf{Properties of the condition number}: Let $A$ be a matrix, and $\kappa(A)$ be the condition number that measures the system $Ax = b$. The following two properties hold
            \begin{enumerate}
                \item $\kappa(A) \geq 1$
                \item $\kappa(I) = 1$
                \item $\kappa(A) = \kappa(A^{-1}) $
            \end{enumerate}
        \item \textbf{Well-conditioned and ill-conditioned in terms of $\kappa(A)$}: If $\kappa(A)$ is large, then $(P)$ is ill-conditioned. If $\kappa(A)$ is small (close to one), then $(P)$ is well-conditioned.


    \end{itemize}

    \pagebreak 
    \subsubsection{Theorems}
    \begin{itemize}
        \item \textbf{Theorem \textit{(Relative Error Bound I)}}: Let $A$ be non-singular, $b \ne 0$, and $Ax = b$. If $A(x + \delta  x) = b + \delta  b$, then
            \begin{align*}
                \frac{\norm{ \delta  x}}{\norm{ x}} \leq \kappa(A) \frac{\norm{\delta  b }}{\norm{ b}}
            \end{align*}
            where $\kappa(A) = \norm{A^{-1}}\norm{A}$.
    \end{itemize}

    \pagebreak 
    \subsubsection{Algorithms and complexities}
    \begin{itemize}
        \item \relax
    \end{itemize}



    \pagebreak 
    \unsect{Julia}
    \bigbreak \noindent 
    \subsection{Types}
    \bigbreak \noindent 
    \fig{.5}{./figures/tree.png}
    \begin{itemize}
        \item \textbf{Subtype constraint <:} $A$ <: $B$ means $A$ is a subtype of $B$
            \bigbreak \noindent 
            \begin{jlcode}
            Int <: Number #true
            \end{jlcode}
    \end{itemize}

    \pagebreak 
    \subsection{Functions}
    \begin{itemize}
        \item \relax    
    \end{itemize}

    \pagebreak 
    \subsection{Linear Algebra}
    \bigbreak \noindent 
    \subsubsection{Matrix creation and operations}
    \begin{itemize}
        \item \textbf{Array constructors}:
            \begin{itemize}
                \item Array\{T\}(undef, dims...)
                \item Vector\{T\}(undef, n)
                \item Matrix\{T\}(undef, m, n)
            \end{itemize}
        \item \textbf{Zeros/ones/fills}
            \begin{itemize}
                \item zeros(n), zeros(m,n)
                \item ones(n), ones(m,n)
                \item fill(x, dims...)
            \end{itemize}
        \item \textbf{Uniform ranges}:
            \begin{itemize}
                \item collect(1:n) $\to$ vector
                \item collect(1:m, 1:n) $\to$ matrix (grid)
            \end{itemize}
    \end{itemize}



















    
\end{document}
