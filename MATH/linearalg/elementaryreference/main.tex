\documentclass{report}

\input{~/dev/latex/template/preamble.tex}
\input{~/dev/latex/template/macros.tex}

\title{\Huge{}}
\author{\huge{Nathan Warner}}
\date{\huge{}}
\fancyhf{}
\rhead{}
\fancyhead[R]{\itshape Warner} % Left header: Section name
\fancyhead[L]{\itshape\leftmark}  % Right header: Page number
\cfoot{\thepage}
\renewcommand{\headrulewidth}{0pt} % Optional: Removes the header line
%\pagestyle{fancy}
%\fancyhf{}
%\lhead{Warner \thepage}
%\rhead{}
% \lhead{\leftmark}
%\cfoot{\thepage}
%\setborder
% \usepackage[default]{sourcecodepro}
% \usepackage[T1]{fontenc}

% Change the title
\hypersetup{
    pdftitle={Elementary Linear Algebra Reference}
}

\begin{document}
    % \maketitle
        \begin{titlepage}
       \begin{center}
           \vspace*{1cm}
    
           \textbf{Elementary Linear Algebra Reference}
    
           \vspace{0.5cm}
                
                
           \vspace{1.5cm}
    
           \textbf{Nathan Warner}
    
           \vfill
                
                
           \vspace{0.8cm}
         
           \includegraphics[width=0.4\textwidth]{~/niu/seal.png}
                
           Computer Science \\
           Northern Illinois University\\
           United States\\
           
                
       \end{center}
    \end{titlepage}
    \tableofcontents
    \pagebreak 
    \unsect{Vectors}
    \begin{itemize}
        \item \textbf{Magnitude}: For a vector $x\in\mathbb{R}^{n},\; x=\begin{pmatrix} x_{1} \\ x_{2} \\ \vdots \\x_{n} \end{pmatrix}$, the norm (magnitude) of $x$ is 
            \begin{align*}
                \norm{x} = \sqrt{x_{1}^{2} + x_{2}^{2} + ... + x_{n}^{2}} 
            \end{align*}
        \item \textbf{Triangle inequality}: For vectors $x,y \in \mathbb{R}^{n}$, we have the inequality
            \begin{align*}
                \norm{x + y} \leq \norm{x} + \norm{y}
            \end{align*}
        \item \textbf{Properties of Vector Operations}:
        Let $\mathbf{u}$, $\mathbf{v}$, and $\mathbf{w}$ be vectors in a plane. Let $r$ and $s$ be scalars.
        \begin{enumerate}
            \item $\mathbf{u} + \mathbf{v} = \mathbf{v} + \mathbf{u}$ \quad (Commutative property)
            \item $(\mathbf{u} + \mathbf{v}) + \mathbf{w} = \mathbf{u} + (\mathbf{v} + \mathbf{w})$ \quad (Associative property)
            \item $\mathbf{u} + \mathbf{0} = \mathbf{u}$ \quad (Additive identity property)
            \item $\mathbf{u} + (-\mathbf{u}) = \mathbf{0}$ \quad (Additive inverse property)
            \item $r(s\mathbf{u}) = (rs)\mathbf{u}$ \quad (Associativity of scalar multiplication)
            \item $(r + s)\mathbf{u} = r\mathbf{u} + s\mathbf{u}$ \quad (Distributive property)
            \item $r(\mathbf{u} + \mathbf{v}) = r\mathbf{u} + r\mathbf{v}$ \quad (Distributive property)
            \item $1\mathbf{u} = \mathbf{u}, \quad 0\mathbf{u} = \mathbf{0}$ \quad (Identity and zero properties)
        \end{enumerate}
    \item \textbf{Finding components of a vector given the magnitude and the angle $\theta$}: If $v \in \mathbb{R}^{2},\; v = \begin{pmatrix} x \\ y \end{pmatrix}$, then
        \begin{align*}
            &x = \norm{\vec{v}}\cos{\theta } \\
            &y = \norm{\vec{v}}\sin{\theta }
        .\end{align*}
    \item \textbf{Unit vector}: A unit vector is a vector with magnitude $1$. For any nonzero vector $\vec{v}$, we can use scalar multiplication to find a unit vector $\vec{u}$ that has the same direction as $\vec{v}:$. To do this, we multiply the vector by the reciprocal of its magnitude:
        \[
            \vec{u} = \frac{1}{\lVert \vec{v} \rVert} \vec{v}.
        \]
    \item \textbf{Properties of the dot product}:
        Let $\vec{u}$, $\vec{v}$, and $\vec{w}$ be vectors, and let $c$ be a scalar.
        \begin{enumerate}
            \item Commutative property: $\vec{u} \cdot \vec{v} = \vec{v} \cdot \vec{u}$
            \item Distributive property: $\vec{u} \cdot (\vec{v} + \vec{w}) = \vec{u} \cdot \vec{v} + \vec{u} \cdot \vec{w}$
            \item Associative property of scalar multiplication: $(c\vec{u} \cdot \vec{v}) = (c\vec{u}) \cdot \vec{v} = \vec{u} \cdot (c\vec{v})$
            \item Property of magnitude: $\vec{v} \cdot \vec{v} = \|\vec{v}\|^2$
        \end{enumerate}
    \item \textbf{Evaluating a dot product}: 
        The dot product of two vectors is the product of the magnitude of each vector and the cosine of the angle between them:
        \begin{align*}
            \vec{u} \cdot \vec{v} = \norm{\vec{u}} \cdot \norm{\vec{v}} \cdot \cos{\theta }
        .\end{align*}
    \item \textbf{Find the measure of the angle between two nonzero vectors}:
        \begin{align*}
            \cos{\theta } = \frac{\vec{u} \cdot \vec{v}}{\norm{\vec{u}}\norm{\vec{v}}}
        .\end{align*}
        \bigbreak \noindent 
        \textbf{Note}: We are considering $0 \leq \theta  \leq \pi $
    \item \textbf{Vector Projection}: The vector projection of $\mathbf{v}$ onto $\mathbf{u}$ has the same initial point as $\mathbf{u}$ and $\mathbf{v}$ and the same direction as $\mathbf{u}$, and represents the component of $\mathbf{v}$ that acts in the direction of $\mathbf{u}:$.
        \begin{align*}
            \text{proj}_{\vec{u}}\vec{v} = \frac{\vec{v} \cdot \vec{u}}{\norm{\vec{u}}^{2}}\vec{u}
        .\end{align*}
        We say "The vector projection of $\vec{v}$ onto $\vec{u}$"
    \item \textbf{Scalar projection notation}: This is the length of the vector projection and is denoted
        \begin{align*}
            \norm{\text{proj}_{\vec{u}}\vec{v}} = \text{comp}_{\vec{u}}\vec{v} = \frac{\vec{u} \cdot \vec{v}}{\norm{\vec{u}}}
        .\end{align*}
    \item \textbf{Decompose some vector $\vec{v}$ into orthogonal components such that one of the component vectors has the same direction as  $\vec{u}$}:
        \begin{itemize}
            \item First, we compute $\vec{p} = \text{proj}_{\vec{u}}\vec{v} $
            \item Then, we define $\vec{q}  = \vec{v} - \vec{p}$ 
            \item Check that $\vec{q}$ and $\vec{p}$ are orthogonal by finding $\vec{q} \cdot \vec{p}$
        \end{itemize}
    \item \textbf{Two vectors are orthogonal if}:
        \begin{align*}
            \vec{u} \cdot \vec{v} = 0
        .\end{align*}
    \item \textbf{Two vectors are parallel if}: Two vectors $v,u$ are parallel if there exists some scalar $\alpha \in \mathbb{R}$ such that $\alpha u = v $
        \bigbreak \noindent 
        \begin{itemize}
            \item If $ \alpha > 0$, then $v$ points in the same direction as $u$
            \item If $ \alpha < 0$, then $v$ points in the opposite direction of $u$
        \end{itemize}
    \item \textbf{Scalar projection componets of a vector}:
        \begin{align*}
            \vec{v} = \langle \text{comp}_{\hat{i}}\vec{v}, \text{comp}_{\hat{j}}\vec{v}, \text{comp}_{\hat{k}}\vec{v}\rangle
        .\end{align*}
    \item \textbf{The Cross Product}: produces a vector perpendicular to both vectors involved in the multiplication
        \bigbreak \noindent 
        Let $\mathbf{u} = \langle u_1, u_2, u_3 \rangle$ and $\mathbf{v} = \langle v_1, v_2, v_3 \rangle$.
        Then, the cross product $\mathbf{u} \times \mathbf{v}$ is vector
        \begin{align*}
            \mathbf{u} \times \mathbf{v} &= (u_2 v_3 - u_3 v_2) \mathbf{i} - (u_1 v_3 - u_3 v_1) \mathbf{j} + (u_1 v_2 - u_2 v_1) \mathbf{k}  \\
                                         &= \langle u_2 v_3 - u_3 v_2, -(u_1 v_3 - u_3 v_1), u_1 v_2 - u_2 v_1 \rangle.
        .\end{align*}
        \bigbreak \noindent 
        \textbf{Note:} The cross product only works in $\mathbb{R}^{3}$, additionally, we measure the angle between $\vec{u}$ and $\vec{v}$ in $\vec{u} \times \vec{v}$ from $\vec{u}$ to $\vec{v}$
    \item \textbf{Cross product using matrix and determinant}, suppose we have vectors $\vec{u}$ und $\vec{v}:$. Then we can express them in matrix form as
        \begin{align*}
            \vec{u} \times \vec{v}  =
            \begin{bmatrix}
                \hat{i} & \hat{j} & \hat{k} \\
                u_{x} & u_{y} & u_{z} \\
                v_{x} & v_{y} & v_{z}
            \end{bmatrix}
        .\end{align*}
        Then we can find the determinant of this matrix to compute the cross product
        \begin{align*}
            \vec{u} \times \vec{v} = (u_{y}v_{z} - u_{z}v_{y})\hat{i} - (u_{x}v_{z}-u_{z}v_{x})\hat{k} + (u_{x}v_{y} - u_{y}v_{x})\hat{j}
        .\end{align*}

    \item \textbf{Properties of the Cross Product}:
        Let $\mathbf{u}$, $\mathbf{v}$, and $\mathbf{w}$ be vectors in space, and let $c$ be a scalar.
        \begin{enumerate}
            \item Anticommutative property: $\mathbf{u} \times \mathbf{v} = -(\mathbf{v} \times \mathbf{u})$
            \item Distributive property: $\mathbf{u} \times (\mathbf{v} + \mathbf{w}) = \mathbf{u} \times \mathbf{v} + \mathbf{u} \times \mathbf{w}$
            \item Multiplication by a constant: $c(\mathbf{u} \times \mathbf{v}) = (c\mathbf{u}) \times \mathbf{v} = \mathbf{u} \times (c\mathbf{v})$
            \item Cross product of the zero vector: $\mathbf{u} \times \mathbf{0} = \mathbf{0} \times \mathbf{u} = \mathbf{0}$
            \item Cross product of a vector with itself: $\mathbf{v} \times \mathbf{v} = \mathbf{0}$
            \item Scalar triple product: $\mathbf{u} \cdot (\mathbf{v} \times \mathbf{w}) = (\mathbf{u} \times \mathbf{v}) \cdot \mathbf{w}$
        \end{enumerate}
    \item \textbf{Magnitude of the Cross Product}:
        Let $\mathbf{u}$ and $\mathbf{v}$ be vectors, and let $\theta$ be the angle between them. Then, $\|\mathbf{u} \times \mathbf{v}\| = \|\mathbf{u}\| \cdot \|\mathbf{v}\| \cdot \sin \theta.$
    \item \textbf{Triple Scalar Product}:

        The triple scalar product of vectors $\mathbf{u}$, $\mathbf{v}$, and $\mathbf{w}$ is $\mathbf{u} \cdot (\mathbf{v} \times \mathbf{w})$.
        \bigbreak \noindent 
        The triple scalar product is the determinant of the  $3\times 3$ matrix formed by the components of the vectors

    \item \textbf{triple scalar product identities}: 
        \begin{enumerate}[label=(\alph*)]
            \item $\mathbf{u} \cdot (\mathbf{v} \times \mathbf{w}) = -\mathbf{u} \cdot (\mathbf{w} \times \mathbf{v})$
            \item $\mathbf{u} \cdot (\mathbf{v} \times \mathbf{w}) = \mathbf{v} \cdot (\mathbf{w} \times \mathbf{u} = \mathbf{w} \cdot (\mathbf{u} \times \mathbf{v}))$
        \end{enumerate}
    \item \textbf{The zero vector is considered to be parallel to all vectors}:
    \item \textbf{vector equation of a line}:
        \begin{align*}
            \mathbf{r} = \mathbf{r}_{0} + t\mathbf{v} 
        .\end{align*}
        Where $\mathbf{v}$ is the direction vector (vector parallel to the line), $t$ is some scalar, and $\mathbf{r}$, $\mathbf{r}_{0}$ are position vectors
    \item \textbf{Parametric and Symmetric Equations of a Line}:
        A line $L$ parallel to vector $\mathbf{v}=\langle a,b,c \rangle$ and passing through point $P(x_0,y_0,z_0)$ can be described by the following parametric equations:
        \[
            x=x_0+ta, \quad y=y_0+tb, \quad \text{and} \quad z=z_0+tc.
        \]
        If the constants $a$, $b$, and $c$ are all nonzero, then $L$ can be described by the symmetric equation of the line:
        \[
            \frac{x-x_0}{a} = \frac{y-y_0}{b} = \frac{z-z_0}{c}.
        \]
        \bigbreak \noindent 
        \textbf{Note:} The parametric equations of a line are not unique. Using a different parallel vector or a different point on the line leads to a different, equivalent representation. Each set of parametric equations leads to a related set of symmetric equations, so it follows that a symmetric equation of a line is not unique either.
    \item \textbf{Vector equation of a line reworked}: Suppose we have some line, with points $P(x_{0}, y_{0}, z_{0})$, $Q(x_{1}, y_{1}, z_{1})$. Where $\mathbf{p} = \left\langle x_{0}, y_{0}, z_{0}\right\rangle $ and $\mathbf{Q} = \left\langle x_{1}, y_{1}, z_{1}\right\rangle $ are the correponding position vectors. Suppose we also have $\mathbf{r}: = \left\langle x,y,z \right\rangle $. Then our vector equation for a line becomes 
        \begin{align*}
            \mathbf{r} = \mathbf{p} + t\left(\vec{PQ}\right)
        .\end{align*} 
        By properties of vectors, we get the vector equation of a line passing through points $P$ and $Q$ to be 
        \begin{align*}
            \mathbf{r} = (1-t)\mathbf{p} + t\mathbf{q}
        .\end{align*}
        \item \textbf{Distance from a Point to a Line}:
            Let $L$ be a line in space passing through point $P$ with direction vector $\mathbf{v}$. If $M$ is any point not on $L$, then the distance from $M$ to $L$ is
            \[
                d = \frac{\left\| \overrightarrow{PM} \times \mathbf{v} \right\|}{\left\| \mathbf{v} \right\|}
            \]
        \item \textbf{Vector equation of a plane}:
            Given a point $P$ and vector $\mathbf{n}$, the set of all points $Q$ satisfying the equation $\mathbf{n} \cdot \overrightarrow{PQ} = 0$ forms a plane. The equation
            \[
                \mathbf{n} \cdot \overrightarrow{PQ} = 0
            \]
            is known as the vector equation of a plane.
    \item \textbf{Scalar equation of a plane}:
        The scalar equation of a plane containing point $P=(x_0, y_0, z_0)$ with normal vector $\mathbf{n}=\langle a, b, c \rangle$ is
        \[
            a(x-x_0) + b(y-y_0) + c(z-z_0) = 0.
        \]
    \item \textbf{General form of the equation of a plane}:
        This equation (the one above) can be expressed as $ax + by + cz + d = 0$, where $d = -ax_0 - by_0 - cz_0$. This form of the equation is sometimes called the general form of the equation of a plane.



    \end{itemize}



    \pagebreak 
    \unsect{Solutions to linear systems}
    \begin{itemize}
        \item \textbf{Possible solutions to a linear system of two unknowns}: The linear system can have a \textbf{unique solution, no solution, or infinitely many solutions}.
            \item \textbf{Does the solution set form a line, plane, hyperplane, or something else?}: The formation of the solution set depends on the number of free variables,
                \begin{itemize}
                    \item \textbf{No free variables (one unique solution)}: Intersects at a point
                    \item \textbf{One free variable (Uncountable solutions)}: Solution set is a line (1-dimensional subspace)
                    \item \textbf{Two free variable (Uncountable solutions)}: Solution set forms a plane (2-dimensional subspace)
                    \item \textbf{Three free variable (Uncountable solutions)}: Solution set is a three dimensional subspace (In $\mathbb{R}^{3}$ it would be the whole space)
                    \item \textbf{$k$ free variables}: Solution set is a $k$-dimensional subspace in $\mathbb{R}^{n} $
                        \bigbreak \noindent 
                        \textbf{Note:} A \( k \)-dimensional subspace in \( \mathbb{R}^n \) means that the solution set spans a \( k \)-dimensional space within the \( n \)-dimensional ambient space \( \mathbb{R}^n \).

                \end{itemize}

        \item \textbf{Determine if three planes intersect at a unique point}: For this, we find all three normal vectors $\vec{\mathbf{n}}_{1}, \vec{\mathbf{n}}_{2}$, and $\vec{\mathbf{n}}_{3}$. Then we find the triple scalar product, that is
            \begin{align*}
                \vec{\mathbf{n}}_{1} \cdot (\vec{\mathbf{n}}_{2} \times \vec{\mathbf{n}}_{3})
            .\end{align*}
            If this value is non-zero, we have intersection at a unique point. If the value is zero, we either have no intersection, or intersection at a line.

    \end{itemize}

    \pagebreak 
    \unsect{Linearity}
    \begin{itemize}
        \item \textbf{The properties of linear equations}:
            A function \( f: \mathbb{R}^n \rightarrow \mathbb{R} \) representing a linear equation is linear, meaning it satisfies the following properties for all vectors \( \mathbf{x}, \mathbf{y} \in \mathbb{R}^n \) and all scalars \( c \in \mathbb{R} \):
            \begin{itemize}
                \item \textbf{Additivity:} \( f(\mathbf{x} + \mathbf{y}) = f(\mathbf{x}) + f(\mathbf{y}) \)
                \item \textbf{Homogeneity of Degree 1:} \( f(c\mathbf{x}) = cf(\mathbf{x}) \)
                    \bigbreak \noindent 
                    It follows from this that $f(c\mathbf{x})$, when $c=0$ implies $f(0 \mathbf{x}) = 0 f(\mathbf{x}) = 0$. Thus, we add the property
                \item \textbf{Scale by zero}: $f(0) = 0$
            \end{itemize}
            These properties define a linear function and imply that the graph of a linear equation is a straight line (in 2D) or a plane (in 3D).
    \end{itemize}

    \pagebreak 
    \unsect{Matrix algebra}
    \begin{itemize}
        \item \textbf{Laws of matrix addition}: 
            \begin{itemize}
                \item \textbf{Addition with the zero matrix}:  $0 + A = A $
                \item \textbf{Communitive law for matrix addition}: $A+B = B+A$
                \item \textbf{Associativity of matrix addition}: $(A+B) + C = A + (B+C) $
            \end{itemize}
        \item \textbf{Laws of matrix subtraction}:
            \begin{itemize}
                \item $A - 0 = A $
                \item $A -A = 0 $
                \item $B-A = (-1)(A-B) $
            \end{itemize}
                \item \textbf{Matrix difference (subtraction)}: We can give a definition to the subtraction operator by just defining it as using matrix addition and multiplication by a scalar $A - B = A + (-1B) $
                \item \textbf{Note on matrix multiplication}: Matrix multiplication is general \textbf{not} communitive, it can be, but it isn't always. Also, in the real numbers, we know for
                    \begin{align*}
                        ab = 0
                    .\end{align*}
                    Then either $a$ is zero, $b$ is zero, or they are both zero. This is not always the case with matrix multiplication, it is possible to multiply two non-zero matrices and get the zero matrix as a result.
        \item \textbf{Properties of matrix multiplication}:
            \begin{enumerate}
                \item If \( A \), \( B \), and \( C \) are matrices of the appropriate sizes, then 
                    \[
                        A(BC) = (AB)C.
                    \]
                \item If \( A \), \( B \), and \( C \) are matrices of the appropriate sizes, then 
                    \[
                        (A + B)C = AC + BC.
                    \]
                \item If \( A \), \( B \), and \( C \) are matrices of the appropriate sizes, then 
                    \[
                        C(A + B) = CA + CB.
                    \]
            \end{enumerate}
        \item \textbf{Properties of Scalar Multiplication}:
            If \( r \) and \( s \) are real numbers and \( A \) and \( B \) are matrices of the appropriate sizes, then
            \begin{enumerate}
                \item \( r(sA) = (rs)A \)
                \item \( (r + s)A = rA + sA \)
                \item \( r(A + B) = rA + rB \)
                \item \( A(rB) = r(AB) = (rA)B \)
            \end{enumerate}
        \item \textbf{Note on cancellation}: If \( a \), \( b \), and \( c \) are real numbers for which \( ab = ac \) and \( a \neq 0 \), it follows that \( b = c \). That is, we can cancel out the nonzero factor \( a \). However, the cancellation law does not hold for matrices. 
        \item \textbf{Differences between matrix multiplication and multiplication of real numbers}:
            We summarize some of the differences between matrix multiplication and the multiplication of real numbers as follows: For matrices \( A \), \( B \), and \( C \) of the appropriate sizes,
            \begin{enumerate}
                \item \( AB \) need not equal \( BA \).
                \item \( AB \) may be the zero matrix with \( A \neq 0 \) and \( B \neq 0 \).
                \item \( AB \) may equal \( AC \) with \( B \neq C \).
            \end{enumerate}
    \end{itemize}

    \pagebreak 
    \unsect{Transpose}
    \begin{itemize}
        \item \textbf{Squared magnitude of a vector}:  
            \begin{align*}
                \norm{x}^{2} = x^{\top}x
            \end{align*}
        \item \textbf{Transpose of product of matrices}:
            \begin{align*}
                (AB)^{\top} = B^{\top}A^{\top}
            \end{align*}
            \bigbreak \noindent 
            \textbf{Consequence}:
            \begin{align*}
                (ABC)^{\top} = C^{\top}(AB)^{\top} = C^{\top}B^{\top}A^{\top}
            \end{align*}
        \item \textbf{Properties of Transpose}:
            If \( r \) is a scalar and \( A \) and \( B \) are matrices of the appropriate sizes, then
            \begin{enumerate}
                \item \( (A^T)^T = A \)
                \item \( (A + B)^T = A^T + B^T \)
                \item \( (AB)^T = B^T A^T \)
                \item \( (rA)^T = rA^T \)
            \end{enumerate}



    \end{itemize}

    \pagebreak 
    \unsect{Linear maps}
    \begin{itemize}
        \item \textbf{Composition}: Let $L:\ \mathbb{R}^{n} \to \mathbb{R}^{m},\quad v \to L(v)$, and $K:\ \mathbb{R}^{m} \to \mathbb{R}^{p},\quad L(v) \to K(L(v))$. We see that $L \in\mathbb{R}^{m\times n} $, and $K \in \mathbb{R}^{p\times m}$.
            \bigbreak \noindent 
            The composition is 
            \begin{align*}
                K(L(v)) = (K \circ L)(v) = (KL)(v)
            \end{align*}
        \item \textbf{2D rotation map}: 
            \begin{align*}
                R(\theta ) = \begin{bmatrix} \cos{\left(\theta \right)}  & -\sin{\left(\theta \right)} \\ \sin{\left(\theta \right)} & \cos{\left(\theta \right)} \end{bmatrix}
            \end{align*}
        \item \textbf{3D rotation, but keeping one variable constant, ie rotating about one of the coordinate axis}. 
            \bigbreak \noindent 
            All there cases below will require a $3\times 3$ matrix
            \begin{itemize}
                \item \textbf{Rotation about the x-axis (rotation in the $yz$-plane)}: 
                    \begin{align*}
                        R_{x}(\theta ) =  \begin{bmatrix} 1 & 0 & 0 \\ 0 & \cos{\left(\theta \right)} & -\sin{\left(\theta \right)} \\ 0 & \sin{\left(\right)} & \cos{\left(\theta \right)}\end{bmatrix}
                    .\end{align*}
                \item \textbf{Rotation about the y-axis (rotation in the $xz$-plane)}
                    \begin{align*}
                        R_{y}(\theta ) = \begin{bmatrix} \cos{\left(\theta   \right)} & 0 & -\sin{\left(\theta \right)} \\ 0 & 1 & 0 \\ \sin{\left(\theta \right)} & 0 & \cos{\left(\theta \right)}\end{bmatrix}
                    .\end{align*}
                \item \textbf{Rotation about the z-axis (rotation in the $xy$-plane)}
                    \begin{align*}
                        R_{z}(\theta ) \begin{bmatrix} \cos{\left(\theta \right)} & -\sin{\left(\theta \right)} & 0 \\ \sin{\left(\theta \right)} & \cos{\left(\theta \right)} & 0 \\ 0 & 0 & 1\end{bmatrix}
                    .\end{align*}
            \end{itemize}
            \bigbreak \noindent 
            \textbf{Notes on consecutive rotations}: Two consecutive rotations about different axises is \textbf{not} communitive, however if you rotation about the same axis it is.
        \item \textbf{What are the columns of a linear map}: If a linear map \(T: \mathbb{R}^n \to \mathbb{R}^m\) is represented by a matrix \(A\), then the columns of \(A\) are exactly the vectors you obtain by applying \(T\) to the standard basis vectors \(e_1, e_2, \dots, e_n\). That is, 
            \begin{align*}
                T(e_j) = A e_j = \text{column } j \text{ of } A.
            \end{align*}
        \item \textbf{Linear Operations}: A linear map from a vector space to itself, $L:\ V \to V$ is called a \textit{linear operation}, or just an \textit{operation}

    \end{itemize}

    \pagebreak 
    \unsect{Injective, surjective, bijective}
    \begin{itemize}
        \item \textbf{Standard Functions}:
            \begin{itemize}
                \item A function $f:\ A \to B$ is \textbf{surjective (onto)} if for all $b \in B$, there exists an $a \in A$ such that $f(a) = b$.  
                    Equivalently, $f$ is surjective if $f(A) = B$.

                \item A function $f:\ A \to B$ is \textbf{injective (one-to-one)} if 
                    \[
                        f(a_{1}) = f(a_{2}) \;\; \implies \;\; a_{1} = a_{2}.
                    \]
                    Equivalently, distinct elements of $A$ map to distinct elements of $B$.

                \item A function $f:\ A \to B$ is \textbf{bijective} if it is both injective and surjective.  
                    In this case, $f$ is called a \textbf{bijection} from $A$ to $B$, and $f$ has an \textbf{inverse function} $f^{-1}:\ B \to A$.

                \item A function $f:\ A \to B$ is \textbf{constant} if there exists $b_{0} \in B$ such that $f(a) = b_{0}$ for all $a \in A$.

                \item A function $f:\ A \to B$ is \textbf{identity} on $A$ if $f(a) = a$ for all $a \in A$.  
                    This function is usually denoted by $\mathrm{id}_{A}$.

                \item A function $f:\ A \to B$ is \textbf{many-to-one} if two or more distinct elements in $A$ can map to the same element of $B$.

                \item A function $f:\ A \to B$ is \textbf{one-to-one correspondence} if it is bijective.  
                    In this case, $A$ and $B$ have the same cardinality.
            \end{itemize}
        \item \textbf{Linear maps}: Consider a linear map $L:\ \mathbb{R}^{n} \to \mathbb{R}^{k}$
            \begin{itemize}
                \item $\mathbb{R}^{n} \to \mathbb{R}^{k}$ where $n < k$ can never be onto, maybe 1-1 though
                \item $\mathbb{R}^{n} \to \mathbb{R}^{k}$ where $n > k$ can never be 1-1, maybe onto though
            \end{itemize}
            Consider a linear map $L:\; \mathbb{R}^{n} \to \mathbb{R}^{m}$, with $A \in \mathbb{R}^{m\times n}$. By the rank-nullity theorem, 
            \begin{align*}
                \text{dim}(\mathbb{R}^{n}) = \text{dim}(\text{Im}(L)) + \text{dim}(\text{ker}(L))
            .\end{align*}
            So, $n = \text{dim}(\text{col}(A)) + \text{dim}(\text{ker}(L)) $. If a matrix is wide $(m > n)$, then a matrix has full rank if $\text{rank}(A) = n = \text{dim}(\text{Im}(L))$. If $A$ has full rank, then
            \begin{align*}
                n = n + \text{dim}(\text{ker}(L))
            .\end{align*}
            So, $\text{dim}(\text{ker}(L)) = 0$, so $L$ is injective. But, $\text{dim}(\text{Im}(L)) = n \ne m$. Since $m$ is the dimension of the codomain, which is different from the dimension of the image, $L$ is not surjective. 
            \bigbreak \noindent 
            Consider a map where $m < n$. If the matrix representing the map has full rank, then $\text{rank}(A) = m = \text{dim}(\text{Im}(L))$. Thus,
            \begin{align*}
               n = m  + \text{dim}(\text{ker}(L))
            .\end{align*}
            So, $\text{dim}(\text{ker}(L)) \geq 1$, and $L$ is not injective. But, $L$ is surjective since the dimension of the image is equal to the dimension of the codomain, so the image of the map is the same space as the codomain.

        \item \textbf{Surjective and bijective in terms of matrix transformations}:
            \bigbreak \noindent 
            If a matrix $A$ represents a linear transformation from $\mathbb{R}^n \to \mathbb{R}^m$, the transformation is onto if for every vector $b \in \mathbb{R}^m$ in the codomain, there exists at least one vector $x \in \mathbb{R}^n$ such that $Ax = b$.
            \bigbreak \noindent 
            \textbf{Geometric Interpretation}: Surjectivity means the matrix transformation "covers" the entire codomain, hitting every possible point. In 2D, this would mean the entire plane is covered by the transformation.
            \bigbreak \noindent 
            A matrix $A$ is injective if for every $x_1$ and $x_2$ in the domain, if $Ax_1 = Ax_2$, then $x_1 = x_2$. This means that no two distinct input vectors can be mapped to the same output vector.
            \bigbreak \noindent 
            \textbf{Geometric Interpretation}: Injectivity means the transformation doesn’t collapse any part of the domain into a lower dimension, so no information is lost.
        \item \textbf{If dimensions are the same}: If $\mathbb{R}^{n} \to \mathbb{R}^{n}$, if its onto, its most likely 1-1.
        \item \textbf{Check if a linear map is injective or surjective (Formal)}: To check whether a linear transformation is surjective or injective, we use specific properties of the matrix representing the transformation.
            \bigbreak \noindent 
            Let $T:\ V \to W$ be a linear transformation, where $V$ and $W$ are vector spaces and $A$ is the matrix representation of $T$
            \begin{itemize}
                \item \textbf{Checking Injectivity (One-to-One)}: A linear transformation $T$ is injective (one-to-one) if:
                    \begin{align*}
                        T(\mathbf{v}_{1}) = T(\mathbf{v}_{2}) \implies \mathbf{v}_{1} = \mathbf{v}_{2}
                    .\end{align*}
                    \bigbreak \noindent 
                    Equivalently, $T$ is injective if the only solution to $T(\mathbf{v})=0$ (the null space or kernel of $T$) is $\mathbf{v}=0$.
                    \bigbreak \noindent 
                    Alternatively, a matrix $A$ is injective if the rank of the matrix (the number of linearly independent columns) is equal to the number of columns of the matrix. This means the matrix has full column rank
                    \bigbreak \noindent 
                    Lastly, we can just check the determinant if the matrix is square.
                \item \textbf{Check surjectivity}:
                    A linear transformation \( T \) is surjective (onto) if for every vector \( \mathbf{w} \in W \) (the codomain), there exists a vector \( \mathbf{v} \in V \) (the domain) such that:
                    \[
                        T(\mathbf{v}) = \mathbf{w}.
                    \]
                    This means that \( T \) "covers" the entire codomain \( W \), or in other words, the image of \( T \) is the entire space \( W \).
                    \bigbreak \noindent 
                    \textbf{Steps to check surjectivity:}
                    \begin{enumerate}
                        \item \textbf{Image:} The transformation \( T \) is surjective if the image (column space or range) of the matrix \( A \) spans the entire codomain \( W \). This means 
                            \[
                                \text{im}(A) = W.
                            \]
                        \item \textbf{Rank:} For surjectivity, the rank of the matrix must be equal to the dimension of the codomain. If \( A \) is an \( m \times n \) matrix, the matrix is surjective if its rank is equal to \( m \) (the number of rows).
                        \item \textbf{Determinant for square matrices:} If \( A \) is a square matrix, \( T \) is surjective if 
                            \[
                                \det(A) \neq 0,
                            \]
                            because a non-zero determinant implies the matrix has full rank, covering the entire codomain.
                    \end{enumerate}
            \end{itemize}
        \item \textbf{Injective and surjective for square matrices}: For square matrices (say $A \in \mathbb{R}^{n\times n}$), injectivity and surjectivity are actually equivalent, since the number of linearly independent rows equals the number of linearly independent columns which both equal $n$ if the matrix has full rank.




    \end{itemize}

    \pagebreak 
    \unsect{Matrix rank}
    \begin{itemize}
        \item \textbf{Row-echelon form}:
            Row echelon form (REF) is a standardized way of arranging the rows of a matrix using elementary row operations. A matrix is in row echelon form if it satisfies these conditions:
            \begin{itemize}
                \item All zero rows are at the bottom. (If a row is entirely zeros, it appears below any nonzero row.)
                \item The first nonzero entry in each nonzero row is 1 (called a leading 1 or pivot).
                \item Each leading 1 is to the right of the leading 1 in the row above it. (So the pivots “stair-step” down to the right as you move down the rows.)
                \item All entries below a pivot are zero.
            \end{itemize}
            The matrix $A \in \mathbb{R}^{4\times 4}$
            \begin{align*}
                A = \begin{bmatrix}
                    1 & 2 & 3 & 4 \\
                    0 & 1 & 5 & 6 \\
                    0 & 0 & 0 & 1 \\
                    0 & 0 & 0 & 0
                \end{bmatrix}
            \end{align*}
            is in row-echelon form
        \item A non square matrix cannot be invertible.
        \item \textbf{Rank of a square matrix}:  For an $n \times n$ square matrix, the \textbf{rank} is the dimension of its row space (or column space---they are equal).
            \bigbreak \noindent 
            If $\text{rank}(A) = n$, the matrix is \textbf{full rank} and invertible.
            \begin{itemize}
                \item The linear map $A: \mathbb{R}^n \to \mathbb{R}^n$ is:
                    \begin{itemize}
                        \item \textbf{Injective} (one-to-one): no two distinct inputs map to the same output.
                        \item \textbf{Surjective} (onto): every element of $\mathbb{R}^n$ is hit by some input.
                        \item Together, injective + surjective = \textbf{bijective}, meaning $A$ is invertible.
                    \end{itemize}
            \end{itemize}
            \bigbreak \noindent 
            \textbf{Note:} If $A \in \mathbb{R}^{n\times n}$ has full rank, then $A$ is nonsingular and $\det(A) \ne 0$
            \bigbreak \noindent 
            If $\text{rank}(A) < n$, then:
            \begin{itemize}
                \item The matrix is singular (non-invertible).
                \item The map is neither injective (kernel is nontrivial) nor surjective (image is a proper subspace).
            \end{itemize}
            \bigbreak \noindent 
            \textbf{Note:} If $A$ is rank-deficient, then $A$ is singular and $\det(A) = 0$.

        \item \textbf{Rank of a non-square matrix}:
                 For an $m \times n$ matrix $A$, the \textbf{rank} is the dimension of the row space (or column space). 
                    \[
                        \text{rank}(A) \leq \min(m,n).
                    \]
                    Consider the map $A: \mathbb{R}^n \to \mathbb{R}^m$:
                    \begin{itemize}
                        \item If $m > n$ (tall matrix):
                            \begin{itemize}
                                \item The maximum rank is $n$.
                                \item If $\text{rank}(A) = n$, then the map is \textbf{injective} (no two inputs give the same output), but not surjective, since the codomain has higher dimension than the image.
                            \end{itemize}
                        \item If $m < n$ (wide matrix):
                            \begin{itemize}
                                \item The maximum rank is $m$.
                                \item If $\text{rank}(A) = m$, then the map is \textbf{surjective} (fills all of $\mathbb{R}^m$), but not injective, since extra input dimensions collapse.
                            \end{itemize}
                    \end{itemize}
        \item \textbf{Determine the rank of a square matrix}: We check the determinant. If $\det(A) \ne 0$, the matrix $A$ has full rank and is nonsingular. If $\det(A) = 0$, the matrix $A$ is rank-deficient and is singular.
        \item \textbf{Determine the rank of a non-square matrix}: To determine the rank of a matrix, we need to make sure the $m\times n$ matrix has $m$ linearly independent rows. To do this, we can perform guassian elimination to get the augmented matrix in row echelon form. The rank of the matrix, $\text{rank}(A)$ is the number of non zero rows.
        \item \textbf{Column rank and row rank}: The number of linearly independent columns is called the column rank, and the number of linearly independent rows is called the row rank.
            \bigbreak \noindent 
            For any matrix $A$, the number of linearly independent rows is equal to the number of linearly independent columns. This number is called the rank of $A$
        \item \textbf{What does the rank tell use about solutions}: If we have full rank, then given a target, there will be a unique solution. If we are rank-deficient, there may be no solution, or infinitely many soluitions.




            % \item \textbf{Rank of a matrix}: The rank of a matrix refers to the number of linearly independent rows or columns in the matrix. Essentially, the rank tells you the "dimension" of the space spanned by the rows or columns.
        %     \begin{itemize}
        %         \item \textbf{Row rank:} The number of linearly independent rows.
        %         \item \textbf{Column rank:} The number of linearly independent columns.
        %     \end{itemize}
        %     For any matrix, the row rank is always equal to the column rank. This common number is simply called the rank of the matrix.
        % \item \textbf{Full Rank}: A matrix is said to have full rank if its rank is as large as possible, meaning that all of its rows or columns are linearly independent.
        %     \bigbreak \noindent 
        %     The rank of an $m \times n$ matrix is bounded by 
        %     \begin{align*}
        %         Rank(A) \leq min(m,n)
        %     .\end{align*}
        %     \bigbreak \noindent 
        %     If a matrix has full rank, it means the matrix is capable of fully transforming vectors in the space without collapsing dimensions.
        %     \bigbreak \noindent 
        %     If the matrix has full rank, it is invertible. This is because the matrix does not collapse any part of the vector space it transforms.
        %     \bigbreak \noindent 
        %     If the matrix has 
        %     \begin{align*}
        %         det(A) \ne 0
        %     .\end{align*}
        %     It has full rank
        %     \bigbreak \noindent 
        %     When a matrix has full rank, it essentially means that the system of equations it represents is well-behaved, and every input gets mapped to exactly one output without overlap
        %     \bigbreak \noindent 
        %     When a matrix has full rank, it has as many independent equations as there are variables (in a square matrix case). This means that each variable (input) has its own effect on the output, and no two variables end up pointing to the same place.
        % \item \textbf{Rank-Deficient}: A matrix is rank-deficient if its rank is less than the maximum possible rank. In other words, some of the rows or columns are linearly dependent (one can be expressed as a combination of others).
        %     \bigbreak \noindent 
        %     When a matrix is rank-deficient, it means the transformation it represents collapses some part of the space into a lower dimension. 
        %     \begin{itemize}
        %         \item In $\mathbb{R}^{2}$, a rank-1 matrix would map all points onto a line, losing one dimension. $\mathbb{R}^{3}$, a rank-2 matrix would map all points onto a plane, collapsing one dimension of the space.
        %     \end{itemize}
        %     If the matrix has 
        %     \begin{align*}
        %         det(A) = 0
        %     .\end{align*}
            %     It is rank deficient

        \item \textbf{Rank theorem}: Let $A$ be a matrix that represents a linear map $L$, the rank of the matrix is given by
            \begin{align*}
                \text{Rank}(A) = \text{dim}(\text{Im($L $)})
            .\end{align*}
            Furthermore
            \begin{align*}
                \text{Rank}(A) = \text{Rank}(A^{T})
            .\end{align*}
        \item \textbf{Solutions of a tall matrix}: Suppose $L:\; \mathbb{R}^{n} \to \mathbb{R}^{m}$ is represented by $A \in \mathbb{R}^{m\times n}$, with $m > n$. If $Ax = b$ has a unique solution for a given $b$, then two things must be true 
            \begin{enumerate}
                \item $b \in \text{Im}(L)$
                \item $\text{Rank}(A) = n$ (full column rank)
            \end{enumerate}
            If $A$ has full column rank, then $L$ is injective but not surjective, so $Ax = 0$ implies $x = 0$, and $\text{Im}(L) \subsetneq \mathbb{R}^{m}$.
            \bigbreak \noindent 
            Suppose that $A$ was rank deficient. Then, there are multiple members of $\mathbb{R}^{n}$ that map to zero. If $x_{0}$ is a solution to $Ax = b$, then $x_{0} + v$, for $v \in \text{Ker}(L)$ also solves $Ax = b$, since
            \begin{align*}
                A(x_{0} + v) = b \implies Ax_{0} + Av = b \implies Ax_{0} + 0 = b \implies Ax_{0} = b
            .\end{align*}
            Thus, infinitely many solutions, since $\text{rank}(A) < n$ implies $\text{dim(ker)}(L) \geq 1 $, so there are infinitely many vectors in the Kernel.
        \item \textbf{Solutions of a wide matrix}: Suppose that $L:\; \mathbb{R}^{n} \to \mathbb{R}^{m}$ is represented by a matrix $A \in \mathbb{R}^{m\times n}$, where $m < n$. In this case, $L$ can be surjective (if $\text{rank}(A) = m$), but not injective. Thus, the only possibilities are no solution, or infinitely many solutions.
            \bigbreak \noindent 
            If $A$ is rank deficient, then $L$ is not surjective, so $\text{Im}(L) \subsetneq \mathbb{R}^{m}$. In this case, if $b \not\in \text{Im}(L)$, no solution exists. If $A$ has full rank, then $\text{Im}(L) = \mathbb{R}^{m}$. So, for any $b \in \mathbb{R}^{m}$, there are infinitely many solutions. Suppose that $x_{0}$ solves $Ax = b$. Then, $x_{0} + v$, for $v\in \text{ker}(L)$ also solves $Ax = b$, since
            \begin{align*}
                A(x_{0} + v) = b \implies Ax_{0} + Av = b \implies Ax_{0} + 0 = b \implies Ax_{0} = b
            .\end{align*}
        \item \textbf{Solutions of a square matrix}: Suppose that $L:\; \mathbb{R}^{n} \to \mathbb{R}^{n}$ is represented by a matrix $A \in \mathbb{R}^{n\times n}$. If $\det(A) \ne 0$, then $A$ is both injective and surjective, and thus bijective. In this case, there is a unique solution for any given vector $b \in \mathbb{R}^{n}$.
            \bigbreak \noindent 
            If $\det(A) = 0$, $A$ is neither surjective nor injective. In this case, for a given $b$, there is either no solution, or infinitely many solutions.
            \begin{enumerate}
                \item If $b \notin \text{Im}(L)$, no solution.
                \item If $b \in \text{Im}(L)$, infinitely many solutions.
            \end{enumerate}
        \item \textbf{Rank with rank-nullity theorem}: For $A\in \mathbb{R}^{m\times n}$, using the rank-nullity theorem, we have
            \begin{align*}
                \text{rank}(A) = n - \text{dim}(\text{ker}(A))        
            .\end{align*}
            So, we see that the rank of a matrix measures how many dimensions survive under the linear map.



    \end{itemize}

    \pagebreak 
    \unsect{Linear dependence, span, intro to fundamental spaces}
    \begin{itemize}
        \item \textbf{Linear dependence}: Linear dependence tells us that we will lose information because it implies that some vectors (or columns/rows of a matrix) do not add any new, unique directions to the space. These dependent vectors can be expressed as combinations of other vectors, meaning they don't span new dimensions, and as a result, the transformation collapses part of the input space into a lower-dimensional output space.
        \item \textbf{Linear independence}: No redundancy, all directions are unique
            \bigbreak \noindent 
        A set of vectors $\{v_1, v_2, \dots, v_k\}$ in a vector space is linearly independent if no vector in the set can be written as a linear combination of the others.
        \bigbreak \noindent 
        Equivalently, the only solution to the equation
        \[
            c_1 v_1 + c_2 v_2 + \dots + c_k v_k = 0
        \]
        is $c_1 = c_2 = \dots = c_k = 0$, where $c_i$ are scalars.
        \item \textbf{Span}: The span of a set of vectors is the collection of all possible linear combinations of those vectors
            \bigbreak \noindent 
            if a matrix has full rank, the span of its columns (or rows) is the entire codomain (or the entire vector space that the matrix maps to)
            \bigbreak \noindent 
            The vectors in a linearly independent set span a space whose dimension equals the number of vectors in the set.
            \bigbreak \noindent 
            If a set contains more vectors than the dimension of the vector space, the set cannot be linearly independent (e.g., in $\mathbb{R}^n$, any set with more than $n$ vectors is dependent).
            \bigbreak \noindent 
            If a set contains fewer vectors than the dimension of the vector space, it cannot span the entire space.
    \item \textbf{Prove that a set of vectors is linearly dependent}: 
To show that a set of vectors is linearly independent, you need to prove that the only solution to the linear combination equating the set to the zero vector is the trivial solution
\bigbreak \noindent 
For a set of vectors $\{v_1, v_2, \dots, v_k\}$, consider the equation:
\[
c_1 v_1 + c_2 v_2 + \dots + c_k v_k = 0
\]
where $c_1, c_2, \dots, c_k$ are scalars (unknowns).
\bigbreak \noindent 
    Place the vectors as columns in a matrix $A = [v_1 \ v_2 \ \dots \ v_k]$.
    \bigbreak \noindent 
    The equation then becomes
    \[
    A c = 0
    \]
    where $c = 
    \begin{bmatrix}
    c_1 \\ 
    c_2 \\ 
    \vdots \\ 
    c_k 
    \end{bmatrix}
    $
    is the column vector of scalars.
    \bigbreak \noindent 
    Solve the homogeneous system 
    \[
        A  c = 0.
    \]
    If the only solution is 
    \[
        c_1 = c_2 = \dots = c_k = 0
    \]
    (the trivial solution), then the vectors are linearly independent.
    \bigbreak \noindent 
    If the set of vectors forms a square matrix (i.e., the number of vectors equals their dimension), compute the determinant of the matrix.
    \bigbreak \noindent 
    If $\det(A) \neq 0$, the vectors are linearly independent. If $\det(A) = 0$, they are dependent.

    \item \textbf{Geometric Interpretation}:
        In $\mathbb{R}^{2}$, two linearly independent vectors are not collinear (do not lie on the same line through the origin).
        \bigbreak \noindent 
        In $\mathbb{R}^{3}$, three linearly independent vectors are not coplanar (do not lie in the same plane through the origin).

        \item \textbf{Intro to row space, column space, null space (kernel)}:
            \begin{itemize}
                \item \textbf{Column space $(C(A))$}:  The set of all linear combinations of the columns of a matrix $A$. It represents the range of the matrix and consists of all possible outputs of the matrix transformation. It's a subspace of $\mathbb{R}^{m}$ (for an $m\times n$ matrix)
                \item \textbf{Row space $(R(A))$}: The set of all linear combinations of the rows of a matrix $A$. It is the span of the rows of the matrix and forms a subspace of $\mathbb{R}^{n}$
                \item \textbf{Kernel/Null space $(\text{ker}(A)/N(A))$}: The set of all vectors $\mathbf{x}$ such that $A\mathbf{x}=0$, where $A$ is a matrix. It represents the solutions to the homogeneous system and is a subspace of $\mathbb{R}^{n}$
            \end{itemize}
            \bigbreak \noindent 
            Each of these spaces relates to the structure and solvability of linear systems and the transformation properties of the matrix.
    \item \textbf{Orthogonality}: If vectors are orthogonal (mutually perpendicular), they are linearly independent.
        \bigbreak \noindent 
        However, linearly independent vectors do not need to be orthogonal.

    \item \textbf{Linear combination of vectors}: A linear combination of vectors is a sum of those vectors, each multiplied by a scalar. So, for a matrix $A$, if $\mathbf{v}_{1}, \mathbf{v}_{2}, ..., \mathbf{v}_{n}$ are its column vectors, any vector in the column space can be written as:
        \begin{align*}
            c_{1}\mathbf{v}_{1} + c_{2}\mathbf{v}_{2} + ... + c_{n}\mathbf{v}_{n}
        .\end{align*}
        \bigbreak \noindent 
        Where $c_{1}, c_{2}, ... c_{n}$ are scalars
        \bigbreak \noindent 
        As you vary the scalars in the linear combination of the matrix's columns, you generate all possible vectors in the column space (also known as the range) of the matrix.
        \bigbreak \noindent 
        \textbf{Note:} The same applies for the row space
    \item \textbf{Row space vs column space vs null space}:
        \begin{itemize}
            \item The column space tells you what the matrix outputs.
            \item The row space tells you about the constraints or conditions that the solutions to the matrix system must satisfy.
            \item The null space tells you what the matrix "loses". If a vector $\mathbf{x}$ is in the null space, it gets mapped to the zero vector, meaning it is "annihilated" by the matrix. Vectors in the null space represent dependencies between the columns of the matrix. If the matrix has non-trivial solutions to $A\mathbf{x}=0$, it indicates that the columns are linearly dependent.
                \bigbreak \noindent 
                Geometrically, the null space represents all the directions in which the matrix compresses space to a lower dimension.
                For example, in $\mathbb{R}^{3}$, if the null space is a line, the matrix compresses all points along that line to the origin.
                \bigbreak \noindent 
                If the null space is non-trivial, it indicates the matrix transformation has lost some dimensions.
                \bigbreak \noindent 
                \textbf{Note:} A non-trivial null space refers to a null space that contains vectors other than just the zero vector.
        \end{itemize}
    \item \textbf{More on the row space}: In a system of linear equations, the row space reveals various types of constraints, depending on the number of independent equations and how the rows of the matrix relate to one another. 
        \begin{itemize}
            \item \textbf{Unique Solution (Full Rank, Independent Rows)}: If the rows of the matrix are linearly independent and span the entire row space, the system has a unique solution. This means the constraints from the equations are sufficient to pin down exactly one solution.
            \item \textbf{No Solution (Inconsistent System):}
            \item \textbf{Infinite Solutions (Dependent Rows, Underdetermined System):} If some rows are linearly dependent, the system will have fewer constraints than unknowns, leading to infinitely many solutions. In this case, the system is underdetermined, meaning there aren’t enough independent constraints to specify a unique solution, allowing multiple solutions (often forming a plane, line, or higher-dimensional space).
            \item \textbf{Zero Solutions (Trivial System):} In a homogeneous system , if the rows are independent but fewer than the number of variables, there is only the trivial solution. This means that the row space spans a subspace of dimension less than the total space, so the only solution is the zero vector.
        \end{itemize}
    \item \textbf{Linear dependence in rows vs columns}:
        The rows of a matrix are linearly dependent if at least one row can be written as a linear combination of the other rows.
        \bigbreak \noindent 
        This means there is redundancy in the information that the rows provide.
        \bigbreak \noindent 
        If the rows of a matrix are linearly dependent, the row space has a lower dimension than the number of rows, meaning the system has fewer independent constraints than it might appear.
        \bigbreak \noindent 
        Consider the matrix
        \begin{align*}
            \begin{bmatrix} 1 & 2 & 3 \\ 2 & 4 & 6 \\ 3 & 6 & 9 \end{bmatrix}
        .\end{align*}
        \bigbreak \noindent 
        Here, the second row is $2\times$ the first row, and the third row is $3\times$ the first row. Therefore, the rows are linearly dependent because each row is a scalar multiple of the first row.
        \bigbreak \noindent 
        This means there’s only one independent constraint in the row space. The row space is spanned by a single vector (the first row), even though the matrix has three rows. Geometrically, the row space collapses to a lower dimension (a line in 3D space).
        \bigbreak \noindent 
        If the rows are linearly dependent, the system of equations might be underdetermined, leading to infinite solutions or no solutions.
        \bigbreak \noindent 
        The columns of a matrix are linearly dependent if at least one column can be written as a linear combination of the other columns.
        \bigbreak \noindent 
        This implies that some columns do not contribute "new" information, and there is a loss of dimensionality in the column space.
        \bigbreak \noindent 
        If the columns are linearly dependent, the column space has a lower dimension than the number of columns, meaning the matrix cannot map onto all of $\mathbb{R}^{m}$ (the output space).
        \bigbreak \noindent 
        Consider the matrix
        \begin{align*}
            \begin{bmatrix} 1 & 2 & 3 \\ 1 & 2 & 3 \\ 1 & 2 & 3\end{bmatrix}
        .\end{align*}
        \bigbreak \noindent 
        Here, the second and third columns are linearly dependent on the first column (they are multiples of the same vector). In fact, each column is identical in this case, so all the columns are linearly dependent.
        \bigbreak \noindent 
        This means the column space of this matrix is spanned by a single vector, even though there are three columns.
        \bigbreak \noindent 
        Geometrically, the matrix can only map vectors to a line in $\mathbb{R}^{3}$, rather than a full plane or space.
        \bigbreak \noindent 
        \textbf{Summary:}
        \begin{itemize}
            \item \textbf{Row dependence} affects the row space, which corresponds to the constraints on the system of equations. Linearly dependent rows mean some equations are redundant, reducing the number of independent constraints.
            \item \textbf{Column dependence} affects the column space, which corresponds to the set of possible outputs of the matrix. Linearly dependent columns mean the matrix has reduced rank, implying the system might not span the full space, and it could have a non-trivial null space (leading to infinite or no solutions).
        \end{itemize}
        \bigbreak \noindent 
        \textbf{Recall}: To check if a linear map has full rank, it is sufficient to check whether all the columns of the matrix representing the linear map are linearly independent
        \bigbreak \noindent 
        \textbf{Important:} The dimension of both the row space and the column space of a matrix is equal to the number of linearly independent rows and linearly independent columns, respectively. This common dimension is called the rank of the matrix.
    \item \textbf{Rank of the null space}: The null space of a matrix has dimension $n-$rank, where $n$ is the number of columns in the matrix. This is a consequence of the Rank-Nullity Theorem (Not yet stated).
    \item \textbf{Nullity}: The nullity of a matrix is the dimension of the null space (i.e., the number of independent vectors that get mapped to the zero vector by the matrix).
    \item \textbf{Zero nullity}: If the nullity is zero, this means that the null space has dimension 0. This implies that the only vector in the null space is the zero vector itself, $\mathbf{0}$
    \item \textbf{non-zero nullity}: 
        We know that there are infinitely many vectors in the kernel (null space) of a matrix when the nullity (dimension of the null space) is greater than zero.
    \end{itemize}

    \pagebreak 
    \unsect{Row space, Column space, Null space, and the Rank Nullity Theorem}
    \begin{itemize}
        \item \textbf{Null space (Kernel)}: Suppose $L :\; V \to W$ is a linear map between vector spaces $V$ and $W$. Then the kernel of $L$ is defined as
            \begin{align*}
                \text{Ker}(L) = \{v\in V:\ L(v) = 0\} \subseteq V
            \end{align*}
            Note that the kernel is a vector space.
        \item \textbf{Image}: Suppose $L :\;  V \to W$ is a linear map between vector spaces $V$ and $W$. Then, the image of $L$, also called the range of $L$ is the subspace 
            \begin{align*}
                \text{Im}(L) = \{w \in W:\; w = L(v),\; v \in V\} \subseteq W
            \end{align*}
            In other words, all vectors $w\in W$ such that $w$ is the image of a vector in $v$. If $L$ is onto, then every vector in the codomain $W$ is the image of a vector in $v$, thus $R_{L}$ will be all vectors in $W$. If $W$ has dimension $n$ and $W$ is onto, then $\text{dim}(R_{L}) = \text{dim}(W)$. If $L$ is not onto, then $\text{dim}(R_{L}) \leq \text{dim}(w)$
        \item \textbf{Column space}: If $L:\; V \to W$ is a linear map between vector spaces $V $ and $W$, and $L$ is represented by a matrix $A \in \mathbb{R}^{m\times n} $, then the \textbf{column space} of $A$, denoted $\text{Col}(A) $ is precisely the Image of $L$. That is,
            \begin{align*}
                \text{Col}(A) = \text{Im}(L)
            \end{align*}
            The column space of $A$ is the set of all linear combinations of the columns of $A$. If $A$ has columns $c_{1}, c_{2}, ..., c_{n}$, then $\text{Col}(A)$ is 
            \begin{align*}
                \text{Col}(A) = \{w \in W:\ w = s_{1}c_{1} + s_{2}c_{2} + ... + s_{n}c_{n}\}
            \end{align*}
            i.e it is the span of the columns of $A$
        \item \textbf{Row space}: Let $L:\; V \to W$ be a linear map between vector spaces $V $ and $W $. If $L$ is represented by a matrix $A \in \mathbb{R}^{m\times n} $.
            \bigbreak \noindent 
            The row space of $A$ is the span of the rows of $A$. The row space of $A$ is defined as
            \begin{align*}
                \text{Row}(A) = \text{Col}(A^{\top}) = \text{Im}(L^{\top})
            \end{align*}
            Where $L^{\top}$ is the transpose map $L^{\top}:\; W^{*} \to V^{*} $
        \item \textbf{The orthogonal complement of the kernel is the row space}: For a linear map $L:\; V \to W$ between vector spaces $V$ and $W$, we have that
            \begin{align*}
                \text{Ker}(L)^{\perp} = \text{Im}(L^{\top}) = \text{Row}(A)
            \end{align*}
            Where $A$ is the matrix that represents $L$.
        \item \textbf{Rank nullity theorem}: 
            The \textbf{Rank-Nullity Theorem} states that for a linear map \( L: V \to W \) between two vector spaces \( V \) and \( W \), the dimension of the domain \( V \) is the sum of the rank and the nullity of \( L \). Mathematically, it is expressed as:
            \[
                \dim(V) = \dim(\text{ker}(L)) + \dim(\text{Im}(L))
            \]
            Where:
            \begin{itemize}
                \item \( \dim(V) \) is the dimension of the domain vector space \( V \),
                \item \( \dim(\text{ker}(L)) \) is the \textbf{nullity} of \( L \), i.e., the dimension of the \textbf{null space} (the set of vectors in \( V \) that map to the zero vector in \( W \)),
                \item \( \dim(\text{im}(L)) \) is the \textbf{rank} of \( L \), i.e., the dimension of the \textbf{image} (the set of all vectors in \( W \) that are the image of some vector in \( V \)).
            \end{itemize}
            In simpler terms, the dimension of the vector space \( V \) is the sum of the number of independent vectors that are mapped to zero and the number of independent vectors that are mapped to non-zero vectors.
            \bigbreak \noindent 
            Thus, the dimensions of the kernel and image of a linear map must sum to the dimension of the domain.

        \item \textbf{Basis for the column space}:
                        We can find a description of the image by row reducing the matrix associated with a linear map. The number of leading zeros in the rows can give us not only the dimension of the image, but also a basis by using the columns from the original matrix where the leading ones appear in the row reduced matrix. Consider an example.
            \bigbreak \noindent 
            Suppose we have the matrix
            \begin{align*}
                A &= \begin{pmatrix} 1 & 2 & 0 & 1 & 0 \\ 1 & 2 & 1 & 2 & 1 \\ 2 & 4 & 1 & 3 & 1 \\ 3 & 6 & 2 & 5 & 1\end{pmatrix}
            .\end{align*}
            Which represents a linear map $L:\ \mathbb{R}^{5} \to \mathbb{R}^{4}$ with respect to the standard basis. Row reducing this matrix yields
            \begin{align*}
                \begin{pmatrix}
                    1 & 2 & 0 & 1 & 0 \\ 0 & 0 & 1 & 1 & 0 \\ 0 & 0 & 0 & 0 & 1 \\ 0 & 0 & 0 & 0 & 0
                \end{pmatrix}
            .\end{align*}
            \bigbreak \noindent 
            As we can see, we have three leading ones, in columns 1,3, and 5. Thus, the dimension of the image is three. Furthermore, we can use these columns from the original matrix as a basis for the image. Thus,
            \begin{align*}
                \begin{pmatrix} 1 \\ 1 \\ 2 \\ 3\end{pmatrix}, \begin{pmatrix} 0 \\ 1 \\ 1 \\ 2 \end{pmatrix}, \begin{pmatrix} 0 \\ 1 \\ 1 \\ 1\end{pmatrix}
            .\end{align*}
            Is a basis for the image. The other columns are linearly dependent on these three vectors. 
            \bigbreak \noindent 
            \textbf{Note:} If the matrix is surjective, the standard basis forms a basis for the column space.
            \bigbreak \noindent 
            If the matrix is injective, the kernel has no basis.



    \end{itemize}


    \pagebreak 
    \unsect{Inverses}
    \begin{itemize}
        \item \textbf{Matrix inverse}: A matrix $B \in \mathbb{R}^{n\times n}$ is the inverse of a matrix $A \in \mathbb{R}^{n\times n}$ provided
            \begin{align*}
                BA = AB = I
            \end{align*}
        \item \textbf{Left and right inverse}:
            \textbf{Left Inverse:} A matrix \( B \) is a left inverse of \( A \) if \( BA = I \), where \( I \) is the identity matrix. This means \( B \) "undoes" \( A \) when multiplied from the left.
            \bigbreak \noindent 
            \textbf{Right Inverse:} A matrix \( C \) is a right inverse of \( A \) if \( AC = I \). This means \( C \) "undoes" \( A \) when multiplied from the right.
        \item \textbf{(Finite dimensions) One sided inverse theorem for matrices (inveribility criterion)}: It states that for square matrices in finite dimensions, the existence of either a left inverse or a right inverse implies the existence of the other, and hence the matrix is invertible. Specifically, if a square matrix \( A \) has a left inverse \( B \) (such that \( BA = I \)) or a right inverse \( C \) (such that \( AC = I \)), then both inverses must be equal, making \( A \) invertible with the unique inverse \( A^{-1} = B = C \).
        \item \textbf{(Finite dimensions) Inverse Uniqueness Theorem for matrices}: If \( A \) is an invertible \( n \times n \) matrix, then there is exactly one matrix \( B \) such that \( AB = BA = I \). This unique matrix is called the inverse of \( A \) and is denoted by \( A^{-1} \).
        \item \textbf{Solving $Ax = b$ for $A \in \mathbb{R}^{n\times n}$}: Assume $A \in \mathbb{R}^{n\times}$ is nonsingular, then
            \begin{align*}
                Ax = b \implies x = A^{-1}b
            \end{align*}
        \item \textbf{Matrix products}: If $A,B \in \mathbb{R}^{n\times n}$, and $AB$ nonsingular, then $A$ and $B$ are nonsingular.
        \item $(AB)^{-1}  = B^{-1}A^{-1}$
    \end{itemize}

    \pagebreak 
    \unsect{Determinant}
    \begin{itemize}
        \item \textbf{Determinant of the identity matrix}: The determinant of the identity matrix is one 
            \begin{align*}
                \text{det}(I) = 1
            .\end{align*}
        \item \textbf{Product of two determinants}: The product of two determinants $\text{det}(A)\text{det}(B) = \text{det}(AB)$. This is particularly useful when we want to find the determinant of a product of two matrices, we can assert, for two matrices $A$, and $B$
            \begin{align*}
                \text{det}(AB) = \text{det}(A)\text{det}(B)        
            .\end{align*}
        \item \textbf{Determinant of the inverse matrix}: If we have some matrix $A$, which has an inverse $A^{-1}$, then
            \begin{align*}
                \text{det}(AA^{-1}) &= \text{det}(I) = 1 = \text{det}(A)\text{det}(A^{-1}) \\
                \implies \text{det}(A^{-1}) &= \frac{1}{\text{det}(A)}
            .\end{align*}
        \item \textbf{Determinant of triangular matrices}: The determinant of a triangular matrix is the product of the main diagonal.
        \item \textbf{Determinant of the transpose}: The determinant of the transpose of a matrix is the same as the original matrix.
            \begin{align*}
                \det(A^{\top})  = \det(A)
            \end{align*}
        \item \textbf{Determinant after changing basis}: Suppose we have some matrix $\begin{pmatrix} a & b \\ c & d\end{pmatrix} $. If we introduce a new basis matrix $B$, then the map becomes $B^{-1}AB$. And the deterimanant is
            \begin{align*}
                \det(B^{-1}AB) &= \det(B^{-1})\det(A)\det(B) \\
                               &= \det(B^{-1}B)\det(A) \\
                               &= \det(I)\det(A) \\
                               &= \det(A)
            .\end{align*}
            Thus, the determinant remains the same. We say that the determinant is invariant under a change of basis.


    \end{itemize}

    \pagebreak 
    \unsect{Other matrix properties}
    \begin{itemize}
                \item \textbf{The trace of a matrix}: The trace of a square matrix is the sum of its diagonal elements. For an 
            $n \times n$ matrix $A$, the trace is defined as:
            \[
                \text{Tr}(A) = \sum_{i=1}^{n} A_{ii}
            \]
            where $A_{ii}$ are the diagonal entries of $A$. The trace is only defined for square matrices and has several useful properties, such as being invariant under a change of basis.
        \item \textbf{Cyclic property of the trace}: The trace is invariant under cyclic permutations. Observe
            \begin{align*}
                \text{Tr}(AB) = \text{Tr}(BA)
            .\end{align*}
            \bigbreak \noindent 
            Proof:
            \begin{align*}
                \text{Tr}(AB) &= \sum_{i=1}^{n}(ab)_{ii}  \\
                &=\sum_{i=1}^{n}\sum_{k=1}^{n}a_{ik}b_{ki}
            .\end{align*}
            \bigbreak \noindent 
            \textbf{Remark}. For finite sums, the order of summation can be swapped because summation is commutative and associative when dealing with real or complex numbers. This means that the sum of a collection of terms does not depend on the order in which the terms are added. So, rearranging the summation over $i$ and $k$ doesn't change the value of the overall sum. Thus
            \begin{align*}
                \text{Tr}(AB) &= \sum_{i=1}^{n}\sum_{k=1}^{n}a_{ik}b_{ki} \\
                              &= \sum_{k=1}^{n}\sum_{i=1}^{n}a_{ik}b_{ki} \\
                              &= \sum_{k=1}^{n}\sum_{i=1}^{n}b_{ki} a_{ik} \\
                              &= \sum_{k=1}^{n}(ba)_{kk} \\
                              &= \text{Tr}(BA)
            .\end{align*}

    \end{itemize}

    \pagebreak 
    \unsect{Eigenvalues and Eigenvectors}
    \begin{itemize}
         \item \textbf{Eigenvectors, Eigenvalues}:
            An \textbf{eigenvector} of a square matrix \( A \) is a non-zero vector \( \mathbf{v} \) such that when \( A \) acts on \( \mathbf{v} \), the result is a scalar multiple of \( \mathbf{v} \). Mathematically, this is written as:
            \[
                A \mathbf{v} = \lambda \mathbf{v} \quad \mathbf{v} \ne \mathbf{0}
            \]
            where \( \lambda \) is a scalar known as the eigenvalue corresponding to the eigenvector \( \mathbf{v} \).
            \bigbreak \noindent 
            An \textbf{eigenvalue} \( \lambda \) is the scalar that represents the factor by which the eigenvector is scaled during the transformation. The eigenvalue corresponds to each eigenvector and provides information about the nature of the transformation (scaling, rotation, etc.).
            \bigbreak \noindent 
            Since the left side is matrix multiplication and the right side is vector multiplication by a scalar, we can rewrite the equation above as
            \begin{align*}
                A\mathbf{v} &= (\lambda I)\mathbf{v} \\
                \implies A\mathbf{v} - \lambda I \mathbf{v} &= \mathbf{0} \\
                \implies \mathbf{v}(A-\lambda I) &= \mathbf{0}
            .\end{align*}
            \bigbreak \noindent 
            This is a homogeneous system. If the transformation map ($A-\lambda I$) is one-to-one and thus invertible, the only solution would be the trivial solution ($\mathbf{v}  = \mathbf{0})$. In order to have non-zero solutions for $\mathbf{v}$ (eigenvectors), the system above would need to not be one-to-one (multiple solutions to the solution vector $\mathbf{0}$), and thus
            \begin{align*}
                \text{det}(A - \lambda I) = 0
            .\end{align*}
        \item \textbf{Characteristic equation, characteristic polynomial}:
        The characteristic equation of a square matrix \( A \) is the equation obtained by setting the determinant of \( A - \lambda I \) equal to zero:
            \[
                \det(A - \lambda I) = 0,
            \]
            where \( \lambda \) represents the eigenvalues of \( A \) and \( I \) is the identity matrix. Solving this equation gives the eigenvalues of \( A \).
             The characteristic polynomial is the polynomial in \( \lambda \) obtained from the determinant \( \det(A - \lambda I) \). It is typically expressed as:
            \[
                p(\lambda) = \det(A - \lambda I),
            \]
            and its roots are the eigenvalues of the matrix \( A \).
        \item \textbf{Finding eigenvectors and eigenvalues}: To find the eigenvalues $\lambda$, we need to solve the characteristic equation:
            \begin{align*}
                \det(A - \lambda I) =0
            .\end{align*}
            \bigbreak \noindent 
            This equation determines the values of $\lambda$ for which the matrix $A-\lambda I$ is singular (non-invertible), which leads to non-zero solutions for $\mathbf{v}$.
            \bigbreak \noindent 
            Once the eigenvalues $\lambda$ are found, substitute each $\lambda$ into the equation $(A-\lambda I)\mathbf{v}=0$ and solve for $\mathbf{v}$. These are the eigenvectors corresponding to each eigenvalue.
        \item \textbf{Repeated eigenvalues (multiplicity)}: A repeated eigenvalue is an eigenvalue that appears more than once when you solve
            \begin{align*}
                \det(A - \lambda I) = 0
            .\end{align*}
            If $(\lambda - \lambda_{i})^{k}$ is a factor of the characteristic polynomial, then $\lambda_{i}$ is a repeated eigenvalue with algebraic multiplicity $k$.
            \bigbreak \noindent 
            The notation for algebraic multiplicity is 
            \begin{align*}
                \text{mult}_{a}(\lambda) = m_{a}(\lambda)
            .\end{align*}
            And the notation for geometric multiplicity is
            \begin{align*}
                \text{mult}_{g}(\lambda) = m_{g}(\lambda)
            .\end{align*}
            The geometric multiplicity is the dimension of the eigenspace for a particular eigenvalue:
            \begin{align*}
                m_{g}(\lambda_{i}) = \text{dim}(\text{ker}(A - \lambda_{i}I))
            .\end{align*}
            For an eigenvalue $\lambda$ of $A$, the eigenspace for $\lambda$ is 
            \begin{align*}
                E_{\lambda} = \{x\ne 0:\; Ax = \lambda x\}
            .\end{align*}
            Since a member of the space satisfies
            \begin{align*}
                Ax = \lambda x 
            .\end{align*}
            We can rearrange to get
            \begin{align*}
                Ax - \lambda x = (A-\lambda I)x= 0
            .\end{align*}
            Thus, the members of the space $E_{\lambda}$ are precisely the vectors in the kernel of $A - \lambda I$. This is why the dimension of the space is the dimension of the kernel of $A - \lambda I$.

            \bigbreak \noindent 
            If the characteristic polynomial is
            \begin{align*}
                (\lambda-2)^{3}(\lambda + 1)
            ,\end{align*}
            then
            \begin{align*}
                m_{a}(2)=3,\; m_{a}(-1) = 1
            .\end{align*}
            If the dimension of the eigenspace for $\lambda = 2$ is 2, then
            \begin{align*}
                m_{g}(2)=  2
            .\end{align*}
        \item \textbf{More on geometric multiplicity}: Geometric multiplicity of an eigenvalue quantifies the dimension of the eigenspace associated with that eigenvalue. It measures how many linearly independent eigenvectors correspond to the same eigenvalue.
            \bigbreak \noindent 
            Suppose that $A \in \mathbb{F}^{n\times n} $ and let $\lambda$ be an eigenvalue of $A$. The geometric multiplicity of $\lambda$ is 
            \begin{align*}
                \text{dim}(E_{\lambda}) = \text{dim}(\text{ker}(A - \lambda I))
            .\end{align*}
            The eigenspace $E_{\lambda}$ is a linear subspace of the ambient space. 
            \begin{itemize}
                \item \textbf{Geometric multiplicity = 1:} the eigenspace is a line through the origin.
                \item \textbf{Geometric multiplicity =2:} the eigenspace is a plane through the origin.
                \item Higher values correspond to higher-dimensional invariant subspaces.
            \end{itemize}
            For any eigenvalue $\lambda$, 
            \begin{align*}
                \lambda \leq m_{g}(\lambda) \leq m_{a}(\lambda)
            .\end{align*}
            An eigenvalue always has at least one eigenvector. If geometric multiplicity is strictly smaller than algebraic multiplicity, the matrix is defective.

        \item \textbf{The set of eigenvalues (The spectrum)}: The set of eigenvalues of a matrix $A$ is denoted by
            \begin{align*}
                \text{spec}(A) = \sigma(A) = \{\lambda \in \mathbb{C}:\; \det(A - \lambda I) = 0\}
            .\end{align*}
            This is called the \textbf{Spectrum} of $A$. Since this set does not show repeated eigenvalues, we can list the eigenvalues
            \begin{align*}
                \lambda_{1}, \lambda_{2}, \ldots, \lambda_{n}
            .\end{align*}
        \item \textbf{The spectrum of the transpose}: Consider a matrix $A$, the spectrum of $A$ is
            \begin{align*}
                \sigma(A) = \{\lambda\in\mathbb{C}:\; \det(A- \lambda I) = 0\}
            .\end{align*}
            Now, consider $A^{T}$,
            \begin{align*}
                \sigma(A^{T}) = \{\lambda \in \mathbb{C}:\; \det(A^{T}- \lambda I) = 0\}
            .\end{align*}
            But,
            \begin{align*}
                \det(A^{T}-\lambda I) = \det((A - \lambda I)^{T})
            .\end{align*}
            Since the determinant of the transpose of a matrix is identical to the determinant of the matrix, 
            \begin{align*}
                \det(A^{T} - \lambda I) = \det((A - \lambda I)^{T}) = \det(A - \lambda I)
            .\end{align*}
            Thus,
            \begin{align*}
                \sigma(A^{T}) = \sigma(A)
            .\end{align*}
        \item \textbf{The spectrum of the inverse}: Consider a invertible matrix $A$, with spectrum 
            \begin{align*}
                \sigma(A) = \{\lambda \in \mathbb{C}:\; \det(A - \lambda I) = 0\}
            .\end{align*}
            Then, the spectrum of $A^{-1}$ is the set
            \begin{align*}
                \sigma(A^{-1}) = \{\frac{1}{\lambda}:\; \lambda \in \sigma(A)\}
            .\end{align*}
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} Let $\lambda$ be an eigenvalue for $A$. Then, for some $v\ne 0$
            \begin{align*}
                Av = \lambda v
            .\end{align*}
            Multiply both sides by the inverse of $A $
            \begin{align*}
                A^{-1}Av = A^{-1}\lambda v \\
                \implies v = A^{-1}\lambda v
            .\end{align*}
            Now, divide both sides by $\lambda$,
            \begin{align*}
                A^{-1}v = \frac{1}{\lambda}v
            .\end{align*}
            Thus, the eigenvalues for $A^{-1}$ are the reciprocals of the eigenvalues of $A$, call the $i^{\text{th}}$ eigenvalue for $A^{-1}$ $\mu_{i}$. That is,
            \begin{align*}
                \mu_{i} = \frac{1}{\lambda_{i}(A)}
            .\end{align*}
            \bigbreak \noindent 
            \textbf{Note:} If the eigenvalues of $A$ are listed in ascending order, $\lambda_{1} \geq \lambda_{2} \geq \cdots \geq \lambda_{n}$, then the ascending order for the eigenvalues of $A^{-1}$ is 
            \begin{align*}
                \frac{1}{\lambda_{n}} \geq \cdots \geq \frac{1}{\lambda_{2}} \geq \frac{1}{\lambda_{1}}
            .\end{align*}

        \item \textbf{Number of eigenvalues for a matrix}: Over the complex numbers, every $n\times n$ matrix has exactly $n$ eigenvalues counted with multiplicity.
            \bigbreak \noindent 
            This is guaranteed by the Fundamental Theorem of Algebra applied to the characteristic polynomial:
            \begin{align*}
                \det(A - \lambda I)
            .\end{align*}
            Over the real numbers, a real matrix may have fewer real eigenvalues because some eigenvalues may be complex.
        \item \textbf{Symmetric matrices}: If $A = A^{T}$ (or $A = A^{*} $ over $\mathbb{C}$), then
            \begin{enumerate}
                \item All eigenvalues are real.
                \item There are exactly $n$ real eigenvalues counting multiplicity.
                \item The matrix is diagonalizable by an orthogonal (or unitary) matrix.
            \end{enumerate}
        \item \textbf{Triangular matrices with real diagonal}: If a matrix is upper or lower triangular, then the eigenvalues are the entries on the diagonal . So if the diagonal entries are real, you get $n$ real eigenvalues.
        \item \textbf{Note about eigenvalues}: Only square matrices have eigenvalues.
        \item \textbf{Spectral mapping}: Consider the spectrum of $A$,
            \begin{align*}
                \sigma(A) = \{\lambda\in \mathbb{C}:\; \det(A - \lambda I) = 0\}
            .\end{align*}
            The spectrum of $A^{2}$, $\sigma(A^{2})$ is the set
            \begin{align*}
                \sigma(A^{2}) = \{\lambda^{2} \in \mathbb{C}:\; \lambda \in \sigma(A)\}
            .\end{align*}
            \bigbreak \noindent 
            \textbf{\textit{Proof.}} Let $\lambda$ be in the spectrum of $A$. Then for any nonzero vector $v\in \mathbb{R}^{n}$,
            \begin{align*}
                Av = \lambda v
            .\end{align*}
            For $A^{2}$,
            \begin{align*}
                A^{2}v = AAv = A \lambda v  = \lambda Av = \lambda \lambda v = \lambda^{2}v
            .\end{align*}
            In fact, one can use this argument to show that
            \begin{align*}
                \sigma(A^{k}) = \{\lambda^{k} \in \mathbb{C}:\; \lambda \in \sigma(A)\}
            .\end{align*}
        \item \textbf{Rank of a matrix and eigenvalues}: Let $A \in \mathbb{R}^{m\times n}$, and $r$ be the rank of $A$, $r = \text{rank}(A) $. If zero is an eigenvalue, $\lambda =0 $, then
            \begin{align*}
                Ax = \lambda x = 0
            .\end{align*}
            So, $\lambda = 0$ corresponds exactly to vectors $x\ne 0$ such that
            \begin{align*}
                Ax = 0
            .\end{align*}
            Thus, 
            \begin{align*}
                \text{ker}(A) \ne \{0\} \iff 0 \text{ is an eigenvalue}
            .\end{align*}
            If zero is an eigenvalue, then 
            \begin{align*}
                E_{0} = \{x \in \mathbb{R}^{n}:\; Ax = 0\}
            .\end{align*}
            Thus,
            \begin{align*}
                E_{0} = \text{ker}(A)
            ,\end{align*}
            and 
            \begin{align*}
                \text{dim}(E_{0}) = \text{dim}(\text{ker}(A))
            .\end{align*}
            Therefore, the rank of $A$ is given by
            \begin{align*}
                \text{rank}(A) = n - \text{dim}(\text{ker}(A)) = n - \text{dim}(E_{0})
            ,\end{align*}
            where $\text{dim}(E_{0})$ is the geometric multiplicity of $\lambda = 0$. So, we see that
            \begin{align*}
                m_{g}(0) = n-\text{rank}(A)
            ,\end{align*}
            where $m_{g}(0) $ is the geometric multiplicity of $\lambda= 0$.
        \item \textbf{Number of eigenvalues revisited}: Let $A \in \mathbb{C}^{n\times n}$ with rank $r$. The number of nonzero eigenvalues is given by the rank $r$, while the remaining $n-r$ eigenvalues are zero.




    \end{itemize}

    \pagebreak 
    \unsect{Basis}
    \begin{itemize}
        \item \textbf{Basis}: A basis of a vector space is a set of linearly independent vectors that span the entire space. This means any vector in the space can be uniquely expressed as a linear combination of the basis vectors. A basis provides a reference framework for representing vectors in that space.
            \bigbreak \noindent 
            For an $n$-dimensional vector space, a basis will consist of exactly $n$ vectors. The coordinates of a vector relative to a basis are the coefficients used in this linear combination.
        \item \textbf{Basis property}: Any set of $n$ linearly independent vectors in an $n$-dimensional vector space forms a basis for that space. A basis is a minimal spanning set of vectors.

        \item \textbf{Standard basis (Implied basis)}: From vector calculus, we know that $\hat{i}$, $\hat{j}$ are the unit vectors the describe the 2 dimensional cartesian plane. Where
            \begin{align*}
                \hat{i} &= \begin{bmatrix} 1 \\ 0 \end{bmatrix} \\
                \hat{j} &= \begin{bmatrix} 0 \\ 1 \end{bmatrix}
            .\end{align*}
            Thus, the standard implied basis when working in $\mathbb{R}^{2}$ is the matrix
            \begin{align*}
                \begin{bmatrix} 1 & 0 \\ 0 & 1\end{bmatrix}
            .\end{align*}
            And a vector $\vec{\mathbf{v}} = \begin{bmatrix} x \\ y \end{bmatrix} $ can be repesented as scaling these basis vectors and then adding them. Ie
            \begin{align*}
                \begin{bmatrix} x \\ y \end{bmatrix} = x \begin{bmatrix} 1 \\ 0\end{bmatrix} + y \begin{bmatrix} 0 \\ 1 \end{bmatrix}
            .\end{align*}
            \bigbreak \noindent 
            Thus, the standard basis for $\mathbb{R}^{n}$ is the $n\times n$ identity matrix $I$
        \item \textbf{Basis notation}: When with vectors, the choice of basis determines how we interpret where the vector sits. For the standard basis, the componenets of the vector is precisely where it will be. Thus, for the standard basis, we write
            \begin{align*}
                [\vec{\mathbf{v}}]
            .\end{align*}
            If the basis were non-standard, we would need to specify it. We write
            \begin{align*}
                [\vec{\mathbf{v}}]_{B}
            .\end{align*}
            Where $B$ is then defined as the matrix representing the basis.
        \item \textbf{Change of basis explained}: We have
            \begin{itemize}
                \item $B^{-1}v = v_{B}$
                \item $Bv_{B} = v$
            \end{itemize}
            \bigbreak \noindent 
            Suppose we have a vector $v = \begin{pmatrix} a \\ b \end{pmatrix}$ in the standard basis. Define a new basis $b_{1} = \begin{pmatrix} u \\ v \end{pmatrix}, b_{2} = \begin{pmatrix} x \\ y\end{pmatrix}$, then $B = \begin{pmatrix} u & x \\ v & y\end{pmatrix} $. Where $v$ under this basis becomes $v_{B} = \begin{pmatrix} \alpha \\ \beta\end{pmatrix} $. Then
            \begin{align*}
                \begin{pmatrix} a \\ b \end{pmatrix} &= \alpha \begin{pmatrix} u \\ v\end{pmatrix} + \beta \begin{pmatrix} x \\ y\end{pmatrix} \\
                                                     &= \begin{pmatrix} \alpha u + \beta x \\ \alpha v + \beta y \end{pmatrix} \\
                                                     &= \begin{pmatrix} u & x \\ v & y \end{pmatrix}\begin{pmatrix} \alpha \\ \beta \end{pmatrix} \\
                \therefore v &= Bv_{B}
            .\end{align*}
            From this, 
            \begin{align*}
                v & = Bv_{B} \\
                B^{-1}v &= B^{-1}Bv_{B} \\
                B^{-1}v &= I v_{B} \\
                \therefore B^{-1}v &= v_{B}
            .\end{align*}
            \bigbreak \noindent 
            \textbf{Note:} The new basis must span the same space as the old basis for the change of basis to work correctly
        \item \textbf{Basis in maps}: Suppose we have some linear map
            \[
                L: V \to W
            \]
            where \(V\) and \(W\) are vector spaces. Suppose \(V\) has a basis \(\beta = \{b_{1}, \dots, b_{n}\}\), and \(W\) has a basis \(\gamma = \{c_{1}, \dots, c_{n}\}\). Then, the matrix representation of \(L\) with respect to these bases is denoted as
            \[
                [L]_{\beta}^{\gamma}
            \]
            Now, suppose we define new bases \(\beta^{\prime}\) for \(V\) and \(\gamma^{\prime}\) for \(W\). We want to find the matrix representation of the linear map \(L\) in the new bases.
            \bigbreak \noindent 
            We have the following transformations:
            \[
                V_{\beta^{\prime}} \to W_{\gamma^{\prime}} \quad \text{and} \quad V_{\beta} \to W_{\gamma}
            \]
            To find the matrix of the map \(L: V_{\beta^{\prime}} \to W_{\gamma^{\prime}}\), we need to relate the new basis vectors to the old basis vectors. Specifically, we perform the following steps:
            \begin{itemize}
                \item To change from the new basis \(\beta^{\prime}\) to the old basis \(\beta\), we multiply by the change-of-basis matrix \(B\), so we have \(B[v]_{\beta^{\prime}} = [v]_{\beta}\).
                \item To change from the old basis \(\gamma\) to the new basis \(\gamma^{\prime}\), we multiply by the inverse of the change-of-basis matrix \(C^{-1}\), so we have \(C^{-1}[w]_{\gamma} = [w]_{\gamma^{\prime}}\).
            \end{itemize}
            \bigbreak \noindent 
            Thus, the matrix representation of \(L\) in the new bases \(\beta^{\prime}\) and \(\gamma^{\prime}\) is given by:
            \[
                [L]_{\beta^{\prime}}^{\gamma^{\prime}} = C^{-1}[L]_{\beta}^{\gamma}B
            \]
        \item \textbf{Diagonalization and eigenbases}: We want to find some new basis $B$ such that the linear map becomes diagonal. That is
            \begin{align*}
                B^{-1}LB = L_{D}
            .\end{align*}
            \bigbreak \noindent 
            We can achieve this via eigenvectors. By changing the basis to one formed by the eigenvectors, we simplify the linear map so that it acts independently on each direction (each eigenvector). In this new basis, the map scales each eigenvector by its corresponding eigenvalue, without mixing different directions. This independence is what makes the matrix diagonal, making the transformation much easier to understand and work with.
            \bigbreak \noindent 
            Consider a matrix \( A \) and a basis formed from its eigenvectors, say 
            \( \mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n \). For simplicity, let’s assume \( A \) has \( n \) linearly 
            independent eigenvectors (which guarantees diagonalization).
            \bigbreak \noindent 
            We can express any vector \( \mathbf{x} \) in terms of this new basis as a linear combination of the eigenvectors
            \[
                \mathbf{x} = c_1 \mathbf{v}_1 + c_2 \mathbf{v}_2 + \cdots + c_n \mathbf{v}_n
            \]
            When we apply the matrix \( A \) to \( \mathbf{x} \), because each eigenvector \( \mathbf{v}_i \) satisfies 
            \( A \mathbf{v}_i = \lambda_i \mathbf{v}_i \), we get:
            \[
                A \mathbf{x} = c_1 \lambda_1 \mathbf{v}_1 + c_2 \lambda_2 \mathbf{v}_2 + \cdots + c_n \lambda_n \mathbf{v}_n
            \]
            This shows that \( A \) acts on each eigenvector individually by multiplying 
            it by its corresponding eigenvalue. The action of \( A \) is now "separated" 
            along each eigenvector direction.
            \bigbreak \noindent 
            To represent $A$ in the new basis (the eigenvector basis), we express the transformation in matrix form with respect to this new basis. Let’s call the matrix $P$, where the columns of $P$ are the eigenvectors of $A$
            \bigbreak \noindent 
            The matrix $A$ in the original basis acts in a complicated way, but in the eigenvector basis, the transformation is simplified. Specifically, in the eigenvector basis, applying $A$ scales each eigenvector by its corresponding eigenvalue. This means that, with respect to this basis, the matrix representing $A$ becomes diagonal
            \bigbreak \noindent 
            \begin{align*}
                P^{-1}AP &= D \\
                \implies  A &= PDP^{-1}
            .\end{align*}
            \bigbreak \noindent 
            where \( D \) is a diagonal matrix with the eigenvalues \( \lambda_1, \lambda_2, \dots, \lambda_n \) on the diagonal:
            \[
                D = \begin{pmatrix}
                    \lambda_1 & 0 & \cdots & 0 \\
                    0 & \lambda_2 & \cdots & 0 \\
                    \vdots & \vdots & \ddots & \vdots \\
                    0 & 0 & \cdots & \lambda_n
                \end{pmatrix}
            \]
            \bigbreak \noindent 
            Consider a matrix $\begin{pmatrix} 1 & 0 \\ 0 & 2 \end{pmatrix}$. When this map acts on a vector $\begin{pmatrix} x \\ y \end{pmatrix}$,
            \begin{align*}
                \begin{pmatrix} 1 & 0 \\ 0 & 2 \end{pmatrix} \begin{pmatrix} x \\ y\end{pmatrix} &= \begin{pmatrix} 1x \\ 2y\end{pmatrix}
            .\end{align*}
            Ie the vector is scaled by $1$ in the $x$ direction, and $2$ in the $y$ direction
        \item \textbf{Note about diagonalization and eigenbases}: Eigenbases (or eigenspaces) are typically defined only for linear maps of the form 
            \( L: V \to V \), where the map \( L \) acts on a vector space \( V \) and maps vectors within that same space. 
            \bigbreak \noindent 
            If a linear map \( L \) is of the form \( L: V \to W \), where \( W \) is a different vector space (or even a subspace of \( V \)), the concept of eigenvectors and eigenvalues doesn’t apply in the usual sense. The reason is that the output of \( L \) is not necessarily a scalar multiple of the input vector \( v \), and it may not even belong to the same vector space.
            \bigbreak \noindent 
            For eigenvectors and eigenvalues to be meaningful, you need the transformation to act within a single vector space, ensuring that the transformed vector remains in the same space, allowing us to compare it directly to the original vector.
        \item \textbf{Diagonalization of a symmetric matrix}: If a matrix $A \in \mathbb{R}^{n\times n}$ is symmetric with real eigenvalues, then the eigenvectors of $A$ are orthogonal.
            \bigbreak \noindent 
            Thus, we can find an orthonormal set of basis vectors easily.

    \end{itemize}

    \pagebreak 
    \unsect{Vector spaces and subspaces}
    \begin{itemize}
        \item \textbf{Vector Space}: A vector space is a set of vectors that satisfy
            \begin{enumerate}
                \item Space has a zero vector
                \item Closed under addition
                \item Closed under multiplication by a scalar
            \end{enumerate}
            We also need the space to have the algebric axioms.
            \begin{itemize}
                \item \textbf{Commutativity of addition}: \( u + v = v + u \)
                \item \textbf{Associativity of addition}: \( (u + v) + w = u + (v + w) \)
                \item \textbf{Additive identity}: There exists a vector \( 0 \in V \) such that \( v + 0 = v \) for all \( v \in V \).
                \item \textbf{Additive inverse}: For every \( v \in V \), there exists a vector \( -v \in V \) such that \( v + (-v) = 0 \).
                \item \textbf{Associativity of scalar multiplication}: \( c(dv) = (cd)v \)
                \item \textbf{Distributivity of scalar multiplication over vector addition}: \( c(u + v) = cu + cv \)
                \item \textbf{Distributivity of scalar multiplication over scalar addition}: \( (c + d)v = cv + dv \)
                \item \textbf{Scalar identity}: \( 1v = v \), where 1 is the multiplicative identity in the field.
            \end{itemize}
            \bigbreak \noindent 
            If we have these properties and algebraic axioms, we have a valid vector space.
        \item \textbf{Abstract vector space}: An abstract vector space is a generalization of this concept, where the elements (vectors) may not have a concrete geometric form, such as functions, polynomials, or matrices, but still follow the same axioms
            \bigbreak \noindent 
            For example, A set of matrices can be defined as an abstract vector space
        \item \textbf{Basis of a vector space, span of the basis}: The basis of a vector space is a choice of $n$ vectors $b_{1},...b_{n}$ such that
            \begin{align*}
                \mathbf{v} &= s_{1}b_{1} + ...  + s_{n}b_{n}
            .\end{align*}
            \bigbreak \noindent 
            If we are able to generate all vectors in the space by simple scaling the vectors by some constant and adding them, the basis $b_{1}, ...,b_{n} $ are said to \textbf{span} the vector space.
        \item \textbf{The number of linearly independent vectors per space}: In $\mathbb{R}^{n}$ the maximum number of linearly independent vectors we can have is $n$. For example, in $\mathbb{R}^{2}$, the maximum number of linearly independent vectors we can have is $2$. This is why we need exactly $n$ vectors to form a basis in $\mathbb{R}^{n}$, and having more than $n$ will also result in the case of allowing us to find and throw out the linearly dependent ones.
            \bigbreak \noindent 
            In other words, There are only $n$ linearly independent vectors in $\mathbb{R}^{n}$ because the dimension of $\mathbb{R}^{n}$ $n$, which means that the space has exactly $n$ independent directions, or degrees of freedom
        \item \textbf{Definition of dimension}: The dimension of a vector space is the number of vectors in a basis for that space. A basis is a set of linearly independent vectors that spans the entire space. In $\mathbb{R}^{n} $ , any valid basis must have exactly $n$ vectors, because it takes $n$ vectors to fully describe the space.
            \bigbreak \noindent 
            A set of vectors is linearly independent if no vector in the set can be written as a linear combination of the others. In $\mathbb{R}^{n}$ , if you have more than $n$ vectors, at least one of those vectors can always be written as a linear combination of the others, meaning they will be linearly dependent. This is because there are only $n$ independent directions in $\mathbb{R}^{n}$
            \bigbreak \noindent 
            In $\mathbb{R}^{2}$, the dimension of the space is 2, meaning that any valid set of linearly independent vectors can have at most two vectors. This is because two vectors are sufficient to fully describe the space—they form a basis. Any other vector in $\mathbb{R}^{2}$ can be expressed as a linear combination of these two vectors.
            \bigbreak \noindent 
            Once you have two linearly independent vectors, adding any third vector will result in linear dependence, because that third vector will lie in the span of the first two vectors.
        \item \textbf{Discern valid basis}: To give a valid basis for a vector space, we must list a collection of vectors that satisfy
            \begin{enumerate}
                \item Basis should span the whole space
                \item No redundant basis vectors
                \item Linearly independent set
            \end{enumerate}
        \item \textbf{Isomorphic vector spaces}:
            Two vector spaces are isomorphic if there is a one-to-one correspondence (a bijection) between them that preserves the structure of vector addition and scalar multiplication. This means that if vector spaces $V$ and $W$ are isomorphic, there exists a map (called an isomorphism) $\phi: V \to W$ such that:
            \begin{enumerate}
                \item $\phi$ is bijective: Every element in $W$ has a unique preimage in $V$, and every element in $V$ is mapped to a unique element in $W$.
                \[
                    \forall w \in W, \ \exists\ v \in V \text{ such that } \phi(v) = w
                \]
                \item $\phi$ preserves addition: For any two vectors $u, v \in V$,
                \[
                    \phi(u + v) = \phi(u) + \phi(v)
                \]
                \item $\phi$ preserves scalar multiplication: For any scalar $c \in \mathbb{F}$ and any vector $v \in V$,
                \[
                    \phi(cv) = c\phi(v)
                \]
            \end{enumerate}
            \bigbreak \noindent 
            \textbf{Example}: $M_{2\times 2}$ is isomorphic to $\mathbb{R}^{4} $
            \bigbreak \noindent 
            A matrix in $m_{2\times 2}$ can be written as 
            \begin{align*}
                \begin{pmatrix} a & b \\ c & d \end{pmatrix}
            .\end{align*}
            Where $a,b,c,d \in \mathbb{R}$. This matrix can be uniquely represented as a 4-dimensional vector:
            \begin{align*}
                (a,b,c,d) \in \mathbb{R}^{4}
            .\end{align*}
            \bigbreak \noindent 
            The correspondence between the matrix and the 4-dimensional vector is a linear bijection that preserves both vector addition and scalar multiplication. Thus, there is a one-to-one correspondence between $M_{2\times 2}$ and $\mathbb{R}^{4}$ , and the two spaces are isomorphic.
            \bigbreak \noindent 
            Moreover, 
            The isomorphism between $M_{2 \times 2}$ (the space of $2 \times 2$ matrices) and $\mathbb{R}^4$ (the 4-dimensional real vector space) can be described by a linear map that transforms a matrix into a 4-dimensional vector by simply mapping the matrix entries to the components of the vector.
            \bigbreak \noindent 
            Let’s define the linear map $\phi: M_{2 \times 2} \to \mathbb{R}^4$.
            \bigbreak \noindent 
            For any matrix
            \[
                A = \begin{pmatrix}
                    a & b \\
                    c & d
                \end{pmatrix} \in M_{2 \times 2},
            \]
            the corresponding vector in $\mathbb{R}^4$ under the map $\phi$ would be:
            \[
                \phi(A) = (a, b, c, d) \in \mathbb{R}^4.
            \]
            This map simply "flattens" the matrix into a 4-tuple of real numbers, with the components arranged in a consistent order (for example, row by row or column by column). In this case, we are mapping the entries of the matrix row by row.
            \bigbreak \noindent 
            Conversely, given any vector $(a, b, c, d) \in \mathbb{R}^4$, the corresponding matrix in $M_{2 \times 2}$ under the inverse map $\phi^{-1}$ would be:
            \[
                \phi^{-1}(a, b, c, d) = \begin{pmatrix}
                    a & b \\
                    c & d
                \end{pmatrix}.
            \]
            \textbf{Properties of the Map}:
            \begin{itemize}
                \item \textbf{Bijectivity:} Every matrix corresponds to a unique vector, and every vector corresponds to a unique matrix.
                \item \textbf{Additivity:} If $A_1 = \begin{pmatrix} a_1 & b_1 \\ c_1 & d_1 \end{pmatrix}$ and $A_2 = \begin{pmatrix} a_2 & b_2 \\ c_2 & d_2 \end{pmatrix}$, then
                    \[
                        \phi(A_1 + A_2) = \phi\left(\begin{pmatrix} a_1 + a_2 & b_1 + b_2 \\ c_1 + c_2 & d_1 + d_2 \end{pmatrix}\right) = (a_1 + a_2, b_1 + b_2, c_1 + c_2, d_1 + d_2),
                    \]
                    which is the same as $\phi(A_1) + \phi(A_2)$.
                \item  \textbf{Scalar Multiplication:} For any scalar $\lambda \in \mathbb{R}$,
            \end{itemize}
            \[
                \phi(\lambda A) = \phi\left(\begin{pmatrix} \lambda a & \lambda b \\ \lambda c & \lambda d \end{pmatrix}\right) = (\lambda a, \lambda b, \lambda c, \lambda d),
            \]
            \bigbreak \noindent 
            which is the same as $\lambda \phi(A)$.
            \bigbreak \noindent 
            Thus, $\phi$ is a linear isomorphism between $M_{2 \times 2}$ and $\mathbb{R}^4$.
            \bigbreak \noindent 
            \textbf{Note:} We only have isomorphism if the dimensions are the same.
        \item \textbf{Eigenspaces}:
            The eigenspace corresponding to an eigenvalue \( \lambda \) of a linear operator \( T \) is the set of all eigenvectors associated with \( \lambda \), along with the zero vector:
            \[
                E_{\lambda} = \{ v \in V \mid T(v) = \lambda v \}.
            \]
            Each eigenvalue \( \lambda \) has its own eigenspace.
        \item \textbf{More on eigenspaces}: Let $A \in \mathbb{R}^{n\times n}$, if $\lambda$ is an eigenvalue for $A$, then the eigenspace for $\lambda$ is
            \begin{align*}
                E_{\lambda} = \{x \in \mathbb{R}^{n}:\; Ax = \lambda x\}
            .\end{align*}
            So, $x \in E_{\lambda}$ implies that 
            \begin{align*}
                Ax =  \lambda x \iff Ax - \lambda x = 0 \iff (A - \lambda I)x = 0
            .\end{align*}
            Thus,
            \begin{align*}
                E_{\lambda} = \text{ker}(A - \lambda I)
            .\end{align*}
        \item \textbf{More on function spaces}: Suppose we define a vector space $P_{4}$, which contains all polynomials of degree four or less. It has a basis
            \begin{align*}
                1, x, x^{2}, x^{3}, x^{4}
            .\end{align*}
            Thus, is dim 5. The verification of this being a legitimate vector space is not shown. Suppose we define two subspaces of $P_{4}$, $E$ and $O$. Where $E$ is the vector subspace of $P_{4}$ that contains all even functions, and $O$ is the vector subspace of $P_{4}$ that contains all the odd functions. Recall that a function is even iff $f(-x) = f(x)$, and a function is odd iff $f(-x)  = -f(x)$. Both subspaces can be shown that they are indeed vector subspaces. In both subspaces, we use the zero function from $P_{4}$, because the zero function is defined here to be both even and odd. 
            \bigbreak \noindent 
            $E$ has basis
            \begin{align*}
                1, x^{2}, x^{4}
            .\end{align*}
            Which is dim 3. Note that all constant functions are even, because they satisify the property of even functions $f(-x) = f(x)$.
            \bigbreak \noindent 
            $O$ has basis
            \begin{align*}
                x, x^{3}
            .\end{align*}
            Which is dim 2. Let's define a mapping
            \begin{align*}
                &L:\ P_{4} \to P_{4} \\
                &L(p(x)) = p^{\prime}(x)
            .\end{align*}
            Which can easily be shown to be linear. Note that this map is not surjective or injective, and thus not bijective. The codomain can not be filled, because no functinos of degree four can be reached by differentiating polynomials of degree four or less. Not injective because all constant functions yield the same derivative (0).
            \bigbreak \noindent 
            Now, we apply the differentiation operator \( L(p(x)) = p'(x) \) to each basis element:
            \begin{itemize}
                \item \( L(1) = 0 \) because the derivative of a constant is 0.
                \item \( L(x) = 1 \).
                \item \( L(x^2) = 2x \).
                \item \( L(x^3) = 3x^2 \).
                \item \( L(x^4) = 4x^3 \).
            \end{itemize}
            Next, express each of these derivatives as a linear combination of the basis elements \( \{1, x, x^2, x^3, x^4\} \):
            \begin{itemize}
                \item \( L(1) = 0 = 0 \cdot 1 + 0 \cdot x + 0 \cdot x^2 + 0 \cdot x^3 + 0 \cdot x^4 \).
                \item \( L(x) = 1 = 1 \cdot 1 + 0 \cdot x + 0 \cdot x^2 + 0 \cdot x^3 + 0 \cdot x^4 \).
                \item \( L(x^2) = 2x = 0 \cdot 1 + 2 \cdot x + 0 \cdot x^2 + 0 \cdot x^3 + 0 \cdot x^4 \).
                \item \( L(x^3) = 3x^2 = 0 \cdot 1 + 0 \cdot x + 3 \cdot x^2 + 0 \cdot x^3 + 0 \cdot x^4 \).
                \item \( L(x^4) = 4x^3 = 0 \cdot 1 + 0 \cdot x + 0 \cdot x^2 + 4 \cdot x^3 + 0 \cdot x^4 \).
            \end{itemize}
            \bigbreak \noindent 
            We now construct the matrix of the linear map $L$ with respect to the basis $\mathcal{B}$. Each column of the matrix corresponds to the image of one of the basis elements, written as a linear combination of the basis elements
            \begin{align*}
                [L]_{\mathcal{B}} &= \begin{pmatrix} 0 & 1 & 0 & 0 & 0 \\ 0 & 0 & 2 & 0 & 0 \\ 0 & 0 & 0 & 3 & 0 \\ 0& 0 & 0 & 0 & 4\\ 0 & 0 & 0 & 0 &0 \end{pmatrix}
            .\end{align*}
            \bigbreak \noindent 
            \begin{itemize}
                \item The first column corresponds to \( L(1) = 0 \).\\
                \item The second column corresponds to \( L(x) = 1 \).\\
                \item The third column corresponds to \( L(x^2) = 2x \).\\
                \item The fourth column corresponds to \( L(x^3) = 3x^2 \).\\
                \item The fifth column corresponds to \( L(x^4) = 4x^3 \).\\
            \end{itemize}
            This matrix represents the differentiation operator on the vector space \( P_4 \) in the basis \( \{1, x, x^2, x^3, x^4\} \).
            \bigbreak \noindent 
            Any polynomial \( p(x) \in P_4 \) can be written as a linear combination of the basis elements \( \{1, x, x^2, x^3, x^4\} \), so we can express the polynomial as a vector of coefficients. For example, if:
            \[
                p(x) = a_0 + a_1x + a_2x^2 + a_3x^3 + a_4x^4,
            \]
            then the polynomial corresponds to the vector:
            \[
                \mathbf{p} = \begin{pmatrix}
                    a_0 \\
                    a_1 \\
                    a_2 \\
                    a_3 \\
                    a_4
                \end{pmatrix}.
            \]
            To apply the linear map \( L \), you multiply the matrix representing \( L \) by the vector of coefficients for \( p(x) \).
            \bigbreak \noindent 
            If \( [L]_{\mathcal{B}} \) is the matrix of the linear map \( L \), and \( \mathbf{p} \) is the vector of coefficients for \( p(x) \), then the result is:
            \[
                L(p(x)) = [L]_{\mathcal{B}} \cdot \mathbf{p}.
            \]
            \bigbreak \noindent 
            A linear map from a vector space to itself can be represented as a square matrix whose size corresponds to the dimension of the space. In this case, since \( L \) maps \( P_4 \) to itself, and \( P_4 \) has dimension 5, the matrix representing \( L \) must be \( 5 \times 5 \).
            \bigbreak \noindent 
            Now we define a few more maps, first $L:\ E \to O$, which has a matrix
            \begin{align*}
                \begin{pmatrix}
                    0 & 2 & 0 \\ 0 & 0 & 4
                \end{pmatrix}
            .\end{align*}
            Then $L:\ P_{4} \to P_{3}$, which has matrix
            \begin{align*}
                \begin{pmatrix}
                    0 & 1 & 0 & 0 & 0 \\ 0 & 0 & 2 & 0 & 0 \\ 0 & 0&0&3 & 0 \\ 0 & 0 & 0 & 0 & 4
                \end{pmatrix}
            .\end{align*}
            Then, $L:\ O \to E $
            \begin{align*}
                \begin{pmatrix}
                    1 & 0 \\ 0 & 3 \\ 0 & 0 
                \end{pmatrix}
            .\end{align*}
            Then, $K:\ P_{3} \to P_{4}$, where $K(p(x)) = \int_{0}^{x}p(u)\ du$, defining the integral in this way leads to no constant of integration. Having a constant of integration would lead to a map that is no longer linear, because $K(0) \ne 0$
            \begin{align*}
                \begin{pmatrix}
                    0 & 0 & 0 & 0\\ 1 & 0 & 0 & 0\\ 0 & \frac{1}{2} & 0 & 0 \\ 0 & 0 &  \frac{1}{3} & 0 \\ 0 & 0 & 0 & \frac{1}{4}
                \end{pmatrix}
            .\end{align*}
            \bigbreak \noindent 
            Now, we take a look at $L \circ K:\ P_{3} \to P_{3}$
            \begin{align*}
                &\begin{pmatrix} 0 & 1 & 0 & 0 & 0 \\ 0 & 0 & 2 & 0 & 0 \\ 0 & 0&0&3 & 0 \\ 0 & 0 & 0 & 0 & 4 \end{pmatrix} \begin{pmatrix} 0 & 0 & 0 & 0\\ 1 & 0 & 0 & 0\\ 0 & \frac{1}{2} & 0 & 0 \\ 0 & 0 &  \frac{1}{3} & 0 \\ 0 & 0 & 0 & \frac{1}{4} \end{pmatrix} \\
                =&\begin{pmatrix}
                    1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0  & 1
                \end{pmatrix} = I
            .\end{align*}
            Thus, we say that $L$ is a left inverse of $K$
            \bigbreak \noindent 
            What about $K\circ L:\ P_{4} \to P_{4}$?
            \begin{align*}
                 &\begin{pmatrix} 0 & 0 & 0 & 0\\ 1 & 0 & 0 & 0\\ 0 & \frac{1}{2} & 0 & 0 \\ 0 & 0 &  \frac{1}{3} & 0 \\ 0 & 0 & 0 & \frac{1}{4} \end{pmatrix}                 \begin{pmatrix} 0 & 1 & 0 & 0 & 0 \\ 0 & 0 & 2 & 0 & 0 \\ 0 & 0&0&3 & 0 \\ 0 & 0 & 0 & 0 & 4 \end{pmatrix} \\
                 &=0 \ne I
            .\end{align*}
            \bigbreak \noindent 
            Thus, $L$ is not a right inverse of $K$
        \item \textbf{A quick remark}: We remark that to express a map as a matrix it must satisfy
            \begin{enumerate}
                \item Linear
                \item Known basis
            \end{enumerate}
        \item \textbf{More on the dot product of function spaces}: For two functions \( f \) and \( g \) in a function space, the dot product (also referred to as the inner product) is typically defined as an integral of their product over a specific interval. Specifically, for real-valued functions \( f \) and \( g \) on an interval \([a, b]\), the inner product is defined as:
            \begin{align*}
                \left\langle p, q \right\rangle = \int_{a}^{b} p(x)q(x) \, dx
            .\end{align*}
            If we define the interval of our function space $x\in [0,1]$, for example $P_{2},\ x\in [0,1]$. Then the inner product is
            \begin{align*}
                \left\langle p,p \right\rangle = \int_{0}^{1} p(x)q(x) \, dx
            .\end{align*}
        \item \textbf{Properties of the inner product}: 
            \begin{itemize}
                \item $\left\langle p,0 \right\rangle = \int_{a}^{b} 0p(x) \, dx  = 0\int_{a}^{b}  p(x)q(x)\, dx= 0$
                \item $\left\langle q,p \right\rangle = \int_{a}^{b} q(x)p(x) \, dx = \int_{a}^{b} p(x)q(x) \, dx = \left\langle p,q \right\rangle$. Thus, we say the inner product is symmetric
                \item $\left\langle p,p \right\rangle = \int_{a}^{b} p^{2}(x) \, dx \geq 0 $ and only equal to zero if $p=0 \ \forall \ x\in [a,b]$
                \item $\left\langle sp,q \right\rangle = \left\langle p,sq \right\rangle = s\left\langle p,q \right\rangle$. Note that $s$ cannot depend on $x$
                \item $\left\langle p + q, r  \right\rangle  = \left\langle p,r \right\rangle  + \left\langle q,r \right\rangle$
            \end{itemize}
        \item \textbf{Norm of the inner product}: Recall in vector calculus
            \begin{align*}
                \mathbf{u} \cdot \mathbf{u} &= \abs{\mathbf{u}}^{2} \\
                \implies \sqrt{\mathbf{u}\cdot \mathbf{u}} &= \abs{\mathbf{u}}
            .\end{align*}
            Given item three above, $\left\langle p,p \right\rangle = \int_{a}^{b} p^{2}(x) \, dx$. Thus, $\abs{\left\langle p,p \right\rangle}  = \sqrt{\int_{a}^{b} p^{2}(x) \, dx}$
        \item \textbf{Angle between function space vectors}: Recall from vector calculus 
            \begin{align*}
                v \cdot w&= \abs{v}\abs{w}\cos{\left(\theta \right)} \\
                \implies \cos{\left(\theta \right)} &= \frac{v \cdot w}{\abs{v}\abs{w}}
            .\end{align*}
        \item \textbf{General Inner product}: If $V$ is a vector space, then the inner product is an operation
            \begin{align*}
                \left\langle  v_{1}, v_{2}\right\rangle:\ V\times V \to \mathbb{R}
            .\end{align*}
            With norm $\abs{v} = \sqrt{\left\langle v,v \right\rangle} $. And with properties described above. Note that only the zero vector has norm zero.
        \item \textbf{Cauchy Schwarz inequality}: The Cauchy-Schwarz inequality states that for any two vectors u and v in an inner product space, the absolute value of their inner product is less than or equal to the product of their norms:
            \begin{align*}
                \abs{\left\langle v,w \right\rangle} \leq \abs{v}\abs{w}
            .\end{align*}
            \bigbreak \noindent 
            Equality holds if and only if $v$ and $w$ are linearly dependent.
        \item \textbf{Inner product space}: An inner product space, also called an \textit{inner space}, is a vector space equipped with an additional structure called an inner product. The inner product is a way to define a notion of "angle" and "length" in the vector space, generalizing the dot product in Euclidean space.
        \item \textbf{Vector plane}: A vector plane (or simply a plane) refers to a two-dimensional flat surface that extends infinitely in all directions within a higher-dimensional space, such as three-dimensional space (\(\mathbb{R}^3\)). It can be thought of as a set of all possible linear combinations of two linearly independent vectors.
            Formally, a vector plane can be described as the span of two non-parallel vectors \(\mathbf{v}_1\) and \(\mathbf{v}_2\) in a vector space. If these vectors belong to \(\mathbb{R}^3\), their span is the set of all vectors of the form:
            \[
                \mathbf{r} = c_1 \mathbf{v}_1 + c_2 \mathbf{v}_2
            \]
            where \(c_1\) and \(c_2\) are scalars (real numbers). This defines a plane that passes through the origin.
            \bigbreak \noindent 
            We can find the spanning vectors of a plane given by 
            \begin{align*}
                ax + by + cz = 0
            .\end{align*}
            By first finding its normal vector $\vec{\mathbf{n}} = (a,b,c)$ which is normal to all points on the plane. we need to find two linearly independent vectors that lie on the plane and are perpendicular to $\vec{\mathbf{n}}$
            \bigbreak \noindent 
            To find vectors that satisfy the plane equation, we can make some simple choices by assigning values to two of the coordinates and solving for the third.
            \bigbreak \noindent 
            For the plane \( 2x + 3y + z = 0 \), let's find two vectors that lie on this plane.
            \begin{itemize}
                \item \textbf{First vector}: Choose \( x = 1 \) and \( y = 0 \).
                    Substituting these into the plane equation:
                    \[
                        2(1) + 3(0) + z = 0 \quad \Rightarrow \quad 2 + z = 0 \quad \Rightarrow \quad z = -2
                    \]
                    So one vector on the plane is:
                    \[
                        \mathbf{v}_1 = (1, 0, -2)
                    \]
                \item \textbf{Second vector}: Now choose \( x = 0 \) and \( y = 1 \).
                    Substituting these into the plane equation:
                    \[
                        2(0) + 3(1) + z = 0 \quad \Rightarrow \quad 3 + z = 0 \quad \Rightarrow \quad z = -3
                    \]
                    So another vector on the plane is:
                    \[
                        \mathbf{v}_2 = (0, 1, -3)
                    \]
            \end{itemize}
            These two vectors, \(\mathbf{v}_1 = (1, 0, -2)\) and \(\mathbf{v}_2 = (0, 1, -3)\), are linearly independent and lie on the plane, so they span the plane.
            \bigbreak \noindent
            \textbf{Note:} A vector plane is a vector space

    \end{itemize}

    \pagebreak 
    \unsect{The Adjoint (transpose map)}
    \begin{itemize}
                \item \textbf{Transpose in inner spaces}: Suppose we have a linear map $L:\ V \to W$, where $V$, $W$ are inner spaces. We know $L:\ V \to W$, and $L^{T}:\  W \to V$. Over in $W$, we have the inner product $\left\langle L(v), w\right\rangle_{W}$, and in $V$ we have $\left\langle v, L^{T}(w) \right\rangle $. We assert
            \begin{align*}
                \left\langle L(v), w \right\rangle_{W} = \left\langle v, L^{T}(w) \right\rangle_{V}
            .\end{align*}
        \item \textbf{Definition of the transpose}: The \textbf{adjoint} (or transpose) of a linear map \( L: V \to W \) between two inner product spaces \( V \) and \( W \), denoted \( L^T: W \to V \), is the unique linear map that satisfies the following condition for all \( v \in V \) and \( w \in W \):
            \[
                \langle L(v), w \rangle_W = \langle v, L^T(w) \rangle_V
            \]
            In other words, the adjoint \( L^T \) transfers the action of \( L \) across the inner product while preserving the result.
        \item \textbf{Linear Functionals}: A \textbf{functional} is a special kind of linear map:
            \[
                f: V \to \mathbb{F},
            \]
            where 
            \begin{itemize}
                \item $V$ is a vector space over a field $\mathbb{F}$ (such as $\mathbb{R}$ or $\mathbb{C}$), and
                \item $f$ is linear, meaning
                    \[
                        f(\alpha v + \beta w) = \alpha f(v) + \beta f(w),
                    \]
                    for all $v, w \in V$ and scalars $\alpha, \beta \in \mathbb{F}$.
            \end{itemize}
            Thus, a functional is a linear map from a vector space into its field of scalars.
        \item \textbf{Dual space}:
                Let $V$ be a vector space over a field $\mathbb{F}$ (often $\mathbb{R}$ or $\mathbb{C}$).  
                \bigbreak \noindent 
                The \textbf{dual space} of $V$, denoted $V^*$, is the set of all linear functionals on $V$:
                \[
                    V^* = \{ f : V \to \mathbb{F} \;\mid\; f \text{ is linear} \}.
                \]
                That is, each element of $V^*$ is a linear map that takes a vector in $V$ and produces a scalar in $\mathbb{F}$.
        \item \textbf{The adjoint}: Consider the map
            \begin{align*}
               L:\ V \to W 
            .\end{align*}
            We define 
            \begin{align*}
                L^{*}:\ W^{*} \to V^{*}
            .\end{align*}
            Which we call the adjoint
            \bigbreak \noindent 
            Furthermore, we define
            \begin{align*}
                L^{*}(\mu):\ V \to \mathbb{R}
            .\end{align*}
            For $\mu \in W^{*}$. Since $\mu \in W^{*}$, it is a linear map that acts on a vector in $W$, and takes us to a scalar. We define
            \begin{align*}
                L^{*}(\mu)v = \mu(Lv) \in \mathbb{R}
            .\end{align*}
            \bigbreak \noindent 
            In real inner spaces with orthonormal basis, 
            \begin{align*}
                L^{*} = L^{T}
            .\end{align*}
            \bigbreak \noindent 
            We have seen this fact in real spaces, recall for a linear operation $L:\ V \to V$, for $v,w \in V $
            \begin{align*}
                \left\langle L(v),w \right\rangle = \left\langle v,L^{\top}(w) \right\rangle
            .\end{align*}
            \bigbreak \noindent 
            If the basis given is not orthonormal, we must convert to an orthonormal basis before using this fact.
            \bigbreak \noindent 
            In complex inner spaces with orthonormal basis,
            \begin{align*}
                L^{*} = \overline{L^{T}}
            .\end{align*}
        \item \textbf{Self-adjoint and normal maps}
            \begin{itemize}
                \item A map $L$ is \textbf{self-adjoint} if $L^{*} = L$. In the real setting, we call this map $symmetric$.
                \item A map $L$ is said to be \textbf{normal} if $L^{*}L = LL^{*}$
            \end{itemize}
            \bigbreak \noindent 
            All symmetric maps are normal, but not all normal maps are symmetric
        \item \textbf{Real spectral theorem}: Consider a linear operation $L:\ V\to V$. The following are equivalent (TFAE)
            \begin{enumerate}
                \item $L$ is symmetric
                \item $V$ has an orthonormal basis consisting of eigenvectors of $L$
                \item $L$ is represented by a diagonal matrix with respect to some orthonormal basis.
            \end{enumerate}
        \item \textbf{Complex spectral theorem}: The only change we make is to number one.
            \begin{enumerate}
                \item $L$ is normal
            \end{enumerate}       




    \end{itemize}

    \pagebreak 
    \unsect{Orthogonality}
    \begin{itemize}
                 \item \textbf{Gram-schmidt orthogonalization}: The Gram-Schmidt process is a method used to take a set of linearly independent vectors and convert them into an orthogonal (or orthonormal) set of vectors. It's useful in linear algebra for generating an orthogonal basis from a given set of vectors in an inner product space. Here's how the process works step by step:
            \bigbreak \noindent 
            Start with the first vector:
            \[
                \mathbf{u}_1 = \mathbf{v}_1
            \]
            This becomes the first orthogonal vector.
            \bigbreak \noindent 
            For each subsequent vector \( \mathbf{v}_k \),
            \bigbreak \noindent 
            subtract the projection of \( \mathbf{v}_k \) onto the previously found orthogonal vectors:

            \[
                \mathbf{u}_k = \mathbf{v}_k - \sum_{j=1}^{k-1} \text{proj}_{\mathbf{u}_j}(\mathbf{v}_k)
            \]
            where \( \text{proj}_{\mathbf{u}_j}(\mathbf{v}_k) \) is the projection of \( \mathbf{v}_k \) onto \( \mathbf{u}_j \):
            \[
                \text{proj}_{\mathbf{u}_j}(\mathbf{v}_k) = \frac{\langle \mathbf{v}_k, \mathbf{u}_j \rangle}{\langle \mathbf{u}_j, \mathbf{u}_j \rangle} \mathbf{u}_j
            \]
            If an orthonormal set is desired, normalize each orthogonal vector:
            \[
                \mathbf{e}_k = \frac{\mathbf{u}_k}{\|\mathbf{u}_k\|}
            \]
                \item \textbf{Perpendicular inner spaces (Orthogonal complement)}: Suppose we have an ambient space $V$, then we take a subset of $V$, namely $U\subset V$, $U\ne \varnothing$. We define
            \begin{align*}
                U^{\perp} = \{v\in V :\ \langle v,u\rangle = 0, \ \forall \ u\in U\}
            .\end{align*}
        \item \textbf{Orthogonality and Direct Sum:}
            One important property of orthogonal complements is that the entire space 
            \(\mathbb{R}^n\) can be written as the direct sum of the subspace \(V\) and its orthogonal complement \(V^\perp\). This means:
            \[
                \mathbb{R}^n = V \oplus V^\perp.
            \]
            In other words, every vector in \(\mathbb{R}^n\) can be uniquely written as the sum of one vector from \(V\) and one vector from \(V^\perp\).
            \bigbreak \noindent 
            Since these two subspaces are orthogonal and span the entire space, their dimensions must add up to the dimension of the ambient space \(\mathbb{R}^n\).
            \[
                \dim(V) + \dim(V^\perp) = n.
            \]
        \item \textbf{Orthogonal complement of the kernel}: Recall that the kernel of a linear map acting on vector spaces, $L:\ V \to W$ is the set of vectors $v\in V$ such that $L(v) = 0$. Formally, for a linear map $L:\ V \to W$, $\text{ker}(L) = \{v\in V:\ L(v) = 0\}$. It represents the set of vectors in the domain space that get annihilated by the map. If a map is injective (one-to-one), then the only member of the kernel is the zero vector. Note that the kernel of a linear map is also well defined for linear operations of the form $L:\ V \to V$. Ie a mapping of a vector space onto itself.
            \bigbreak \noindent 
            We can find the orthogonal complement of the null space, 
            \begin{align*}
                N^{\perp}(L) = \text{ker}^{\perp}(L) = \{v\in V:\ \left\langle v, n \right\rangle = 0 \ \forall \ n \in N(L)\}
            .\end{align*}
            If the kernel only contains the zero vector (the map is injective), then the orthogonal complement would be the whole space $V$, because every vector is perpendicular to the zero vector, even the zero vector itself.
        \item \textbf{Orthonormal matrix}: An orthonormal matrix is a square matrix whose columns (and rows) form an orthonormal set of vectors. 
            \begin{itemize}
                \item \textbf{Orthogonality:} Each column is orthogonal to every other column (their dot product is zero).
                \item \textbf{Normalization:} Each column is a unit vector (its length is 1).
            \end{itemize}
            An orthonormal matrix has the following properties
            \begin{itemize}
                \item $A^{T}A = AA^{T} = I $
                \item $A^{-1} = A^{T}$
                \item The determinant is $\pm 1$
            \end{itemize}
    \end{itemize}


    
    \pagebreak 
    \unsect{Special matrices}
    \begin{itemize}
        \item \textbf{Symmetric matrix}: A matrix $A \in \mathbb{R}^{n\times n}$ is symmetric if $A^{\top} = A$
        \item \textbf{Skew symmetric matrix}: A matrix $A \in \mathbb{R}^{n\times n}$ is skew symmetric if $A^{\top} = -A$
        \item \textbf{Properties of symmetric matrices}:
            \begin{itemize}
                \item \textbf{Real Eigenvalues:} A symmetric matrix has real eigenvalues. This follows from the fact that symmetric matrices are self-adjoint in real vector spaces.
                \item \textbf{Orthogonal Eigenvectors:} The eigenvectors corresponding to distinct eigenvalues of a symmetric matrix are orthogonal.
                \item \textbf{Diagonalizability}: Every symmetric matrix is diagonalizable, meaning it can be written as
                    \begin{align*}
                        A = Q\Lambda Q^{\top}
                    .\end{align*}
                    Where $Q$ is an orthogonal matrix (with $Q^{\top}Q = I $) whose columns are the eigenvectors of $A$, and $\Lambda$ is a diagonal matrix whose entries are the eigenvalues of $A$
                \item \textbf{Closed under Addition and Scalar Multiplication:} If $A$ and $B$ are symmetric matrices of the same size, then $A+B$ and $cA$ (where $c$ is a scalar) are also symmetric. 
                \item \textbf{Closed under Multiplication (under specific conditions):} If $A$ and $B$ are symmetric and commute ( $AB=BA$), then $AB$ is symmetric.
                \item \textbf{Closed under Powers:} If 
                    $A$ is symmetric, then $A^{n}$ (where $n$ is a positive integer) is also symmetric.
                \item \textbf{Inverse of symmetric matrices}: If $A$ is symmetric and invertible, then  $A^{-1}$ is also symmetric.
            \end{itemize}

    \end{itemize}







    
\end{document}
