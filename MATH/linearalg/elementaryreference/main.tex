\documentclass{report}

\input{~/dev/latex/template/preamble.tex}
\input{~/dev/latex/template/macros.tex}

\title{\Huge{}}
\author{\huge{Nathan Warner}}
\date{\huge{}}
\fancyhf{}
\rhead{}
\fancyhead[R]{\itshape Warner} % Left header: Section name
\fancyhead[L]{\itshape\leftmark}  % Right header: Page number
\cfoot{\thepage}
\renewcommand{\headrulewidth}{0pt} % Optional: Removes the header line
%\pagestyle{fancy}
%\fancyhf{}
%\lhead{Warner \thepage}
%\rhead{}
% \lhead{\leftmark}
%\cfoot{\thepage}
%\setborder
% \usepackage[default]{sourcecodepro}
% \usepackage[T1]{fontenc}

% Change the title
\hypersetup{
    pdftitle={Elementary Linear Algebra Reference}
}

\begin{document}
    % \maketitle
        \begin{titlepage}
       \begin{center}
           \vspace*{1cm}
    
           \textbf{Elementary Linear Algebra Reference}
    
           \vspace{0.5cm}
                
                
           \vspace{1.5cm}
    
           \textbf{Nathan Warner}
    
           \vfill
                
                
           \vspace{0.8cm}
         
           \includegraphics[width=0.4\textwidth]{~/niu/seal.png}
                
           Computer Science \\
           Northern Illinois University\\
           United States\\
           
                
       \end{center}
    \end{titlepage}
    \tableofcontents
    \pagebreak 
    \unsect{Vectors}
    \begin{itemize}
        \item \textbf{Magnitude}: For a vector $x\in\mathbb{R}^{n},\; x=\begin{pmatrix} x_{1} \\ x_{2} \\ \vdots \\x_{n} \end{pmatrix}$, the norm (magnitude) of $x$ is 
            \begin{align*}
                \norm{x} = \sqrt{x_{1}^{2} + x_{2}^{2} + ... + x_{n}^{2}} 
            \end{align*}
        \item \textbf{Triangle inequality}: For vectors $x,y \in \mathbb{R}^{n}$, we have the inequality
            \begin{align*}
                \norm{x + y} \leq \norm{x} + \norm{y}
            \end{align*}
        \item \textbf{Properties of Vector Operations}:
        Let $\mathbf{u}$, $\mathbf{v}$, and $\mathbf{w}$ be vectors in a plane. Let $r$ and $s$ be scalars.
        \begin{enumerate}
            \item $\mathbf{u} + \mathbf{v} = \mathbf{v} + \mathbf{u}$ \quad (Commutative property)
            \item $(\mathbf{u} + \mathbf{v}) + \mathbf{w} = \mathbf{u} + (\mathbf{v} + \mathbf{w})$ \quad (Associative property)
            \item $\mathbf{u} + \mathbf{0} = \mathbf{u}$ \quad (Additive identity property)
            \item $\mathbf{u} + (-\mathbf{u}) = \mathbf{0}$ \quad (Additive inverse property)
            \item $r(s\mathbf{u}) = (rs)\mathbf{u}$ \quad (Associativity of scalar multiplication)
            \item $(r + s)\mathbf{u} = r\mathbf{u} + s\mathbf{u}$ \quad (Distributive property)
            \item $r(\mathbf{u} + \mathbf{v}) = r\mathbf{u} + r\mathbf{v}$ \quad (Distributive property)
            \item $1\mathbf{u} = \mathbf{u}, \quad 0\mathbf{u} = \mathbf{0}$ \quad (Identity and zero properties)
        \end{enumerate}
    \item \textbf{Finding components of a vector given the magnitude and the angle $\theta$}: If $v \in \mathbb{R}^{2},\; v = \begin{pmatrix} x \\ y \end{pmatrix}$, then
        \begin{align*}
            &x = \norm{\vec{v}}\cos{\theta } \\
            &y = \norm{\vec{v}}\sin{\theta }
        .\end{align*}
    \item \textbf{Unit vector}: A unit vector is a vector with magnitude $1$. For any nonzero vector $\vec{v}$, we can use scalar multiplication to find a unit vector $\vec{u}$ that has the same direction as $\vec{v}:$. To do this, we multiply the vector by the reciprocal of its magnitude:
        \[
            \vec{u} = \frac{1}{\lVert \vec{v} \rVert} \vec{v}.
        \]
    \item \textbf{Properties of the dot product}:
        Let $\vec{u}$, $\vec{v}$, and $\vec{w}$ be vectors, and let $c$ be a scalar.
        \begin{enumerate}
            \item Commutative property: $\vec{u} \cdot \vec{v} = \vec{v} \cdot \vec{u}$
            \item Distributive property: $\vec{u} \cdot (\vec{v} + \vec{w}) = \vec{u} \cdot \vec{v} + \vec{u} \cdot \vec{w}$
            \item Associative property of scalar multiplication: $(c\vec{u} \cdot \vec{v}) = (c\vec{u}) \cdot \vec{v} = \vec{u} \cdot (c\vec{v})$
            \item Property of magnitude: $\vec{v} \cdot \vec{v} = \|\vec{v}\|^2$
        \end{enumerate}
    \item \textbf{Evaluating a dot product}: 
        The dot product of two vectors is the product of the magnitude of each vector and the cosine of the angle between them:
        \begin{align*}
            \vec{u} \cdot \vec{v} = \norm{\vec{u}} \cdot \norm{\vec{v}} \cdot \cos{\theta }
        .\end{align*}
    \item \textbf{Find the measure of the angle between two nonzero vectors}:
        \begin{align*}
            \cos{\theta } = \frac{\vec{u} \cdot \vec{v}}{\norm{\vec{u}}\norm{\vec{v}}}
        .\end{align*}
        \bigbreak \noindent 
        \textbf{Note}: We are considering $0 \leq \theta  \leq \pi $
    \item \textbf{Vector Projection}: The vector projection of $\mathbf{v}$ onto $\mathbf{u}$ has the same initial point as $\mathbf{u}$ and $\mathbf{v}$ and the same direction as $\mathbf{u}$, and represents the component of $\mathbf{v}$ that acts in the direction of $\mathbf{u}:$.
        \begin{align*}
            \text{proj}_{\vec{u}}\vec{v} = \frac{\vec{v} \cdot \vec{u}}{\norm{\vec{u}}^{2}}\vec{u}
        .\end{align*}
        We say "The vector projection of $\vec{v}$ onto $\vec{u}$"
    \item \textbf{Scalar projection notation}: This is the length of the vector projection and is denoted
        \begin{align*}
            \norm{\text{proj}_{\vec{u}}\vec{v}} = \text{comp}_{\vec{u}}\vec{v} = \frac{\vec{u} \cdot \vec{v}}{\norm{\vec{u}}}
        .\end{align*}
    \item \textbf{Decompose some vector $\vec{v}$ into orthogonal components such that one of the component vectors has the same direction as  $\vec{u}$}:
        \begin{itemize}
            \item First, we compute $\vec{p} = \text{proj}_{\vec{u}}\vec{v} $
            \item Then, we define $\vec{q}  = \vec{v} - \vec{p}$ 
            \item Check that $\vec{q}$ and $\vec{p}$ are orthogonal by finding $\vec{q} \cdot \vec{p}$
        \end{itemize}
    \item \textbf{Two vectors are orthogonal if}:
        \begin{align*}
            \vec{u} \cdot \vec{v} = 0
        .\end{align*}
    \item \textbf{Two vectors are parallel if}: Two vectors $v,u$ are parallel if there exists some scalar $\alpha \in \mathbb{R}$ such that $\alpha u = v $
        \bigbreak \noindent 
        \begin{itemize}
            \item If $ \alpha > 0$, then $v$ points in the same direction as $u$
            \item If $ \alpha < 0$, then $v$ points in the opposite direction of $u$
        \end{itemize}
    \item \textbf{Scalar projection componets of a vector}:
        \begin{align*}
            \vec{v} = \langle \text{comp}_{\hat{i}}\vec{v}, \text{comp}_{\hat{j}}\vec{v}, \text{comp}_{\hat{k}}\vec{v}\rangle
        .\end{align*}
    \item \textbf{The Cross Product}: produces a vector perpendicular to both vectors involved in the multiplication
        \bigbreak \noindent 
        Let $\mathbf{u} = \langle u_1, u_2, u_3 \rangle$ and $\mathbf{v} = \langle v_1, v_2, v_3 \rangle$.
        Then, the cross product $\mathbf{u} \times \mathbf{v}$ is vector
        \begin{align*}
            \mathbf{u} \times \mathbf{v} &= (u_2 v_3 - u_3 v_2) \mathbf{i} - (u_1 v_3 - u_3 v_1) \mathbf{j} + (u_1 v_2 - u_2 v_1) \mathbf{k}  \\
                                         &= \langle u_2 v_3 - u_3 v_2, -(u_1 v_3 - u_3 v_1), u_1 v_2 - u_2 v_1 \rangle.
        .\end{align*}
        \bigbreak \noindent 
        \textbf{Note:} The cross product only works in $\mathbb{R}^{3}$, additionally, we measure the angle between $\vec{u}$ and $\vec{v}$ in $\vec{u} \times \vec{v}$ from $\vec{u}$ to $\vec{v}$
    \item \textbf{Cross product using matrix and determinant}, suppose we have vectors $\vec{u}$ und $\vec{v}:$. Then we can express them in matrix form as
        \begin{align*}
            \vec{u} \times \vec{v}  =
            \begin{bmatrix}
                \hat{i} & \hat{j} & \hat{k} \\
                u_{x} & u_{y} & u_{z} \\
                v_{x} & v_{y} & v_{z}
            \end{bmatrix}
        .\end{align*}
        Then we can find the determinant of this matrix to compute the cross product
        \begin{align*}
            \vec{u} \times \vec{v} = (u_{y}v_{z} - u_{z}v_{y})\hat{i} - (u_{x}v_{z}-u_{z}v_{x})\hat{k} + (u_{x}v_{y} - u_{y}v_{x})\hat{j}
        .\end{align*}

    \item \textbf{Properties of the Cross Product}:
        Let $\mathbf{u}$, $\mathbf{v}$, and $\mathbf{w}$ be vectors in space, and let $c$ be a scalar.
        \begin{enumerate}
            \item Anticommutative property: $\mathbf{u} \times \mathbf{v} = -(\mathbf{v} \times \mathbf{u})$
            \item Distributive property: $\mathbf{u} \times (\mathbf{v} + \mathbf{w}) = \mathbf{u} \times \mathbf{v} + \mathbf{u} \times \mathbf{w}$
            \item Multiplication by a constant: $c(\mathbf{u} \times \mathbf{v}) = (c\mathbf{u}) \times \mathbf{v} = \mathbf{u} \times (c\mathbf{v})$
            \item Cross product of the zero vector: $\mathbf{u} \times \mathbf{0} = \mathbf{0} \times \mathbf{u} = \mathbf{0}$
            \item Cross product of a vector with itself: $\mathbf{v} \times \mathbf{v} = \mathbf{0}$
            \item Scalar triple product: $\mathbf{u} \cdot (\mathbf{v} \times \mathbf{w}) = (\mathbf{u} \times \mathbf{v}) \cdot \mathbf{w}$
        \end{enumerate}
    \item \textbf{Magnitude of the Cross Product}:
        Let $\mathbf{u}$ and $\mathbf{v}$ be vectors, and let $\theta$ be the angle between them. Then, $\|\mathbf{u} \times \mathbf{v}\| = \|\mathbf{u}\| \cdot \|\mathbf{v}\| \cdot \sin \theta.$
    \item \textbf{Triple Scalar Product}:

        The triple scalar product of vectors $\mathbf{u}$, $\mathbf{v}$, and $\mathbf{w}$ is $\mathbf{u} \cdot (\mathbf{v} \times \mathbf{w})$.
        \bigbreak \noindent 
        The triple scalar product is the determinant of the  $3\times 3$ matrix formed by the components of the vectors

    \item \textbf{triple scalar product identities}: 
        \begin{enumerate}[label=(\alph*)]
            \item $\mathbf{u} \cdot (\mathbf{v} \times \mathbf{w}) = -\mathbf{u} \cdot (\mathbf{w} \times \mathbf{v})$
            \item $\mathbf{u} \cdot (\mathbf{v} \times \mathbf{w}) = \mathbf{v} \cdot (\mathbf{w} \times \mathbf{u} = \mathbf{w} \cdot (\mathbf{u} \times \mathbf{v}))$
        \end{enumerate}
    \item \textbf{The zero vector is considered to be parallel to all vectors}:
    \item \textbf{vector equation of a line}:
        \begin{align*}
            \mathbf{r} = \mathbf{r}_{0} + t\mathbf{v} 
        .\end{align*}
        Where $\mathbf{v}$ is the direction vector (vector parallel to the line), $t$ is some scalar, and $\mathbf{r}$, $\mathbf{r}_{0}$ are position vectors
    \item \textbf{Parametric and Symmetric Equations of a Line}:
        A line $L$ parallel to vector $\mathbf{v}=\langle a,b,c \rangle$ and passing through point $P(x_0,y_0,z_0)$ can be described by the following parametric equations:
        \[
            x=x_0+ta, \quad y=y_0+tb, \quad \text{and} \quad z=z_0+tc.
        \]
        If the constants $a$, $b$, and $c$ are all nonzero, then $L$ can be described by the symmetric equation of the line:
        \[
            \frac{x-x_0}{a} = \frac{y-y_0}{b} = \frac{z-z_0}{c}.
        \]
        \bigbreak \noindent 
        \textbf{Note:} The parametric equations of a line are not unique. Using a different parallel vector or a different point on the line leads to a different, equivalent representation. Each set of parametric equations leads to a related set of symmetric equations, so it follows that a symmetric equation of a line is not unique either.
    \item \textbf{Vector equation of a line reworked}: Suppose we have some line, with points $P(x_{0}, y_{0}, z_{0})$, $Q(x_{1}, y_{1}, z_{1})$. Where $\mathbf{p} = \left\langle x_{0}, y_{0}, z_{0}\right\rangle $ and $\mathbf{Q} = \left\langle x_{1}, y_{1}, z_{1}\right\rangle $ are the correponding position vectors. Suppose we also have $\mathbf{r}: = \left\langle x,y,z \right\rangle $. Then our vector equation for a line becomes 
        \begin{align*}
            \mathbf{r} = \mathbf{p} + t\left(\vec{PQ}\right)
        .\end{align*} 
        By properties of vectors, we get the vector equation of a line passing through points $P$ and $Q$ to be 
        \begin{align*}
            \mathbf{r} = (1-t)\mathbf{p} + t\mathbf{q}
        .\end{align*}
        \item \textbf{Distance from a Point to a Line}:
            Let $L$ be a line in space passing through point $P$ with direction vector $\mathbf{v}$. If $M$ is any point not on $L$, then the distance from $M$ to $L$ is
            \[
                d = \frac{\left\| \overrightarrow{PM} \times \mathbf{v} \right\|}{\left\| \mathbf{v} \right\|}
            \]
        \item \textbf{Vector equation of a plane}:
            Given a point $P$ and vector $\mathbf{n}$, the set of all points $Q$ satisfying the equation $\mathbf{n} \cdot \overrightarrow{PQ} = 0$ forms a plane. The equation
            \[
                \mathbf{n} \cdot \overrightarrow{PQ} = 0
            \]
            is known as the vector equation of a plane.
    \item \textbf{Scalar equation of a plane}:
        The scalar equation of a plane containing point $P=(x_0, y_0, z_0)$ with normal vector $\mathbf{n}=\langle a, b, c \rangle$ is
        \[
            a(x-x_0) + b(y-y_0) + c(z-z_0) = 0.
        \]
    \item \textbf{General form of the equation of a plane}:
        This equation (the one above) can be expressed as $ax + by + cz + d = 0$, where $d = -ax_0 - by_0 - cz_0$. This form of the equation is sometimes called the general form of the equation of a plane.



    \end{itemize}



    \pagebreak 
    \unsect{Solutions to linear systems}
    \begin{itemize}
        \item \textbf{Possible solutions to a linear system of two unknowns}: The linear system can have a \textbf{unique solution, no solution, or infinitely many solutions}.
            \item \textbf{Does the solution set form a line, plane, hyperplane, or something else?}: The formation of the solution set depends on the number of free variables,
                \begin{itemize}
                    \item \textbf{No free variables (one unique solution)}: Intersects at a point
                    \item \textbf{One free variable (Uncountable solutions)}: Solution set is a line (1-dimensional subspace)
                    \item \textbf{Two free variable (Uncountable solutions)}: Solution set forms a plane (2-dimensional subspace)
                    \item \textbf{Three free variable (Uncountable solutions)}: Solution set is a three dimensional subspace (In $\mathbb{R}^{3}$ it would be the whole space)
                    \item \textbf{$k$ free variables}: Solution set is a $k$-dimensional subspace in $\mathbb{R}^{n} $
                        \bigbreak \noindent 
                        \textbf{Note:} A \( k \)-dimensional subspace in \( \mathbb{R}^n \) means that the solution set spans a \( k \)-dimensional space within the \( n \)-dimensional ambient space \( \mathbb{R}^n \).

                \end{itemize}

        \item \textbf{Determine if three planes intersect at a unique point}: For this, we find all three normal vectors $\vec{\mathbf{n}}_{1}, \vec{\mathbf{n}}_{2}$, and $\vec{\mathbf{n}}_{3}$. Then we find the triple scalar product, that is
            \begin{align*}
                \vec{\mathbf{n}}_{1} \cdot (\vec{\mathbf{n}}_{2} \times \vec{\mathbf{n}}_{3})
            .\end{align*}
            If this value is non-zero, we have intersection at a unique point. If the value is zero, we either have no intersection, or intersection at a line.

    \end{itemize}

    \pagebreak 
    \unsect{Linearity}
    \begin{itemize}
        \item \textbf{The properties of linear equations}:
            A function \( f: \mathbb{R}^n \rightarrow \mathbb{R} \) representing a linear equation is linear, meaning it satisfies the following properties for all vectors \( \mathbf{x}, \mathbf{y} \in \mathbb{R}^n \) and all scalars \( c \in \mathbb{R} \):
            \begin{itemize}
                \item \textbf{Additivity:} \( f(\mathbf{x} + \mathbf{y}) = f(\mathbf{x}) + f(\mathbf{y}) \)
                \item \textbf{Homogeneity of Degree 1:} \( f(c\mathbf{x}) = cf(\mathbf{x}) \)
                    \bigbreak \noindent 
                    It follows from this that $f(c\mathbf{x})$, when $c=0$ implies $f(0 \mathbf{x}) = 0 f(\mathbf{x}) = 0$. Thus, we add the property
                \item \textbf{Scale by zero}: $f(0) = 0$
            \end{itemize}
            These properties define a linear function and imply that the graph of a linear equation is a straight line (in 2D) or a plane (in 3D).
    \end{itemize}

    \pagebreak 
    \unsect{Matrix algebra}
    \begin{itemize}
        \item \textbf{Laws of matrix addition}: 
            \begin{itemize}
                \item \textbf{Addition with the zero matrix}:  $0 + A = A $
                \item \textbf{Communitive law for matrix addition}: $A+B = B+A$
                \item \textbf{Associativity of matrix addition}: $(A+B) + C = A + (B+C) $
            \end{itemize}
        \item \textbf{Laws of matrix subtraction}:
            \begin{itemize}
                \item $A - 0 = A $
                \item $A -A = 0 $
                \item $B-A = (-1)(A-B) $
            \end{itemize}
                \item \textbf{Matrix difference (subtraction)}: We can give a definition to the subtraction operator by just defining it as using matrix addition and multiplication by a scalar $A - B = A + (-1B) $
                \item \textbf{Note on matrix multiplication}: Matrix multiplication is general \textbf{not} communitive, it can be, but it isn't always. Also, in the real numbers, we know for
                    \begin{align*}
                        ab = 0
                    .\end{align*}
                    Then either $a$ is zero, $b$ is zero, or they are both zero. This is not always the case with matrix multiplication, it is possible to multiply two non-zero matrices and get the zero matrix as a result.
        \item \textbf{Properties of matrix multiplication}:
            \begin{enumerate}
                \item If \( A \), \( B \), and \( C \) are matrices of the appropriate sizes, then 
                    \[
                        A(BC) = (AB)C.
                    \]
                \item If \( A \), \( B \), and \( C \) are matrices of the appropriate sizes, then 
                    \[
                        (A + B)C = AC + BC.
                    \]
                \item If \( A \), \( B \), and \( C \) are matrices of the appropriate sizes, then 
                    \[
                        C(A + B) = CA + CB.
                    \]
            \end{enumerate}
        \item \textbf{Properties of Scalar Multiplication}:
            If \( r \) and \( s \) are real numbers and \( A \) and \( B \) are matrices of the appropriate sizes, then
            \begin{enumerate}
                \item \( r(sA) = (rs)A \)
                \item \( (r + s)A = rA + sA \)
                \item \( r(A + B) = rA + rB \)
                \item \( A(rB) = r(AB) = (rA)B \)
            \end{enumerate}
        \item \textbf{Note on cancellation}: If \( a \), \( b \), and \( c \) are real numbers for which \( ab = ac \) and \( a \neq 0 \), it follows that \( b = c \). That is, we can cancel out the nonzero factor \( a \). However, the cancellation law does not hold for matrices. 
        \item \textbf{Differences between matrix multiplication and multiplication of real numbers}:
            We summarize some of the differences between matrix multiplication and the multiplication of real numbers as follows: For matrices \( A \), \( B \), and \( C \) of the appropriate sizes,
            \begin{enumerate}
                \item \( AB \) need not equal \( BA \).
                \item \( AB \) may be the zero matrix with \( A \neq 0 \) and \( B \neq 0 \).
                \item \( AB \) may equal \( AC \) with \( B \neq C \).
            \end{enumerate}
    \end{itemize}

    \pagebreak 
    \unsect{Transpose}
    \begin{itemize}
        \item \textbf{Squared magnitude of a vector}:  
            \begin{align*}
                \norm{x}^{2} = x^{\top}x
            \end{align*}
        \item \textbf{Transpose of product of matrices}:
            \begin{align*}
                (AB)^{\top} = B^{\top}A^{\top}
            \end{align*}
            \bigbreak \noindent 
            \textbf{Consequence}:
            \begin{align*}
                (ABC)^{\top} = C^{\top}(AB)^{\top} = C^{\top}B^{\top}A^{\top}
            \end{align*}
        \item \textbf{Properties of Transpose}:
            If \( r \) is a scalar and \( A \) and \( B \) are matrices of the appropriate sizes, then
            \begin{enumerate}
                \item \( (A^T)^T = A \)
                \item \( (A + B)^T = A^T + B^T \)
                \item \( (AB)^T = B^T A^T \)
                \item \( (rA)^T = rA^T \)
            \end{enumerate}



    \end{itemize}

    \pagebreak 
    \unsect{Linear maps}
    \begin{itemize}
        \item \textbf{Composition}: Let $L:\ \mathbb{R}^{n} \to \mathbb{R}^{m},\quad v \to L(v)$, and $K:\ \mathbb{R}^{m} \to \mathbb{R}^{p},\quad L(v) \to K(L(v))$. We see that $L \in\mathbb{R}^{m\times n} $, and $K \in \mathbb{R}^{p\times m}$.
            \bigbreak \noindent 
            The composition is 
            \begin{align*}
                K(L(v)) = (K \circ L)(v) = (KL)(v)
            \end{align*}
        \item \textbf{2D rotation map}: 
            \begin{align*}
                R(\theta ) = \begin{bmatrix} \cos{\left(\theta \right)}  & -\sin{\left(\theta \right)} \\ \sin{\left(\theta \right)} & \cos{\left(\theta \right)} \end{bmatrix}
            \end{align*}
        \item \textbf{3D rotation, but keeping one variable constant, ie rotating about one of the coordinate axis}. 
            \bigbreak \noindent 
            All there cases below will require a $3\times 3$ matrix
            \begin{itemize}
                \item \textbf{Rotation about the x-axis (rotation in the $yz$-plane)}: 
                    \begin{align*}
                        R_{x}(\theta ) =  \begin{bmatrix} 1 & 0 & 0 \\ 0 & \cos{\left(\theta \right)} & -\sin{\left(\theta \right)} \\ 0 & \sin{\left(\right)} & \cos{\left(\theta \right)}\end{bmatrix}
                    .\end{align*}
                \item \textbf{Rotation about the y-axis (rotation in the $xz$-plane)}
                    \begin{align*}
                        R_{y}(\theta ) = \begin{bmatrix} \cos{\left(\theta   \right)} & 0 & -\sin{\left(\theta \right)} \\ 0 & 1 & 0 \\ \sin{\left(\theta \right)} & 0 & \cos{\left(\theta \right)}\end{bmatrix}
                    .\end{align*}
                \item \textbf{Rotation about the z-axis (rotation in the $xy$-plane)}
                    \begin{align*}
                        R_{z}(\theta ) \begin{bmatrix} \cos{\left(\theta \right)} & -\sin{\left(\theta \right)} & 0 \\ \sin{\left(\theta \right)} & \cos{\left(\theta \right)} & 0 \\ 0 & 0 & 1\end{bmatrix}
                    .\end{align*}
            \end{itemize}
            \bigbreak \noindent 
            \textbf{Notes on consecutive rotations}: Two consecutive rotations about different axises is \textbf{not} communitive, however if you rotation about the same axis it is.
    \end{itemize}

    \pagebreak 
    \unsect{Injective, surjective, bijective}
    \begin{itemize}
    \end{itemize}





    
\end{document}
