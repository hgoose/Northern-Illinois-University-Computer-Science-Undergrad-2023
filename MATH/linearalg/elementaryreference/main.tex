\documentclass{report}

\input{~/dev/latex/template/preamble.tex}
\input{~/dev/latex/template/macros.tex}

\title{\Huge{}}
\author{\huge{Nathan Warner}}
\date{\huge{}}
\fancyhf{}
\rhead{}
\fancyhead[R]{\itshape Warner} % Left header: Section name
\fancyhead[L]{\itshape\leftmark}  % Right header: Page number
\cfoot{\thepage}
\renewcommand{\headrulewidth}{0pt} % Optional: Removes the header line
%\pagestyle{fancy}
%\fancyhf{}
%\lhead{Warner \thepage}
%\rhead{}
% \lhead{\leftmark}
%\cfoot{\thepage}
%\setborder
% \usepackage[default]{sourcecodepro}
% \usepackage[T1]{fontenc}

% Change the title
\hypersetup{
    pdftitle={Elementary Linear Algebra Reference}
}

\begin{document}
    % \maketitle
        \begin{titlepage}
       \begin{center}
           \vspace*{1cm}
    
           \textbf{Elementary Linear Algebra Reference}
    
           \vspace{0.5cm}
                
                
           \vspace{1.5cm}
    
           \textbf{Nathan Warner}
    
           \vfill
                
                
           \vspace{0.8cm}
         
           \includegraphics[width=0.4\textwidth]{~/niu/seal.png}
                
           Computer Science \\
           Northern Illinois University\\
           United States\\
           
                
       \end{center}
    \end{titlepage}
    \tableofcontents
    \pagebreak 
    \unsect{Vectors}
    \begin{itemize}
        \item \textbf{Magnitude}: For a vector $x\in\mathbb{R}^{n},\; x=\begin{pmatrix} x_{1} \\ x_{2} \\ \vdots \\x_{n} \end{pmatrix}$, the norm (magnitude) of $x$ is 
            \begin{align*}
                \norm{x} = \sqrt{x_{1}^{2} + x_{2}^{2} + ... + x_{n}^{2}} 
            \end{align*}
        \item \textbf{Triangle inequality}: For vectors $x,y \in \mathbb{R}^{n}$, we have the inequality
            \begin{align*}
                \norm{x + y} \leq \norm{x} + \norm{y}
            \end{align*}
        \item \textbf{Properties of Vector Operations}:
        Let $\mathbf{u}$, $\mathbf{v}$, and $\mathbf{w}$ be vectors in a plane. Let $r$ and $s$ be scalars.
        \begin{enumerate}
            \item $\mathbf{u} + \mathbf{v} = \mathbf{v} + \mathbf{u}$ \quad (Commutative property)
            \item $(\mathbf{u} + \mathbf{v}) + \mathbf{w} = \mathbf{u} + (\mathbf{v} + \mathbf{w})$ \quad (Associative property)
            \item $\mathbf{u} + \mathbf{0} = \mathbf{u}$ \quad (Additive identity property)
            \item $\mathbf{u} + (-\mathbf{u}) = \mathbf{0}$ \quad (Additive inverse property)
            \item $r(s\mathbf{u}) = (rs)\mathbf{u}$ \quad (Associativity of scalar multiplication)
            \item $(r + s)\mathbf{u} = r\mathbf{u} + s\mathbf{u}$ \quad (Distributive property)
            \item $r(\mathbf{u} + \mathbf{v}) = r\mathbf{u} + r\mathbf{v}$ \quad (Distributive property)
            \item $1\mathbf{u} = \mathbf{u}, \quad 0\mathbf{u} = \mathbf{0}$ \quad (Identity and zero properties)
        \end{enumerate}
    \item \textbf{Finding components of a vector given the magnitude and the angle $\theta$}: If $v \in \mathbb{R}^{2},\; v = \begin{pmatrix} x \\ y \end{pmatrix}$, then
        \begin{align*}
            &x = \norm{\vec{v}}\cos{\theta } \\
            &y = \norm{\vec{v}}\sin{\theta }
        .\end{align*}
    \item \textbf{Unit vector}: A unit vector is a vector with magnitude $1$. For any nonzero vector $\vec{v}$, we can use scalar multiplication to find a unit vector $\vec{u}$ that has the same direction as $\vec{v}:$. To do this, we multiply the vector by the reciprocal of its magnitude:
        \[
            \vec{u} = \frac{1}{\lVert \vec{v} \rVert} \vec{v}.
        \]
    \item \textbf{Properties of the dot product}:
        Let $\vec{u}$, $\vec{v}$, and $\vec{w}$ be vectors, and let $c$ be a scalar.
        \begin{enumerate}
            \item Commutative property: $\vec{u} \cdot \vec{v} = \vec{v} \cdot \vec{u}$
            \item Distributive property: $\vec{u} \cdot (\vec{v} + \vec{w}) = \vec{u} \cdot \vec{v} + \vec{u} \cdot \vec{w}$
            \item Associative property of scalar multiplication: $(c\vec{u} \cdot \vec{v}) = (c\vec{u}) \cdot \vec{v} = \vec{u} \cdot (c\vec{v})$
            \item Property of magnitude: $\vec{v} \cdot \vec{v} = \|\vec{v}\|^2$
        \end{enumerate}
    \item \textbf{Evaluating a dot product}: 
        The dot product of two vectors is the product of the magnitude of each vector and the cosine of the angle between them:
        \begin{align*}
            \vec{u} \cdot \vec{v} = \norm{\vec{u}} \cdot \norm{\vec{v}} \cdot \cos{\theta }
        .\end{align*}
    \item \textbf{Find the measure of the angle between two nonzero vectors}:
        \begin{align*}
            \cos{\theta } = \frac{\vec{u} \cdot \vec{v}}{\norm{\vec{u}}\norm{\vec{v}}}
        .\end{align*}
        \bigbreak \noindent 
        \textbf{Note}: We are considering $0 \leq \theta  \leq \pi $
    \item \textbf{Vector Projection}: The vector projection of $\mathbf{v}$ onto $\mathbf{u}$ has the same initial point as $\mathbf{u}$ and $\mathbf{v}$ and the same direction as $\mathbf{u}$, and represents the component of $\mathbf{v}$ that acts in the direction of $\mathbf{u}:$.
        \begin{align*}
            \text{proj}_{\vec{u}}\vec{v} = \frac{\vec{v} \cdot \vec{u}}{\norm{\vec{u}}^{2}}\vec{u}
        .\end{align*}
        We say "The vector projection of $\vec{v}$ onto $\vec{u}$"
    \item \textbf{Scalar projection notation}: This is the length of the vector projection and is denoted
        \begin{align*}
            \norm{\text{proj}_{\vec{u}}\vec{v}} = \text{comp}_{\vec{u}}\vec{v} = \frac{\vec{u} \cdot \vec{v}}{\norm{\vec{u}}}
        .\end{align*}
    \item \textbf{Decompose some vector $\vec{v}$ into orthogonal components such that one of the component vectors has the same direction as  $\vec{u}$}:
        \begin{itemize}
            \item First, we compute $\vec{p} = \text{proj}_{\vec{u}}\vec{v} $
            \item Then, we define $\vec{q}  = \vec{v} - \vec{p}$ 
            \item Check that $\vec{q}$ and $\vec{p}$ are orthogonal by finding $\vec{q} \cdot \vec{p}$
        \end{itemize}
    \item \textbf{Two vectors are orthogonal if}:
        \begin{align*}
            \vec{u} \cdot \vec{v} = 0
        .\end{align*}
    \item \textbf{Two vectors are parallel if}: Two vectors $v,u$ are parallel if there exists some scalar $\alpha \in \mathbb{R}$ such that $\alpha u = v $
        \bigbreak \noindent 
        \begin{itemize}
            \item If $ \alpha > 0$, then $v$ points in the same direction as $u$
            \item If $ \alpha < 0$, then $v$ points in the opposite direction of $u$
        \end{itemize}
    \item \textbf{Scalar projection componets of a vector}:
        \begin{align*}
            \vec{v} = \langle \text{comp}_{\hat{i}}\vec{v}, \text{comp}_{\hat{j}}\vec{v}, \text{comp}_{\hat{k}}\vec{v}\rangle
        .\end{align*}
    \item \textbf{The Cross Product}: produces a vector perpendicular to both vectors involved in the multiplication
        \bigbreak \noindent 
        Let $\mathbf{u} = \langle u_1, u_2, u_3 \rangle$ and $\mathbf{v} = \langle v_1, v_2, v_3 \rangle$.
        Then, the cross product $\mathbf{u} \times \mathbf{v}$ is vector
        \begin{align*}
            \mathbf{u} \times \mathbf{v} &= (u_2 v_3 - u_3 v_2) \mathbf{i} - (u_1 v_3 - u_3 v_1) \mathbf{j} + (u_1 v_2 - u_2 v_1) \mathbf{k}  \\
                                         &= \langle u_2 v_3 - u_3 v_2, -(u_1 v_3 - u_3 v_1), u_1 v_2 - u_2 v_1 \rangle.
        .\end{align*}
        \bigbreak \noindent 
        \textbf{Note:} The cross product only works in $\mathbb{R}^{3}$, additionally, we measure the angle between $\vec{u}$ and $\vec{v}$ in $\vec{u} \times \vec{v}$ from $\vec{u}$ to $\vec{v}$
    \item \textbf{Cross product using matrix and determinant}, suppose we have vectors $\vec{u}$ und $\vec{v}:$. Then we can express them in matrix form as
        \begin{align*}
            \vec{u} \times \vec{v}  =
            \begin{bmatrix}
                \hat{i} & \hat{j} & \hat{k} \\
                u_{x} & u_{y} & u_{z} \\
                v_{x} & v_{y} & v_{z}
            \end{bmatrix}
        .\end{align*}
        Then we can find the determinant of this matrix to compute the cross product
        \begin{align*}
            \vec{u} \times \vec{v} = (u_{y}v_{z} - u_{z}v_{y})\hat{i} - (u_{x}v_{z}-u_{z}v_{x})\hat{k} + (u_{x}v_{y} - u_{y}v_{x})\hat{j}
        .\end{align*}

    \item \textbf{Properties of the Cross Product}:
        Let $\mathbf{u}$, $\mathbf{v}$, and $\mathbf{w}$ be vectors in space, and let $c$ be a scalar.
        \begin{enumerate}
            \item Anticommutative property: $\mathbf{u} \times \mathbf{v} = -(\mathbf{v} \times \mathbf{u})$
            \item Distributive property: $\mathbf{u} \times (\mathbf{v} + \mathbf{w}) = \mathbf{u} \times \mathbf{v} + \mathbf{u} \times \mathbf{w}$
            \item Multiplication by a constant: $c(\mathbf{u} \times \mathbf{v}) = (c\mathbf{u}) \times \mathbf{v} = \mathbf{u} \times (c\mathbf{v})$
            \item Cross product of the zero vector: $\mathbf{u} \times \mathbf{0} = \mathbf{0} \times \mathbf{u} = \mathbf{0}$
            \item Cross product of a vector with itself: $\mathbf{v} \times \mathbf{v} = \mathbf{0}$
            \item Scalar triple product: $\mathbf{u} \cdot (\mathbf{v} \times \mathbf{w}) = (\mathbf{u} \times \mathbf{v}) \cdot \mathbf{w}$
        \end{enumerate}
    \item \textbf{Magnitude of the Cross Product}:
        Let $\mathbf{u}$ and $\mathbf{v}$ be vectors, and let $\theta$ be the angle between them. Then, $\|\mathbf{u} \times \mathbf{v}\| = \|\mathbf{u}\| \cdot \|\mathbf{v}\| \cdot \sin \theta.$
    \item \textbf{Triple Scalar Product}:

        The triple scalar product of vectors $\mathbf{u}$, $\mathbf{v}$, and $\mathbf{w}$ is $\mathbf{u} \cdot (\mathbf{v} \times \mathbf{w})$.
        \bigbreak \noindent 
        The triple scalar product is the determinant of the  $3\times 3$ matrix formed by the components of the vectors

    \item \textbf{triple scalar product identities}: 
        \begin{enumerate}[label=(\alph*)]
            \item $\mathbf{u} \cdot (\mathbf{v} \times \mathbf{w}) = -\mathbf{u} \cdot (\mathbf{w} \times \mathbf{v})$
            \item $\mathbf{u} \cdot (\mathbf{v} \times \mathbf{w}) = \mathbf{v} \cdot (\mathbf{w} \times \mathbf{u} = \mathbf{w} \cdot (\mathbf{u} \times \mathbf{v}))$
        \end{enumerate}
    \item \textbf{The zero vector is considered to be parallel to all vectors}:
    \item \textbf{vector equation of a line}:
        \begin{align*}
            \mathbf{r} = \mathbf{r}_{0} + t\mathbf{v} 
        .\end{align*}
        Where $\mathbf{v}$ is the direction vector (vector parallel to the line), $t$ is some scalar, and $\mathbf{r}$, $\mathbf{r}_{0}$ are position vectors
    \item \textbf{Parametric and Symmetric Equations of a Line}:
        A line $L$ parallel to vector $\mathbf{v}=\langle a,b,c \rangle$ and passing through point $P(x_0,y_0,z_0)$ can be described by the following parametric equations:
        \[
            x=x_0+ta, \quad y=y_0+tb, \quad \text{and} \quad z=z_0+tc.
        \]
        If the constants $a$, $b$, and $c$ are all nonzero, then $L$ can be described by the symmetric equation of the line:
        \[
            \frac{x-x_0}{a} = \frac{y-y_0}{b} = \frac{z-z_0}{c}.
        \]
        \bigbreak \noindent 
        \textbf{Note:} The parametric equations of a line are not unique. Using a different parallel vector or a different point on the line leads to a different, equivalent representation. Each set of parametric equations leads to a related set of symmetric equations, so it follows that a symmetric equation of a line is not unique either.
    \item \textbf{Vector equation of a line reworked}: Suppose we have some line, with points $P(x_{0}, y_{0}, z_{0})$, $Q(x_{1}, y_{1}, z_{1})$. Where $\mathbf{p} = \left\langle x_{0}, y_{0}, z_{0}\right\rangle $ and $\mathbf{Q} = \left\langle x_{1}, y_{1}, z_{1}\right\rangle $ are the correponding position vectors. Suppose we also have $\mathbf{r}: = \left\langle x,y,z \right\rangle $. Then our vector equation for a line becomes 
        \begin{align*}
            \mathbf{r} = \mathbf{p} + t\left(\vec{PQ}\right)
        .\end{align*} 
        By properties of vectors, we get the vector equation of a line passing through points $P$ and $Q$ to be 
        \begin{align*}
            \mathbf{r} = (1-t)\mathbf{p} + t\mathbf{q}
        .\end{align*}
        \item \textbf{Distance from a Point to a Line}:
            Let $L$ be a line in space passing through point $P$ with direction vector $\mathbf{v}$. If $M$ is any point not on $L$, then the distance from $M$ to $L$ is
            \[
                d = \frac{\left\| \overrightarrow{PM} \times \mathbf{v} \right\|}{\left\| \mathbf{v} \right\|}
            \]
        \item \textbf{Vector equation of a plane}:
            Given a point $P$ and vector $\mathbf{n}$, the set of all points $Q$ satisfying the equation $\mathbf{n} \cdot \overrightarrow{PQ} = 0$ forms a plane. The equation
            \[
                \mathbf{n} \cdot \overrightarrow{PQ} = 0
            \]
            is known as the vector equation of a plane.
    \item \textbf{Scalar equation of a plane}:
        The scalar equation of a plane containing point $P=(x_0, y_0, z_0)$ with normal vector $\mathbf{n}=\langle a, b, c \rangle$ is
        \[
            a(x-x_0) + b(y-y_0) + c(z-z_0) = 0.
        \]
    \item \textbf{General form of the equation of a plane}:
        This equation (the one above) can be expressed as $ax + by + cz + d = 0$, where $d = -ax_0 - by_0 - cz_0$. This form of the equation is sometimes called the general form of the equation of a plane.



    \end{itemize}



    \pagebreak 
    \unsect{Solutions to linear systems}
    \begin{itemize}
        \item \textbf{Possible solutions to a linear system of two unknowns}: The linear system can have a \textbf{unique solution, no solution, or infinitely many solutions}.
            \item \textbf{Does the solution set form a line, plane, hyperplane, or something else?}: The formation of the solution set depends on the number of free variables,
                \begin{itemize}
                    \item \textbf{No free variables (one unique solution)}: Intersects at a point
                    \item \textbf{One free variable (Uncountable solutions)}: Solution set is a line (1-dimensional subspace)
                    \item \textbf{Two free variable (Uncountable solutions)}: Solution set forms a plane (2-dimensional subspace)
                    \item \textbf{Three free variable (Uncountable solutions)}: Solution set is a three dimensional subspace (In $\mathbb{R}^{3}$ it would be the whole space)
                    \item \textbf{$k$ free variables}: Solution set is a $k$-dimensional subspace in $\mathbb{R}^{n} $
                        \bigbreak \noindent 
                        \textbf{Note:} A \( k \)-dimensional subspace in \( \mathbb{R}^n \) means that the solution set spans a \( k \)-dimensional space within the \( n \)-dimensional ambient space \( \mathbb{R}^n \).

                \end{itemize}

        \item \textbf{Determine if three planes intersect at a unique point}: For this, we find all three normal vectors $\vec{\mathbf{n}}_{1}, \vec{\mathbf{n}}_{2}$, and $\vec{\mathbf{n}}_{3}$. Then we find the triple scalar product, that is
            \begin{align*}
                \vec{\mathbf{n}}_{1} \cdot (\vec{\mathbf{n}}_{2} \times \vec{\mathbf{n}}_{3})
            .\end{align*}
            If this value is non-zero, we have intersection at a unique point. If the value is zero, we either have no intersection, or intersection at a line.

    \end{itemize}

    \pagebreak 
    \unsect{Linearity}
    \begin{itemize}
        \item \textbf{The properties of linear equations}:
            A function \( f: \mathbb{R}^n \rightarrow \mathbb{R} \) representing a linear equation is linear, meaning it satisfies the following properties for all vectors \( \mathbf{x}, \mathbf{y} \in \mathbb{R}^n \) and all scalars \( c \in \mathbb{R} \):
            \begin{itemize}
                \item \textbf{Additivity:} \( f(\mathbf{x} + \mathbf{y}) = f(\mathbf{x}) + f(\mathbf{y}) \)
                \item \textbf{Homogeneity of Degree 1:} \( f(c\mathbf{x}) = cf(\mathbf{x}) \)
                    \bigbreak \noindent 
                    It follows from this that $f(c\mathbf{x})$, when $c=0$ implies $f(0 \mathbf{x}) = 0 f(\mathbf{x}) = 0$. Thus, we add the property
                \item \textbf{Scale by zero}: $f(0) = 0$
            \end{itemize}
            These properties define a linear function and imply that the graph of a linear equation is a straight line (in 2D) or a plane (in 3D).
    \end{itemize}

    \pagebreak 
    \unsect{Matrix algebra}
    \begin{itemize}
        \item \textbf{Laws of matrix addition}: 
            \begin{itemize}
                \item \textbf{Addition with the zero matrix}:  $0 + A = A $
                \item \textbf{Communitive law for matrix addition}: $A+B = B+A$
                \item \textbf{Associativity of matrix addition}: $(A+B) + C = A + (B+C) $
            \end{itemize}
        \item \textbf{Laws of matrix subtraction}:
            \begin{itemize}
                \item $A - 0 = A $
                \item $A -A = 0 $
                \item $B-A = (-1)(A-B) $
            \end{itemize}
                \item \textbf{Matrix difference (subtraction)}: We can give a definition to the subtraction operator by just defining it as using matrix addition and multiplication by a scalar $A - B = A + (-1B) $
                \item \textbf{Note on matrix multiplication}: Matrix multiplication is general \textbf{not} communitive, it can be, but it isn't always. Also, in the real numbers, we know for
                    \begin{align*}
                        ab = 0
                    .\end{align*}
                    Then either $a$ is zero, $b$ is zero, or they are both zero. This is not always the case with matrix multiplication, it is possible to multiply two non-zero matrices and get the zero matrix as a result.
        \item \textbf{Properties of matrix multiplication}:
            \begin{enumerate}
                \item If \( A \), \( B \), and \( C \) are matrices of the appropriate sizes, then 
                    \[
                        A(BC) = (AB)C.
                    \]
                \item If \( A \), \( B \), and \( C \) are matrices of the appropriate sizes, then 
                    \[
                        (A + B)C = AC + BC.
                    \]
                \item If \( A \), \( B \), and \( C \) are matrices of the appropriate sizes, then 
                    \[
                        C(A + B) = CA + CB.
                    \]
            \end{enumerate}
        \item \textbf{Properties of Scalar Multiplication}:
            If \( r \) and \( s \) are real numbers and \( A \) and \( B \) are matrices of the appropriate sizes, then
            \begin{enumerate}
                \item \( r(sA) = (rs)A \)
                \item \( (r + s)A = rA + sA \)
                \item \( r(A + B) = rA + rB \)
                \item \( A(rB) = r(AB) = (rA)B \)
            \end{enumerate}
        \item \textbf{Note on cancellation}: If \( a \), \( b \), and \( c \) are real numbers for which \( ab = ac \) and \( a \neq 0 \), it follows that \( b = c \). That is, we can cancel out the nonzero factor \( a \). However, the cancellation law does not hold for matrices. 
        \item \textbf{Differences between matrix multiplication and multiplication of real numbers}:
            We summarize some of the differences between matrix multiplication and the multiplication of real numbers as follows: For matrices \( A \), \( B \), and \( C \) of the appropriate sizes,
            \begin{enumerate}
                \item \( AB \) need not equal \( BA \).
                \item \( AB \) may be the zero matrix with \( A \neq 0 \) and \( B \neq 0 \).
                \item \( AB \) may equal \( AC \) with \( B \neq C \).
            \end{enumerate}
    \end{itemize}

    \pagebreak 
    \unsect{Transpose}
    \begin{itemize}
        \item \textbf{Squared magnitude of a vector}:  
            \begin{align*}
                \norm{x}^{2} = x^{\top}x
            \end{align*}
        \item \textbf{Transpose of product of matrices}:
            \begin{align*}
                (AB)^{\top} = B^{\top}A^{\top}
            \end{align*}
            \bigbreak \noindent 
            \textbf{Consequence}:
            \begin{align*}
                (ABC)^{\top} = C^{\top}(AB)^{\top} = C^{\top}B^{\top}A^{\top}
            \end{align*}
        \item \textbf{Properties of Transpose}:
            If \( r \) is a scalar and \( A \) and \( B \) are matrices of the appropriate sizes, then
            \begin{enumerate}
                \item \( (A^T)^T = A \)
                \item \( (A + B)^T = A^T + B^T \)
                \item \( (AB)^T = B^T A^T \)
                \item \( (rA)^T = rA^T \)
            \end{enumerate}



    \end{itemize}

    \pagebreak 
    \unsect{Linear maps}
    \begin{itemize}
        \item \textbf{Composition}: Let $L:\ \mathbb{R}^{n} \to \mathbb{R}^{m},\quad v \to L(v)$, and $K:\ \mathbb{R}^{m} \to \mathbb{R}^{p},\quad L(v) \to K(L(v))$. We see that $L \in\mathbb{R}^{m\times n} $, and $K \in \mathbb{R}^{p\times m}$.
            \bigbreak \noindent 
            The composition is 
            \begin{align*}
                K(L(v)) = (K \circ L)(v) = (KL)(v)
            \end{align*}
        \item \textbf{2D rotation map}: 
            \begin{align*}
                R(\theta ) = \begin{bmatrix} \cos{\left(\theta \right)}  & -\sin{\left(\theta \right)} \\ \sin{\left(\theta \right)} & \cos{\left(\theta \right)} \end{bmatrix}
            \end{align*}
        \item \textbf{3D rotation, but keeping one variable constant, ie rotating about one of the coordinate axis}. 
            \bigbreak \noindent 
            All there cases below will require a $3\times 3$ matrix
            \begin{itemize}
                \item \textbf{Rotation about the x-axis (rotation in the $yz$-plane)}: 
                    \begin{align*}
                        R_{x}(\theta ) =  \begin{bmatrix} 1 & 0 & 0 \\ 0 & \cos{\left(\theta \right)} & -\sin{\left(\theta \right)} \\ 0 & \sin{\left(\right)} & \cos{\left(\theta \right)}\end{bmatrix}
                    .\end{align*}
                \item \textbf{Rotation about the y-axis (rotation in the $xz$-plane)}
                    \begin{align*}
                        R_{y}(\theta ) = \begin{bmatrix} \cos{\left(\theta   \right)} & 0 & -\sin{\left(\theta \right)} \\ 0 & 1 & 0 \\ \sin{\left(\theta \right)} & 0 & \cos{\left(\theta \right)}\end{bmatrix}
                    .\end{align*}
                \item \textbf{Rotation about the z-axis (rotation in the $xy$-plane)}
                    \begin{align*}
                        R_{z}(\theta ) \begin{bmatrix} \cos{\left(\theta \right)} & -\sin{\left(\theta \right)} & 0 \\ \sin{\left(\theta \right)} & \cos{\left(\theta \right)} & 0 \\ 0 & 0 & 1\end{bmatrix}
                    .\end{align*}
            \end{itemize}
            \bigbreak \noindent 
            \textbf{Notes on consecutive rotations}: Two consecutive rotations about different axises is \textbf{not} communitive, however if you rotation about the same axis it is.
        \item \textbf{What are the columns of a linear map}: If a linear map \(T: \mathbb{R}^n \to \mathbb{R}^m\) is represented by a matrix \(A\), then the columns of \(A\) are exactly the vectors you obtain by applying \(T\) to the standard basis vectors \(e_1, e_2, \dots, e_n\). That is, 
            \begin{align*}
                T(e_j) = A e_j = \text{column } j \text{ of } A.
            \end{align*}
    \end{itemize}

    \pagebreak 
    \unsect{Injective, surjective, bijective}
    \begin{itemize}
        \item \textbf{Standard Functions}:
            \begin{itemize}
                \item A function $f:\ A \to B$ is \textbf{surjective (onto)} if for all $b \in B$, there exists an $a \in A$ such that $f(a) = b$.  
                    Equivalently, $f$ is surjective if $f(A) = B$.

                \item A function $f:\ A \to B$ is \textbf{injective (one-to-one)} if 
                    \[
                        f(a_{1}) = f(a_{2}) \;\; \implies \;\; a_{1} = a_{2}.
                    \]
                    Equivalently, distinct elements of $A$ map to distinct elements of $B$.

                \item A function $f:\ A \to B$ is \textbf{bijective} if it is both injective and surjective.  
                    In this case, $f$ is called a \textbf{bijection} from $A$ to $B$, and $f$ has an \textbf{inverse function} $f^{-1}:\ B \to A$.

                \item A function $f:\ A \to B$ is \textbf{constant} if there exists $b_{0} \in B$ such that $f(a) = b_{0}$ for all $a \in A$.

                \item A function $f:\ A \to B$ is \textbf{identity} on $A$ if $f(a) = a$ for all $a \in A$.  
                    This function is usually denoted by $\mathrm{id}_{A}$.

                \item A function $f:\ A \to B$ is \textbf{many-to-one} if two or more distinct elements in $A$ can map to the same element of $B$.

                \item A function $f:\ A \to B$ is \textbf{one-to-one correspondence} if it is bijective.  
                    In this case, $A$ and $B$ have the same cardinality.
            \end{itemize}
        \item \textbf{Linear maps}: Consider a linear map $L:\ \mathbb{R}^{n} \to \mathbb{R}^{k}$
            \begin{itemize}
                \item $\mathbb{R}^{n} \to \mathbb{R}^{k}$ where $n < k$ can never be onto, maybe 1-1 though
                \item $\mathbb{R}^{n} \to \mathbb{R}^{k}$ where $n > k$ can never be 1-1, maybe onto though
            \end{itemize}
        \item \textbf{Surjective and bijective in terms of matrix transformations}:
            \bigbreak \noindent 
            If a matrix $A$ represents a linear transformation from $\mathbb{R}^n \to \mathbb{R}^m$, the transformation is onto if for every vector $b \in \mathbb{R}^m$ in the codomain, there exists at least one vector $x \in \mathbb{R}^n$ such that $Ax = b$.
            \bigbreak \noindent 
            \textbf{Geometric Interpretation}: Surjectivity means the matrix transformation "covers" the entire codomain, hitting every possible point. In 2D, this would mean the entire plane is covered by the transformation.
            \bigbreak \noindent 
            A matrix $A$ is injective if for every $x_1$ and $x_2$ in the domain, if $Ax_1 = Ax_2$, then $x_1 = x_2$. This means that no two distinct input vectors can be mapped to the same output vector.
            \bigbreak \noindent 
            \textbf{Geometric Interpretation}: Injectivity means the transformation doesn’t collapse any part of the domain into a lower dimension, so no information is lost.
        \item \textbf{If dimensions are the same}: If $\mathbb{R}^{n} \to \mathbb{R}^{n}$, if its onto, its most likely 1-1.
        \item \textbf{Check if a linear map is injective or surjective (Formal)}: To check whether a linear transformation is surjective or injective, we use specific properties of the matrix representing the transformation.
            \bigbreak \noindent 
            Let $T:\ V \to W$ be a linear transformation, where $V$ and $W$ are vector spaces and $A$ is the matrix representation of $T$
            \begin{itemize}
                \item \textbf{Checking Injectivity (One-to-One)}: A linear transformation $T$ is injective (one-to-one) if:
                    \begin{align*}
                        T(\mathbf{v}_{1}) = T(\mathbf{v}_{2}) \implies \mathbf{v}_{1} = \mathbf{v}_{2}
                    .\end{align*}
                    \bigbreak \noindent 
                    Equivalently, $T$ is injective if the only solution to $T(\mathbf{v})=0$ (the null space or kernel of $T$) is $\mathbf{v}=0$.
                    \bigbreak \noindent 
                    Alternatively, a matrix $A$ is injective if the rank of the matrix (the number of linearly independent columns) is equal to the number of columns of the matrix. This means the matrix has full column rank
                    \bigbreak \noindent 
                    Lastly, we can just check the determinant if the matrix is square.
                \item \textbf{Check surjectivity}:
                    A linear transformation \( T \) is surjective (onto) if for every vector \( \mathbf{w} \in W \) (the codomain), there exists a vector \( \mathbf{v} \in V \) (the domain) such that:
                    \[
                        T(\mathbf{v}) = \mathbf{w}.
                    \]
                    This means that \( T \) "covers" the entire codomain \( W \), or in other words, the image of \( T \) is the entire space \( W \).
                    \bigbreak \noindent 
                    \textbf{Steps to check surjectivity:}
                    \begin{enumerate}
                        \item \textbf{Image:} The transformation \( T \) is surjective if the image (column space or range) of the matrix \( A \) spans the entire codomain \( W \). This means 
                            \[
                                \text{im}(A) = W.
                            \]
                        \item \textbf{Rank:} For surjectivity, the rank of the matrix must be equal to the dimension of the codomain. If \( A \) is an \( m \times n \) matrix, the matrix is surjective if its rank is equal to \( m \) (the number of rows).
                        \item \textbf{Determinant for square matrices:} If \( A \) is a square matrix, \( T \) is surjective if 
                            \[
                                \det(A) \neq 0,
                            \]
                            because a non-zero determinant implies the matrix has full rank, covering the entire codomain.
                    \end{enumerate}
            \end{itemize}
        \item \textbf{Injective and surjective for square matrices}: For square matrices (say $A \in \mathbb{R}^{n\times n}$), injectivity and surjectivity are actually equivalent, since the number of linearly independent rows equals the number of linearly independent columns which both equal $n$ if the matrix has full rank.




    \end{itemize}

    \pagebreak 
    \unsect{Matrix rank}
    \begin{itemize}
        \item \textbf{Row-echelon form}:
            Row echelon form (REF) is a standardized way of arranging the rows of a matrix using elementary row operations. A matrix is in row echelon form if it satisfies these conditions:
            \begin{itemize}
                \item All zero rows are at the bottom. (If a row is entirely zeros, it appears below any nonzero row.)
                \item The first nonzero entry in each nonzero row is 1 (called a leading 1 or pivot).
                \item Each leading 1 is to the right of the leading 1 in the row above it. (So the pivots “stair-step” down to the right as you move down the rows.)
                \item All entries below a pivot are zero.
            \end{itemize}
            The matrix $A \in \mathbb{R}^{5} \to \mathbb{R}^{4}$
            \begin{align*}
                A = \begin{bmatrix}
                    1 & 2 & 3 & 4 \\
                    0 & 1 & 5 & 6 \\
                    0 & 0 & 0 & 1 \\
                    0 & 0 & 0 & 0
                \end{bmatrix}
            \end{align*}
            is in row-echelon form
        \item A non square matrix cannot be invertible.
        \item \textbf{Rank of a square matrix}:  For an $n \times n$ square matrix, the \textbf{rank} is the dimension of its row space (or column space---they are equal).
            \bigbreak \noindent 
            If $\text{rank}(A) = n$, the matrix is \textbf{full rank} and invertible.
            \begin{itemize}
                \item The linear map $A: \mathbb{R}^n \to \mathbb{R}^n$ is:
                    \begin{itemize}
                        \item \textbf{Injective} (one-to-one): no two distinct inputs map to the same output.
                        \item \textbf{Surjective} (onto): every element of $\mathbb{R}^n$ is hit by some input.
                        \item Together, injective + surjective = \textbf{bijective}, meaning $A$ is invertible.
                    \end{itemize}
            \end{itemize}
            \bigbreak \noindent 
            \textbf{Note:} If $A \in \mathbb{R}^{n\times n}$ has full rank, then $A$ is nonsingular and $\det(A) \ne 0$
            \bigbreak \noindent 
            If $\text{rank}(A) < n$, then:
            \begin{itemize}
                \item The matrix is singular (non-invertible).
                \item The map is neither injective (kernel is nontrivial) nor surjective (image is a proper subspace).
            \end{itemize}
            \bigbreak \noindent 
            \textbf{Note:} If $A$ is rank-deficient, then $A$ is singular and $\det(A) = 0$.

        \item \textbf{Rank of a non-square matrix}:
                 For an $m \times n$ matrix $A$, the \textbf{rank} is the dimension of the row space (or column space). 
                    \[
                        \text{rank}(A) \leq \min(m,n).
                    \]
                    Consider the map $A: \mathbb{R}^n \to \mathbb{R}^m$:
                    \begin{itemize}
                        \item If $m > n$ (tall matrix):
                            \begin{itemize}
                                \item The maximum rank is $n$.
                                \item If $\text{rank}(A) = n$, then the map is \textbf{injective} (no two inputs give the same output), but not surjective, since the codomain has higher dimension than the image.
                            \end{itemize}
                        \item If $m < n$ (wide matrix):
                            \begin{itemize}
                                \item The maximum rank is $m$.
                                \item If $\text{rank}(A) = m$, then the map is \textbf{surjective} (fills all of $\mathbb{R}^m$), but not injective, since extra input dimensions collapse.
                            \end{itemize}
                    \end{itemize}
        \item \textbf{Determine the rank of a square matrix}: We check the determinant. If $\det(A) \ne 0$, the matrix $A$ has full rank and is nonsingular. If $\det(A) = 0$, the matrix $A$ is rank-deficient and is singular.
        \item \textbf{Determine the rank of a non-square matrix}: To determine the rank of a matrix, we need to make sure the $m\times n$ matrix has $m$ linearly independent rows. To do this, we can perform guassian elimination to get the augmented matrix in row echelon form. The rank of the matrix, $\text{rank}(A)$ is the number of non zero rows.
        \item \textbf{Column rank and row rank}: The number of linearly independent columns is called the column rank, and the number of linearly independent rows is called the row rank.
            \bigbreak \noindent 
            For any matrix $A$, the number of linearly independent rows is equal to the number of linearly independent columns. This number is called the rank of $A$
        \item \textbf{What does the rank tell use about solutions}: If we have full rank, then given a target, there will be a unique solution. If we are rank-deficient, there may be no solution, or infinitely many soluitions.




            % \item \textbf{Rank of a matrix}: The rank of a matrix refers to the number of linearly independent rows or columns in the matrix. Essentially, the rank tells you the "dimension" of the space spanned by the rows or columns.
        %     \begin{itemize}
        %         \item \textbf{Row rank:} The number of linearly independent rows.
        %         \item \textbf{Column rank:} The number of linearly independent columns.
        %     \end{itemize}
        %     For any matrix, the row rank is always equal to the column rank. This common number is simply called the rank of the matrix.
        % \item \textbf{Full Rank}: A matrix is said to have full rank if its rank is as large as possible, meaning that all of its rows or columns are linearly independent.
        %     \bigbreak \noindent 
        %     The rank of an $m \times n$ matrix is bounded by 
        %     \begin{align*}
        %         Rank(A) \leq min(m,n)
        %     .\end{align*}
        %     \bigbreak \noindent 
        %     If a matrix has full rank, it means the matrix is capable of fully transforming vectors in the space without collapsing dimensions.
        %     \bigbreak \noindent 
        %     If the matrix has full rank, it is invertible. This is because the matrix does not collapse any part of the vector space it transforms.
        %     \bigbreak \noindent 
        %     If the matrix has 
        %     \begin{align*}
        %         det(A) \ne 0
        %     .\end{align*}
        %     It has full rank
        %     \bigbreak \noindent 
        %     When a matrix has full rank, it essentially means that the system of equations it represents is well-behaved, and every input gets mapped to exactly one output without overlap
        %     \bigbreak \noindent 
        %     When a matrix has full rank, it has as many independent equations as there are variables (in a square matrix case). This means that each variable (input) has its own effect on the output, and no two variables end up pointing to the same place.
        % \item \textbf{Rank-Deficient}: A matrix is rank-deficient if its rank is less than the maximum possible rank. In other words, some of the rows or columns are linearly dependent (one can be expressed as a combination of others).
        %     \bigbreak \noindent 
        %     When a matrix is rank-deficient, it means the transformation it represents collapses some part of the space into a lower dimension. 
        %     \begin{itemize}
        %         \item In $\mathbb{R}^{2}$, a rank-1 matrix would map all points onto a line, losing one dimension. $\mathbb{R}^{3}$, a rank-2 matrix would map all points onto a plane, collapsing one dimension of the space.
        %     \end{itemize}
        %     If the matrix has 
        %     \begin{align*}
        %         det(A) = 0
        %     .\end{align*}
        %     It is rank deficient

    \end{itemize}

    \pagebreak 
    \unsect{Linear dependence, span, fundamental spaces}
    \begin{itemize}
        \item \textbf{Linear dependence}: Linear dependence tells us that we will lose information because it implies that some vectors (or columns/rows of a matrix) do not add any new, unique directions to the space. These dependent vectors can be expressed as combinations of other vectors, meaning they don't span new dimensions, and as a result, the transformation collapses part of the input space into a lower-dimensional output space.
        \item \textbf{Linear independence}: No redundancy, all directions are unique
        \item \textbf{Span}: The span of a set of vectors is the collection of all possible linear combinations of those vectors
            \bigbreak \noindent 
            if a matrix has full rank, the span of its columns (or rows) is the entire codomain (or the entire vector space that the matrix maps to)
        \item \textbf{Intro to row space, column space, null space (kernel)}:
            \begin{itemize}
                \item \textbf{Column space $(C(A))$}:  The set of all linear combinations of the columns of a matrix $A$. It represents the range of the matrix and consists of all possible outputs of the matrix transformation. It's a subspace of $\mathbb{R}^{m}$ (for an $m\times n$ matrix)
                \item \textbf{Row space $(R(A))$}: The set of all linear combinations of the rows of a matrix $A$. It is the span of the rows of the matrix and forms a subspace of $\mathbb{R}^{n}$
                \item \textbf{Kernel/Null space $(\text{ker}(A)/N(A))$}: The set of all vectors $\mathbf{x}$ such that $A\mathbf{x}=0$, where $A$ is a matrix. It represents the solutions to the homogeneous system and is a subspace of $\mathbb{R}^{n}$
            \end{itemize}
            \bigbreak \noindent 
            Each of these spaces relates to the structure and solvability of linear systems and the transformation properties of the matrix.
    \item \textbf{Linear combination of vectors}: A linear combination of vectors is a sum of those vectors, each multiplied by a scalar. So, for a matrix $A$, if $\mathbf{v}_{1}, \mathbf{v}_{2}, ..., \mathbf{v}_{n}$ are its column vectors, any vector in the column space can be written as:
        \begin{align*}
            c_{1}\mathbf{v}_{1} + c_{2}\mathbf{v}_{2} + ... + c_{n}\mathbf{v}_{n}
        .\end{align*}
        \bigbreak \noindent 
        Where $c_{1}, c_{2}, ... c_{n}$ are scalars
        \bigbreak \noindent 
        As you vary the scalars in the linear combination of the matrix's columns, you generate all possible vectors in the column space (also known as the range) of the matrix.
        \bigbreak \noindent 
        \textbf{Note:} The same applies for the row space
    \item \textbf{Row space vs column space vs null space}:
        \begin{itemize}
            \item The column space tells you what the matrix outputs.
            \item The row space tells you about the constraints or conditions that the solutions to the matrix system must satisfy.
            \item The null space tells you what the matrix "loses". If a vector $\mathbf{x}$ is in the null space, it gets mapped to the zero vector, meaning it is "annihilated" by the matrix. Vectors in the null space represent dependencies between the columns of the matrix. If the matrix has non-trivial solutions to $A\mathbf{x}=0$, it indicates that the columns are linearly dependent.
                \bigbreak \noindent 
                Geometrically, the null space represents all the directions in which the matrix compresses space to a lower dimension.
                For example, in $\mathbb{R}^{3}$, if the null space is a line, the matrix compresses all points along that line to the origin.
                \bigbreak \noindent 
                If the null space is non-trivial, it indicates the matrix transformation has lost some dimensions.
                \bigbreak \noindent 
                \textbf{Note:} A non-trivial null space refers to a null space that contains vectors other than just the zero vector.
        \end{itemize}
    \item \textbf{More on the row space}: In a system of linear equations, the row space reveals various types of constraints, depending on the number of independent equations and how the rows of the matrix relate to one another. 
        \begin{itemize}
            \item \textbf{Unique Solution (Full Rank, Independent Rows)}: If the rows of the matrix are linearly independent and span the entire row space, the system has a unique solution. This means the constraints from the equations are sufficient to pin down exactly one solution.
            \item \textbf{No Solution (Inconsistent System):}
            \item \textbf{Infinite Solutions (Dependent Rows, Underdetermined System):} If some rows are linearly dependent, the system will have fewer constraints than unknowns, leading to infinitely many solutions. In this case, the system is underdetermined, meaning there aren’t enough independent constraints to specify a unique solution, allowing multiple solutions (often forming a plane, line, or higher-dimensional space).
            \item \textbf{Zero Solutions (Trivial System):} In a homogeneous system , if the rows are independent but fewer than the number of variables, there is only the trivial solution. This means that the row space spans a subspace of dimension less than the total space, so the only solution is the zero vector.
        \end{itemize}
    \item \textbf{Linear dependence in rows vs columns}:
        The rows of a matrix are linearly dependent if at least one row can be written as a linear combination of the other rows.
        \bigbreak \noindent 
        This means there is redundancy in the information that the rows provide.
        \bigbreak \noindent 
        If the rows of a matrix are linearly dependent, the row space has a lower dimension than the number of rows, meaning the system has fewer independent constraints than it might appear.
        \bigbreak \noindent 
        Consider the matrix
        \begin{align*}
            \begin{bmatrix} 1 & 2 & 3 \\ 2 & 4 & 6 \\ 3 & 6 & 9 \end{bmatrix}
        .\end{align*}
        \bigbreak \noindent 
        Here, the second row is $2\times$ the first row, and the third row is $3\times$ the first row. Therefore, the rows are linearly dependent because each row is a scalar multiple of the first row.
        \bigbreak \noindent 
        This means there’s only one independent constraint in the row space. The row space is spanned by a single vector (the first row), even though the matrix has three rows. Geometrically, the row space collapses to a lower dimension (a line in 3D space).
        \bigbreak \noindent 
        If the rows are linearly dependent, the system of equations might be underdetermined, leading to infinite solutions or no solutions.
        \bigbreak \noindent 
        The columns of a matrix are linearly dependent if at least one column can be written as a linear combination of the other columns.
        \bigbreak \noindent 
        This implies that some columns do not contribute "new" information, and there is a loss of dimensionality in the column space.
        \bigbreak \noindent 
        If the columns are linearly dependent, the column space has a lower dimension than the number of columns, meaning the matrix cannot map onto all of $\mathbb{R}^{m}$ (the output space).
        \bigbreak \noindent 
        Consider the matrix
        \begin{align*}
            \begin{bmatrix} 1 & 2 & 3 \\ 1 & 2 & 3 \\ 1 & 2 & 3\end{bmatrix}
        .\end{align*}
        \bigbreak \noindent 
        Here, the second and third columns are linearly dependent on the first column (they are multiples of the same vector). In fact, each column is identical in this case, so all the columns are linearly dependent.
        \bigbreak \noindent 
        This means the column space of this matrix is spanned by a single vector, even though there are three columns.
        \bigbreak \noindent 
        Geometrically, the matrix can only map vectors to a line in $\mathbb{R}^{3}$, rather than a full plane or space.
        \bigbreak \noindent 
        \textbf{Summary:}
        \begin{itemize}
            \item \textbf{Row dependence} affects the row space, which corresponds to the constraints on the system of equations. Linearly dependent rows mean some equations are redundant, reducing the number of independent constraints.
            \item \textbf{Column dependence} affects the column space, which corresponds to the set of possible outputs of the matrix. Linearly dependent columns mean the matrix has reduced rank, implying the system might not span the full space, and it could have a non-trivial null space (leading to infinite or no solutions).
        \end{itemize}
        \bigbreak \noindent 
        \textbf{Recall}: To check if a linear map has full rank, it is sufficient to check whether all the columns of the matrix representing the linear map are linearly independent
        \bigbreak \noindent 
        \textbf{Important:} The dimension of both the row space and the column space of a matrix is equal to the number of linearly independent rows and linearly independent columns, respectively. This common dimension is called the rank of the matrix.
    \item \textbf{Rank of the null space}: The null space of a matrix has dimension $n-$rank, where $n$ is the number of columns in the matrix. This is a consequence of the Rank-Nullity Theorem (Not yet stated).
    \item \textbf{Nullity}: The nullity of a matrix is the dimension of the null space (i.e., the number of independent vectors that get mapped to the zero vector by the matrix).
    \item \textbf{Zero nullity}: If the nullity is zero, this means that the null space has dimension 0. This implies that the only vector in the null space is the zero vector itself, $\mathbf{0}$
    \item \textbf{non-zero nullity}: 
        We know that there are infinitely many vectors in the kernel (null space) of a matrix when the nullity (dimension of the null space) is greater than zero.

    \end{itemize}


    \pagebreak 
    \unsect{Inverses}
    \begin{itemize}
        \item \textbf{Matrix inverse}: A matrix $B \in \mathbb{R}^{n\times n}$ is the inverse of a matrix $A \in \mathbb{R}^{n\times n}$ provided
            \begin{align*}
                BA = AB = I
            \end{align*}
        \item \textbf{Left and right inverse}:
            \textbf{Left Inverse:} A matrix \( B \) is a left inverse of \( A \) if \( BA = I \), where \( I \) is the identity matrix. This means \( B \) "undoes" \( A \) when multiplied from the left.
            \bigbreak \noindent 
            \textbf{Right Inverse:} A matrix \( C \) is a right inverse of \( A \) if \( AC = I \). This means \( C \) "undoes" \( A \) when multiplied from the right.
        \item \textbf{(Finite dimensions) One sided inverse theorem for matrices (inveribility criterion)}: It states that for square matrices in finite dimensions, the existence of either a left inverse or a right inverse implies the existence of the other, and hence the matrix is invertible. Specifically, if a square matrix \( A \) has a left inverse \( B \) (such that \( BA = I \)) or a right inverse \( C \) (such that \( AC = I \)), then both inverses must be equal, making \( A \) invertible with the unique inverse \( A^{-1} = B = C \).
        \item \textbf{(Finite dimensions) Inverse Uniqueness Theorem for matrices}: If \( A \) is an invertible \( n \times n \) matrix, then there is exactly one matrix \( B \) such that \( AB = BA = I \). This unique matrix is called the inverse of \( A \) and is denoted by \( A^{-1} \).
        \item \textbf{Solving $Ax = b$ for $A \in \mathbb{R}^{n\times n}$}: Assume $A \in \mathbb{R}^{n\times}$ is nonsingular, then
            \begin{align*}
                Ax = b \implies x = A^{-1}b
            \end{align*}
        \item \textbf{Matrix products}: If $A,B \in \mathbb{R}^{n\times n}$, and $AB$ nonsingular, then $A$ and $B$ are nonsingular.
        \item $(AB)^{-1}  = B^{-1}A^{-1}$
    \end{itemize}

    \pagebreak 
    \unsect{Determinant}
    \begin{itemize}
        \item \textbf{Determinant of the identity matrix}: The determinant of the identity matrix is one 
            \begin{align*}
                \text{det}(I) = 1
            .\end{align*}
        \item \textbf{Product of two determinants}: The product of two determinants $\text{det}(A)\text{det}(B) = \text{det}(AB)$. This is particularly useful when we want to find the determinant of a product of two matrices, we can assert, for two matrices $A$, and $B$
            \begin{align*}
                \text{det}(AB) = \text{det}(A)\text{det}(B)        
            .\end{align*}
        \item \textbf{Determinant of the inverse matrix}: If we have some matrix $A$, which has an inverse $A^{-1}$, then
            \begin{align*}
                \text{det}(AA^{-1}) &= \text{det}(I) = 1 = \text{det}(A)\text{det}(A^{-1}) \\
                \implies \text{det}(A^{-1}) &= \frac{1}{\text{det}(A)}
            .\end{align*}
        \item \textbf{Determinant of triangular matrices}: The determinant of a triangular matrix is the product of the main diagonal.
        \item \textbf{Determinant of the transpose}: The determinant of the transpose of a matrix is the same as the original matrix.
            \begin{align*}
                \det(A^{\top})  = \det(A)
            \end{align*}
        \item \textbf{Determinant after changing basis}: Suppose we have some matrix $\begin{pmatrix} a & b \\ c & d\end{pmatrix} $. If we introduce a new basis matrix $B$, then the map becomes $B^{-1}AB$. And the deterimanant is
            \begin{align*}
                \det(B^{-1}AB) &= \det(B^{-1})\det(A)\det(B) \\
                               &= \det(B^{-1}B)\det(A) \\
                               &= \det(I)\det(A) \\
                               &= \det(A)
            .\end{align*}
            Thus, the determinant remains the same. We say that the determinant is invariant under a change of basis.


    \end{itemize}

    \pagebreak 
    \unsect{Other matrix properties}
    \begin{itemize}
                \item \textbf{The trace of a matrix}: The trace of a square matrix is the sum of its diagonal elements. For an 
            $n \times n$ matrix $A$, the trace is defined as:
            \[
                \text{Tr}(A) = \sum_{i=1}^{n} A_{ii}
            \]
            where $A_{ii}$ are the diagonal entries of $A$. The trace is only defined for square matrices and has several useful properties, such as being invariant under a change of basis.
        \item \textbf{Cyclic property of the trace}: The trace is invariant under cyclic permutations. Observe
            \begin{align*}
                \text{Tr}(AB) = \text{Tr}(BA)
            .\end{align*}
            \bigbreak \noindent 
            Proof:
            \begin{align*}
                \text{Tr}(AB) &= \sum_{i=1}^{n}(ab)_{ii}  \\
                &=\sum_{i=1}^{n}\sum_{k=1}^{n}a_{ik}b_{ki}
            .\end{align*}
            \bigbreak \noindent 
            \textbf{Remark}. For finite sums, the order of summation can be swapped because summation is commutative and associative when dealing with real or complex numbers. This means that the sum of a collection of terms does not depend on the order in which the terms are added. So, rearranging the summation over $i$ and $k$ doesn't change the value of the overall sum. Thus
            \begin{align*}
                \text{Tr}(AB) &= \sum_{i=1}^{n}\sum_{k=1}^{n}a_{ik}b_{ki} \\
                              &= \sum_{k=1}^{n}\sum_{i=1}^{n}a_{ik}b_{ki} \\
                              &= \sum_{k=1}^{n}\sum_{i=1}^{n}b_{ki} a_{ik} \\
                              &= \sum_{k=1}^{n}(ba)_{kk} \\
                              &= \text{Tr}(BA)
            .\end{align*}

    \end{itemize}

    \pagebreak 
    \unsect{Eigenvalues and Eigenvectors}
    \begin{itemize}
         \item \textbf{Eigenvectors, Eigenvalues}:
            An \textbf{eigenvector} of a square matrix \( A \) is a non-zero vector \( \mathbf{v} \) such that when \( A \) acts on \( \mathbf{v} \), the result is a scalar multiple of \( \mathbf{v} \). Mathematically, this is written as:
            \[
                A \mathbf{v} = \lambda \mathbf{v} \quad \mathbf{v} \ne \mathbf{0}
            \]
            where \( \lambda \) is a scalar known as the eigenvalue corresponding to the eigenvector \( \mathbf{v} \).
            \bigbreak \noindent 
            An \textbf{eigenvalue} \( \lambda \) is the scalar that represents the factor by which the eigenvector is scaled during the transformation. The eigenvalue corresponds to each eigenvector and provides information about the nature of the transformation (scaling, rotation, etc.).
            \bigbreak \noindent 
            Since the left side is matrix multiplication and the right side is vector multiplication by a scalar, we can rewrite the equation above as
            \begin{align*}
                A\mathbf{v} &= (\lambda I)\mathbf{v} \\
                \implies A\mathbf{v} - \lambda I \mathbf{v} &= \mathbf{0} \\
                \implies \mathbf{v}(A-\lambda I) &= \mathbf{0}
            .\end{align*}
            \bigbreak \noindent 
            This is a homogeneous system. If the transformation map ($A-\lambda I$) is one-to-one and thus invertible, the only solution would be the trivial solution ($\mathbf{v}  = \mathbf{0})$. In order to have non-zero solutions for $\mathbf{v}$ (eigenvectors), the system above would need to not be one-to-one (multiple solutions to the solution vector $\mathbf{0}$), and thus
            \begin{align*}
                \text{det}(A - \lambda I) = 0
            .\end{align*}
        \item \textbf{Characteristic equation, characteristic polynomial}:
        The characteristic equation of a square matrix \( A \) is the equation obtained by setting the determinant of \( A - \lambda I \) equal to zero:
            \[
                \det(A - \lambda I) = 0,
            \]
            where \( \lambda \) represents the eigenvalues of \( A \) and \( I \) is the identity matrix. Solving this equation gives the eigenvalues of \( A \).
             The characteristic polynomial is the polynomial in \( \lambda \) obtained from the determinant \( \det(A - \lambda I) \). It is typically expressed as:
            \[
                p(\lambda) = \det(A - \lambda I),
            \]
            and its roots are the eigenvalues of the matrix \( A \).
        \item \textbf{Finding eigenvectors and eigenvalues}: To find the eigenvalues $\lambda$, we need to solve the characteristic equation:
            \begin{align*}
                \det(A - \lambda I) =0
            .\end{align*}
            \bigbreak \noindent 
            This equation determines the values of $\lambda$ for which the matrix $A-\lambda I$ is singular (non-invertible), which leads to non-zero solutions for $\mathbf{v}$.
            \bigbreak \noindent 
            Once the eigenvalues $\lambda$ are found, substitute each $\lambda$ into the equation $(A-\lambda I)\mathbf{v}=0$ and solve for $\mathbf{v}$. These are the eigenvectors corresponding to each eigenvalue.

    \end{itemize}

    \unsect{Basis}
    \begin{itemize}
        \item \textbf{Basis}: A basis of a vector space is a set of linearly independent vectors that span the entire space. This means any vector in the space can be uniquely expressed as a linear combination of the basis vectors. A basis provides a reference framework for representing vectors in that space.
            \bigbreak \noindent 
            For an $n$-dimensional vector space, a basis will consist of exactly $n$ vectors. The coordinates of a vector relative to a basis are the coefficients used in this linear combination.
        \item \textbf{Standard basis (Implied basis)}: From vector calculus, we know that $\hat{i}$, $\hat{j}$ are the unit vectors the describe the 2 dimensional cartesian plane. Where
            \begin{align*}
                \hat{i} &= \begin{bmatrix} 1 \\ 0 \end{bmatrix} \\
                \hat{j} &= \begin{bmatrix} 0 \\ 1 \end{bmatrix}
            .\end{align*}
            Thus, the standard implied basis when working in $\mathbb{R}^{2}$ is the matrix
            \begin{align*}
                \begin{bmatrix} 1 & 0 \\ 0 & 1\end{bmatrix}
            .\end{align*}
            And a vector $\vec{\mathbf{v}} = \begin{bmatrix} x \\ y \end{bmatrix} $ can be repesented as scaling these basis vectors and then adding them. Ie
            \begin{align*}
                \begin{bmatrix} x \\ y \end{bmatrix} = x \begin{bmatrix} 1 \\ 0\end{bmatrix} + y \begin{bmatrix} 0 \\ 1 \end{bmatrix}
            .\end{align*}
            \bigbreak \noindent 
            Thus, the standard basis for $\mathbb{R}^{n}$ is the $n\times n$ identity matrix $I$
        \item \textbf{Basis notation}: When with vectors, the choice of basis determines how we interpret where the vector sits. For the standard basis, the componenets of the vector is precisely where it will be. Thus, for the standard basis, we write
            \begin{align*}
                [\vec{\mathbf{v}}]
            .\end{align*}
            If the basis were non-standard, we would need to specify it. We write
            \begin{align*}
                [\vec{\mathbf{v}}]_{B}
            .\end{align*}
            Where $B$ is then defined as the matrix representing the basis.
        \item \textbf{Change of basis explained}: We have
            \begin{itemize}
                \item $B^{-1}v = v_{B}$
                \item $Bv_{B} = v$
            \end{itemize}
            \bigbreak \noindent 
            Suppose we have a vector $v = \begin{pmatrix} a \\ b \end{pmatrix}$ in the standard basis. Define a new basis $b_{1} = \begin{pmatrix} u \\ v \end{pmatrix}, b_{2} = \begin{pmatrix} x \\ y\end{pmatrix}$, then $B = \begin{pmatrix} u & x \\ v & y\end{pmatrix} $. Where $v$ under this basis becomes $v_{B} = \begin{pmatrix} \alpha \\ \beta\end{pmatrix} $. Then
            \begin{align*}
                \begin{pmatrix} a \\ b \end{pmatrix} &= \alpha \begin{pmatrix} u \\ v\end{pmatrix} + \beta \begin{pmatrix} x \\ y\end{pmatrix} \\
                                                     &= \begin{pmatrix} \alpha u + \beta x \\ \alpha v + \beta y \end{pmatrix} \\
                                                     &= \begin{pmatrix} u & x \\ v & y \end{pmatrix}\begin{pmatrix} \alpha \\ \beta \end{pmatrix} \\
                \therefore v &= Bv_{B}
            .\end{align*}
            From this, 
            \begin{align*}
                v & = Bv_{B} \\
                B^{-1}v &= B^{-1}Bv_{B} \\
                B^{-1}v &= I v_{B} \\
                \therefore B^{-1}v &= v_{B}
            .\end{align*}
            \bigbreak \noindent 
            \textbf{Note:} The new basis must span the same space as the old basis for the change of basis to work correctly
        \item \textbf{Basis in maps}: Suppose we have some linear map
            \[
                L: V \to W
            \]
            where \(V\) and \(W\) are vector spaces. Suppose \(V\) has a basis \(\beta = \{b_{1}, \dots, b_{n}\}\), and \(W\) has a basis \(\gamma = \{c_{1}, \dots, c_{n}\}\). Then, the matrix representation of \(L\) with respect to these bases is denoted as
            \[
                [L]_{\beta}^{\gamma}
            \]
            Now, suppose we define new bases \(\beta^{\prime}\) for \(V\) and \(\gamma^{\prime}\) for \(W\). We want to find the matrix representation of the linear map \(L\) in the new bases.
            \bigbreak \noindent 
            We have the following transformations:
            \[
                V_{\beta^{\prime}} \to W_{\gamma^{\prime}} \quad \text{and} \quad V_{\beta} \to W_{\gamma}
            \]
            To find the matrix of the map \(L: V_{\beta^{\prime}} \to W_{\gamma^{\prime}}\), we need to relate the new basis vectors to the old basis vectors. Specifically, we perform the following steps:
            \begin{itemize}
                \item To change from the new basis \(\beta^{\prime}\) to the old basis \(\beta\), we multiply by the change-of-basis matrix \(B\), so we have \(B[v]_{\beta^{\prime}} = [v]_{\beta}\).
                \item To change from the old basis \(\gamma\) to the new basis \(\gamma^{\prime}\), we multiply by the inverse of the change-of-basis matrix \(C^{-1}\), so we have \(C^{-1}[w]_{\gamma} = [w]_{\gamma^{\prime}}\).
            \end{itemize}
            \bigbreak \noindent 
            Thus, the matrix representation of \(L\) in the new bases \(\beta^{\prime}\) and \(\gamma^{\prime}\) is given by:
            \[
                [L]_{\beta^{\prime}}^{\gamma^{\prime}} = C^{-1}[L]_{\beta}^{\gamma}B
            \]
        \item \textbf{Diagonalization and eigenbases}: We want to find some new basis $B$ such that the linear map becomes diagonal. That is
            \begin{align*}
                B^{-1}LB = L_{D}
            .\end{align*}
            \bigbreak \noindent 
            We can achieve this via eigenvectors. By changing the basis to one formed by the eigenvectors, we simplify the linear map so that it acts independently on each direction (each eigenvector). In this new basis, the map scales each eigenvector by its corresponding eigenvalue, without mixing different directions. This independence is what makes the matrix diagonal, making the transformation much easier to understand and work with.
            \bigbreak \noindent 
            Consider a matrix \( A \) and a basis formed from its eigenvectors, say 
            \( \mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n \). For simplicity, let’s assume \( A \) has \( n \) linearly 
            independent eigenvectors (which guarantees diagonalization).
            \bigbreak \noindent 
            We can express any vector \( \mathbf{x} \) in terms of this new basis as a linear combination of the eigenvectors
            \[
                \mathbf{x} = c_1 \mathbf{v}_1 + c_2 \mathbf{v}_2 + \cdots + c_n \mathbf{v}_n
            \]
            When we apply the matrix \( A \) to \( \mathbf{x} \), because each eigenvector \( \mathbf{v}_i \) satisfies 
            \( A \mathbf{v}_i = \lambda_i \mathbf{v}_i \), we get:
            \[
                A \mathbf{x} = c_1 \lambda_1 \mathbf{v}_1 + c_2 \lambda_2 \mathbf{v}_2 + \cdots + c_n \lambda_n \mathbf{v}_n
            \]
            This shows that \( A \) acts on each eigenvector individually by multiplying 
            it by its corresponding eigenvalue. The action of \( A \) is now "separated" 
            along each eigenvector direction.
            \bigbreak \noindent 
            To represent $A$ in the new basis (the eigenvector basis), we express the transformation in matrix form with respect to this new basis. Let’s call the matrix $P$, where the columns of $P$ are the eigenvectors of $A$
            \bigbreak \noindent 
            The matrix $A$ in the original basis acts in a complicated way, but in the eigenvector basis, the transformation is simplified. Specifically, in the eigenvector basis, applying $A$ scales each eigenvector by its corresponding eigenvalue. This means that, with respect to this basis, the matrix representing $A$ becomes diagonal
            \bigbreak \noindent 
            \begin{align*}
                P^{-1}AP &= D \\
                \implies  A &= PDP^{-1}
            .\end{align*}
            \bigbreak \noindent 
            where \( D \) is a diagonal matrix with the eigenvalues \( \lambda_1, \lambda_2, \dots, \lambda_n \) on the diagonal:
            \[
                D = \begin{pmatrix}
                    \lambda_1 & 0 & \cdots & 0 \\
                    0 & \lambda_2 & \cdots & 0 \\
                    \vdots & \vdots & \ddots & \vdots \\
                    0 & 0 & \cdots & \lambda_n
                \end{pmatrix}
            \]
            \bigbreak \noindent 
            Consider a matrix $\begin{pmatrix} 1 & 0 \\ 0 & 2 \end{pmatrix}$. When this map acts on a vector $\begin{pmatrix} x \\ y \end{pmatrix}$,
            \begin{align*}
                \begin{pmatrix} 1 & 0 \\ 0 & 2 \end{pmatrix} \begin{pmatrix} x \\ y\end{pmatrix} &= \begin{pmatrix} 1x \\ 2y\end{pmatrix}
            .\end{align*}
            Ie the vector is scaled by $1$ in the $x$ direction, and $2$ in the $y$ direction
        \item \textbf{Note about diagonalization and eigenbases}: Eigenbases (or eigenspaces) are typically defined only for linear maps of the form 
            \( L: V \to V \), where the map \( L \) acts on a vector space \( V \) and maps vectors within that same space. 
            \bigbreak \noindent 
            If a linear map \( L \) is of the form \( L: V \to W \), where \( W \) is a different vector space (or even a subspace of \( V \)), the concept of eigenvectors and eigenvalues doesn’t apply in the usual sense. The reason is that the output of \( L \) is not necessarily a scalar multiple of the input vector \( v \), and it may not even belong to the same vector space.
            \bigbreak \noindent 
            For eigenvectors and eigenvalues to be meaningful, you need the transformation to act within a single vector space, ensuring that the transformed vector remains in the same space, allowing us to compare it directly to the original vector.
        \item \textbf{Diagonalization of a symmetric matrix}: If a matrix $A \in \mathbb{R}^{n\times n}$ is symmetric with real eigenvalues, then the eigenvectors of $A$ are orthogonal.
            \bigbreak \noindent 
            Thus, we can find an orthonormal set of basis vectors easily.

    \end{itemize}

    \pagebreak 
    \unsect{Vector spaces and subspaces}
    \begin{itemize}
        \item \textbf{Vector Space}: A vector space is a set of vectors that satisfy
            \begin{enumerate}
                \item Space has a zero vector
                \item Closed under addition
                \item Closed under multiplication by a scalar
            \end{enumerate}
            We also need the space to have the algebric axioms.
            \begin{itemize}
                \item \textbf{Commutativity of addition}: \( u + v = v + u \)
                \item \textbf{Associativity of addition}: \( (u + v) + w = u + (v + w) \)
                \item \textbf{Additive identity}: There exists a vector \( 0 \in V \) such that \( v + 0 = v \) for all \( v \in V \).
                \item \textbf{Additive inverse}: For every \( v \in V \), there exists a vector \( -v \in V \) such that \( v + (-v) = 0 \).
                \item \textbf{Associativity of scalar multiplication}: \( c(dv) = (cd)v \)
                \item \textbf{Distributivity of scalar multiplication over vector addition}: \( c(u + v) = cu + cv \)
                \item \textbf{Distributivity of scalar multiplication over scalar addition}: \( (c + d)v = cv + dv \)
                \item \textbf{Scalar identity}: \( 1v = v \), where 1 is the multiplicative identity in the field.
            \end{itemize}
            \bigbreak \noindent 
            If we have these properties and algebraic axioms, we have a valid vector space.
        \item \textbf{Abstract vector space}: An abstract vector space is a generalization of this concept, where the elements (vectors) may not have a concrete geometric form, such as functions, polynomials, or matrices, but still follow the same axioms
            \bigbreak \noindent 
            For example, A set of matrices can be defined as an abstract vector space
        \item \textbf{Basis of a vector space, span of the basis}: The basis of a vector space is a choice of $n$ vectors $b_{1},...b_{n}$ such that
            \begin{align*}
                \mathbf{v} &= s_{1}b_{1} + ...  + s_{n}b_{n}
            .\end{align*}
            \bigbreak \noindent 
            If we are able to generate all vectors in the space by simple scaling the vectors by some constant and adding them, the basis $b_{1}, ...,b_{n} $ are said to \textbf{span} the vector space.
        \item \textbf{The number of linearly independent vectors per space}: In $\mathbb{R}^{n}$ the maximum number of linearly independent vectors we can have is $n$. For example, in $\mathbb{R}^{2}$, the maximum number of linearly independent vectors we can have is $2$. This is why we need exactly $n$ vectors to form a basis in $\mathbb{R}^{n}$, and having more than $n$ will also result in the case of allowing us to find and throw out the linearly dependent ones.
            \bigbreak \noindent 
            In other words, There are only $n$ linearly independent vectors in $\mathbb{R}^{n}$ because the dimension of $\mathbb{R}^{n}$ $n$, which means that the space has exactly $n$ independent directions, or degrees of freedom
        \item \textbf{Definition of dimension}: The dimension of a vector space is the number of vectors in a basis for that space. A basis is a set of linearly independent vectors that spans the entire space. In $\mathbb{R}^{n} $ , any valid basis must have exactly $n$ vectors, because it takes $n$ vectors to fully describe the space.
            \bigbreak \noindent 
            A set of vectors is linearly independent if no vector in the set can be written as a linear combination of the others. In $\mathbb{R}^{n}$ , if you have more than $n$ vectors, at least one of those vectors can always be written as a linear combination of the others, meaning they will be linearly dependent. This is because there are only $n$ independent directions in $\mathbb{R}^{n}$
            \bigbreak \noindent 
            In $\mathbb{R}^{2}$, the dimension of the space is 2, meaning that any valid set of linearly independent vectors can have at most two vectors. This is because two vectors are sufficient to fully describe the space—they form a basis. Any other vector in $\mathbb{R}^{2}$ can be expressed as a linear combination of these two vectors.
            \bigbreak \noindent 
            Once you have two linearly independent vectors, adding any third vector will result in linear dependence, because that third vector will lie in the span of the first two vectors.
        \item \textbf{Discern valid basis}: To give a valid basis for a vector space, we must list a collection of vectors that satisfy
            \begin{enumerate}
                \item Basis should span the whole space
                \item No redundant basis vectors
                \item Linearly independent set
            \end{enumerate}
        \item \textbf{Isomorphic vector spaces}:
            Two vector spaces are isomorphic if there is a one-to-one correspondence (a bijection) between them that preserves the structure of vector addition and scalar multiplication. This means that if vector spaces $V$ and $W$ are isomorphic, there exists a map (called an isomorphism) $\phi: V \to W$ such that:
            \begin{enumerate}
                \item $\phi$ is bijective: Every element in $W$ has a unique preimage in $V$, and every element in $V$ is mapped to a unique element in $W$.
                \[
                    \forall w \in W, \ \exists\ v \in V \text{ such that } \phi(v) = w
                \]
                \item $\phi$ preserves addition: For any two vectors $u, v \in V$,
                \[
                    \phi(u + v) = \phi(u) + \phi(v)
                \]
                \item $\phi$ preserves scalar multiplication: For any scalar $c \in \mathbb{F}$ and any vector $v \in V$,
                \[
                    \phi(cv) = c\phi(v)
                \]
            \end{enumerate}
            \bigbreak \noindent 
            \textbf{Example}: $M_{2\times 2}$ is isomorphic to $\mathbb{R}^{4} $
            \bigbreak \noindent 
            A matrix in $m_{2\times 2}$ can be written as 
            \begin{align*}
                \begin{pmatrix} a & b \\ c & d \end{pmatrix}
            .\end{align*}
            Where $a,b,c,d \in \mathbb{R}$. This matrix can be uniquely represented as a 4-dimensional vector:
            \begin{align*}
                (a,b,c,d) \in \mathbb{R}^{4}
            .\end{align*}
            \bigbreak \noindent 
            The correspondence between the matrix and the 4-dimensional vector is a linear bijection that preserves both vector addition and scalar multiplication. Thus, there is a one-to-one correspondence between $M_{2\times 2}$ and $\mathbb{R}^{4}$ , and the two spaces are isomorphic.
            \bigbreak \noindent 
            Moreover, 
            The isomorphism between $M_{2 \times 2}$ (the space of $2 \times 2$ matrices) and $\mathbb{R}^4$ (the 4-dimensional real vector space) can be described by a linear map that transforms a matrix into a 4-dimensional vector by simply mapping the matrix entries to the components of the vector.
            \bigbreak \noindent 
            Let’s define the linear map $\phi: M_{2 \times 2} \to \mathbb{R}^4$.
            \bigbreak \noindent 
            For any matrix
            \[
                A = \begin{pmatrix}
                    a & b \\
                    c & d
                \end{pmatrix} \in M_{2 \times 2},
            \]
            the corresponding vector in $\mathbb{R}^4$ under the map $\phi$ would be:
            \[
                \phi(A) = (a, b, c, d) \in \mathbb{R}^4.
            \]
            This map simply "flattens" the matrix into a 4-tuple of real numbers, with the components arranged in a consistent order (for example, row by row or column by column). In this case, we are mapping the entries of the matrix row by row.
            \bigbreak \noindent 
            Conversely, given any vector $(a, b, c, d) \in \mathbb{R}^4$, the corresponding matrix in $M_{2 \times 2}$ under the inverse map $\phi^{-1}$ would be:
            \[
                \phi^{-1}(a, b, c, d) = \begin{pmatrix}
                    a & b \\
                    c & d
                \end{pmatrix}.
            \]
            \textbf{Properties of the Map}:
            \begin{itemize}
                \item \textbf{Bijectivity:} Every matrix corresponds to a unique vector, and every vector corresponds to a unique matrix.
                \item \textbf{Additivity:} If $A_1 = \begin{pmatrix} a_1 & b_1 \\ c_1 & d_1 \end{pmatrix}$ and $A_2 = \begin{pmatrix} a_2 & b_2 \\ c_2 & d_2 \end{pmatrix}$, then
                    \[
                        \phi(A_1 + A_2) = \phi\left(\begin{pmatrix} a_1 + a_2 & b_1 + b_2 \\ c_1 + c_2 & d_1 + d_2 \end{pmatrix}\right) = (a_1 + a_2, b_1 + b_2, c_1 + c_2, d_1 + d_2),
                    \]
                    which is the same as $\phi(A_1) + \phi(A_2)$.
                \item  \textbf{Scalar Multiplication:} For any scalar $\lambda \in \mathbb{R}$,
            \end{itemize}
            \[
                \phi(\lambda A) = \phi\left(\begin{pmatrix} \lambda a & \lambda b \\ \lambda c & \lambda d \end{pmatrix}\right) = (\lambda a, \lambda b, \lambda c, \lambda d),
            \]
            \bigbreak \noindent 
            which is the same as $\lambda \phi(A)$.
            \bigbreak \noindent 
            Thus, $\phi$ is a linear isomorphism between $M_{2 \times 2}$ and $\mathbb{R}^4$.
            \bigbreak \noindent 
            \textbf{Note:} We only have isomorphism if the dimensions are the same.


    \end{itemize}


    
    \pagebreak 
    \unsect{Special matrices}
    \begin{itemize}
        \item \textbf{Symmetric matrix}: A matrix $A \in \mathbb{R}^{n\times n}$ is symmetric if $A^{\top} = A$
        \item \textbf{Skew symmetric matrix}: A matrix $A \in \mathbb{R}^{n\times n}$ is skew symmetric if $A^{\top} = -A$
    \end{itemize}







    
\end{document}
