\documentclass{report}

\input{~/dev/latex/template/preamble.tex}
\input{~/dev/latex/template/macros.tex}

\title{\Huge{}}
\author{\huge{Nathan Warner}}
\date{\huge{}}
\pagestyle{fancy}
\fancyhf{}
\rhead{}
\fancyhead[R]{\itshape Warner} % Right header: Last Name
\fancyhead[L]{\itshape\leftmark}  % Right header: Section Name
\cfoot{\thepage}
\renewcommand{\headrulewidth}{0pt} % Remove the header line

% Change the title
\hypersetup{
	pdftitle={Math 2}
}


\begin{document}
    % \maketitle
        \begin{titlepage}
       \begin{center}
           \vspace*{1cm}
    
           \textbf{Comprehensive Compendium:} \\
            Calculus II
    
           \vspace{0.5cm}
            
                
           \vspace{1.5cm}
    
           \textbf{Nathan Warner}
    
           \vfill
                
                
           \vspace{0.8cm}
         
           \includegraphics[width=0.4\textwidth]{~/niu/seal.png}
                
           Computer Science \\
           Northern Illinois University\\
           August 28,2023 \\
           United States\\
           
                
       \end{center}
    \end{titlepage}
    \tableofcontents
    \pagebreak \bigbreak \noindent
    \unsect{Calculus II}
    \bigbreak \noindent 

    \bigbreak \noindent 
    \subsection{Chapter 1 Key Equations}
    \bigbreak \noindent 
    \begin{itemize}
        \item \textbf{Mean Value Theorem For Integrals}: If  $f(x)$ is continuous over an interval  [a,b], then there is at least one point  $c\in[a,b]$ such that 
            \begin{align*}
                f(c) = \frac{1}{b-a}\int f(x)\ dx
            .\end{align*}
        \item \textbf{Integrals resulting in inverse trig functions}
                \begin{enumerate}
        \item \begin{align*}
                \int \frac{dx}{\sqrt{a^{2}-x^{2}}} = \sin^{-1}{\frac{x}{\abs{a}}} + C
        .\end{align*}
    \item \begin{align*}
        \int \frac{dx}{a^{2}+x^{2}} = \frac{1}{a}\tan^{-1}{\frac{x}{a}} + C
    .\end{align*}
    \item \begin{align*}
            \int \frac{dx}{x\sqrt{x^{2}-a^{2}}} = \frac{1}{\abs{a}}\sec^{-1}{\frac{\abs{x}}{a}} + C
    .\end{align*}
    \end{enumerate}
    \end{itemize}

    % \pagebreak \bigbreak \noindent 
    % \subsection{Chapter 2 Key Terms / Ideas}
    % \bigbreak \noindent 
    % \begin{itemize}
    %     \item \textbf{Finding limits of integration for region between two functions}: Usually, we want our limits of integration to be the points where the functions intersect
    %     \item A \textbf{"complex region"} between curves usually refers to an area that is not easily described by a single, continuous function over the interval of interest.
    %     \item \textbf{compound regions} are regions bounded by the graphs of functions that cross one another
    %     \item \textbf{Cross-section:} The intersection of a plane and a solid object.
    %     \item a \textbf{cylinder} is a three-dimensional shape that has two parallel, congruent bases connected by a curved surface. The bases are usually circles, but they can be other shapes as well
    %     \item The line segment connecting the centers of the two bases is called the \textbf{"axis" of the cylinder.}
    %     \item \textbf{Slicing method:} A method of calculating the volume of a solid that involves cutting the solid into pieces, estimating the volume of each piece, then adding these estimates to arrive at an estimate of the total volume; as the number of slices goes to infinity, this estimate becomes an integral that gives the exact value of the volume.
    %     \begin{enumerate}
    %         \item Examine the solid and determine the shape of a cross-section of the solid. It is often helpful to draw a picture if one is not provided.
    %         \item Determine a formula for the area of the cross-section.
    %         \item Integrate the area formula over the appropriate interval to get the volume.
    %     \end{enumerate}
    %     \item \textbf{Solid of revolution:} A solid generated by revolving a region in a plane around a line in that plane.
    %     \item \textbf{Disk method:} A special case of the slicing method used with solids of revolution when the slices are disks.
    %     \item A \textbf{Washer (Annuli)} is a disk with holes in the center.
    %     \item \textbf{Washer method:} A special case of the slicing method used with solids of revolution when the slices are washers.
    %     \item \textbf{Method of cylindrical shells:} A method of calculating the volume of a solid of revolution by dividing the solid into nested cylindrical shells; this method is different from the methods of disks or washers in that we integrate with respect to the opposite variable.
    %     % \item A \textbf{cylinder} is defined as any solid that can be generated by translating a plane region along a line perpendicular to the region, called the \textbf{axis of the cylinder}.
    %      \item \textbf{Arc length:} The arc length of a curve can be thought of as the distance a person would travel along the path of the curve.
    %     \item \textbf{Surface area:} The surface area of a solid is the total area of the outer layer of the object; for objects such as cubes or bricks, the surface area of the object is the sum of the areas of all of its faces.
    %     % \item \textbf{Catenary:} A curve in the shape of the function \(y = a \cosh(x/a)\) is a catenary; a cable of uniform density suspended between two supports assumes the shape of a catenary.
    %     % \item \textbf{Center of mass:} The point at which the total mass of the system could be concentrated without changing the moment.
    %     % \item \textbf{Centroid:} The centroid of a region is the geometric center of the region; laminas are often represented by regions in the plane; if the lamina has a constant density, the center of mass of the lamina depends only on the shape of the corresponding planar region; in this case, the center of mass of the lamina corresponds to the centroid of the representative region.
    %     % \item \textbf{Density function:} A density function describes how mass is distributed throughout an object; it can be a linear density, expressed in terms of mass per unit length; an area density, expressed in terms of mass per unit area; or a volume density, expressed in terms of mass per unit volume; weight-density is also used to describe weight (rather than mass) per unit volume.
    %     % \item \textbf{Doubling time:} If a quantity grows exponentially, the doubling time is the amount of time it takes the quantity to double, and is given by \(\frac{\ln 2}{k}\).
    %     % \item \textbf{Exponential decay:} Systems that exhibit exponential decay follow a model of the form \(y = y_0 e^{-kt}\).
    %     % \item \textbf{Exponential growth:} Systems that exhibit exponential growth follow a model of the form \(y = y_0 e^{kt}\).
    %     % \item \textbf{Frustum:} A portion of a cone; a frustum is constructed by cutting the cone with a plane parallel to the base.
    %     % \item \textbf{Half-life:} If a quantity decays exponentially, the half-life is the amount of time it takes the quantity to be reduced by half. It is given by \(\frac{\ln 2}{k}\).
    %     % \item \textbf{Hooke's Law:} This law states that the force required to compress (or elongate) a spring is proportional to the distance the spring has been compressed (or stretched) from equilibrium; in other words, \(F = kx\), where \(k\) is a constant.
    %     % \item \textbf{Hydrostatic pressure:} The pressure exerted by water on a submerged object.
    %     % \item \textbf{Lamina:} A thin sheet of material; laminas are thin enough that, for mathematical purposes, they can be treated as if they are two-dimensional.
    %     % \item \textbf{Moment:} If \(n\) masses are arranged on a number line, the moment of the system with respect to the origin is given by \(M = \sum_{i=1}^{n} m_i x_i\); if, instead, we consider a region in the plane, bounded above by a function \(f(x)\) over an interval \([a, b]\), then the moments of the region with respect to the \(x\)- and \(y\)-axes are given by \(M_x = \rho \int_{a}^{b} \frac{[f(x)]^2}{2} dx\) and \(M_y = \rho \int_{a}^{b} x f(x) dx\), respectively.
    %     % \item \textbf{Symmetry principle:} The symmetry principle states that if a region \(R\) is symmetric about a line \(l\), then the centroid of \(R\) lies on \(l\).
    %     % \item \textbf{Theorem of Pappus for volume:} This theorem states that the volume of a solid of revolution formed by revolving a region around an external axis is equal to the area of the region multiplied by the distance traveled by the centroid of the region.
    %     % \item \textbf{Work:} The amount of energy it takes to move an object; in physics, when a force is constant, work is expressed as the product of force and distance.
    % \end{itemize}

    \pagebreak \bigbreak \noindent 
    \subsection{Chapter 2 Key Equations}
    \bigbreak \noindent 
    \begin{itemize}

    \item \textbf{Area between two curves, integrating on the x-axis}
    \begin{align}
        A = \int_{a}^{b} [f(x) - g(x)] \, dx
    \end{align}
    Where $f(x) \geq g(x)$
    \begin{align*}
        A = \int_{a}^{b}\ [g(x) - f(x)]\ dx
    .\end{align*}
    for $g(x) \geq f(x)$

    \item \textbf{Area between two curves, integrating on the y-axis}
    \begin{align}
        A = \int_{c}^{d} [u(y) - v(y)] \, dy
    \end{align}

    \item \textbf{Areas of compound regions}
        \begin{align*}
          \int_{a}^{b}\ \abs{f(x)-g(x)}\ dx 
        .\end{align*}
    \item \textbf{Area of complex regions}
        \begin{align*}
            \int_{a}^{b}\ f(x)\ dx + \int_{b}^{c}\ g(x)\ dx
        .\end{align*}
    \item \textbf{Slicing Method}
        \begin{align*}
            V(s) = \summation{n}{i=1}\ A(x_{i}^{*})\ \Delta x  = \int_{a}^{b}\ A(x)\ dx
        .\end{align*}
    \item \textbf{Disk Method along the x-axis}
    \begin{align}
        V = \int_{a}^{b} \pi [f(x)]^2 \, dx
    \end{align}

    \item \textbf{Disk Method along the y-axis}
    \begin{align}
        V = \int_{c}^{d} \pi [g(y)]^2 \, dy
    \end{align}

    \item \textbf{Washer Method along the x-axis}
    \begin{align}
        V = \int_{a}^{b} \pi [(f(x))^2 - (g(x))^2] \, dx
    \end{align}

    \item \textbf{Washer Method along the y-axis}
    \begin{align}
        V = \int_{c}^{d} \pi [(u(y))^2 - (v(y))^2] \, dy
    \end{align}

    \item \textbf{Radius if revolved around other line (Washer Method)}
        \begin{align*}
           If:\ x=-k\\
           Then:\ r = Function + k
        .\end{align*}
        \begin{align*}
           If:\ x=k\\
           Then:\ r = k - Function
        .\end{align*}

    \item \textbf{Method of Cylindrical Shells (x-axis)}
    \begin{align}
        V = \int_{a}^{b} 2\pi x f(x) \, dx
    \end{align}

    \item \textbf{Method of Cylindrical Shells (y-axis)}
    \begin{align}
        V = \int_{c}^{d} 2\pi y g(y) \, dy
    \end{align}

    \item \textbf{Region revolved around other line (method of cylindrical shells):}
        \begin{align*}
            If:\ x=-k \\
            Then:\ V = \int_{a}^{b}\ 2\pi (x+k)(f(x))\ dx
        .\end{align*}
        \begin{align*}
             If:\ x=k \\
            Then:\ V = \int_{a}^{b}\ 2\pi (k-x)(f(x))\ dx
        .\end{align*}
    \item \textbf{A Region of Revolution Bounded by the Graphs of Two Functions (method cylindrical shells)}
        \begin{align*}
            V = \int_{a}^{b}\ 2\pi x\left[f(x)-g(x)\right]\ dx
        .\end{align*}

    \item \textbf{Arc Length of a Function of x}
    \begin{align}
        \text{Arc Length} = \int_{a}^{b} \sqrt{1 + [f'(x)]^2} \, dx
    \end{align}

    \item \textbf{Arc Length of a Function of y}
    \begin{align}
        \text{Arc Length} = \int_{c}^{d} \sqrt{1 + [g'(y)]^2} \, dy
    \end{align}

	\item \textbf{Surface Area of a Function of x (Around x)}
    \begin{align}
        \text{Surface Area} = \int_{a}^{b} 2\pi f(x) \sqrt{1 + [f'(x)]^2} \, dx
    \end{align}

    \item \textbf{Surface Area of a Function of x (Around y)}
    \begin{align}
        \text{Surface Area} = \int_{a}^{b} 2\pi \sqrt{1 + [f'(x)]^2} \, dx \\
        \text{Or: } \int_{a}^{b}\ 2\pi u(y)\sqrt{1+(u^{\prime}(y))^{2}}\ dy
    \end{align}

    \item \textbf{Natural logarithm function}
    \begin{align}
        \ln x = \int_{1}^{x} \frac{1}{t} \, dt\
    \end{align}

    \item \textbf{Exponential function}
    \begin{align}
        y = e^x, \quad \ln y = \ln(e^x) = x\
    \end{align}

    \item  \textbf{Logarithm Differentiation}
    \begin{align*}
        f^{\prime}(x) = f(x) \cdot \frac{d}{dx}\ln{\left(f^{\prime}(x)\right)}
    .\end{align*}
    \textbf{Note:} Use properties of logs before you differentiate whats inside the logarithm

    % \item \textbf{Mass of a one-dimensional object}
    % \begin{align}
    %     m = \int_{a}^{b} \rho(x) \, dx
    % \end{align}
    %
    % \item \textbf{Mass of a circular object}
    % \begin{align}
    %     m = \int_{0}^{r} 2\pi x \rho(x) \, dx
    % \end{align}
    %
    % \item \textbf{Work done on an object}
    % \begin{align}
    %     W = \int_{a}^{b} F(x) \, dx
    % \end{align}
    %
    % \item \textbf{Hydrostatic force on a plate}
    % \begin{align}
    %     F = \int_{a}^{b} \rho w(x) s(x) \, dx
    % \end{align}
    %
    % \item \textbf{Mass of a lamina}
    % \begin{align}
    %     m = \rho \int_{a}^{b} f(x) \, dx
    % \end{align}
    %
    % \item \textbf{Moments of a lamina}
    % \begin{align}
    %     M_x = \rho \int_{a}^{b} \frac{[f(x)]^2}{2} \, dx, \quad M_y = \rho \int_{a}^{b} x f(x) \, dx
    % \end{align}
    %
    % \item \textbf{Center of mass of a lamina}
    % \begin{align}
    %     \bar{x} = \frac{M_y}{m},\ \ \text{and}\ \  \bar{y} = \frac{M_x}{m}
    % \end{align}

    \end{itemize}


    % \pagebreak \bigbreak \noindent 
    % \subsection{Chapter 3 Key Terms}
    % \bigbreak \noindent 
    % \begin{itemize}
    %     \item \textbf{integration by parts}: a technique of integration that allows the exchange of one integral for another using the formula 
    %     \item \textbf{integration table}: a table that lists integration formulas.
    %     \item \textbf{power reduction formula}: a rule that allows an integral of a power of a trigonometric function to be exchanged for an integral involving a lower power.
    %     \item \textbf{trigonometric integral}: an integral involving powers and products of trigonometric functions.
    %     \item \textbf{trigonometric substitution}: an integration technique that converts an algebraic integral containing expressions of the form \( \sqrt{a^2 - x^2} \), \( \sqrt{a^2 + x^2} \), or \( \sqrt{x^2 - a^2} \) into a trigonometric integral.
    %     \item \textbf{partial fraction decomposition}: a technique used to break down a rational function into the sum of simple rational functions.
    %     \item \textbf{improper integral}: an integral over an infinite interval or an integral of a function containing an infinite discontinuity on the interval; an improper integral is defined in terms of a limit. The improper integral converges if this limit is a finite real number; otherwise, the improper integral diverges.
    %      % \item \textbf{absolute error}: if \( B \) is an estimate of some quantity having an actual value of \( A \), then the absolute error is given by \( |A-B| \).
    %     % \item \textbf{computer algebra system (CAS)}: technology used to perform many mathematical tasks, including integration.
    %     % \item \textbf{midpoint rule}: a rule that uses a Riemann sum of the form 
    %     % \item \textbf{numerical integration}: the variety of numerical methods used to estimate the value of a definite integral, including the midpoint rule, trapezoidal rule, and Simpson’s rule.
    %     % \item \textbf{relative error}: error as a percentage of the absolute value, given by 
    %     % \item \textbf{Simpson’s rule}: a rule that approximates \( \int_{a}^{b} f(x) \, dx \) using the integrals of a piecewise quadratic function. The approximation \( S_n \) to \( \int_{a}^{b} f(x) \, dx \) is given by 
    %     % \item \textbf{trapezoidal rule}: a rule that approximates \( \int_{a}^{b} f(x) \, dx \) using trapezoids.
    % \end{itemize}
    %
    \pagebreak \bigbreak \noindent 
    \subsection{Chapter 3 Key Equations}
    \bigbreak \noindent 
    \begin{itemize}
        \item \textbf{Integration by parts formula} 
            \begin{align*}
                \int u \, dv &= uv - \int v \, du 
            .\end{align*}
        \item \textbf{Integration by parts for definite integral}
            \begin{align*}
                \int_{a}^{b} u \, dv &= uv\big|_{a}^{b} - \int_{a}^{b} v \, du
            \end{align*}
        \item \textbf{To integrate products involving  sin(ax), sin(bx), cos(ax), and  cos(bx), use the substitutions:}
            \begin{itemize}
                \item \textbf{Sine Products}
                \begin{align*}
                    \sin(ax) \sin(bx) &= \frac{1}{2} \cos((a-b)x) - \frac{1}{2} \cos((a+b)x)
                \end{align*}

                \item \textbf{Sine and Cosine Products}
                \begin{align*}
                    \sin(ax) \cos(bx) &= \frac{1}{2} \sin((a-b)x) + \frac{1}{2} \sin((a+b)x)
                \end{align*}

                \item \textbf{Cosine Products}
                \begin{align*}
                    \cos(ax) \cos(bx) &= \frac{1}{2} \cos((a-b)x) + \frac{1}{2} \cos((a+b)x)
                \end{align*}

                \item \textbf{Power Reduction Formula (sine)}
                    \begin{align*}
                        &\int \sin^{n}{x}\ dx = -\frac{1}{n}\sin^{n-1}{x}\cos{x} + \frac{n-1}{n}\int \sin^{n-2}{x}\ dx \\
                        &\int_{0}^{\frac{\pi}{2}}\ \sin^{n}{x}\ dx = \frac{n-1}{n}\int_{0}^{\frac{\pi}{2}}\ \sin^{n-2}{x}\ dx
                    .\end{align*}
                \item \textbf{Power Reduction Formula (cosine)}
                    \begin{align*}
                        &\int \cos^{n}{x}\ dx = \frac{1}{n}\cos^{n-1}{x}\sin{x} + \frac{n-1}{n}\int \cos^{n-2}{x}\ dx \\
                        &\int_{0}^{\frac{\pi}{2}}\ \cos^{n}{x}\ dx = \frac{n-1}{n}\int_{0}^{\frac{\pi}{2}}\ \cos^{n-2}{x}\ dx
                    .\end{align*}
                \item \textbf{Power Reduction Formula (secant)}
                \begin{align*}
                    \int \sec^{n}{x}\ dx &= \frac{1}{n-1}\sec^{n-1}{x}\sin{x}+\frac{n-2}{n-1}\int \sec^{n-2}{x}\ dx \\
                    \int \sec^{n}{x}\ dx &= \frac{1}{n-1}\sec^{n-2}{x}\tan{x}+\frac{n-2}{n-1}\int \sec^{n-2}{x}\ dx
                \end{align*}

                \item \textbf{Power Reduction Formula (tangent)}
                \begin{align*}
                    \int \tan^n x \, dx &= \frac{1}{n-1} \tan^{n-1}x - \int \tan^{n-2}x \, dx
                \end{align*}
            \end{itemize}
            \item \textbf{Trigonometric Substitution}
            \begin{itemize}
                \item $\sqrt{a^{2} - x^{2}}$ use $x =a\sin{\theta }$ with domain restriction $\bigg[-\frac{\pi}{2},\frac{\pi}{2}\bigg] $
                \item $\sqrt{a^{2} + x^{2}}$ use $x=a\tan{\theta}$ with domain restriction $\left(-\frac{\pi}{2}, \frac{\pi}{2}\right)$
                \item $\sqrt{x^{2} - a^{2}}$ use $x =a\sec{\theta}$ with domain restriction $\bigg[0,\frac{\pi}{2}\bigg) \cup \bigg[\pi,\frac{3\pi}{2}\bigg)$ 
            \end{itemize}

        \item \textbf{Steps for fraction decomposition}
            \begin{enumerate}
                \item Ensure $deg(Q) < deg(P)$, if not, long divide
                \item Factor denominator
                \item Split up fraction into factors
                \item Multiply through to clear denominator
                \item Group terms and equalize
                \item Solve for constants
                \item Plug constants into split up fraction
                \item Compute integral
            \end{enumerate}

        \item \textbf{Solving for constants}
            Either:
            \begin{itemize}
                \item Plug in values (often the roots)
                \item Equalize 
            \end{itemize}
        \item \textbf{Cases for partial fractions}
            \begin{itemize}
                \item Non repeated linear factors
                \item Repeated linear factors
                \item Nonfactorable quadratic factors
            \end{itemize}
        \item \textbf{Midpoint rule}
            \begin{align*}
                M_{n} = \summation{n}{i=1}\ f(m_{i})\ \Delta x 
            .\end{align*}
        \item \textbf{Absolute error}
            \begin{align*}
                err = \bigg|\text{Actual} - \text{Estimated}\bigg|
            .\end{align*}
        \item \textbf{Relative error}
            \begin{align*}
                err = \bigg|\frac{\text{Actual} - \text{Estimated}}{\text{Actual}}\bigg| \cdot 100\%
            .\end{align*}
        \item \textbf{Error upper bound for midpoint rule}
        \begin{align*}
            E_{M} \leq \frac{M(b-a)^3}{24n^2}
        \end{align*}
        Where $M$ is the maximum value of the second derivative
        \item \textbf{Trapezoidal rule}
        \begin{align*}
            T_n \frac{1}{2} \Delta x \left( f(x_0) + 2f(x_1) + 2f(x_2) + \cdots + 2f(x_{n-1}) + f(x_n) \right)
        \end{align*}
        \item \textbf{Error upper bound for trapezoidal rule}
        \begin{align*}
            E_{T} \leq \frac{M(b-a)^3}{12n^2}
        \end{align*}
        Where $M$ is the maximum value of the second derivative
        \item \textbf{Simpson’s rule}
        \begin{align*}
            S_n = \frac{\Delta x}{3} \left( f(x_0) + 4f(x_1) + 2f(x_2) + 4f(x_3) + 2f(x_4) + 4f(x_5) + \cdots + 2f(x_{n-2}) + 4f(x_{n-1}) + f(x_n) \right)
        \end{align*}
        \item \textbf{Error upper bound for Simpson’s rule}
        \begin{align*}
            E_{S} \leq \frac{M(b-a)^5}{180n^4}
        \end{align*}
        Where $M$ is the maximum value of the fourth derivative
    \item \textbf{Finding $n$ with error bound functions}
        \begin{enumerate}
            \item Find $f^{\prime\prime}(x)$
            \item Find maximum values of $f^{\prime\prime}(x)$ in the interval
            \item Plug into error bound function 
            \item Set value $\leq$ desired accuracy (ex: 0.01)
            \item Solve: 
            \item If we were to truncate, we would use the ceil function $\ceil*{n}$ DO NOT FLOOR
        \end{enumerate}
    \item \textbf{Improper integrals (Infinite interval)}
        \begin{itemize}
            \item $\int_{a}^{+\infty}\ f(x)\ dx  = \lim\limits_{t \to +\infty}{\int_{a}^{t}\ f(x)\ dx}$  
            \item $\int_{-\infty}^{b}\ f(x)\ dx = \lim\limits_{t \to -\infty}{\int_{t}^{b}\ f(x)\ dx}$ 
            \item $\int_{-\infty}^{+\infty}\ f(x)\ dx = \int_{-\infty}^{0}\ f(x)\ dx + \int_{0}^{+\infty}\ f(x)\ dx$
        \end{itemize}
    \item \textbf{Improper integral (discontinuous)}
        \begin{itemize}
            \item Let $f(x)$ be continuous on $[a,b)$, then;
                \begin{align*}
                    \int_{a}^{b}\ f(x)\ dx = \lim\limits_{t \to b^{-}}{\int_{a}^{t}\ f(x)\ dx}\
                .\end{align*}
            \item Let $f(x)$ be continuous on $(a,b]$, then;
                \begin{align*}
                    \int_{a}^{b}\ f(x)\ dx = \lim\limits_{t \to b^{+}}{\int_{t}^{b}\ f(x)\ dx}\
                .\end{align*}
                In each case, if the limit exists, then the improper integral is said to converge. If the limit does not exist, then the improper integral is said to diverge.
            \item Let $f(x)$ be continuous on $[a,b]$ except at a point $c \in (a,b)$, then;
                \begin{align*}
                    \int_{a}^{b}\ f(x)\ dx = \int_{a}^{c}\ f(x)\ dx  +\int_{c}^{b}\ f(x)\ dx
                .\end{align*}
                If either integral diverges, then $\int_{a}^{b}\ f(x)\ dx $ diverges
        \end{itemize}
        \item \textbf{Comparison theorem}
        Let $f(x)$ and $g(x)$ be continuous over $[a,+\infty)$. Assume that $0 \leq f(x) \leq g(x)$ for $x \geq a$.
        \begin{itemize}
            \item If $\int_a^{+\infty} f(x) \, dx = \lim_{t \to +\infty} \int_a^t f(x) \, dx = +\infty$,  \\
                then $\int_a^{+\infty} g(x) \, dx = \lim_{t \to +\infty} \int_a^t g(x) \, dx = +\infty$.
            \item If $\int_a^{+\infty} g(x) \, dx = \lim_{t \to +\infty} \int_a^t g(x) \, dx = L$, where $L$ is a real number,  \\
                then $\int_a^{+\infty} f(x) \, dx = \lim_{t \to +\infty} \int_a^t f(x) \, dx = M$ for some real number $M \leq L$.
        \end{itemize}
        \pagebreak \bigbreak \noindent 
    \item \textbf{P-integrals}
        \begin{itemize}
    \item $\int_{0}^{+\infty} \frac{1}{x^{p}}\ dx =  
        \begin{cases}
            \frac{1}{p-1} & \text{if } p>1 \\
            +\infty & \text{if } p \leq 1
        \end{cases}$
    \item $\int_{0}^{1} \frac{1}{x^p}\ dx =    
        \begin{cases}
            \frac{1}{1-p} & \text{if } p<1 \\
            +\infty & \text{if } p \geq 1
        \end{cases}$
    \item $\int_{a}^{+\infty} \frac{1}{x^{p}}\ dx =  
        \begin{cases}
            \frac{a^{1-p}}{p-1} & \text{if } p>1 \\
            +\infty & \text{if } p \leq 1
        \end{cases}$
    \item $\int_{0}^{a} \frac{1}{x^p}\ dx =    
        \begin{cases}
            \frac{a^{1-p}}{1-p} & \text{if } p<1 \\
            +\infty & \text{if } p \geq 1
        \end{cases}$
\end{itemize}
    \item \textbf{Bypass L'Hospital's Rule}
        \begin{align*}
            \ln{(\ln{(x)})},\ \ln{(x)},\ \cdots\ x^{\frac{1}{100}},\ x^{\frac{1}{3}},\ \sqrt{x},\ 1,\ x^{2},\ x^{3},\ \cdots\ e^{x},\ e^{2x},\ e^{3x},\ \cdots,\ e^{x^{2}},\ \cdots\ e^{e^{x}}
        .\end{align*}
        Essentially what it means is things on the right grow faster than things on the left. Thus, if we have say:
        \begin{align*}
            \lim\limits_{x \to \infty}{\frac{x^{2}}{e^{2x}}} 
        .\end{align*}
        We can be sure that it is zero. Because this is $x^{2}\cdot e^{-2x}$. If we take  $ \lim\limits_{x \to \infty}{x^{2}e^{-2x}}$, we get $\infty \cdot 0$. As we see by the sequence $e^{-2x}$ overrules $x^{2}$ and we can say the limit is zero.
    % \item \textbf{something to consider for limits}: Suppose we have $f:\ A \rightarrow B:\ x \mapsto f(x)$. It would not be meaninful to consider some $ \lim\limits_{x \to b+n}{f(x)}$ for $n>0 $. Thus we shall conclude that the limit is undefined. For example, the domain of arcsine is $[-1,1]$, thus any $\lim\limits_{x \to a}{f(x)}$ for $(-\infty,-1)\cup (1,\infty)$ would be undefined
    \item \textbf{Consideration for Limits}: Let \(f: A \rightarrow B\) be a function defined by \(x \mapsto f(x)\). If a point \(c\) lies outside the domain \(A\), then the expression \(\lim\limits_{x \to c} f(x)\) is not meaningful, and we classify this limit as undefined. For instance, the function arcsine has a domain of \([-1,1]\). Therefore, limits like \(\lim\limits_{x \to a} \sin^{-1}(x)\) where \(a \notin [-1,1]\) are undefined.
    \item \textbf{Why does}
        \begin{align*}
            &\lim\limits_{x \to 2}{\tan^{-1}{\frac{1}{x-2}}} 
        .\end{align*}
        \begin{minipage}[]{0.47\textwidth}
            \begin{align*}
                &=\lim\limits_{x \to 2^{-}}{\tan^{-1}{\frac{1}{x-2}}} \\
                &= \lim\limits_{x \to -\infty}{\tan^{-1}{x}} \\
                &= -\pi/2
            .\end{align*}
        \end{minipage}
        \begin{minipage}[]{0.47\textwidth}
            \begin{align*}
                &=\lim\limits_{x \to 2^{+}}{\tan^{-1}{\frac{1}{x-2}}} \\
                &=\lim\limits_{x \to +\infty}{\tan^{-1}{x}} \\
                &=\frac{\pi}{2}
            .\end{align*}
        \end{minipage}
    \end{itemize}

    % \pagebreak \bigbreak \noindent 
    % \subsection{Chapter 5 Key Terms}
    % \bigbreak \noindent 
    % \begin{itemize}
    % 
    % \item Alternating series: 
    % \[
    % \text{A series of the form } \sum_{n=1}^{\infty} (-1)^{n+1} b_n \text{ or } \sum_{n=1}^{\infty} (-1)^n b_n, \text{ where } b_n \geq 0, \text{ is called an alternating series.}
    % \]
    % 
    % \item Alternating series test: 
    % \[
    % \text{For an alternating series of either form, if } b_{n+1} \leq b_n \text{ for all integers } n \geq 1 \text{ and } b_n \to 0, \text{ then an alternating series converges.}
    % \]
    % 
    % \item Arithmetic sequence: 
    % \[
    % \text{A sequence in which the difference between every pair of consecutive terms is the same is called an arithmetic sequence.}
    % \]
    % 
    % \item Bounded above: 
    % \[
    % \text{A sequence } \{a_n\} \text{ is bounded above if there exists a constant } M \text{ such that } a_n \leq M \text{ for all positive integers } n.
    % \]
    % 
    % \item Bounded below: 
    % \[
    % \text{A sequence } \{a_n\} \text{ is bounded below if there exists a constant } M \text{ such that } M \leq a_n \text{ for all positive integers } n.
    % \]
    % 
    % \item Bounded sequence: 
    % \[
    % \text{A sequence } \{a_n\} \text{ is bounded if there exists a constant } M \text{ such that } |a_n| \leq M \text{ for all positive integers } n.
    % \]
    % 
    % 
    % \item Convergence of a series: 
    % \[
    % \text{A series converges if the sequence of partial sums for that series converges.}
    % \]
    % 
    % \item Convergent sequence: 
    % \[
    % \text{A convergent sequence is a sequence } \{a_n\} \text{ for which there exists a real number } L \text{ such that } a_n \text{ is arbitrarily close to } L \text{ as long as } n \text{ is sufficiently large.}
    % \]
    % 
    % \item Divergence of a series: 
    % \[
    % \text{A series diverges if the sequence of partial sums for that series diverges.}
    % \]
    % 
    % \item Divergence test: 
    % \[
    % \text{If } \lim_{n \to \infty} a_n \neq 0, \text{ then the series } \sum_{n=1}^{\infty} a_n \text{ diverges.}
    % \]
    % 
    % \item Divergent sequence: 
    % \[
    % \text{A sequence that is not convergent is divergent.}
    % \]
    % 
    % \item Explicit formula: 
    % \[
    % \text{A sequence may be defined by an explicit formula such that } a_n = f(n).
    % \]
    % 
    % \item Geometric sequence: 
    % \[
    % \text{A sequence } \{a_n\} \text{ in which the ratio } \frac{a_{n+1}}{a_n} \text{ is the same for all positive integers } n \text{ is called a geometric sequence.}
    % \]
    % 
    % \item Geometric series: 
    % \[
    % \text{A geometric series is a series that can be written in the form } \sum_{n=1}^{\infty} ar^{n-1} = a + ar + ar^2 + ar^3 + \cdots.
    % \]
    % 
    % \item Harmonic series: 
    % \[
    % \text{The harmonic series takes the form } \sum_{n=1}^{\infty} \frac{1}{n} = 1 + \frac{1}{2} + \frac{1}{3} + \cdots.
    % \]
    % 
    % \item Index variable: 
    % \[
    % \text{The subscript used to define the terms in a sequence is called the index.}
    % \]
    % 
    % \item Infinite series: 
    % \[
    % \text{An infinite series is an expression of the form } a_1 + a_2 + a_3 + \cdots = \sum_{n=1}^{\infty} a_n.
    % \]
    % 
    % \item Integral test: 
    % \[
    % \text{For a series } \sum_{n=1}^{\infty} a_n \text{ with positive terms } a_n, \text{ if there exists a continuous, decreasing function } f \text{ such that } f(n) = a_n \text{ for all positive integers } n, \text{ then } \sum_{n=1}^{\infty} a_n \text{ and } \int_{1}^{\infty} f(x) \, dx \text{ either both converge or both diverge.}
    % \]
    % 
    % \item Limit comparison test: 
    % \[
    % \text{Suppose } a_n, b_n \geq 0 \text{ for all } n \geq 1. \text{ If } \
    % \]
    % \end{itemize}

    \pagebreak \bigbreak \noindent 
    \subsection{Chapter 5 Key Equations}
    \bigbreak \noindent 
    \begin{itemize}
    \item \textbf{Sequence notation}
        \begin{align*}
            \{a_{n}\}_{n=1}^{\infty},\ \text{or simply } \{a_{n}\}
        .\end{align*}
    \item \textbf{Sequence notation (ordered list)}
        \begin{align*}
            a_{1},\ a_{2},\ a_{3},\ \cdots,\ a_{n},\ \cdots
        .\end{align*}
    \item \textbf{Arithemetic Sequence Difference}
        \begin{align*}
            d = a_{n} - a_{n-1}
        .\end{align*}
    \item \textbf{Arithmetic sequence (common difference between subsequent terms) general form}
        \begin{align*}
            &\text{Index starting at 0}:\ a_{n} = a + nd \\
            &\text{Index starting at 1}:\ a_{n} = a + (n-1)d \\
        .\end{align*}
    \item \textbf{Arithmetic sequence (common difference between subsequent terms) recursive form}
        \begin{align*}
            a_{n} = a_{n-1} + d
        .\end{align*}
    \item \textbf{Sum of arithmetic sequence}
        \begin{align*}
            &S_{n} = \frac{n}{2}\left[a + a_{n}\right] \\
            &S_{n} = \frac{n}{2}\left[2a + (n-1)d\right]
        .\end{align*}
    \item \textbf{Geometric sequence form common ratio}
        \begin{align*}
            r = \frac{a_{n}}{a_{n-1}}
        .\end{align*}
    \item \textbf{Geometric sequence general form}
        \begin{align*}
            &a_{n} = ar^{n}\ \text{(Index starting at 0)} \\
            &a_{n} = a^{n+1} \text{(index starting at 0 and a=r)} \\
            &a_{n} = ar^{n-1}\ \text{(Index starting at 1)} \\
            &a_{n} = a^{n} \text{(index starting at 1 and a=r)}
        .\end{align*}
    \item \textbf{Geometric sequence recursive form}
        \begin{align*}
            &a_{n} = ra_{n-1}
        .\end{align*}
    \item \textbf{Sum of geometric sequence (finite terms)}
        \begin{align*}
            S_{n} = \frac{a(1-r^{n})}{1-r}\ \quad r\ne 1
        .\end{align*}
    \item \textbf{Convergence / Divergence}: If 
        \begin{align*}
            \lim\limits_{n \to +\infty}{a_{n}} = L
        .\end{align*}
        We say that the sequence converges, else it diverges
    \item \textbf{Formal definition of limit of sequence}
        \begin{align*}
            \lim\limits_{n \to +\infty}{a_{n}= L} \iff \forall \varepsilon > 0, \exists N \in \mathbb{Z} \mid \abs{a_{n} - L} < \varepsilon,\ \text{if } n \geq n
        .\end{align*}
        Then we can say 
        \begin{align*}
            \lim\limits_{n \to +\infty}{a_{n} = L}\ \text{or } a_{n} \rightarrow L 
        .\end{align*}
    \item \textbf{Limit of a sequence defined by a function}:         Consider a sequence \( \{a_n\} \) such that \( a_n = f(n) \) for all \( n \geq 1 \). If there exists a real number \( L \) such that
        \[
        \lim_{{x \to \infty}} f(x) = L,
        \]
        then \( \{a_n\} \) converges and
        \[
        \lim_{{n \to \infty}} a_n = L.
        \]
    \item \textbf{Algebraic limit laws}:
          Given sequences \( \{a_n\} \) and \( \{b_n\} \) and any real number \( c \), if there exist constants \( A \) and \( B \) such that \( \lim_{{n \to \infty}} a_n = A \) and \( \lim_{{n \to \infty}} b_n = B \), then
          \begin{itemize}
            \item \( \lim_{{n \to \infty}} c = c \)
            \item \( \lim_{{n \to \infty}} c a_n = c \lim_{{n \to \infty}} a_n = cA \)
            \item \( \lim_{{n \to \infty}} (a_n \pm b_n) = \lim_{{n \to \infty}} a_n \pm \lim_{{n \to \infty}} b_n = A \pm B \)
            \item \( \lim_{{n \to \infty}} (a_n \cdot b_n) = (\lim_{{n \to \infty}} a_n) \cdot (\lim_{{n \to \infty}} b_n) = A \cdot B \)
            \item \( \lim_{{n \to \infty}} \frac{a_n}{b_n} = \frac{\lim_{{n \to \infty}} a_n}{\lim_{{n \to \infty}} b_n} = \frac{A}{B} \), provided \( B \neq 0 \) and each \( b_n \neq 0 \).
        \end{itemize}
    \item \textbf{Continuous Functions Defined on Convergent Sequences}:
          Consider a sequence \( \{a_n\} \) and suppose there exists a real number \( L \) such that the sequence \( \{a_n\} \) converges to \( L \). Suppose \( f \) is a continuous function at \( L \). Then there exists an integer \( N \) such that \( f \) is defined at all values \( a_n \) for \( n \geq N \), and the sequence \( \{f(a_n)\} \) converges to \( f(L) \).
    \item \textbf{Squeeze Theorem for Sequences}:           Consider sequences \( \{a_n\} \), \( \{b_n\} \), and \( \{c_n\} \). Suppose there exists an integer \( N \) such that
        \[ a_n \leq b_n \leq c_n \text{ for all } n \geq N. \]
        If there exists a real number \( L \) such that
        \[ \lim_{{n \to \infty}} a_n = L = \lim_{{n \to \infty}} c_n, \]
        then \( \{b_n\} \) converges and \( \lim_{{n \to \infty}} b_n = L \)
    \item \textbf{Bounded above}:           A sequence \( \{a_n\} \) is bounded above if there exists a real number \( M \) such that
        \[ a_n \leq M \]
        for all positive integers \( n \).
    \item \textbf{Bounded below}:
        A sequence \( \{a_n\} \) is bounded below if there exists a real number \( M \) such that
        \[ M \leq a_n \]
        for all positive integers \( n \).
    \item \textbf{Bounded}:
        A sequence \( \{a_n\} \) is a bounded sequence if it is bounded above and bounded below. 
    \item \textbf{Unbounded}:
        If a sequence is not bounded, it is an unbounded sequence.
    \item \textbf{If a sequence  $\{a_{n}\} $ converges, then it is bounded.}
    \item \textbf{Increasing sequence}: A sequence \( \{a_n\} \) is increasing for all \( n \geq n_0 \) if
        \[ a_n \leq a_{n+1} \text{ for all } n \geq n_0. \]
    \item \textbf{Decreasing sequence}: A sequence \( \{a_n\} \) is decreasing for all \( n \geq n_0 \) if
        \[ a_n \geq a_{n+1} \text{ for all } n \geq n_0. \]
    \item \textbf{Monotone sequence}: A sequence \( \{a_n\} \) is a \textbf{monotone sequence} for all \( n \geq n_0 \) if it is increasing for all \( n \geq n_0 \) or decreasing for all \( n \geq n_0 \)
    \item \textbf{Monotone Convergence Theorem}:         If \( \{a_n\} \) is a bounded sequence and there exists a positive integer \( n_0 \) such that \( \{a_n\} \) is monotone for all \( n \geq n_0 \), then \( \{a_n\} \) converges.
    \item \textbf{Infinite Series form:}
        \begin{align*}
            \sum_{n=1}^{\infty} a_n = a_1 + a_2 + a_3 + \cdots.
        .\end{align*}
    \item \textbf{Partial sum ($k^{th}$ partial sum)}
        \begin{align*}
            S_k = \sum_{n=1}^{k} a_n = a_1 + a_2 + a_3 + \cdots + a_k
        .\end{align*}
    \item \textbf{Convergence of infinity series notation}
        \bigbreak \noindent 
        For a series, say...
        \begin{align*}
            \summation{\infty}{n=1}\ a_{n}\ 
        .\end{align*}
        its convergence is determined by the limit of its sequence of partial sums. Specifically, if
        \begin{align*}
            \lim\limits_{n \to +\infty}{S_{n}} = S \rightarrow \summation{\infty}{n=1}\ a_{n}\ = S 
        .\end{align*}
    \item \textbf{Harmonic series}
        \begin{align*}
            \summation{\infty}{n=1}\ \frac{1}{n}  =  \frac{1}{2} + \frac{1}{3} + \frac{1}{4} + \ldots\ 
        .\end{align*}
        Which diverges to $+\infty$
    \item \textbf{Algebraic Properties of Convergent Series}
        Let $ \sum_{n=1}^{\infty} a_n$ and $\sum_{n=1}^{\infty} b_n$ be convergent series. Then the following algebraic properties hold:
        \begin{enumerate}
            \item The series 
            $\sum_{n=1}^{\infty} (a_n + b_n)$ converges and 
            \begin{align*}
                \sum_{n=1}^{\infty} (a_n + b_n) = \sum_{n=1}^{\infty} a_n + \sum_{n=1}^{\infty} b_n. \quad \text{(Sum Rule)}
            .\end{align*}
            \item The series $\sum_{n=1}^{\infty} (a_n - b_n)$ converges and 
                \begin{align*}
                    \sum_{n=1}^{\infty} (a_n - b_n) = \sum_{n=1}^{\infty} a_n - \sum_{n=1}^{\infty} b_n. \quad \text{(Difference Rule)}
                .\end{align*}
            \item For any real number \( c \), the series $\sum_{n=1}^{\infty} c a_n$ converges and 
                \begin{align*}
                    \sum_{n=1}^{\infty} c a_n = c \sum_{n=1}^{\infty} a_n. \quad \text{(Constant Multiple Rule)}
                .\end{align*}
        \end{enumerate}

    \item \textbf{Geometric series convergence or divergence: }
    \begin{align*}
       \summation{\infty}{n=1}\ ar^{n-1} \  = \quad \quad 
        \begin{cases}
             \frac{a}{1-r} & \text{if }  \abs{r} < 1\\
             diverges & \text{if }  \abs{r} \geq 1
        \end{cases}
    .\end{align*}

    \item \textbf{Divergence test}: In the context of sequences, if $\lim_{{n \to \infty}} a_n = c \neq 0$ or the limit does not exist, then the series $\sum_{{n=1}}^{\infty} a_n$ is said to diverge. The converse is not true.
        \bigbreak \noindent 
        Because:
        \begin{align*}
                \lim_{k \to \infty} a_k = \lim_{k \to \infty} (S_k - S_{k-1}) = \lim_{k \to \infty} S_k - \lim_{k \to \infty} S_{k-1} = S - S = 0.
        .\end{align*}
    \item \textbf{Integral Test Prelude}:
        for any integer $k$, the $k$th partial sum $S_k$ satisfies
        \begin{align*}
                S_k = a_1 + a_2 + a_3 + \cdots + a_k < a_1 + \int_{1}^{k} f(x) \, dx < a_1 + \int_{1}^{\infty} f(x) \, dx.
        .\end{align*}
        and
        \begin{align*}
                S_k = a_1 + a_2 + a_3 + \cdots + a_k > \int_{1}^{k+1} f(x) \, dx.
        .\end{align*}

    \item \textbf{Intgeral test}
        Suppose  $\summation{\infty}{n=1}\ a_{n}\  $ is a series with positive terms  $a_{n}$ Suppose there exists a function  $f $
        and a positive integer  $N$ 
      such that the following three conditions are satisfied:
        \begin{enumerate}
            \item \( f \) positive, continuous, and decreasing on $[N,\infty)$
            \item \( f(n) = a_n \) for all integers \( n \geq N \), $N \in \mathbb{Z^{+}} $
        \end{enumerate}
        \begin{align*}
            \text{Then the series} \sum_{n=1}^{\infty} a_n \text{ and the improper integral} \int_{N}^{\infty} f(x) \, dx \text{ either both converge or both diverge.}
        .\end{align*}

    \item \textbf{P-series}
       $\forall p \in \mathbb{R}$, the series 
       \begin{align*}
           \summation{\infty}{n=1}\ \frac{1}{n^{P}}\ 
       .\end{align*}
       Is called a \textbf{p-series}. Furthermore, 
       \begin{align*}
           \sum_{n=1}^{\infty} \frac{1}{n^p} \begin{cases}
        \text{converges if } p>1 \\
        \text{diverges if } p \leq 1.
        \end{cases}
       .\end{align*}

    \item \textbf{P-series extended}
        \begin{align*}
            \summation{\infty}{n=2}\ \frac{1}{n\ln{(n)}^{p}}\ 
            \begin{cases}
        \text{converges if } p>1 \\
        \text{diverges if } p \leq 1.
        \end{cases}
        .\end{align*}
    \item \textbf{Remainder estimate for the integral test}
                Suppose \( \sum_{n=1}^{\infty} a_n \)
        is a convergent series with positive terms. Suppose there exists a function \( f \) and a positive integer $M$
        satisfying the following three conditions:
        \begin{enumerate}
            \item \( f \) is positive, decreasing, and continuous on $[M,\infty)$
            \item \( f(n) = a_n \) for all integers \( n \geq M \).
        \end{enumerate}
        Let \( S_N \) be the \( N \)th partial sum of \( \sum_{n=1}^{\infty} a_n \).
        For all positive integers \( N \),
        \[
        S_N + \int_{N+1}^{\infty} f(x) \, dx < \sum_{n=1}^{\infty} a_n < S_N + \int_{N}^{\infty} f(x) \, dx.
        \]
        In other words, the remainder \( R_N = \sum_{n=1}^{\infty} a_n - S_N = \sum_{n=N+1}^{\infty} a_n \)
        satisfies the following estimate:
        \[
        \int_{N+1}^{\infty} f(x) \, dx < R_N < \int_{N}^{\infty} f(x) \, dx.
        \]
        This is known as the remainder estimate 
        \bigbreak \noindent 
        To find a value of $N$ such that we are withing a desired margin of error, Since we know $R_{n} < \int_{N}^{\infty}\ f(x)\ dx $. Simply compute the improper integral and set the result < the desired error to solve for $N$
    \item \textbf{Find $a_{n}$ given the expression for the partial sum}
        \begin{align*}
            a_{n} = S_{n} - S_{n-1}
        .\end{align*}
    \item \textbf{telescoping series}: Telescoping series are a type of series where each term cancels out a part of another term, leaving only a few terms that do not cancel. When you sum the series, most of the terms collapse or "telescope," which simplifies the calculation of the sum. Here are some key points and generalizations you can note about telescoping series:
        \begin{itemize}
            \item Partial Fraction Decomposition
            \item Cancellation Pattern: In a telescoping series, look for a pattern where a term in one fraction will cancel out with a term in another fraction.
            \item Write out Terms
            \item What is left is $S_{n}$, thus the sum of the series is the $\lim\limits_{n \to \infty}{S_{n}} $
        \end{itemize}
        Try: 
        \begin{align*}
            \summation{\infty}{n=2}\ \frac{1}{n^{2}-1}\ 
        .\end{align*}
        Hint, its not only the first and last terms cancel, we also have a $\frac{\frac{1}{2}}{n}$, when $a_{n-1}$: Answer is $\frac{3}{4}$
        \pagebreak 
    \item \textbf{Comparison test for series}
        \begin{enumerate}
            \item Suppose there exists an integer \( N \) such that \( 0 \leq a_n \leq b_n \) for all \( n \geq N \). If \( \sum_{n=1}^{\infty} b_n \) converges, then \( \sum_{n=1}^{\infty} a_n \) converges. 
            \item  Suppose there exists an integer \( N \) such that \( a_n \geq b_n \geq 0 \) for all \( n \geq N \). If \( \sum_{n=1}^{\infty} b_n \) diverges, then \( \sum_{n=1}^{\infty} a_n \) diverges.
        \end{enumerate}
    \item \textbf{Limit Comparison Test}
         Let \( a_n, b_n \geq 0 \) for all \( n \geq 1 \).
        \begin{itemize}
          \item If \( \lim_{n \to \infty} \frac{a_n}{b_n} = L \neq 0 \), then \( \sum_{n=1}^{\infty} a_n \) and \( \sum_{n=1}^{\infty} b_n \) both converge or both diverge.
          \item If \( \lim_{n \to \infty} \frac{a_n}{b_n} = 0 \) and \( \sum_{n=1}^{\infty} b_n \) converges, then \( \sum_{n=1}^{\infty} a_n \) converges.
          \item If \( \lim_{n \to \infty} \frac{a_n}{b_n} = \infty \) and \( \sum_{n=1}^{\infty} b_n \) diverges, then \( \sum_{n=1}^{\infty} a_n \) diverges.
        \end{itemize}
    \textbf{Note:} Note that if $\frac{a_n}{b_n} \to 0$ and $\sum_{n=1}^{\infty} b_n$ diverges, the limit comparison test gives no information. Similarly, if $\frac{a_n}{b_n} \to \infty$ and $\sum_{n=1}^{\infty} b_n$ converges, the test also provides no information. 
    \bigbreak \noindent 
    Consider the series 
    \begin{align*}
        \summation{\infty}{n=1}\ \frac{n^{4} + 6}{n^{5} + 4}\ 
    .\end{align*}
    To find our $b_{n}$ we can only focus on the leading coefficients. Thus: 
    \begin{align*}
        b_{n} = \frac{n^{4}}{n^{5}} = \frac{1}{n}
    .\end{align*}
    So our test...
    \smallbreak \noindent
    \begin{minipage}[t]{0.47\textwidth}
    \begin{align*}
        &\lim\limits_{n \to \infty}{\frac{a_{n}}{b_{n}}} = \frac{\frac{n^{4} + 6}{n^{5} + 4}}{\frac{1}{n}} \\
        &=\lim\limits_{n \to \infty}{\frac{n(n^{4}+6)}{n^{5} + 4}} \\
        &=\lim\limits_{n \to \infty}{\frac{n^{5}+6n}{n^{5} + 4}} \\
        &=1
    .\end{align*}
    \end{minipage}
    \begin{minipage}[t]{0.47\textwidth}
        Since $\lim\limits_{n \to \infty}{\frac{a_{n}}{b_{n}}} \ne 0 \lor +\infty$. And $\frac{1}{n}$ diverges, we can conclude that $a_{n}$ will also diverge.
    \end{minipage}

    \item \textbf{Determine which series (or function) is greater}

        \begin{itemize}
            \item \textbf{Subtraction}: Given two functions $f(x) = \frac{1}{x}$ and $g(x) = \frac{x^4 + 6}{x^5 + 4}$, we want to compare them by considering the function $h(x) = f(x) - g(x)$:

\[
h(x) = f(x) - g(x) = \frac{1}{x} - \frac{x^4 + 6}{x^5 + 4}
\]

To compare these directly, it would be helpful to have a common denominator:

\[
h(x) = \frac{x^4 + 4 - (x^4 + 6)}{x(x^5 + 4)} = \frac{-2}{x(x^5 + 4)}
\]

Now, we can see that the sign of $h(x)$ depends on the sign of $x$ because the denominator $x(x^5 + 4)$ is always positive for $x \neq 0$. So:

\begin{itemize}
  \item For $x > 0$, $h(x) < 0$, which means $f(x) < g(x)$.
  \item For $x < 0$, $h(x) > 0$, which means $f(x) > g(x)$.
\end{itemize}
    \end{itemize}

    \item \textbf{Alternating Series}
      Any series whose terms alternate between positive and negative values is called an alternating series. An alternating series can be written in the form 
      \begin{align*}
          \sum_{n=1}^{\infty} (-1)^{n+1} b_n = b_1 - b_2 + b_3 - b_4 + \cdots
      .\end{align*}
      or
      \begin{align*}
          \sum_{n=1}^{\infty} (-1)^n b_n = -b_1 + b_2 - b_3 + b_4 - \cdots
      .\end{align*}
      Where  $b_n > 0$  for all positive integers $n$.
  \item \textbf{alternating series test (Leibniz criterion)}
         An alternating series of the form
        \[
        \sum_{n=1}^{\infty} (-1)^{n+1} b_n \quad \text{or} \quad \sum_{n=1}^{\infty} (-1)^n b_n
        \]
        converges if
        \begin{itemize}
            
            \item $0 < b_{n+1} \leq b_n\ \forall\ n \geq 1$
            \item $\lim_{n \to \infty} b_n = 0.$
    \end{itemize}
    \textbf{Note:} We remark that this theorem is true more generally as long as there exists some integer \( N \) such that \( 0 < b_{n+1} \leq b_n \) for all \( n \geq N \).
    \bigbreak \noindent 
    \textbf{Additional note:} The AST allows us to consider just the positive terms to check for these two conditions because if a series of decreasing positive terms that approach zero is alternated in sign, the alternating series will converge. This is a special property of alternating series that does not generally hold for non-alternating series.

    \item \textbf{Show decreasing (For the AST)}: Consider the series
        \begin{align*}
            \summation{\infty}{n=1}\ \frac{(-1)^{n+1}}{n^{2}}\ 
        .\end{align*}
        \bigbreak \noindent 
        So you see we have $b_{n}=  \frac{1}{n^{2}}$. For the AST, we must show that this is decreasing. If $b_{n+1} = \frac{1}{(n+1)^{2}}$. Then we see
        \begin{align*}
           \frac{1}{(n+1)^{2}} < \frac{1}{n^{2}}
        .\end{align*}
        \bigbreak \noindent 
        Thus it is decreasing for $n \geq 1$ ($b_{n+1} < b_{n}$)
        \blacksquare
    \item \textbf{Remainders in alternating series}
        Consider an alternating series of the form
        \[
        \sum_{n=1}^{\infty} (-1)^{n+1} b_n \quad \text{or} \quad \sum_{n=1}^{\infty} (-1)^n b_n,
        \]
        that satisfies the hypotheses of the alternating series test. Let \( S \) denote the sum of the series and \( S_N \) denote the \( N \)-th partial sum. For any integer \( N \geq 1 \), the remainder \( R_N = S - S_N \) satisfies
        \[
        \lvert R_N \rvert \leq b_{N+1}.
        \]
        This tells us that if we stop at the $N^{th}$ term, the error we are making is at most the size of the next term
    \item \textbf{Absolute and conditional convergence}
        \begin{itemize}
            \item A series \(\sum_{n=1}^{\infty} a_n\) exhibits absolute convergence if \(\sum_{n=1}^{\infty} |a_n|\) converges.
            \item A series \(\sum_{n=1}^{\infty} a_n\) exhibits conditional convergence if \(\sum_{n=1}^{\infty} a_n\) converges but \(\sum_{n=1}^{\infty} |a_n|\) diverges.
            \item If $\summation{\infty}{n=1}\ \lvert a_{n} \rvert\ $ converges then $\summation{\infty}{n=1}\ a_{n}\ $ converges
        \end{itemize}
        \bigbreak \noindent 
        \textbf{Note:} if $\abs{a_{n}}$ diverges, we cannot have absolute convergence, thus we must examine to see if normal $a_{n}$ converges, in which case we would have conditional convergence
        \bigbreak \noindent 
        \textbf{Big Note:} If a series not strictly decreasing, we can still check for absolute/conditional convergence. Take $\summation{\infty}{n=1}\ \frac{\sin{n}}{3^{n} + 4}\  $ for example.
    \item \textbf{Ratio test}
       Let \(\sum_{n=1}^{\infty} a_n\) be a series with nonzero terms. Let
       \begin{align*}
           \rho = \lim_{n \to \infty} \left| \frac{a_{n+1}}{a_n} \right|.
       .\end{align*}
        Then:
        \begin{enumerate}[label=\roman*.]
            \item If \(0 \leq \rho < 1\), then \(\sum_{n=1}^{\infty} a_n\) converges absolutely.
            \item If \(\rho > 1\) or \(\rho = \infty\), then \(\sum_{n=1}^{\infty} a_n\) diverges.
            \item If \(\rho = 1\), the test does not provide any information.
        \end{enumerate}
        \bigbreak \noindent 
        \textbf{Note:} The ratio test is useful for series whose terms involve factorials
    \item \textbf{Root test}
        Consider the series \(\sum_{n=1}^{\infty} a_n\). Let
        \begin{align*}
            \rho = \lim_{n \to \infty} \sqrt[n]{|a_n|}.
        .\end{align*}
        \begin{enumerate}[label=\roman*.]
            \item If \(0 \leq \rho < 1\), then \(\sum_{n=1}^{\infty} a_n\) converges absolutely. 
            \item If \(\rho > 1\) or \(\rho = \infty\), then \(\sum_{n=1}^{\infty} a_n\) diverges. 
            \item If \(\rho = 1\), the test does not provide any information.
        \end{enumerate}
        \bigbreak \noindent 
        \textbf{Note:} The root test is useful for series whose terms involve exponentials
    \item \textbf{Which tests require positive terms}
        \begin{itemize}
            \item The \textbf{Integral Test:} This test applies to series where the terms come from a function that is positive, continuous, and decreasing on a certain interval. The convergence or divergence of the series is determined by the convergence or divergence of the corresponding improper integral of the function.
            \item The \textbf{Remainder estimate for the integral test}
            \item The \textbf{Comparison Test:} This test compares the terms of a series to those of another series with known convergence behavior. It requires that the terms of both series be positive or non-negative.
            \item The \textbf{Limit Comparison Test:} Similar to the Comparison Test, this test involves comparing the terms of two series by taking the limit of the ratio of their terms. It requires that the terms of both series be positive.
            \item In \textbf{alternating series}, $b_{n}$ must have only positive terms
        \end{itemize}
    \end{itemize}

    \pagebreak \bigbreak \noindent 
    \subsection{Chapter 6 Key equations}
    \begin{itemize}
        \item \textbf{Euler definition for $e$}
            \begin{align*}
                &e^{a} = \lim\limits_{n \to \infty}{\left(1+\frac{a}{n}\right)^{n}} \\
                &\frac{1}{e^{a}} = \lim\limits_{n \to \infty}{\left(1+\frac{-a}{n}\right)^{n}} \\ 
                &\frac{1}{e^{a}} = \lim\limits_{n \to \infty}{\left(\frac{n}{n+a}\right)^{n}}
            .\end{align*}
        \item \textbf{Other definition for $e $}
            \begin{align*}
                &e = \summation{\infty}{n=0}\ \frac{1}{n!}\  \\
                &e-1 = \summation{\infty}{n=1}\ \frac{1}{n!}\ \\
                &e^{x} = \summation{\infty}{n=0}\ \frac{x^{n}}{n!}\ 
            .\end{align*}
        \item \textbf{Power series}:
            A series of the form
        \begin{align*}
            \sum_{n=0}^{\infty} c_n x^n &= c_0 + c_1 x + c_2 x^2 + \cdots 
        .\end{align*}
        is a power series centered at \( x = 0 \).
        \bigbreak \noindent 
        A series of the form
        \begin{align*}
            \sum_{n=0}^{\infty} c_n (x - a)^n &= c_0 + c_1 (x - a) + c_2 (x - a)^2 + \cdots 
        .\end{align*}
        is a power series centered at \( x = a \).
            
            \item \textbf{Convergence of a Power Series}
                \bigbreak \noindent 
                Consider the power series \(\sum_{n=0}^{\infty} c_n (x - a)^n\). The series satisfies exactly one of the following properties:
                \begin{enumerate}[label=(\roman*)]
                    \item The series converges at \( x = a \) and diverges for all \( x \neq a \).
                    \item The series converges for all real numbers \( x \).
                    \item There exists a real number \( R > 0 \) such that the series converges if \( |x - a| < R \) and diverges if \( |x - a| > R \). At the values \( x \) where \( |x - a| = R \), the series may converge or diverge.
                \end{enumerate}
            \item \textbf{A power series always converges at its center}
            \item \textbf{Radius of convergence}:         Consider the power series \(\sum_{n=0}^{\infty} c_n (x - a)^n\). The set of real numbers \( x \) where the series converges is the interval of convergence. If there exists a real number \( R > 0 \) such that the series converges for \( |x - a| < R \) and diverges for \( |x - a| > R \), then \( R \) is the radius of convergence. If the series converges only at \( x = a \), we say the radius of convergence is \( R = 0 \). If the series converges for all real numbers \( x \), we say the radius of convergence is \( R = \infty \) (Figure 6.2).
            \item \textbf{Finding interval of convergence and radius of convergence}
                \begin{itemize}
                    \item Fact: power series is always convergent on its center
                    \item Use ratio test (values of $\rho$) 
                    \item Use $\rho < 1$ to find Radius of convergence
                    \item Test end points of interval by plugging into original series and seeing whether the series is convergent or divergent
                \end{itemize}
            \item \textbf{If $\rho = 0$, the power series converges for all $x$}
            \item \textbf{If $\rho = \infty$}, the series diverges for all $x \neq a$ 
                \pagebreak 
            \item \textbf{Combining Power Series:}
                    Suppose that the two power series \(\sum_{n=0}^{\infty} c_n x^n\) and \(\sum_{n=0}^{\infty} d_n x^n\) converge to the functions \(f\) and \(g\), respectively, on a common interval \(I\).
                \begin{enumerate}[label=(\roman*)]
                    \item The power series \(\sum_{n=0}^{\infty} (c_n x^n \pm d_n x^n)\) converges to \(f \pm g\) on \(I\).
                    \item For any integer \(m \geq 0\) and any real number \(b\), the power series \(\sum_{n=0}^{\infty} b x^m c_n x^n\) converges to \(b x^m f(x)\) on \(I\).
                        \smallbreak \noindent
                        Eg: If we know $\summation{\infty}{n=0}\ a_{n}x_{n}\  $ has $I = (-1,1) $. Then
                        \begin{align*}
                            &\summation{\infty}{n=0}\ a_{n}3^{n}x^{n}\  \\
                            =&\summation{\infty}{n=0}\ a_{n}(3x)^{n}\  \\
                            &I=(-3,3)
                        .\end{align*}
                    \item For any integer \(m \geq 0\) and any real number \(b\), the series \(\sum_{n=0}^{\infty} c_n (b x^m)^n\) converges to \(f(b x^m)\) for all \(x\) such that \(b x^m\) is in \(I\).
                \end{enumerate}
            \item \textbf{For part I, II, and III, the interval of the combined series is the smaller interval}
            \item \textbf{Problem to remember:} Combining power series
            \item \textbf{Cauchy product (Multiplying power series)}:
                Suppose that the power series \(\sum_{n=0}^{\infty} c_n x^n\) and \(\sum_{n=0}^{\infty} d_n x^n\) converge to \(f\) and \(g\), respectively, on a common interval \(I\). Let
                \begin{align*}
                    &e_n = c_0 d_n + c_1 d_{n-1} + c_2 d_{n-2} + \cdots + c_{n-1} d_1 + c_n d_0  \\
                    &= \sum_{k=0}^{n} c_k d_{n-k}.
                .\end{align*}
                Then
                \[
                \left( \sum_{n=0}^{\infty} c_n x^n \right) \left( \sum_{n=0}^{\infty} d_n x^n \right) = \sum_{n=0}^{\infty} e_n x^n
                \]
                and
                \[
                \sum_{n=0}^{\infty} e_n x^n \text{ converges to } f(x) \cdot g(x) \text{ on } I.
                \]
                The series \(\sum_{n=0}^{\infty} e_n x^n\) is known as the Cauchy product of the series \(\sum_{n=0}^{\infty} c_n x^n\) and \(\sum_{n=0}^{\infty} d_n x^n\).
            \item \textbf{Sterling's Approximation}
                \begin{align*}
                    n! \approx \sqrt{2\pi n}\left(\frac{n}{e}\right)^{n}
                .\end{align*}
            \item \textbf{Gamma function (extension of the factorial function)}
                \begin{align*}
                     &\Gamma(z) = \int_{0}^{\infty}\ e^{-t}t^{z-1}\ dt \\
                     &\text{Thus, } n! = \Gamma(n+1)
                .\end{align*}
            \item \textbf{Cool definition for $e^{x}$}
                \begin{align*}
                    &f^{\prime}(x) = rf(x) \\
                    &\implies f(x) = ce^{rx}
                .\end{align*}
            \pagebreak 
            \item \textbf{Term-by-Term Differentiation and Integration for Power Series.}
                 Suppose that the power series $\sum_{n=0}^{\infty} c_n (x - a)^n$ converges on the interval $(a - R, a + R)$ for some $R > 0$. Let $f$ be the function defined by the series
                \[
                f(x) = \sum_{n=0}^{\infty} c_n (x - a)^n = c_0 + c_1(x - a) + c_2(x - a)^2 + c_3(x - a)^3 + \cdots
                \]
                for $|x - a| < R$. Then $f$ is differentiable on the interval $(a - R, a + R)$ and we can find $f'$ by differentiating the series term-by-term:
                \[
                f'(x) = \sum_{n=1}^{\infty} n c_n (x - a)^{n-1} = c_1 + 2c_2(x - a) + 3c_3(x - a)^2 + \cdots
                \]
                for $|x - a| < R$. Also, to find $\int f(x) \, dx$, we can integrate the series term-by-term. The resulting series converges on $(a - R, a + R)$, and we have
                \[
                \int f(x) \, dx = C + \sum_{n=0}^{\infty} \frac{c_n (x - a)^{n+1}}{n+1} = C + c_0(x - a) + \frac{c_1(x - a)^2}{2} + \frac{c_2(x - a)^3}{3} + \cdots
                \]
                for $|x - a| < R$.
                \bigbreak \noindent 
                \textbf{NOTE!} when a power series is differentiated or integrated term-by-term, it says nothing about what happens at the endpoints.
            \item \textbf{Uniqueness of Power Series}:
                Let $\sum_{n=0}^{\infty} c_n (x - a)^n$ and $\sum_{n=0}^{\infty} d_n (x - a)^n$ be two convergent power series such that
                \[
                \sum_{n=0}^{\infty} c_n (x - a)^n = \sum_{n=0}^{\infty} d_n (x - a)^n
                \]
                for all \( x \) in an open interval containing \( a \). Then \( c_n = d_n \) for all \( n \geq 0 \).
            \item \textbf{When finding the Cauchy product of two power series, we include the zero term when finding the new power series general term. For integrating and differentiating, we do not}
            \item \textbf{Taylor and Maclaurin series}:
                If $f$ has derivatives of all orders at $x=a$, then the Taylor series for the function $f$ at $a$ is
                \begin{equation}
                    \sum_{n=0}^{\infty} \frac{f^{(n)}(a)}{n!}(x-a)^n = f(a) + f'(a)(x-a) + \frac{f''(a)}{2!}(x-a)^2 + \cdots + \frac{f^{(n)}(a)}{n!}(x-a)^n + \cdots.
                \end{equation}
                The Taylor series for $f$ at $0$ is known as the Maclaurin series for $f$.
                \bigbreak \noindent 
            \item \textbf{Uniqueness of Taylor series}:  If a function $f$ has a power series at $a$ that converges to $f$ on some open interval containing $a$, then that power series is the Taylor series for $f$ at $a$.
            \item \textbf{Taylor-Macluarin Polynomials}:         
            If $f$ has $n$ derivatives at $x=a$, then the $n$th Taylor polynomial for $f$ at $a$ is
            \begin{equation}
                p_n(x) = f(a) + f'(a)(x-a) + \frac{f''(a)}{2!}(x-a)^2 + \frac{f'''(a)}{3!}(x-a)^3 + \cdots + \frac{f^{(n)}(a)}{n!}(x-a)^n.
            \end{equation}
            The $n$th Taylor polynomial for $f$ at $0$ is known as the $n$th Maclaurin polynomial for $f$.
            \item \textbf{Taylor’s Theorem with Remainder}:
                Let $f$ be a function that can be differentiated $n+1$ times on an interval $I$ containing the real number $a$. Let $p_n$ be the $n$th Taylor polynomial of $f$ at $a$ and let
                \begin{align*}
                R_n(x) = f(x) - p_n(x)
                \end{align*}
                be the $n$th remainder. Then for each $x$ in the interval $I$, there exists a real number $c$ between $a$ and $x$ such that
                \begin{align*}
                R_n(x) = \frac{f^{(n+1)}(c)}{(n+1)!}(x - a)^{n+1}.
                \end{align*}
                If there exists a real number $M$ such that $\left|f^{(n+1)}(x)\right| \leq M$ for all $x \in I$, then
                \begin{align*}
                \left|R_n(x)\right| \leq \frac{M}{(n+1)!}\left|x - a\right|^{n+1}
                \end{align*}
               $\forall\ x \in I$ 
            \item \textbf{Maclaurin Series/Polynomials for sine}:
                The Taylor series for the sine function is 
                \begin{align*}
                    \sin{(x)} = \summation{\infty}{n=0}\ (-1)^{n}\frac{x^{2n+1}}{(2n+1)!}\ = x-\frac{x^{3}}{3!} + \frac{x^{5}}{5!} - \frac{x^{7}}{7!}  + ... \quad \text{For } x\in\mathbb{R} 
                .\end{align*}
                Where $p_{n}$ obeys
                \begin{align*}
                   &p_{2m+1} = p_{2m+2} \\
                   &=x-\frac{x^{3}}{3!}+\frac{x^{5}}{5!}-\frac{x^{7}}{7!} + ... + \frac{(-1)^{m}x^{2m+1}}{(2m+1)!}
                .\end{align*}
                \textbf{Note:} When discussing specific polynomials, say $P_5 $ for example, we arnt talking about the first $5$ terms in the series above, we are talking about the polynomial \texttt{up to} degree 5. Thus it would have 3 terms
            \item \textbf{Maclaurin Series/Polynomials for cosine}: Similar to the sine function, the Maclaurin series for the cosine function is 
                \begin{align*}
                    \cos{(x)} = \summation{\infty}{n=0}\ (-1)^{n}\frac{x^{2n}}{(2n)!}\ = 1 - \frac{x^{2}}{2!} + \frac{x^{4}}{4!} -\frac{x^{6}}{6!} + ... \quad \text{For } x\in\mathbb{R} 
                .\end{align*}
                Where $p_{n} $ obeys
                \begin{align*}
                    &p_{2m} = p_{2m+1} \\
                    &=1 - \frac{x^{2}}{2!} + \frac{x^{4}}{4!} -\frac{x^{6}}{6!}     + ... + (-1)^{n}\frac{x^{2m}}{(2m)!}
                .\end{align*}
            \item \textbf{Maclaurin Series/Polynomials for $e^{x}$}: We find the Maclaurin series for the exponential function to be 
                \begin{align*}
                    e^{x} = \summation{\infty}{n=0}\ \frac{x^{n}}{n!}\  = 1 + x + \frac{x^{2}}{2!} + \frac{x^{3}}{3!} + ... \quad \text{For } x\in\mathbb{R}
                .\end{align*}
                \bigbreak \noindent 
                \textbf{Note:} this definition is described above but now we have a way of showing its truthiness
            \item \textbf{Convergence of Taylor Series}:
                Suppose that $f$ has derivatives of all orders on an interval $I$ containing $a$. Then the Taylor series
                \[
                \sum_{n=0}^{\infty} \frac{f^{(n)}(a)}{n!}(x-a)^n
                \]
                converges to $f(x)$ for all $x$ in $I$ if and only if
                \[
                \lim_{n \to \infty} R_n(x) = 0
                \]
                for all $x$ in $I$.
                \bigbreak \noindent 
                \textbf{Note:}  With this theorem, we can prove that a Taylor series for $f$ at $a$ converges to $f$ if we can prove that the remainder $R_n(x) \to 0$. To prove that $R_n(x) \to 0$, we typically use the bound
                \[
                |R_n(x)| \leq \frac{M}{(n+1)!}|x - a|^{n+1}
                \]
                from Taylor’s theorem with remainder.
            \item \textbf{Using taylor series to find limits}: Consider the limit $\lim\limits_{x \to 0+}{\frac{\cos{\sqrt{x}} -1}{2x}}$. We know we have a problem if we attempt to use the \texttt{direct substitution property}. Thus, we can substitute $\cos{(\sqrt{x})}$ for its \texttt{Maclaurin series} and see what happens. We know the maclaurin series for $\cos{(x)}$ is 
                \begin{align*}
                   \cos{x} &\sim \summation{\infty}{n=0}\ (-1)^{n}\frac{x^{2n}}{(2n)!}\   = 1 - \frac{x^{2}}{2!} + \frac{x^{4}}{4!} - \frac{x^{6}}{6!} + ... \\
                   \implies \cos{\sqrt{x}} &\sim \summation{\infty}{n=0}\ (-1)^{n}\frac{(x^{\frac{1}{2}})^{2n}}{(2n)!}\   = 1 - \frac{(x^{\frac{1}{2}})^{2}}{2!} + \frac{(x^{\frac{1}{2}})^{4}}{4!} - \frac{(x^{\frac{1}{2}})^{6}}{6!} + ... \\
                                           &=\summation{\infty}{n=0}\ (-1)^{n}\frac{x^{n}}{(2n)!}\   = 1 - \frac{x}{2!} + \frac{x^{2}}{4!} - \frac{x^{3}}{6!} + ... 
                .\end{align*}
                So we have
                \begin{align*}
                    &\lim\limits_{x \to 0^{+}}{\frac{\left(1-\frac{x}{2!}+\frac{x^{2}}{4!}-\frac{x^{3}}{6!} + ...\right)-1}{2x}}\\
                    &=\lim\limits_{x \to 0^{+}}{\frac{\left(-\frac{x}{2!}+\frac{x^{2}}{4!}-\frac{x^{3}}{6!} + ...\right)}{2x}}\\
                    &=\lim\limits_{x \to 0^{+}}\left(-\frac{x}{2!}+\frac{x^{2}}{4!}-\frac{x^{3}}{6!} + ...\right)\cdot \frac{1}{2x}\\
                    &=\lim\limits_{x \to 0^{+}}{-\frac{1}{4}} \\
                    &=-\frac{1}{4}
                .\end{align*}
            \item \textbf{Multiplying a known Taylor series by some other function:} Consider $f(x) = x\cos{x}$. Since we know that the taylor series for $\cos{(x)} = \summation{\infty}{n=0}\ (-1)^{n}\frac{x^{2n}}{(2n)!}\ $, which converges $\forall\ x \in \mathbb{R} $. We can easily just multiply this by $x$ to get the taylor series for $f(x) = x\cos{(x)}$.  Since the Taylor series for $\cos{(x)}$ converges for all real $x$, multiplying it by  $x$ won't affect its convergence properties. The resulting series will still converge for all $x$.
                \bigbreak \noindent 
                \textbf{Note:} The product of the Taylor series and the function will be valid only where both the series converges and the function is well-defined. Probably analyze the convergence of the product series.
                \item \textbf{Multiplying a known Taylor series by some other function where convergence is affected:} Consider the example above. Although this time suppose we multiply $\cos{(x)}$ by $\frac{1}{x} $ instead of $x$. We know the resulting taylor series must not be convergent at $x=0$ because $\frac{1}{x}$ is not defined at zero. It is important to understand that we will not get this conclusion from the ratio test alone. The Ratio Test alone does not account for points where the series or its terms are not defined.
                    \bigbreak \noindent 
                    \textbf{TLDR:} Be mindful about the domain of the function you are multiplying the Taylor series by. Do not only rely on the ratio test to find points of convergence.
                    \pagebreak 
            \item \textbf{Analytic function:} 
                \begin{itemize}
                    \item An analytic function is infinitely differentiable within its domain.
                    \item An analytic function can be represented by a convergent power series (like a Taylor series) around any point in its domain. 
                    \item  The power series representing an analytic function not only exists but also converges to the function within a certain radius around the point of expansion
                \end{itemize}
            \item \textbf{Maclaurin series for $\frac{1}{1-x}$}:
                \begin{align*}
                    \frac{1}{1-x} \sim \summation{\infty}{n=0}\x^{n} \ \quad \text{for } \abs{x} < 1
                .\end{align*}
            \item \textbf{Maclaurin series for $\ln{(1+x)}$}:
                \begin{align*}
                    \ln{(1+x)} \sim \summation{\infty}{n=1}\(-1)^{n+1}\frac{x^{n}}{n} \ \quad \text{for } -1 < x \leq 1
                .\end{align*}
            \item \textbf{Maclaurin series for $\tan^{-1}{x}$}:
                \begin{align*}
                    \tan^{-1}{x} \sim \summation{\infty}{n=0}\(-1)^{n}\frac{x^{2n+1}}{2n+1} \ \quad \text{for } \abs{x} \leq 1
                .\end{align*}

        \end{itemize}
    \item \textbf{Binomial expansion for $(1+x)^{r}$ for $r\in\mathbb{Z^{+}}$}
        \begin{align*}
            (1+x)^{r}= \summation{r}{n=0}\ \binom{r}{n}x^{n}\ \quad r \in \mathbb{Z^{+}}
        .\end{align*}
    \item \textbf{Binomial expansion for $(1+x)^{r}$ for $r\in\mathbb{R}$}
        \begin{align*}
            (1+x)^{r} = \summation{\infty}{n=0}\ \binom{r}{n}x^{n} = 1+rx+ \frac{r(r-1)}{2!}x^2 + \cdots + \frac{r(r-1)\cdots(r-n+1)}{n!}x^n + \cdots
        .\end{align*}
        Where 
        \begin{align*}
            &\binom{r}{n} = \frac{f^{(n)}(0)}{n!} = \frac{r(r-1)(r-2)\cdot ...\cdot (r-n+1)}{n!} 
        .\end{align*}
        \textbf{Note:} When $n=0$, $\binom{r}{0} = 1$, when $n=1$, $\binom{r}{1} = r$, when $n=2$, $\binom{r}{2} = \frac{r(r-1)}{2!} $, etc...
    \item \textbf{The binomial theorem:}
        For any real number \(r\), the Maclaurin series for \( f(x) = (1 + x)^r \) is the binomial series. It converges to \(f\) for \(|x| < 1\), and we write
        \begin{align*}
            (1 + x)^r = \sum_{n=0}^{\infty} \binom{r}{n} x^n = 1 + rx + \frac{r(r-1)}{2!}x^2 + \cdots + \frac{r(r-1) \cdots (r-n+1)}{n!}x^n + \cdots
        \end{align*}
        for \(|x| < 1\).

    \pagebreak \bigbreak \noindent 
    \subsection{Chapter 6 Problems to Remember}
    \begin{itemize}
                    \item \textbf{Problem to remember (Properties of power series):} Evaluate the infinite series by identifying it as the value of an integral of a geometric series.
                \begin{align*}
                    \summation{\infty}{n=0}\ \frac{(-1)^{n}}{2n+1}\ 
                .\end{align*}
                \begin{remark}
                    If we can find which geometric power series's integral (with some bounds) gives us the given series, we can then integrate the function representation to get the value of the original series. Consider the geometric power series
                    \begin{align*}
                        \frac{1}{1+x^{2}} = \frac{1}{1-(-x^{2})} = \summation{\infty}{n=0}\ (-x^{2})^{n}\  = \summation{\infty}{n=0}\ (-1)^{n}x^{2n}\ 
                    .\end{align*}
                    Suppose we then integrate the power series
                    \begin{align*}
                        &\int_{}^{}\ \summation{\infty}{n=0}\ (-1)^{n}x^{2n}\ dx \\
                        &=\summation{\infty}{n=0}\ (-1)^{n}\ \int_{}^{}\ x^{2n}\ dx \\
                        &=\frac{1}{2n+1}x^{2n+1}
                    .\end{align*}
                    Now we must deduce for which bounds will the FTC give us the original series $\summation{\infty}{n=0}\ \frac{(-1)^{n}}{2n+1}\  $. We come to the conclusion
                    \begin{align*}
                        \summation{\infty}{n=0}\ (-1)^{n}\ \int_{0}^{1}\ x^{2n}\ dx = \summation{\infty}{n}\ \frac{(-1)^{n}}{2n+1}\ 
                    .\end{align*}
                    This implies we can integrate the function representation of the geometric power series we just integrated to get the value of the infinite series $\summation{\infty}{n=0}\ \frac{(-1)^{n}}{2n+1}\  $. Thus,
                    \begin{align*}
                        \int_{0}^{1}\ \frac{1}{1+x^{2}}\ dx = \frac{\pi}{4}
                    .\end{align*}
                \end{remark}
            \item \textbf{Problem to remember (Properties of power series):} Find the power series for $f(x) = \ln{x}$ centered at $x=9$ by using term-by-term integration or differentiation.
                \smallbreak \noindent
                \textit{Solution.} The goal is to find a function that resembles one we know (sum of geometric series $\frac{a}{1-x}$) such that if we integrate or differentiate we can get $\ln{x}$. Since we know the integral of $\frac{1}{x}$ is $\ln{x}$, and we can easily manipulate $\frac{1}{x}$ to be in the form $\frac{a}{1-x}$, we choose $\frac{1}{x} $ to be the function to examine. Thus, 
                \begin{align*}
                    \frac{1}{x} &= \frac{1}{9+x-9} = \frac{1}{9-(-(x-9))} = \frac{1/9}{1-\left(\frac{-(x-9)}{9}\right)} \\
                    \text{If } f(x) &= \frac{a}{1-x} \sim \summation{\infty}{n=0}\ a(x^{n})\ = a + ax + ax^{2} + ax^{3} + ... \\
                    \implies f(x) &= \frac{1/9}{1-\left(\frac{-(x-9)}{9}\right)} \sim \summation{\infty}{n=0}\ \frac{1}{9}\ \left(\frac{-(x-9)}{9}\right)^{n} = \summation{\infty}{n=0}\ \frac{(-1)^{n}(x-9)^{n}}{9^{n+1}}\ 
                .\end{align*}
                Then we can throw in some integrals
                \begin{align*}
                    &\int f(x)\ dx = \int \frac{1/9}{1-\left(\frac{-(x-9)}{9}\right)}\ dx = \int \summation{\infty}{n=0}\ \frac{(-1)^{n}(x-9)^{n}}{9^{n+1}}\ \ dx \\
                    &\ln{x} = \summation{\infty}{n=0}\ \frac{(-1)^{n}}{9^{n+1}}\  \int (x-9)^{n}\ dx \\
                    &\ln{x}  =  \summation{\infty}{n=0}\ \frac{(-1)^{n}(x-9)^{n+1}}{(n+1)9^{n+1}}\ + C 
                .\end{align*}
                If we let $x=9$
                \begin{align*}
                    &\ln{9}  =  \summation{\infty}{n=0}\ \frac{(-1)^{n}(9-9)^{n+1}}{(n+1)9^{n+1}}\ + C  \\
                    &\ln{9} = C
                .\end{align*}
                Thus, we have
                \begin{align*}
                    f(x) = \ln{9} + \summation{\infty}{n=0}\ \frac{(-1)^{n}(9-9)^{n+1}}{(n+1)9^{n+1}}\  \\
                .\end{align*}
                \textbf{Note:} the "$+C$" is initially omitted from $\ln{(x)}$ because we're considering a specific antiderivative. When you integrate the power series, you include "$+C$" to account for the general form of the antiderivative. The value of  $C$ is then determined using a specific condition to match the specific antiderivative you're interested in.
            \item \textbf{Problem to remember}: Say we want to find the power series for $7x\ln{1+x}$. We can first find the power series for $\ln{(1+x)} $
                \begin{align*}
                    \frac{d}{dx} \ln{(1+x)} = \frac{1}{1+x} = \frac{1}{1-(-x)}
                .\end{align*}
                We know the power series for  $\frac{1}{1-(-x)} $ is
                \begin{align*}
                    &\summation{\infty}{n=0}\ (-1)^{n}x^{n}\  \\
                    &\implies \int \frac{1}{1+x} = \int \summation{\infty}{n=0}\ (-1)x^{n}\   \\
                    &\ln{(1+x)} = \summation{\infty}{n=0}\ (-1)^{n}\frac{x^{n+1}}{n+1}\  + C  \\
                    &\ln{(1+x)} = \summation{\infty}{n=0}\ (-1)^{n}\frac{x^{n+1}}{n+1}\ 
                .\end{align*}
                Now that we have found the power series for $\ln{(1+x)}$, to find the power series for $7x\ln{(1+x)}$...
                \begin{align*}
                    &7x\summation{\infty}{n=0}\ (-1)^{n}\frac{x^{n+1}}{n+1}\  \\
                    &\summation{\infty}{n=0}\ 7x\left((-1)^{n}\frac{x^{n+1}}{n+1}\right)\  \\
                    &=\summation{\infty}{n=0}\ (-1)^{n}\frac{7x^{n+2}}{n+1}\ 
                .\end{align*}
                \pagebreak \bigbreak \noindent 
            \item \textbf{Problem to remember (Cumbersome taylor polynomial)}: Suppose we have some function $f$, and we would like to find the Taylor polynomial up to degree $3$. Say $f(x) = e^{2x}\cos{(x)}$. We could find each derivative up to degree 3, however, given that we know the taylor series for both $e^{2x}$ and $\cos{(x)}$.
                \begin{align*}
                    &e^{2x} = \summation{\infty}{n=0}\ \frac{(2x)^{n}}{n!}\  = 1+2x + \frac{4x^{2}}{2!} + \frac{8x^{3}}{3!} + ...\\
                    &\cos{(x)} = \summation{\infty}{n=0}\ (-1)^{n}\frac{x^{2n}}{(2n)!} = 1-\frac{x^{2}}{2!} + \frac{x^{4}}{4!} + \frac{x^{6}}{6!} + ...\ 
                .\end{align*}
                We can find $P_{3}(x)$ by multiplying  these Taylor series. Thus,
                \begin{align*}
                    (1+2x + \frac{4x^{2}}{2!} + \frac{8x^{3}}{3!} + ...)(1-\frac{x^{2}}{2!} + \frac{x^{4}}{4!} + \frac{x^{6}}{6!}+...)
                .\end{align*}
                And we can find $P_{3}$ to be 
                \begin{align*}
                    P_{3} = 1 +2x + \frac{3}{2}x^{2} +\frac{1}{3}x^{3}
                .\end{align*}
            \item \textbf{Problem to Remember (Using known Taylor series to find sum of series):} Consider the series $\summation{\infty}{n=0}\ (-1)^{n}\frac{\left(\frac{1}{25}\right)^{n-3}}{2n+1}\ $
                \bigbreak \noindent 
                We notice this resembles the Taylor series for the arctangent function $\tan^{-1}{x} = \summation{\infty}{n=0}\ (-1)^{n}\frac{x^{2n+1}}{2n+1}\ \text{ for } \abs{x} \leq 1$. Thus, we maipulate the series to better conform to the Taylor series for $\tan^{-1}{x}$.
                \begin{align*}
                   &\summation{\infty}{n=0}\ (-1)^{n}\frac{\left(\frac{1}{25}\right)^{n-3}}{2n+1}\  \\
                   &=\summation{\infty}{n=0}\ (-1)^{n}\frac{\left(\frac{1}{5}\right)^{2n-6}}{2n+1}\ \\
                   &=\summation{\infty}{n=0}\ (-1)^{n}\frac{\left(\frac{1}{5}\right)^{2n}5^{6}}{2n+1}\ \\
                   &=\summation{\infty}{n=0}\ (-1)^{n}\frac{\left(\frac{1}{5}\right)^{2n}5^{6}\left(\frac{1}{5}\right)5}{2n+1}\ \\
                   &=\summation{\infty}{n=0}\ (-1)^{n}\frac{\left(\frac{1}{5}\right)^{2n+1}5^{7}}{2n+1}\ \\
                .\end{align*}
                Thus the sum will be 
                \begin{align*}
                    5^{7}\tan^{-1}{\frac{1}{5}}
                .\end{align*}
                
    \end{itemize}





    \pagebreak \bigbreak \noindent 
    \unsect{Vectors}
    \bigbreak \noindent 
    \subsection{The Vocabulary of Vectors}
    \begin{itemize}
        \item A \textbf{vector} is two pieces of information. 
        \begin{enumerate}
            \item Length 
            \item Direction (Magnitude)
        \end{enumerate}
    \end{itemize}

    \bigbreak \noindent 
    \subsection{Vector Notation}
    \begin{itemize}
        \item \textbf{Defining a vector}                
            \begin{align*}
                \vec{v} = [x,y] \text{ or } 
                \begin{bmatrix}
                    x \\ y
                \end{bmatrix}
            .\end{align*}
        \item \textbf{Length of a vector}
            \begin{align*}
                || \vec{v} || \in \mathbb{R^{n}} = \sqrt{v_{1}^{2} + v_{2}^{2} + ... + v_{n}^{2}}
            .\end{align*}
        \item \textbf{Vector addition}
            Suppose we have two vectors $\vec{v} = \begin{bmatrix} x_{1} \\ y_{1} \end{bmatrix} $ and $\vec{u} = \begin{bmatrix} x_{2} \\ y_{2} \end{bmatrix} $. Then
            \begin{align*}
                \vec{v}  +\vec{u} = \begin{bmatrix} x_{1} + x_{2} \\ y_{1} + y_{2} \end{bmatrix}   = \vec{c}             
            .\end{align*}
            \textbf{Note:} we call this new vector the \textbf{resultant}
        \item \textbf{Multiplying by a scalar}
            Suppose we have the vector $\vec{v} = \begin{bmatrix} x \\ y \end{bmatrix} $. Then
            \begin{align*}
               2\vec{v}  = \begin{bmatrix} 2x \\ 2y \end{bmatrix}
            .\end{align*}
        \item \textbf{Vector subtraction}
            \begin{align*}
                \vec{v} - \vec{u} = \begin{bmatrix} x_{1} - x_{2} \\ y_{1} - y_{2} \end{bmatrix}
            .\end{align*}
        \item \textbf{Vector in three dimensions}
            \begin{align*}
                \vec{v} = \begin{bmatrix} x \\ y \\ z \end{bmatrix}
            .\end{align*}
        \item \textbf{Definition for $\mathbb{R}^{n}$}: $\mathbb{R}^{n}$ is the set of all $n-tuples$ of real numbers  
            \begin{align*}
                &\vec{v} = [v_{1}, v_{2}]\ \vec{v} \in \mathbb{R}^{2} \\
                &\vec{u} = [u_{1}, u_{2},u_{3}]\ \vec{u} \in \mathbb{R}^{3} \\
                &\vec{w} = [w_{1}, w_{2},w_{3}, w_{n}]\ \vec{w} \in \mathbb{R}^{n} \\
            .\end{align*}
    \end{itemize}










\end{document}
