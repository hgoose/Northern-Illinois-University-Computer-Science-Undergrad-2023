{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f1e4667-3b27-4c28-8486-6bd3f90e21ac",
   "metadata": {},
   "source": [
    "# Project: Nate Warner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deda05b3-ae33-4fbc-a6c4-370d892c935e",
   "metadata": {},
   "source": [
    "#### [AccurateArithmetic.jl](https://github.com/JuliaMath/AccurateArithmetic.jl): Kahan's summation algorithm for compensated summations and more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a318561-60d0-450d-844c-c2d0d2e7513a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/General.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.11/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.11/Manifest.toml`\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.add(\"AccurateArithmetic\")\n",
    "using AccurateArithmetic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630cc678-fa2a-4a26-815e-bcc7e2da71e9",
   "metadata": {},
   "source": [
    "AccurateArithetic.jl is marketed as \"Floating point math with error-free, faithful, and compensated transforms.\" However, I would like to focus on the packages implementation of Kahan's algorithm. First, let's take a look at the problem Kahan's algorithm solves.\n",
    "\n",
    "Consider a summation of real numbers.\n",
    "$$\\sum_{i=0}^n a_{i},\\quad a_i \\in \\mathbb{R}.$$\n",
    "Which can be naively implemented as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc072a55-1d2c-4bbb-a2ce-26cc90634e43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "naivesum (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function naivesum(S...)\n",
    "    sum = 0.0\n",
    "    for s in S\n",
    "        sum+=s\n",
    "    end\n",
    "    sum\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233a093c-153a-4d1d-b52a-23f63e10ac92",
   "metadata": {},
   "source": [
    "We note that Kahan's algorithm is implemented as _sum\\_kbn_ in AccurateArithmetic.jl."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62330a29-18e0-4a5d-924b-980b59f69d12",
   "metadata": {},
   "source": [
    "Which works fine for integers, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a105dfc-61e6-462f-83bc-f1f0d78a4edb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naivesum(1,2,3,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ce99f3-c6d2-4956-97a4-dc07b0bfbdda",
   "metadata": {},
   "source": [
    "but can cause issues when dealing with floats. When dealing with floating point numbers, there are three major issues that can occur.\n",
    "\n",
    "- Rounding Error Accumulation: When summing many floating point numbers, each sum can introduce rounding error. Over time, these errors can accumulate and greatly affect the final value.\n",
    "- Loss of Significance: Summing $x + y$ when $x >> y$ can cause the information in $y$ to be lost.\n",
    "- Catastrophic Cancellation: When two numbers are approximately equal, the loss in accuracy is called catastrophic cancellation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e8942c-d0eb-4c0d-b041-f169c65d62b0",
   "metadata": {},
   "source": [
    "Before we take a look at the details of kahan's algorithm, let's first look at these three points above in the context of naivesum vs sum\\_kbn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b559e29-84c1-4ac0-838e-6268dfd7264b",
   "metadata": {},
   "source": [
    "First, we look at rounding error accumulation. Consider $\\sum_{i=1}^{1000} 0.1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28650bd7-2933-49b3-a9ae-ce7badd86534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99.9999999999986, 100.0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [0.1 for i in 1:1000]\n",
    "naivesum(x...), sum_kbn(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6aa862d-ed41-4270-8ea3-dbef51232b83",
   "metadata": {},
   "source": [
    "We see that by using naivesum, we get a result that is not quite correct. Since $0.1$ cannot be represented in the computer exactly, each terms introduces a small amount of roundoff error. After summing all 1000 terms, those rounding errors accumulate and the final result is not quite correct. We observe that sum\\_kbn gives an accurate result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5580e7d9-8e15-4e9c-bc74-a48311fe6271",
   "metadata": {},
   "source": [
    "Next, we consider loss of significance and catestrophic cancellation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "513c0645-764a-447e-b0f3-71ba027f542b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [1e16, 1, -1e16]\n",
    "naivesum(x...), sum_kbn(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a67943-fd5c-4adc-b50d-ef72c02597ef",
   "metadata": {},
   "source": [
    "Again, we see that naivesum gives a result that is not quite correct, while sum\\_kbn gives the correct result. When adding 1 to $1\\times10^{16}$, the 1 is lost due to the fact that Float64 only has 16 digits of accuracy. Although the result should be $1e16 + 1$, the sum remains $1e16$. Thus, subtracting $-1e16$ leads to an answer of zero, although it should be one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a616a6b-4f20-44c8-b745-42ecd34ec7b0",
   "metadata": {},
   "source": [
    "Lastly, the example given in the package documentation is the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f1ad7e6e-079f-43b7-b727-ec6f9388cbe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-23.55290330392469, 1.0000000000000044)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# By design, this vector sums to 1\n",
    "x = 5000 |> N->randn(N) .* exp.(10 .* randn(N)) |> x->[x;-x;1.0] |> x->x[sortperm(rand(length(x)))];\n",
    "naivesum(x...), sum_kbn(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effa1268-5534-4ad0-8087-ec87deae519f",
   "metadata": {},
   "source": [
    "Although Kahan's algorithm can sometimes account for loss of significance and castrophic cancellation errors, its main objective is to minimize the accumulation of rounding errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19902d1b-a00d-4a0f-b03a-305c6d8c5789",
   "metadata": {},
   "source": [
    "We now introduce Kahan's algorithm. Kahan's algorithm is a technique designed to improve the numerical accuracy of the sum of a sequence of finite-precision floating-point numbers. We saw in naivesum, small rounding errors accumulate, Kahan's algorithm minimizes these errors by keeping track of a small \"compensation\" value that adjusts the running sum to account for lost low-order bits. The algorithm is fairly straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "782a7d8e-2874-4948-8532-1cd6e65c3930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "kahan_sum (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function kahan_sum(X)\n",
    "    sum = 0.0\n",
    "    # Running compensation for low order bits\n",
    "    c = 0.0\n",
    "    for x in X\n",
    "        # First, we correct the next value by subtracting the compensation,\n",
    "        # where the compensation is the loss from the previous operation\n",
    "        y = x - c\n",
    "        # Then, we add the corrected value to the sum\n",
    "        t = sum + y \n",
    "        # We compute the new compensation with the following formula (in exact arithmetic this would be zero)\n",
    "        c = (t - sum) - y\n",
    "        # Last, update the total\n",
    "        sum = t\n",
    "    end\n",
    "    return sum\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "703ae3d7-fdd3-45ef-b5ce-f5a91bb29ee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99.9999999999986, 100.0)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [0.1 for i in 1:1000]\n",
    "naivesum(x...), kahan_sum(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd76393-342c-421f-8971-1939d1b9434d",
   "metadata": {},
   "source": [
    "The variable $c$ is used to track rounding errors in previous computations. For each $x \\in X$, $y = x-c$ \"adjusts\" the next value $x$ with the current compensation value, ensuring that errors from previous sums are properly accounted for. Although it may seem counterintuitive to subtract $c$ instead of adding, we remark that the way $c$ is computed, $c$ will be negative if the previous sum loses information in the low-order bits.\n",
    "\n",
    "Next, we compute $t = sum + y$. Instead of directly adding the corrected value to the sum, we hold it in a temporary variable so we can use it to find the error in the current iteration (that is, after adding the current value).\n",
    "\n",
    "We see this by looking at the next instruction $c = (t-sum) -y$. Thus, we take the current sum, (which should in theory be exactly $y$), and find the difference after subtracting $y$, this gives us the compensation $c$.\n",
    "\n",
    "Lastly, we update $sum$.\n",
    "\n",
    "Thus, the error growth in this summation algorithm does not depend on $n$, where $n$ is the size of the input vector $x$, it has constant growth $\\mathcal{O}(1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8896b2f9-f7c0-4e49-80e8-5f0c86e46346",
   "metadata": {},
   "source": [
    "Although the algorithm as implemented above is better than naivesum, it still can be improved. Consider the example we talked about earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dbffd4ee-bd09-434f-9235-85006192d4c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 0.0)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [1e16, 1, -1e16]\n",
    "naivesum(x...), kahan_sum(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6f7ae7-219d-46ab-8b3a-85bdd4892dce",
   "metadata": {},
   "source": [
    "We see for the input vector $x = [1e16, 1, -1e16]$, it is still no better than naivesum. We can slightly improve our implementation by using the _Fast Two Sum_ algorithm, which takes two numbers $a$, $b$, where $\\left\\lvert a\\right\\rvert \\geq \\left\\lvert{b}\\right\\rvert$, and finds numbers $s,e$ such that $a + b = s + e$, where $e$ is the error in the sum. An implementation for FastTwoSum is as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ca95101-8a97-4ba4-bc4f-7ae1f1df01f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fast_two_sum (generic function with 1 method)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function fast_two_sum(a,b)\n",
    "    # If b > a, swap b and a\n",
    "    if (b > a)\n",
    "        (a,b) = (b,a)\n",
    "    end\n",
    "\n",
    "    # Compute fl(a+b)\n",
    "    s = a + b\n",
    "    # In theory should be b\n",
    "    t = s - a\n",
    "    # Get the error between the b that was reduced by error and the original b\n",
    "    e = b - t\n",
    "\n",
    "    s,e\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "94461cb3-f238-4f5e-9c7d-79938540bd27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0e16, 1.0)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(s,e) = fast_two_sum(1e16,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2290614-8df3-4c4b-babe-0c234cbf2d8e",
   "metadata": {},
   "source": [
    "So, we have $s = a + b$, which computes the sum $a+b$ and stores the result in $s$. We know that $s$ may have lost some precision due to error, so we take $t = s-a$, which in theory should be $b$, but may actually be $b - \\epsilon$, for some small $\\epsilon$. Thus, computing $e = b-t$ yields $b - (b - \\epsilon) = \\epsilon$, which is precisely the error in the sum $a+b$.\n",
    "\n",
    "With this algorithm, we can improve our implementation of Kahan's algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d7590dda-6032-4e43-b4dd-cff4caed199d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function kahan_sum_f2s(X)\n",
    "    sum = 0.0\n",
    "    # Running compensation\n",
    "    c = 0.0\n",
    "    for x in X\n",
    "        # First, we correct the next value by adding the compensation,\n",
    "        # where the compensation is the loss from the previous operation\n",
    "        # We note that since fast_two_sum gives a positive error (unlike the original implementation), \n",
    "        # we switch minus to plus\n",
    "        y = x + c\n",
    "\n",
    "        # Elegent compensation finding with fast two sum\n",
    "        (sum,c) = fast_two_sum(sum,y)\n",
    "    end\n",
    "    return sum\n",
    "end\n",
    "#x = [1e16, 1, -1e16]\n",
    "X = [1e16, 1.0, -1e16]\n",
    "kahan_sum_f2s(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54df3795-6fbe-442b-8cb9-491049e6f68d",
   "metadata": {},
   "source": [
    "We see the algorithm still does not quite account for large variation in the magnitudes of the vectors elements. The final piece of the puzzle is the _Neumaier summation algorithm_, which expands on the work of Kahan. Neumaiers algorithm is a variation of Kahan's algorithm that adjusts the compensation depending on the relative sizes of the current sum and the next number."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04dad861-3622-4a44-9f46-5ff1655447f0",
   "metadata": {},
   "source": [
    "For $x = [1e16, 1.0,-1e16]$,\n",
    "\n",
    "Iteration 1: $sum = 0.0, \\ c = 0.0$\n",
    "\n",
    "\\begin{align*}\n",
    "    y &= 1\\times10^{16} + 0.0 = 1\\times 10^{16} \\\\\n",
    "    sum &= 0.0 + 1\\times 10^{16} = 1\\times 10^{16} \\\\\n",
    "    c &= 0.0\n",
    ".\\end{align*}\n",
    "\n",
    "Iteration 2: $sum = 1\\times 10^{16},\\ c = 0.0$\n",
    "\n",
    "\\begin{align*}\n",
    "    y &= 1.0 + 0.0 = 1.0 \\\\\n",
    "    sum &= 1\\times 10^{16} + 1.0  = 1\\times 10^{16} \\\\\n",
    "    c &= 1.0\n",
    ".\\end{align*}\n",
    "\n",
    "Iteration 3: $sum = 1\\times 10^{16},\\ c = 1.0$\n",
    "\n",
    "\\begin{align*}\n",
    "    y &= -1\\times10^{16} + 1.0 = -1\\times10^{16}\\\\\n",
    "    sum &= 1\\times 10^{16} - 1\\times 10^{16} = 0 \\\\\n",
    "    c &= 0.0\n",
    ".\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "73ba6d1d-ef5f-4d78-beab-ecd708034ca9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function kahan_neumaier_sum(X)\n",
    "    s = X[1]\n",
    "    c = 0.0\n",
    "    for i in 2:length(X)\n",
    "        x = X[i]\n",
    "\n",
    "        # Note: Neumaier does something like this, but we can make it easier with fast two sum\n",
    "        #t = s + x\n",
    "        #if abs(s) >= abs(x)\n",
    "            #c += (s - t) + x\n",
    "       # else\n",
    "            #c += (x-t) + s\n",
    "        #end\n",
    "        #s = t\n",
    "\n",
    "        s,e = fast_two_sum(s,x)\n",
    "        c+=e\n",
    "    end\n",
    "    return s + c\n",
    "end\n",
    "#x = [.1 for i in 1:1000]\n",
    "x = [1e16, 1, -1e16]\n",
    "kahan_neumaier_sum(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db68b052-8d35-4c80-b3e2-62a4260cd856",
   "metadata": {},
   "source": [
    "We compare our implemenation against _sum\\_kbn_ and naivesum with the following vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7905b09-9707-48f2-ba2e-148c0a866dd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [1e16, 1, -1e16]\n",
    "y = [0.1 for i in 1:1000]\n",
    "z = 5000 |> N->randn(N) .* exp.(10 .* randn(N)) |> z->[z;-z;1.0] |> z->z[sortperm(rand(length(z)))];\n",
    "r = Float64[1e16^(-i) for i in 0:0.001:100]\n",
    "0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ff077cd7-fcf4-47c6-9fa6-9796a6ff7960",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0, 1.0)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(naivesum(x...), kahan_neumaier_sum(x), sum_kbn(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7ccf60ec-0e0e-4c23-a006-7621d833a4df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99.9999999999986, 100.0, 100.0)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(naivesum(y...), kahan_neumaier_sum(y), sum_kbn(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ecc2c0df-ab91-4150-ac78-9c9f3c255b94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.09672063026532755, -0.0928417077712226, 0.9999999999999998)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(naivesum(z...), kahan_neumaier_sum(z), sum_kbn(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "23c4e6af-b45b-4703-87a5-09926880e459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27.6464751629624, 27.646475162962446, 27.646475162962446)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(naivesum(r...), kahan_neumaier_sum(r), sum_kbn(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2af8d2bf-3379-4827-837e-d9a5fc974571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.64647516296244641857573182468082331028776757181158327242734178230478766349055"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(big.(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d955f39e-4784-4dc4-abfe-3221b3297933",
   "metadata": {},
   "source": [
    "We see that for the vector $\\mathbf{z}$, our implementation behaves similar to naivesum, its unclear what _sum_kbn_ does to get that vector to work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1bc17c-dadc-416e-9c38-6cbe8da61402",
   "metadata": {},
   "source": [
    "#### [Errors](https://en.wikipedia.org/wiki/Kahan_summation_algorithm)\n",
    "For the sum\n",
    "$$S_n = \\sum_{i=0}^{n}x_i$$\n",
    "In using compensated summation algorithms such as Kahan's algorithm, we get $S_n + E_n$, where $E_n$ is bounded by\n",
    "$$ |E_{n}|\\leq \\big[2\\varepsilon +O(n\\varepsilon ^{2})\\big]\\sum _{i=1}^{n}|x_{i}|,$$\n",
    "where $\\epsilon$ is the machine epsilon.\n",
    "\n",
    "The relative error is bounded by \n",
    "$${\\displaystyle {\\frac {|E_{n}|}{|S_{n}|}}\\leq {\\big [}2\\varepsilon +O(n\\varepsilon ^{2}){\\big ]}{\\frac {\\sum \\limits _{i=1}^{n}|x_{i}|}{\\left|\\sum \\limits _{i=1}^{n}x_{i}\\right|}}.}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6533cd-791b-40c2-989d-dd7deb095795",
   "metadata": {},
   "source": [
    "#### Closing remarks\n",
    "The (nonfast) TwoSum algorithm is implemented as follows. TwoSum does not make the assumption that $a \\geq b$. Infact, it finds the ammount lost in both $a$ and $b$ and sums them. This is precisely what makes FastTwoSum faster, by making the enforcing $a \\geq b$, it reduces the number of computations required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c2983a9-d665-4037-83f4-99c117971218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0e16, 1.0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function TwoSum(a,b)\n",
    "    s = a + b\n",
    "    t = s - a\n",
    "    e = (a - (s-t)) + (b-t)\n",
    "    (s,e)\n",
    "end\n",
    "TwoSum(1e16, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c33e169-e9db-441d-9633-d96b5f8e2626",
   "metadata": {},
   "source": [
    "_AccurateArithmetic.jl_ does implement TwoSum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee1ca734-1cc6-4a53-8661-c5d3eefc7d15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0e16, 1.0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AccurateArithmetic.two_sum(1e16,1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcd49a8-13c6-40ba-9350-0249b9e4eb69",
   "metadata": {},
   "source": [
    "They also implement other sum algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6675bbdf-5d8f-43e9-a020-1504a8590f4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [0.1 for i in 1:1000];0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b58673de-c09c-47ff-ac09-ab7d54902a0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100.0, 100.00000000000004)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_oro(x), sum_naive(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71758e8e-d5cc-41a8-b04f-40d4b79e9f1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.4",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
