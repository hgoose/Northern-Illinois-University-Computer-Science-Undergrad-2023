\documentclass{report}

\input{~/dev/latex/template/preamble.tex}
\input{~/dev/latex/template/macros.tex}

\title{\Huge{}}
\author{\huge{Nathan Warner}}
\date{\huge{}}
\fancyhf{}
\rhead{}
\fancyhead[R]{\itshape Warner} 
\fancyhead[L]{\itshape\leftmark}  
\cfoot{\thepage}
\renewcommand{\headrulewidth}{0pt} 
%\pagestyle{fancy}
%\fancyhf{}
%\lhead{Warner \thepage}
%\rhead{}
% \lhead{\leftmark}
%\cfoot{\thepage}
%\setborder
% \usepackage[default]{sourcecodepro}
% \usepackage[T1]{fontenc}

% Change the title
\hypersetup{
    pdftitle={Exam 1 Study Guide}
}

\begin{document}
    % \maketitle
        \begin{titlepage}
       \begin{center}
           \vspace*{1cm}
    
           \textbf{Numerical Linear Algebra Exam 1 Study Guide}
    
           \vspace{0.5cm}
            
                
           \vspace{1.5cm}
    
           \textbf{Nathan Warner}
    
           \vfill
                
                
           \vspace{0.8cm}
         
           \includegraphics[width=0.4\textwidth]{~/niu/seal.png}
                
           Computer Science \\
           Northern Illinois University\\
           United States\\
           
                
       \end{center}
    \end{titlepage}
    \tableofcontents
    \pagebreak 
    \unsect{Chapter one: Gaussian Elimination and variants}
    \subsection{Concepts}
    \begin{itemize}
        \item \textbf{Matrix multiplication}: Know how to do matrix-vector, and matrix-matrix multiplication.
        \item \textbf{Block decompositions}
        \item \textbf{Systems of linear equations}
        \item \textbf{Singularity, properties of nonsingular matrices}
        \item \textbf{Triangular systems}
        \item \textbf{Properties of triangular matrices / systems}
        \item \textbf{Forward and backward substitution, flop counts}
        \item \textbf{Positive definite matrices / systems}
        \item \textbf{Properties of positive definite matrices}
        \item \textbf{Principal submatrices, leading principal submatrices, principal minors}
        \item \textbf{Cholesky decomposition and the cholesky factor}
        \item \textbf{Algorithms to compute the Cholesky factor and flop counts}
        \item \textbf{Cholesky factor in a diagonal matrix}
        \item \textbf{Banded matrix}
        \item \textbf{Column envelope}
        \item \textbf{Envelope of Choleksy factor}
        \item \textbf{Elementary operations on a system}
        \item \textbf{$LU$ decomposition without row interchanges (and criterion for this to be possible)}
        \item \textbf{Algorithms to find $LU$ decomposition without row interchanges and flop counts}
        \item \textbf{Partial pivoting}
        \item \textbf{Permutation matrix}
        \item \textbf{Gaussian elimination with partial pivoting}
        \item \textbf{$LU$ decomposition with partial pivoting}

    \end{itemize}

    \bigbreak \noindent 
    \subsection{Definitions}
    \begin{itemize}
        \item \textbf{Positive definite matrix}: A matrix $A$ is \textbf{positive definite} provided that the following two conditions are satisfied
            \begin{enumerate}
                \item $A$ is symmetric. That is, $A = A^{\top} $
                \item $x^{\top}Ax > 0 $ for all $x\ne 0$
            \end{enumerate}
        \item \textbf{Banded matrix, lower and upper bandwidths}: A banded matrix is a sparse matrix whose nonzero entries are confined to a diagonal band, consisting of the main diagonal and a fixed number of diagonals on either side of it.
            \bigbreak \noindent 
            Let $A \in \mathbb{R}^{m \times n}$.  
            Then $A$ is called a \textbf{banded matrix} if there exist nonnegative integers $p, q$ (called the \emph{lower} and \emph{upper bandwidths}) such that
            \[
                a_{ij} = 0 \quad \text{whenever } i - j > p \text{ or } j - i > q.
            \]
            \begin{itemize}
                \item The \emph{lower bandwidth} $p$ is the number of subdiagonals (below the main diagonal) that may contain nonzero entries.
                \item The \emph{upper bandwidth} $q$ is the number of superdiagonals (above the main diagonal) that may contain nonzero entries.
            \end{itemize}
            The \emph{total bandwidth} is sometimes defined as $p + q + 1$, counting the main diagonal as well.
        \item \textbf{Column envelope}: The column envelope of $A $ is the set of indices $(i,j)$ in the upper triangular part of $A$ (including the main diagonal). Define
            \begin{align*}
            \text{colenv}\{A\} = \{(i,j)\} :\; i \leq j \text{ and } a_{kj} \ne 0 \text{ for } k \leq i\}
            \end{align*}




    \end{itemize}

    \bigbreak \noindent 
    \subsection{Properties}
    \begin{itemize}
        \item \textbf{Singularity}: A \textbf{singular matrix} is a square matrix that does not have an inverse.
            \bigbreak \noindent 
            A \textbf{nonsingular} matrix is a square matrix that does have an inverse.
            \bigbreak \noindent 
            The following are equivalent, if any one holds, they all hold
            \begin{itemize}
                \item $Ax = b$ has a unique solution
                \item $\det(A)\ne 0$
                \item $A^{-1}$ exists
                \item There is no nonzero vector $y \in \mathbb{R}^{m}$ such that $Ay=0 $
                \item The columns of $A$ are linearly independent
                \item The rows of $A$ are linearly independent
                \item Given any vector $b$, there is exactly one vector $x$ such that $Ax=b$
            \end{itemize}
            If any one of the following are true, they all are true, and $A$ is nonsingular

        \item \textbf{Properties of positive definite (p.d) matrices}:
            \begin{enumerate}
                \item If $A$ is p.d then $A$ is \textit{nonsingular}
                    \bigbreak \noindent 
                    \textbf{Note:} Since $A$ is nonsingular there is no $y \in \mathbb{R}^{n}$, $y\ne 0$ such that $Ay = 0$
                \item If $A = M^{\top}M$ for some $M$ nonsingular than $A$ is p.d
                \item If $A$ is p.d than $\det(A) > 0$
                \item If $A$ is p.d then all principal submatrices are p.d
                \item If $A$ is p.d then $a_{ii}>0$ for $i=1,2,...,n$. So, if any $a_{ii} \leq 0$, $A$ is not p.d.
                \item $A$ is p.d if and only if all leading principal minors are positive
                \item $A$ is p.d if and only if there exists a unique upper triangular matrix $R$ such that $A = R^{\top}R$ (Cholesky factorization described below)
                \item $A$ is p.d if and only if all eigenvalues of $A$ are positive
                    \bigbreak \noindent 
                    Recall that $\lambda$ is an eigenvalue of $A$ if there exists $x_{\lambda} \ne 0$ such that $Ax_{\lambda}  = \lambda x_{\lambda}$
            \end{enumerate}
     \item \textbf{Properties of a permutation matrix}:
        \begin{enumerate}
            \item \textbf{Orthogonal:} $P^{T} = P^{-1} $
            \item \textbf{Determinant:}: $\det(P)=\pm 1$, depending on whether the permutation is even or odd.
            \item \textbf{Action on vectors:}: $Px$ permutes the coordinates of $x$
            \item \textbf{Action on matrices:} Left multiplication permutes rows; right multiplication permutes columns.
        \end{enumerate}
           \textbf{Note:} Property two is a key property.

    \end{itemize}

    \bigbreak \noindent 
    \subsection{Theorems / Propositions}
    \begin{itemize}
                \item \textbf{Theorem 1.4.7 (\textit{Cholesky Decomposition Theorem})}: Let $A$ be positive definite. Then $A$ can be decomposed in exactly one way into a product
            \begin{align*}
                A = R^{\top}R
            \end{align*}
            such that $R$ is upper triangular and has all main diagonal entries $r_{ii}$ positive. $R$ is called the Cholesky factor of $A$.
        \item \textbf{Theorem}: Let $A \in \mathbb{R}^{n\times n}$ be nonsingular. Then, we can solve the system $Ax = b$, $b \in \mathbb{R}^{n}$ using Gaussian Elimination without row interchanges if and only if all landing principal sub-matrices of $A$ are nonsingular.
        \item \textbf{Theorem 1.7.19 \textit{($LU$ Decomposition Theorem)}}: Let $A$ be an $n\times n$ matrix whose leading principal submatrices are all nonsingular. Then, $A$ can be decomposed in exactly one way into a product $A = LU$ such that $L$ is unit lower triangular and $U$ is upper triangular.
      
    \end{itemize}

    \bigbreak \noindent 
    \subsection{Algorithms / Flop counts}
    \begin{itemize}
        
    \end{itemize}


    \pagebreak 
    \unsect{Chapter 2: Sensitivity of linear systems}
    \subsection{Concepts}
    \begin{itemize}
        \item \textbf{Norms}
        \item \textbf{Vector norms}
        \item \textbf{Properties of vector norms}
        \item \textbf{Cauchy-schwarz inequality}
        \item \textbf{Entrywise matrix norms}
        \item \textbf{Properties of matrix norms}
        \item \textbf{Induced matrix norms}
        \item \textbf{Properties of induced matrix norms}
        \item \textbf{Induced matrix-norms special cases}
        \item \textbf{Numerical error when solving systems}
        \item \textbf{Residual vector}
        \item \textbf{Relative error}
        \item \textbf{Pertubation}
        \item \textbf{Condition number}
        \item \textbf{Properties of the condition number}
        \item \textbf{Relative error bound I}
        \item \textbf{Relative error bound II}
        \item \textbf{Condition for singularity in $\hat{A}$}
        \item \textbf{$\kappa$, well-conditioned, ill-conditioned}
    \end{itemize}

    \bigbreak \noindent 
    \subsection{Definitions}
    \begin{itemize}
        \item \textbf{Norm:} A norm is an operation $\norm{\cdot }:\; \mathbb{R}^{n} \to \mathbb{R}_{+}:\; x \to \norm{x} \geq 0$  that satisfies
            \begin{enumerate}
                \item $\norm{x} = 0 \iff x = 0$
                \item $\norm{\alpha x} = \left\lvert \alpha \right\rvert \norm{x} $
                \item $\norm{x+y} \leq \norm{x} + \norm{y} $ (triangle inequality)
            \end{enumerate}
        \item \textbf{Euclidean norm (2-norm)}: The standard Euclidean distance. For $x \in \mathbb{R}^{n}$,
            \begin{align*}
                \norm{x}_{2} = \sqrt{x_{1}^{2} + x_{2}^{2} + ... + x_{n}^{2}}
            .\end{align*}
        \item \textbf{Manhattan norm (1-norm)}: Denoted $L^{1} $, and also called \textbf{Taxicab norm}. For $x \in \mathbb{R}^{n} $,
            \begin{align*}
                \norm{x}_{1} = \left\lvert x_{1} \right\rvert + \left\lvert x_{2} \right\rvert + ... + \left\lvert x_{n} \right\rvert
            .\end{align*}
        \item \textbf{$L$-Infinity (max) norm ($\infty$-norm)}: Denoted $L^{\infty}$. for $x\in \mathbb{R}^{n}$,
            \begin{align*}
                \norm{x}_{\infty} = \max_{1 \leq i \leq n} \left\lvert x_{i} \right\rvert = \max\{\left\lvert x_{1} \right\rvert, \left\lvert x_{2} \right\rvert, ..., \left\lvert x_{n} \right\rvert\} 
            .\end{align*}
        \item \textbf{$p$-norm}: In $\mathbb{R}^{n}$, A more general norm is
            \begin{align*}
                \norm{x}_{p} = \left(\sum_{i=1}^{n} \left\lvert x_{i} \right\rvert^{p}\right)^{\frac{1}{p}} = \left(\left\lvert x_{1}^{p} \right\rvert + \left\lvert x_{2} \right\rvert^{p} + ... + \left\lvert x_{n} \right\rvert^{p}\right)^{\frac{1}{p}}
            \end{align*}
            for $1 \leq p < \infty $. The general $p$-norm satisfies all three properties of a norm only when $p \geq 1$. For smaller $p$, the triangle inequality does not hold.
        \item \textbf{Induced (operator) matrix norms}: For all $A\in \mathbb{R}^{n\times m}$, we define
            \begin{align*}
                \norm{A}_{p} := \max_{x\in\mathbb{R}^{n} \setminus \{0\}} \frac{\norm{Ax}_{p}}{\norm{x}_{p}}
           .\end{align*}
         \item \textbf{Induced matrix norms special cases}:
            \begin{center}
                \begin{tabular}{p{1cm}|p{5cm}|p{5cm}}
                    \toprule
                    $p$ & \textbf{Name} & \textbf{Explicit formula} \\
                    \midrule
                    $1$ & Maximum column sum &
                    $\displaystyle \|A\|_{1} = \max_{1 \leq j \leq n} \sum_{i=1}^{m} |a_{ij}|$ \\[3ex]
                    $2$ & Spectral norm &
                    $\displaystyle \|A\|_{2} = \sqrt{\lambda_{\max}(A^{T}A)}$ \\[3ex]
                    $\infty$ & Maximum row sum &
                    $\displaystyle \|A\|_{\infty} = \max_{1 \leq i \leq m} \sum_{j=1}^{n} |a_{ij}|$ \\
                    \bottomrule
                \end{tabular}
            \end{center}
        \item \textbf{Cauchy Schwarz inequality for 2-norm (vector norm)}: states
            \begin{align*}
                \left\lvert x^{T}y \right\rvert \leq \norm{x}_{2}\norm{y}_{2} 
            .\end{align*}
        \item \textbf{Residual vector}: Suppose $Ax=b$ yields $\hat{x}$ via numerical methods, then define the residual vector as
            \begin{align*}
                \hat{r} = b-A\hat{x}
            .\end{align*}
            Which, by the way, implies that
            \begin{align*}
                b = \hat{r} + A\hat{x}
            .\end{align*}
        \item \textbf{Intro to measuring solutions}: Consider a problem $(P)$, where
            \begin{align*}
                (P):\; Ax = b
            .\end{align*}
            Numerical techniques yields a solution $\hat{x}$, which may or may not be the true solution to $(P)$. Let $x$ be the true solution to the system. So, $x$ solves $Ax = b$. 
            \bigbreak \noindent 
            We want to measure the distance between the numerical solution $\hat{x} $ and the true solution $x$, we hope that the numerical solution $\hat{x}$ is close to $x$. If the distance is small, then $\hat{x}$ is a good solution.
        \item \textbf{Relative error}: The relative error in $\hat{x}$ is given by
            \begin{align*}
                \frac{\norm{\hat{x} - x}}{\norm{x}} = \frac{\norm{\delta x}}{\norm{x}}
            \end{align*}
            where $\hat{x} = x + \delta  x $, which implies $x = \hat{x} - \delta  x $.
        \item \textbf{Perturbation}: If numerical methods to solve a linear system $Ax = b$ yields $\hat{x}$, then $\hat{x}$ solves $\hat{A}\hat{x} = \hat{b}$. Note that it is possible for $\hat{A} = A$ or $\hat{b} = b $. If both $\hat{A} = A$ and $\hat{b} = b$, then $\hat{x} = x$.
            \bigbreak \noindent 
            $\hat{A}$ and $\hat{b}$ are called perturbed if they are modified versions of the original. If $\hat{A}$ is a perturbed matrix $A$, and $\hat{b}$ is a perturbed vector $b$, then
            \begin{align*}
                \hat{A} &= A + \delta A, \\
                \hat{b} &= b + \delta  b
            .\end{align*}
        \item \textbf{Condition number}: We define the condition number
            \begin{align*}
                \kappa(A) &= \norm{A^{-1}}\norm{A},
            \end{align*}
            which measures how sensitive the system is to perturbations in $A$ or $b$, and how close $A$ is to being singular.
            

    \end{itemize}

    \bigbreak \noindent 
    \subsection{Properties}
    \begin{itemize}
        \item \textbf{Properties of vector norms}: 
            \begin{enumerate}
                \item $\norm{x} \geq 0$
                \item $\norm{x} = 0 \iff x = 0 $
                \item $\norm{\alpha x} = \left\lvert \alpha \right\rvert \norm{x} $
                \item $\norm{x + y} \leq \norm{x} + \norm{y} $ (Triangle inequality)
            \end{enumerate}

        \item \textbf{Properties of matrix norms}: Matrix norms satisfy the three required properties of norms.
            \begin{enumerate}
                \item $\norm{A} = 0 \iff A = 0 $
                \item $\norm{\alpha A} = \left\lvert \alpha \right\rvert \norm{A} $
                \item $\norm{A + B} \leq \norm{A} + \norm{B} $ (Triangle inequality)
            \end{enumerate}
        \item \textbf{Properties of induced matrix norms}
            \begin{itemize}
                \item \textbf{Sub-multiplicativity}: $\norm{AB}_{p} \leq \norm{A}_{p}\norm{B}_{p} $
                \item \textbf{Consistency}: $\norm{Ax}_{p} \leq \norm{A}_{p}\norm{x}_{p} $
                \item \textbf{Normalization}: $\norm{I}_{p} = 1 $
            \end{itemize}
            These are what entrywise ("flattened") norms lack.
        \item \textbf{Properties of the condition number}: Let $A$ be a matrix, and $\kappa(A)$ be the condition number that measures the system $Ax = b$. The following two properties hold
            \begin{enumerate}
                \item $\kappa(A) \geq 1$
                \item $\kappa(I) = 1$
                \item $\kappa(A) = \kappa(A^{-1}) $
            \end{enumerate}


    \end{itemize}

    \bigbreak \noindent 
    \subsection{Theorems / Propositions}
    \begin{itemize}
        \item \textbf{Theorem \textit{(Relative Error Bound I)}}: Let $A$ be nonsingular, $b \ne 0$, and $Ax = b$. If $A(x + \delta  x) = b + \delta  b$, then
            \begin{align*}
                \frac{\norm{ \delta  x}}{\norm{ x}} \leq \kappa(A) \frac{\norm{\delta  b }}{\norm{ b}}
            \end{align*}
        \item \textbf{Theorem (\textit{Singularity of perturbed $A$})}: If
            \begin{align*}
                \frac{\norm{\delta A}}{\norm{A}} < \frac{1}{\kappa(A)}
            \end{align*}
            then $A + \delta  A$ is nonsingular.
        \item \textbf{Theorem \textit{(Relative error bound II)}}: Let $A$ be nonsingular, $b\ne 0$, and $Ax=b$. If $(A + \delta A)(x + \delta x) = b$, and 
            \begin{align*}
                \frac{\norm{\delta  A}}{\norm{A}} < \frac{1}{\kappa(A)},
            \end{align*}
            then
            \begin{align*}
                \frac{\norm{\delta x}}{\norm{x}} \leq \frac{\kappa(A)\frac{\norm{\delta A}}{\norm{A}}}{1-\kappa(A)\frac{\norm{\delta A}}{\norm{A}}}
            .\end{align*}
        \item \textbf{Theorem \textit{(Relative error bound III)}}:  Let $A$ be nonsingular, $b\ne 0$, and $Ax=b$. If $(A + \delta A)(x + \delta x) = b + \delta  b$, and 
            \begin{align*}
                \frac{\norm{\delta  A}}{\norm{A}} < \frac{1}{\kappa(A)},
            \end{align*}
            then
            \begin{align*}
                \frac{\norm{\delta x}}{\norm{x}} \leq \frac{\kappa(A)\left(\frac{\norm{\delta A}}{\norm{A}} + \frac{\norm{\delta b}}{\norm{b}}\right)}{1-\kappa(A)\frac{\norm{\delta A}}{\norm{A}}}
            .\end{align*}

    \end{itemize}

    \bigbreak \noindent 
    \subsection{Algorithms}
    \begin{itemize}
        \item \textbf{Iterative approach to improve $\hat{x}$}: Suppose for a system $Ax = b$ numerical techniques yields an approximation $\hat{x}_{1}$. Then, the residual vector $\hat{r}_{1} = b - A\hat{x}_{1}$, which implies that $b = \hat{r}_{1} + A\hat{x}_{1}$. If $\hat{x}_{2}$ is a different approximation, where $\hat{x}_{2} = \hat{x}_{1} + \delta \hat{x}_{1}$, then
            \begin{align*}
                A\hat{x}_{2} &= b = \hat{r}_{1} + A\hat{x}_{1} \\
                \implies A(\hat{x}_{1} + \delta \hat{x}_{1}) &= \hat{r}_{1} + A\hat{x}_{1} \\
                \implies A\hat{x}_{1} + A\delta\hat{x}_{1} &= \hat{r}_{1} + A\hat{x}_{1} \\
                \implies A\delta\hat{x}_{1} &= \hat{r}_{1}  
            .\end{align*}
            So, we solve the system for $\delta \hat{x}_{1}$. Then, since $\hat{x}_{2} = \hat{x}_{1} + \delta \hat{x}_{1}$, we see that we need to update the first solution by adding the computed $\delta \hat{x}_{1} $.
            \bigbreak \noindent 
            In general, if $\hat{x}_{i}$ is the $i^{\text{th}}$ numerical solution to $Ax = b$, and $\hat{r}_{i}$ is the residual vector to the $i^{\text{th}}$ solution, then 
            \begin{align*}
                \hat{x}_{i+1} = \hat{x}_{i} + A^{-1}\hat{r}_{i}
            .\end{align*}
            In practice, we don't compute $A^{-1}\hat{r}_{i}$, as we know that this is an expensive task. Instead, we solve the system correction system
            \begin{align*}
                A\delta\hat{x}_{i} = \hat{r}_{i} 
            .\end{align*}
            In exact arithmetic, 
            \begin{align*}
                A\delta \hat{x} &= b - A\hat{x} \\
                \implies \delta \hat{x} &= A^{-1}\left(b - A\hat{x}\right) \\
                                        &= A^{-1}b - A^{-1}A\hat{x} \\
                                        &= x - \hat{x}
            .\end{align*}
            So, 
            \begin{align*}
                \hat{x}_{\text{new}} &= \hat{x} + \delta \hat{x} = \hat{x} + x - \hat{x} = x
            .\end{align*}
            Thus, in exact arithmetic, we converge to the true solution in one step.
            \bigbreak \noindent 
            In practice, computations are done in floating-point arithmetic, so both the residual and the correction are computed approximately. 
            Let 
            \[
                A\,\delta\hat{x} = r + \delta r,
            \]
            where $\delta r$ represents rounding or truncation errors. When we update
            \[
                \hat{x}_{\text{new}} = \hat{x} + \delta\hat{x},
            \]
            we hope that the new residual
            \[
                r_{\text{new}} = b - A\hat{x}_{\text{new}}
            \]
            is smaller than the previous residual. Each iteration ideally improves the approximation because
            \begin{align*}
                \delta \hat{x} \approx A^{-1}(b - A\hat{x}) = x - \hat{x}
            .\end{align*}
            When floating-point errors are small enough relative to the conditioning of $A$, this correction moves $\hat{x} $ closer to $x$. But, if $A$ is ill-conditioned, the corrections may no longer reduce the error — in fact, they can make it worse.
    \end{itemize}



    
\end{document}
