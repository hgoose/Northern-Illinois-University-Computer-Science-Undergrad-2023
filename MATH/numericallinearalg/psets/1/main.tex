 \documentclass{report}
 
 \input{~/dev/latex/template/preamble.tex}
 \input{~/dev/latex/template/macros.tex}
 
 \title{\Huge{}}
 \author{\huge{Nathan Warner}}
 \date{\huge{}}
 \fancyhf{}
 \rhead{}
 \fancyhead[R]{\itshape Warner} % Left header: Section name
 \fancyhead[L]{\itshape\leftmark}  % Right header: Page number
 \cfoot{\thepage}
 \renewcommand{\headrulewidth}{0pt} % Optional: Removes the header line
 %\pagestyle{fancy}
 %\fancyhf{}
 %\lhead{Warner \thepage}
 %\rhead{}
 % \lhead{\leftmark}
 %\cfoot{\thepage}
 %\setborder
 % \usepackage[default]{sourcecodepro}
 % \usepackage[T1]{fontenc}
 
 % Change the title
 \hypersetup{
     pdftitle={}
 }

 \geometry{
  left=1in,
  right=1in,
  top=1in,
  bottom=1in
}
 
 \begin{document}
     % \maketitle
     %     \begin{titlepage}
     %    \begin{center}
     %        \vspace*{1cm}
     % 
     %        \textbf{}
     % 
     %        \vspace{0.5cm}
     %         
     %             
     %        \vspace{1.5cm}
     % 
     %        \textbf{Nathan Warner}
     % 
     %        \vfill
     %             
     %             
     %        \vspace{0.8cm}
     %      
     %        \includegraphics[width=0.4\textwidth]{~/niu/seal.png}
     %             
     %        Computer Science \\
     %        Northern Illinois University\\
     %        United States\\
     %        
     %             
     %    \end{center}
     % \end{titlepage}
     % \tableofcontents
    \pagebreak \bigbreak \noindent
    Nate Warner \ \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad  MATH 434 \quad  \quad \quad \quad \quad \quad \quad \quad \quad \ \ \quad \quad Fall 2025
    \begin{center}
        \textbf{Problem set 1 - Due: Sunday, September 28}
    \end{center}
    \bigbreak \noindent 
    \begin{mdframed}
        1.2.4. Prove that if $A^{-1}$ exists, then there can be no nonzero $y$ for which $Ay = 0$
    \end{mdframed}
    \bigbreak \noindent 
    \textbf{\textit{Proof.}} Assume that $A \in \mathbb{R}^{n\times n}$, and $A^{-1}$ exists. Assume for the sake of contradiction that there exists $y \in \mathbb{R}^{n}$, $y\ne 0$ such that $Ay=  0$. So,
    \begin{align*}
        Ay &= 0, \\
        \implies A^{-1}Ay &= A^{-1}0 \\
        \implies Iy &= 0 \\
        \implies y &= 0
        % Ax &= 0
    \end{align*}
    But, $y\ne 0$, a contradiction. Therefore, if $A^{-1}$ exists, then there can be no nonzero $y$ for which $Ay = 0$. $\endpf $

    % We know that $Ax = 0$ for $x \in \mathbb{R}^{n}$, $x = 0$. So,
    % Thus,
    % \begin{align*}
    %     Ay &= Ax \\
    %     \implies A^{-1}Ay &= A^{-1}Ax \\
    %     \implies Iy &= Ix \\
    %     \implies y &= x
    % \end{align*}


    \bigbreak \noindent 
    \begin{mdframed}
        1.2.5. Prove that if $A^{-1}$ exists, then $\det(A) \ne 0$.
    \end{mdframed}
    \bigbreak \noindent 
    \textbf{\textit{Proof.}} Assume that $A\in \mathbb{R}^{n\times n}$, and $A^{-1}$ exists.
    \bigbreak \noindent 
    Suppose for the sake of contradiction that $\det(A) = 0$. We know that $AA^{-1} = I$, and that $\det(AB) = \det(A)\det(B)$. So,
    \begin{align*}
        AA^{-1} &= I \\
        \implies \det(AA^{-1}) &= \det(I) \\
        \implies \det(A)\det(A^{-1}) &= 1 \\
        \implies 0\det(A^{-1}) &= 1 \\
        \implies 0 &= 1
    \end{align*}
    A contradiction. Therefore, if $A^{-1}$ exists, then $\det(A) \ne 0$ $\endpf$

    \bigbreak \noindent 
    \begin{mdframed}
        1.2.11. Check that the equations in Example 1.2.10 are correct. Check that the coefficient matrix of the system is nonsingular
    \end{mdframed}
    \bigbreak \noindent 
    The stiffness matrix $A$ is given as
    \begin{align*}
        A = \begin{bmatrix}
            8 & -4 & 0 \\
            -4 & 8 & -4 \\
            0 & -4 & 8
        \end{bmatrix}
    .\end{align*}
    The system $Ax = b$ is given as
        \begin{align*}
        \begin{bmatrix} 8 & -4 & 0 \\ -4 & 8 & -4 \\ 0 & -4 & 8 \end{bmatrix}
        \begin{bmatrix} x_{1} \\ x_{2} \\x_{3} \end{bmatrix}
         = 
         \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}
    .\end{align*}
    So, the system of linear equations is 
    \begin{align*}
        8x_{1} -4x_{2} &= 1 \\
        -4x_{1} + 8x_{2} - 4x_{3} &= 2\\
        -4x_{2} + 8 x_{3} &= 3
    \end{align*}
    Since the second equation is derived, we only need to check the first and third equations.
    \bigbreak \noindent 
    For cart one, the spring exerts a leftward force of $-4 \frac{N}{m}$, and is stretched by the amount that cart one moves, which is given by $x_{1} \; m$. So, the leftward force is $-4x_{1}\; N$. On the right, there is a force given by the spring of $4 \frac{N}{m}$, and is stretched $(x_{2} - x_{1})\; m$ meters. So, the rightward force is $4(x_{2} - x_{1}) \; N$. Finally, an external force of $1N$ is applied to the cart, so the equilibrium equation for cart one is
    \begin{align*}
        -4x_{1} + 4(x_{2} - x_{1}) + 1 &= 0, \\
        \implies -4x_{1} + 4x_{2} - 4x_{1} + 1 &= 0, \\
        \implies -8x_{1} + 4x_{2} + 1 &= 0, \\
        \implies 8x_{1} -4x_{2} &= 1
    .\end{align*}
    Which is precisely the first equation in the given system.
    \bigbreak \noindent 
    Regarding the third cart, there is a leftward force from the spring $-4 \; \frac{N}{m}$, and the spring is stretched $ (x_{3} - x_{2})\; m$, so the leftward force is $-4(x_{3} - x_{2}) \; N $. On the right, there is a force from the spring $4 \; \frac{N}{m}$, and the spring is compressed $-x_{3} \; m$, so the rightward force is $-4x_{3} \; N$. Thus, the equilibrium equation for the third cart is
    \begin{align*}
        -4(x_{3} - x_{2}) - 4x_{3} + 3 &= 0, \\
        \implies -4x_{3} + 4x_{2} -4x_{3} + 3 &= 0, \\
        \implies -8x_{3} + 4x_{2} + 3 &= 0, \\
        \implies 8x_{3} - 4x_{2} &= 3
    .\end{align*}
    Which is precisely the third equation in the given system.
    \bigbreak \noindent 
    Therefore, the equations are verified.
    \bigbreak \noindent 
    We can check that the coefficient matrix of the system is nonsingular by computing the determinant and verifying that it is nonzero.
    \begin{align*}
        \det\left( \begin{bmatrix} 8 & -4 & 0 \\ -4 & 8 & -4 \\ 0 & -4 & 8 \end{bmatrix}\right) &= 8(8(8) - (-4)(-4)) +4(-4(8) - (-4)(0)) + 0 (-4(-4)-8(0)) \\
                               &=256 \ne 0
    .\end{align*}
    So, $A$ is nonsingular.


    \pagebreak \bigbreak \noindent 
    \begin{mdframed}
        1.3.4. Use pencil and paper to solve the system
        \[
            \begin{bmatrix}
                2 & 0 & 0 & 0 \\
                -1 & 2 & 0 & 0 \\
                3 & 1 & -1 & 0 \\
                4 & 1 & -3 & 3
            \end{bmatrix}
            \begin{bmatrix}
                y_{1} \\
                y_{2} \\
                y_{3} \\
                y_{4}
            \end{bmatrix}
            =
            \begin{bmatrix}
                2 \\
                3 \\
                2 \\
                9
            \end{bmatrix}
        \]
        by forward substitution
    \end{mdframed}
    \bigbreak \noindent 
    We solve the system by forward substitution
    \begin{align*}
        2y_{1} &= 2 \implies y_{1} = 1, \\
        -y_{1} + 2y_{2} &= 3 \implies y_{2} = \frac{3+y_{1}}{2} = \frac{3+1}{2} = 2, \\
        3y_{1} + y_{2} - y_{3} &= 2 \implies y_{3} = \frac{2-3y_{1}-y_{2}}{-1} = \frac{2-3(1)-2}{-1} = 3, \\
        4y_{1} + y_{2} - 3y_{3} + 3y_{4} &= 9 \implies y_{4} = \frac{9-4y_{1}-y_{2}+3y_{3}}{3} = \frac{9-4(1)-2+3(3)}{3} = 4
    .\end{align*}
    Thus, $y = \begin{pmatrix} 1 \\ 2 \\ 3 \\ 4 \end{pmatrix} $. We verify multiplying $Ay$. We have,
    \begin{align*}
        \begin{bmatrix}
            2 & 0 & 0 & 0 \\
            -1 & 2 & 0 & 0 \\
            3 & 1 & -1 & 0 \\
            4 & 1 & -3 & 3
        \end{bmatrix}
        \begin{bmatrix}
            1 \\ 2 \\ 3 \\4
        \end{bmatrix}
        &= \begin{bmatrix}
            2(1) \\
            -1(1) + 2(2) \\
            3(1) + 1(2)-1(3) \\
            4(1) + 1(2) -3(3) + 3(4)
        \end{bmatrix}
        = \begin{bmatrix}
            2 \\ 3 \\ 2 \\ 9
        \end{bmatrix}
    .\end{align*}

    \pagebreak \bigbreak \noindent 
    \begin{mdframed}
        1.3.11. Use column-oriented forward substitution to solve the system from Exercise 1.3.4.
    \end{mdframed}
    \bigbreak \noindent 
    \begin{remark}
       \textit{(Column oriented forward substitution).} 
        Suppose we have $Lx = b$ when $L$ is lower triangular, we split the matrix into the following blocks
            \begin{align*}
                \begin{bmatrix}
                    \ell_{11} & 0 \\
                    \hat{\ell} & \hat{L}
                \end{bmatrix}
                \begin{bmatrix}
                    x_{1} \\ \hat{x} 
                \end{bmatrix}
                = \begin{bmatrix}
                    b_{1} \\ \hat{b}
                \end{bmatrix}
            .\end{align*}
            With $\hat{\ell} \in \mathbb{R}^{n-1}$, $\hat{L} \in \mathbb{R}^{n-1 \times n-1} $, $\hat{x} \in \mathbb{R}^{n-1}$, $\ell_{11}, x_{1}, b_{1} \in \mathbb{R}$. Note that $\hat{L}$ is also lower triangular.
            \begin{enumerate}
                \item Compute $x_{1} = \frac{b_{1}}{\ell_{11}} $
                \item Compute $\hat{b} - \hat{\ell}x_{1} = \tilde{b} \in \mathbb{R}^{n-1} $
                \item Find $\hat{L}x = \tilde{b} $
                \item Run the algorithm on $\hat{L}$, $\tilde{b}$. That is, $\text{Alg}(\hat{L}, \tilde{b}) $
            \end{enumerate}
            The recursive column oriented forward substitution algorithm requires $\mathcal{O}(n^{2})$ flops.
        \end{remark}
        \bigbreak \noindent 
        Alg$(L,b)$: The system is
        \[
            \begin{bmatrix}
                2 & 0 & 0 & 0 \\
                -1 & 2 & 0 & 0 \\
                3 & 1 & -1 & 0 \\
                4 & 1 & -3 & 3
            \end{bmatrix}
            \begin{bmatrix}
                y_{1} \\
                y_{2} \\
                y_{3} \\
                y_{4}
            \end{bmatrix}
            =
            \begin{bmatrix}
                2 \\
                3 \\
                2 \\
                9
            \end{bmatrix}
        \]
    We have $\ell_{11} = 2$, $b_{1} = 2$, and 
    \begin{align*}
        \hat{L} &= \begin{bmatrix} 2 & 0 & 0 \\1 & -1 & 0 \\ 1 & -3 & 3 \end{bmatrix} \in \mathbb{R}^{3\times 3}, \\
        \hat{\ell} &= \begin{bmatrix} -1 \\ 3 \\ 4\end{bmatrix} \in \mathbb{R}^{3}, \\
        \hat{y} &= \begin{bmatrix} y_{2} \\ y_{3} \\ y_{4} \end{bmatrix}, \in \mathbb{R}^{3}\\
        \hat{b} &= \begin{bmatrix} 3 \\ 2 \\ 9 \end{bmatrix} \in \mathbb{R}^{3}
    .\end{align*}
    By step one, we have 
    \begin{align*}
        y_{1} = \frac{2}{2} = 1
    .\end{align*}
    By step two, we have
    \begin{align*}
        \tilde{b} &= \begin{bmatrix} 3 \\ 2 \\ 9 \end{bmatrix}
        - \begin{bmatrix} -1 \\ 3 \\ 4 \end{bmatrix} \cdot  1 \
        = \begin{bmatrix} 3 + 1 \\ 2 -3(1) \\ 9-4(1) \end{bmatrix}
        = \begin{bmatrix} 4 \\ -1 \\ 5 \end{bmatrix}
    .\end{align*}
    Alg$(\hat{L}, \tilde{b})$: The system is
        \[
            \begin{bmatrix}
                2 & 0 & 0 \\
                1 & -1 & 0 \\
                1 & -3 & 3
            \end{bmatrix}
            \begin{bmatrix}
                y_{2} \\
                y_{3} \\
                y_{4}
            \end{bmatrix}
            =
            \begin{bmatrix}
                4 \\ -1 \\ 5
            \end{bmatrix}
        \]
    We have $\ell_{11} = 2$, $b_{1} = 3$, and 
    \begin{align*}
        \hat{L} &= \begin{bmatrix} -1 & 0 \\ -3 & 3 \end{bmatrix} \in \mathbb{R}^{2\times 2}, \\
        \hat{\ell} &= \begin{bmatrix} 1 \\1 \end{bmatrix} \in \mathbb{R}^{2}, \\
        \hat{y} &= \begin{bmatrix} y_{3} \\ y_{4} \end{bmatrix} \in \mathbb{R}^{2},\\
        \hat{b} &= \begin{bmatrix} 2 \\ 9 \end{bmatrix} \in \mathbb{R}^{2}
    .\end{align*}
    By step one, 
    \begin{align*}
        y_{2} &= \frac{4}{2} = 2
    \end{align*}
    By step two we have
    \begin{align*}
        \tilde{b} &= \begin{bmatrix} -1 \\ 5 \end{bmatrix} - \begin{bmatrix} 1 \\ 1 \end{bmatrix} \cdot 2 = \begin{bmatrix} -1 -2 \\ 5-2 \end{bmatrix} = \begin{bmatrix} -3 \\ 3 \end{bmatrix}
    \end{align*}
    Alg$(\hat{L}, \tilde{b})$: The system is
        \[
            \begin{bmatrix}
                -1 & 0 \\
                -3 & 3
            \end{bmatrix}
            \begin{bmatrix}
                y_{3} \\
                y_{4}
            \end{bmatrix}
            =
            \begin{bmatrix}
                -3 \\ 3
            \end{bmatrix}
        \]
    We have $\ell_{11} = -1$, $b_{1} = -3$, and 
    \begin{align*}
        \hat{L} &= \begin{bmatrix} 3 \end{bmatrix} \in \mathbb{R}^{1}, \\
        \hat{\ell} &= \begin{bmatrix} -3 \end{bmatrix} \in \mathbb{R}^{1}, \\
        \hat{y} &= \begin{bmatrix} y_{4} \end{bmatrix} \in \mathbb{R}^{1}, \\
        \hat{b} &= \begin{bmatrix} 3 \end{bmatrix}
    .\end{align*}
    By step one, we have that
    \begin{align*}
        y_{3} &= \frac{-3}{-1} = 3
    .\end{align*}
    By step two, we have that
    \begin{align*}
        \tilde{b} &= \begin{bmatrix} 3 \end{bmatrix} - \begin{bmatrix} -3 \end{bmatrix} \cdot  3 = \begin{bmatrix} 3 - -3(3) \end{bmatrix} = \begin{bmatrix} 12 \end{bmatrix}
    \end{align*}
    Alg $(\hat{L}, \tilde{b})$: The system is 
    \begin{align*}
        \begin{bmatrix} 3 \end{bmatrix} \begin{bmatrix} y_{4} \end{bmatrix} = \begin{bmatrix} 12 \end{bmatrix}
    \end{align*}
    From this final call we have that
    \begin{align*}
        y_{4} = \frac{12}{3} = 4
    \end{align*}
    So, $y = \begin{bmatrix} 1 \\ 2 \\3 \\4 \end{bmatrix} $, the same result as 1.3.4.

    

    \pagebreak \bigbreak \noindent 
    \begin{mdframed}
        1.3.15. Develop the row-oriented version of back substitution. Write pseudocode in the spirit of (1.3.5) and (1.3.13).
    \end{mdframed}
    \bigbreak \noindent 
                Let $A \in \mathbb{R}^{n\times n},\ x \in \mathbb{R}^{n},\ b \in \mathbb{R}^{n}$, with
            \begin{align*}
                A = \begin{bmatrix}
                    a_{11} & a_{12} & \cdots & a_{1n} \\
                    0 & a_{22} & \cdots & a_{2n} \\
                    \vdots & \vdots & \ddots & \vdots \\
                    0 & 0 & \cdots & a_{nn}
                \end{bmatrix},  \quad \;
                x = \begin{bmatrix}
                    x_{1} \\ x_{2} \\ \vdots \\ x_{n}
                \end{bmatrix}, \quad \;
                b = \begin{bmatrix}
                    b_{1} \\ b_{2} \\ \vdots \\ b_{n}
                \end{bmatrix}
            .\end{align*}
            Then, 
            \begin{align*}
                 \begin{bmatrix}
                    a_{11} & a_{12} & \cdots & a_{1n} \\
                    0 & a_{22} & \cdots & a_{2n} \\
                    \vdots & \vdots & \ddots & \vdots \\
                    0 & 0 & \cdots & a_{nn}
                \end{bmatrix}
                \begin{bmatrix}
                    x_{1} \\ x_{2} \\ \vdots \\ x_{n}
                \end{bmatrix} 
                = \begin{bmatrix}
                    b_{1} \\ b_{2} \\ \vdots \\ b_{n}
                \end{bmatrix}
            \end{align*}
            implies
            \begin{align*}
                a_{11}x_{1} + a_{12}x_{2} + \cdots + a_{1n}x_{n} &= b_{1} \\
                a_{22}x_{2} + a_{23}x_{3} + \cdots + a_{2n}x_{n} &= b_{2} \\
                                                                 &\vdots \\
                a_{nn} x_{n} &= b_{n}
            .\end{align*}
            So, 
            \begin{align*}
                x_{1} &= \frac{b_{1} - (a_{12}x_{2} + a_{13}x_{3} + \cdots + a_{1n}x_{n})}{a_{11}} ,\\
                x_{2} &= \frac{b_{2} - (a_{23}x_{3} + a_{24}x_{4} + \cdots + a_{2n}x_{n})}{a_{22}} ,\\
                x_{n} &= \frac{b_{n}}{a_{nn}}
            .\end{align*}
            In general, we have that
            \begin{align*}
                x_{i} &= \frac{b_{i} - \sum_{j=i+1}^{n}a_{ij}x_{j}}{a_{ii}}, \quad i=n,n-1,...,1
            \end{align*}
            The pseudocode for the above algorithm is
            \bigbreak \noindent 
            \begin{jlcode}
                for |$i=n,...,1$|
                    for |$j=i+1,...,n$|
                        |$b[i] = b[i] - A[i,j] \cdot b[j]$|
                    end
                    if |$A[i,i] = 0$|, set error flag, exit
                    |$b[i] = b[i] / A[i,i]$|
                end
            \end{jlcode}



    \pagebreak \bigbreak \noindent 
    \begin{mdframed}
        1.3.16. Develop the column-oriented version of back substitution Write pseudocode in the spirit of (1.3.5) and (1.3.13).
    \end{mdframed}
    \bigbreak \noindent 
    Let $U \in \mathbb{R}^{n\times n}$ be upper triangular, $x \in \mathbb{R}^{n}$, and $ b \in \mathbb{R}^{n}$ which gives the system
    \begin{align*}
       \begin{bmatrix}
           u_{11} & u_{12} & \cdots & u_{1n} \\
           0 & u_{22} & \cdots & u_{2n} \\
           \vdots & \vdots & \ddots & \vdots \\
           0 & 0 & \cdots & u_{nn}
       \end{bmatrix} 
       \begin{bmatrix}
           x_{1} \\ x_{2} \\ \vdots \\ x_{n}
       \end{bmatrix}
        = 
        \begin{bmatrix}
            b_{1} \\ b_{2} \\ \vdots \\ b_{n}
        \end{bmatrix}
    \end{align*}
    Split the system into the following block decomposition
    \begin{align*}
        \begin{bmatrix}
            \hat{U} & u \\
            0^{\top} & u_{nn}
        \end{bmatrix}
        \begin{bmatrix}
            \hat{x} \\ x_{n}
        \end{bmatrix}
        = 
        \begin{bmatrix}
            \hat{b} \\ b_{n}
        \end{bmatrix}
    \end{align*}
    Then,
    \begin{align*}
        \hat{U}\hat{x} + ux_{n} &= \hat{b} \implies \hat{U}\hat{x} = \hat{b} - ux_{n} = \tilde{b}, \\
        u_{nn}x_{n} &= b_{n} \implies x_{n} = \frac{b_{n}}{u_{nn}}
    \end{align*}
    Thus, the column-oriented backward substitution algorithm is defined by the following steps
    \begin{enumerate}
        \item Compute $x_{n} = \frac{b_{n}}{u_{nn}} $
        \item Compute $\tilde{b} = \hat{b} - ux_{n} $
        \item Run the algorithm on $\hat{U}, \tilde{b}$. That is, $\text{Alg}(\hat{U}, \tilde{b})$
    \end{enumerate}
    \bigbreak \noindent 
    The non-recursive pseudocode in the spirit of 1.3.5 and 1.3.13 is
    \bigbreak \noindent 
    \begin{jlcode}
    for |$i=n,...,1$|
        if |$U[i,i] = 0$|, set error flag, exit

        |$b[i] = b[i] / U[i,i]$|

        for |$j = i-1,...,1$|
            |$b[j] = b[j] - U[j,i] \cdot b[i]$|
        end
    end
    \end{jlcode}

    \pagebreak \bigbreak \noindent 
    \begin{mdframed}
        1.3.17. Solve the upper-triangular system
        \[
            \begin{bmatrix}
                3 & 2 & 1 & 0 \\
                0 & 1 & 2 & 3 \\
                0 & 0 & -2 & 1 \\
                0 & 0 & 0 & 4
            \end{bmatrix}
            \begin{bmatrix}
                x_{1} \\
                x_{2} \\
                x_{3} \\
                x_{4}
            \end{bmatrix}
            =
            \begin{bmatrix}
                -10 \\
                10 \\
                1 \\
                12
            \end{bmatrix}
        \]
        (a) by row-oriented back substitution, (b) by column-oriented back substitution
    \end{mdframed}
    \bigbreak \noindent 
    By row-oriented back substitution, we have
    \begin{align*}
        4x_{4} &= 12 \implies x_{4} = 3, \\
        -2x_{3} + x_{4} &= 1 \implies x_{3} = \frac{1-3}{-2} = 1, \\
        x_{2} +2x_{3} + 3x_{4} &= 10 \implies x_{2} = 10-3(3)-2(1) = -1, \\
        3x_{1} + 2x_{2} + x_{3} &= -10 \implies x_{1} = \frac{-10 -1 -2(-1)}{3} = -3
    .\end{align*}
    So, $x = \begin{pmatrix} -3 \\ -1 \\ 1 \\ 3 \end{pmatrix} $. We can verify the result with matrix multiplication $Ux$, we have
    \begin{align*}
                    \begin{bmatrix}
                3 & 2 & 1 & 0 \\
                0 & 1 & 2 & 3 \\
                0 & 0 & -2 & 1 \\
                0 & 0 & 0 & 4
            \end{bmatrix}
            \begin{bmatrix}
                -3 \\ -1 \\ 1 \\ 3
            \end{bmatrix}
            = 
            \begin{bmatrix}
                3(-3) + 2(-1) + 1(1) \\
                1(-1) +2(1) +3(3) \\
                -2(1)+1(3) \\
                4(3)
            \end{bmatrix}
            =
            \begin{bmatrix}
                -10 \\
                10 \\
                1 \\
                12
            \end{bmatrix}
    .\end{align*}
    Which matches the given $b$. With column-oriented back substitution, we have
    \bigbreak \noindent 
    $\text{Alg}(U,b)$: We have the system
    \begin{align*}
        \begin{bmatrix}
            3 & 2 & 1 & 0 \\
            0 & 1 & 2 & 3 \\
            0 & 0 & -2 & 1 \\
            0 & 0 & 0 & 4
        \end{bmatrix}
        \begin{bmatrix}
            x_{1} \\
            x_{2} \\
            x_{3} \\
            x_{4}
        \end{bmatrix}
        =
        \begin{bmatrix}
            -10 \\
            10 \\
            1 \\
            12
        \end{bmatrix}
    \end{align*}
    Notice that $\hat{U} = \begin{bmatrix} 3 & 2 & 1 \\ 0 & 1 & 2 \\ 0 & 0 & -2 \end{bmatrix} $, $\hat{x} = \begin{bmatrix} x_{1} \\ x_{2} \\ x_{3} \end{bmatrix} $, $\hat{b} = \begin{bmatrix} -10 \\ 10 \\ 1 \end{bmatrix} $, and $u = \begin{bmatrix}
        0 \\ 3 \\ 1
    \end{bmatrix} $
    \bigbreak \noindent 
    By step one, we have that $x_{4} = \frac{12}{4} = 3$.
    \bigbreak \noindent 
    By step two, we have that
    \begin{align*}
        \tilde{b} = \begin{bmatrix}
            -10 \\ 10 \\ 1
        \end{bmatrix}
        -
        \begin{bmatrix}
           0 \\ 3 \\ 1 
        \end{bmatrix}
        \cdot 3
        =
        \begin{bmatrix}
            -10 - 0 \\
            10-3(3) \\
            1-3
        \end{bmatrix}
        = \begin{bmatrix}
            -10 \\
            1 \\
            -2
        \end{bmatrix}
    \end{align*}
    $\text{Alg}(\hat{U}, \tilde{b})$: We have the system
    \begin{align*}
        \begin{bmatrix}
            3 & 2 & 1 \\
            0 & 1 & 2 \\
            0 & 0 & -2
        \end{bmatrix}
        \begin{bmatrix}
            x_{1} \\ x_{2} \\ x_{3}
        \end{bmatrix}
         = 
         \begin{bmatrix}
             -10 \\ 1 \\ -2
         \end{bmatrix}
    \end{align*}
    Notice that $\hat{U} = \begin{bmatrix} 3 & 2 \\ 0 & 1 \end{bmatrix} $, $\hat{x} = \begin{bmatrix} x_{1} \\ x_{2} \end{bmatrix} $, $\hat{b} = \begin{bmatrix} -10 \\ 1 \end{bmatrix} $, and $u = \begin{bmatrix} 1 \\ 2 \end{bmatrix} $.
    \bigbreak \noindent 
    By step one, we have that $x_{3} = \frac{-2}{-2} = 1$
    \bigbreak \noindent 
    By step two, we have that
    \begin{align*}
        \tilde{b} = \begin{bmatrix}
            -10 \\10
        \end{bmatrix}
        - \begin{bmatrix}
            1 \\ 2
        \end{bmatrix} \cdot 1 
        = 
        \begin{bmatrix}
            -10 - 1 \\
            1 -2
        \end{bmatrix}
         = 
         \begin{bmatrix}
             -11 \\
             -1
         \end{bmatrix}
    \end{align*}
    $\text{Alg}(\hat{U}, \tilde{b})$: We have the system
    \begin{align*}
       \begin{bmatrix}
           3 & 2 \\ 0 & 1
       \end{bmatrix} 
       \begin{bmatrix}
           x_{1} \\ x_{2}
       \end{bmatrix}
       = 
       \begin{bmatrix}
           -11 \\ -1
       \end{bmatrix}
    \end{align*}
    Notice that $\hat{U} = \begin{bmatrix} 3 \end{bmatrix}$, $\hat{x} = \begin{bmatrix} x_{1} \end{bmatrix} $, $\hat{b} = \begin{bmatrix} -11 \end{bmatrix} $, $u = \begin{bmatrix} 2 \end{bmatrix} $.
    \bigbreak \noindent 
    By step one, we have that $x_{2} = \frac{-1}{1} =-1$
    \bigbreak \noindent 
    By step two, we have that
    \begin{align*}
        \tilde{b} &= \begin{bmatrix} -11 \end{bmatrix} - \begin{bmatrix} 2\end{bmatrix} \cdot (-1) = \begin{bmatrix}
           -11 + 2  
        \end{bmatrix}
        = 
        \begin{bmatrix}
            -9
        \end{bmatrix}
    \end{align*}
    $\text{Alg}(\hat{U}, \tilde{b})$: In the final call, we have  the system
    \begin{align*}
        \begin{bmatrix}
            3
        \end{bmatrix}
        \begin{bmatrix}
            x_{1}
        \end{bmatrix}
        = 
        \begin{bmatrix}
            -9
        \end{bmatrix}
    \end{align*}
    So, we have that $x_{1} = -\frac{9}{3} = -3$. Thus, 
    \begin{align*}
        x = \begin{pmatrix} -3 \\ -1 \\ 1 \\ 3 \end{pmatrix} 
    \end{align*}
    which is precisely the vector $x$ found in the row-oriented backward substitution.


    \pagebreak \bigbreak \noindent 
    \begin{mdframed}
        1.4.21.  Let
        \[
            A =
            \begin{bmatrix}
                16 & 4 & 8 & 4 \\
                4 & 10 & 8 & 4 \\
                8 & 8 & 12 & 10 \\
                4 & 4 & 10 & 12
            \end{bmatrix},
            \qquad
            b =
            \begin{bmatrix}
                32 \\
                26 \\
                38 \\
                30
            \end{bmatrix}.
        \]
        Notice that $A$ is symmetric, (a) Use the inner-product formulation of Cholesky's method to show that $A$ is positive definite and compute its Cholesky factor, (b) Use forward and back substitution to solve the linear system $Ax =b$. 
    \end{mdframed}
    \bigbreak \noindent 
    \begin{remark}
    \textit{(Inner product formulas to compute $R$ (Cholesky factor))}: We have the formulas
        \begin{align*}
            r_{ii} &= \sqrt{a_{ii} - \sum_{k=1}^{i-1}r_{ki}^{2}} \quad i = 1,2,...,n \\
            r_{ij} &= \frac{a_{ij} - \sum_{k=1}^{i-1}r_{ki}r_{kj}}{r_{ii}} \quad j = i+1,...,n
        \end{align*}
        \bigbreak \noindent 
        The inner product formulas to compute $R$ requires $\mathcal{O}(n^{3})$ flops.
    \end{remark}
    \bigbreak \noindent 
    For the first row, we have
    \begin{align*}
        r_{11} &= \sqrt{a_{11} - \sum_{k=1}^{0}r^{2}_{ki}} = \sqrt{a_{11}} = \sqrt{16} = 4, \\
        r_{12} &= \frac{a_{12} - \sum_{k=1}^{0}r_{ki}r_{kj}}{r_{11}} = \frac{4}{4} = 1, \\
        r_{13} &= \frac{a_{13}}{r_{11}} = \frac{8}{4} = 2, \\
        r_{14} &= \frac{a_{14}}{r_{11}} = \frac{4}{4} = 1
    .\end{align*}
    For the second row, we have
    \begin{align*}
        r_{22} &= \sqrt{a_{22} - \sum_{k=1}^{1}r_{k2}^{2}} = \sqrt{10 - 1^{2}} = \sqrt{9} = 3, \\
        r_{23} &= \frac{a_{23} - \sum_{k=1}^{1}r_{k2}r_{k3}}{r_{22}} = \frac{8-1(2)}{3} = 2, \\
        r_{24} &= \frac{a_{24} - \sum_{k=1}^{1}r_{k2}r_{k4}}{r_{22}} = \frac{4 - 1(1)}{3} = 1
    .\end{align*}
    For the third row, we have
    \begin{align*}
        r_{33} &= \sqrt{a_{33} - \sum_{k=1}^{2}r_{k3}^{2}} = \sqrt{12 - (2^{2} + 2^{2})}  = \sqrt{4} = 2, \\
        r_{34} &= \frac{a_{34} - \sum_{k=1}^{2}r_{k3}r_{k4}}{r_{33}} = \frac{10 - (2(1) + 2(1))}{2} = 3
    .\end{align*}
    For the last row, we have
    \begin{align*}
        r_{44} &= \sqrt{a_{44} - \sum_{k=1}^{3}r_{k4}^{2}} = \sqrt{12 -(1^{2} + 1^{2} + 3^{2})} = \sqrt{1} = 1
    .\end{align*}
    So, the Cholesky factor $R$ is 
    \begin{align*}
        \begin{bmatrix}
            4 & 1 & 2 & 1 \\
            0 & 3 & 2 & 1 \\
            0 & 0 & 2 &3 \\
            0 & 0 & 0 & 1
        \end{bmatrix}
    .\end{align*}
    Thus, the system $Ax = b$ becomes $R^{\top}Rx = b$, which is given by
    \begin{align*}
        \begin{bmatrix}
            4 & 0 & 0 & 0 \\
            1 & 3 & 0 & 0\\
            2 & 2 & 2 & 0\\
            1 & 1 & 3 & 1
        \end{bmatrix}
        \begin{bmatrix}
            4 & 1 & 2 & 1 \\
            0 & 3 & 2 & 1 \\
            0 & 0 & 2 &3 \\
            0 & 0 & 0 & 1
        \end{bmatrix}
        \begin{bmatrix}
            x_{1} \\x_{2} \\ x_{3} \\ x_{4}
        \end{bmatrix}
         = 
         \begin{bmatrix}
            32 \\ 26 \\ 38 \\ 30 
         \end{bmatrix}
    \end{align*}
    First, we solve the upper triangular system $R^{\top}y = b $ with forward substitution. We have the system 
    \begin{align*}
        \begin{bmatrix}
            4 & 0 & 0 & 0 \\
            1 & 3 & 0 & 0\\
            2 & 2 & 2 & 0\\
            1 & 1 & 3 & 1
        \end{bmatrix}
        \begin{bmatrix}
            y_{1} \\ y_{2} \\ y_{3} \\ y_{4}
        \end{bmatrix}
        =
        \begin{bmatrix}
            32 \\ 26 \\ 38 \\ 30
        \end{bmatrix}
    .\end{align*}
    Using forward substitution, we find
    \begin{align*}
        4y_{1} &= 32 \implies y_{1} = 8, \\
        y_{1} + 3y_{2} &= 26 \implies y_{2} = \frac{26-8}{3} = 6, \\
        2y_{1} + 2y_{2} + 2y_{3} &= 38 \implies y_{3} = \frac{38-2(6)-2(8)}{2} = 5, \\
        y_{1} + y_{2} + 3y_{3} + y_{4} &= 30\implies y_{4} = 30-3(5)-6-8 = 1
    .\end{align*}
    So, 
    \begin{align*}
        y = \begin{pmatrix} 8 \\ 6 \\ 5 \\ 1 \end{pmatrix}
    \end{align*}
    Now, we solve $Rx = y$. We have
    \begin{align*}
         \begin{bmatrix}
            4 & 1 & 2 & 1 \\
            0 & 3 & 2 & 1 \\
            0 & 0 & 2 &3 \\
            0 & 0 & 0 & 1
        \end{bmatrix}
        \begin{bmatrix}
           x_{1} \\ x_{2} \\ x_{3} \\ x_{4} 
        \end{bmatrix}
        =
        \begin{bmatrix}
            8 \\ 6\\ 5\\1
        \end{bmatrix}
    \end{align*}
    Using backward substitution, we find
    \begin{align*}
        x_{4} &= 1, \\
        2x_{3} + 3x_{4} &= 5 \implies x_{3} = \frac{5-3(1)}{2} = 1, \\
        3x_{2} + 2x_{3} + x_{4} &= 6 \implies x_{2} = \frac{6-1-2(1)}{3} = 1, \\
        4x_{1} + x_{2} + 2x_{3} + x_{4} &= 8 \implies x_{1} = \frac{8-1-2(1)-1}{4} = 1
    .\end{align*}
    So,
    \begin{align*}
        x = \begin{pmatrix} 1 \\ 1 \\ 1 \\ 1 \end{pmatrix}
    \end{align*}
    Let's verify this solution by multiplying $Ax$ to see that it equals the given $b$
    \begin{align*}
        \begin{bmatrix}
            16 & 4 & 8 & 4 \\
            4 & 10 & 8 & 4 \\
            8 & 8 & 12 & 10 \\
            4 & 4 & 10 & 12
        \end{bmatrix}
        \begin{bmatrix}
            1 \\ 1\\ 1 \\1
        \end{bmatrix}
        = 
        \begin{bmatrix}
            16 + 4 + 8 + 4\\
            4 + 10 + 8 + 4 \\
            8 + 8 + 12 + 10 \\
            4 + 4 + 10 + 12 
        \end{bmatrix}
        = 
        \begin{bmatrix}
            32 \\ 26 \\ 38 \\ 30
        \end{bmatrix}
    .\end{align*}
    The solution is verified.
    

    \pagebreak \bigbreak \noindent 
    \begin{mdframed}
        1.4.31. Use the outer-product form to work part (a) of Exercise 1.4.21. 
    \end{mdframed}
    \bigbreak \noindent 
    \begin{remark}
    \textit{(Recursive column oriented method to find the Cholesky factor $R$ (Outer product method))}. Let $A \in \mathbb{R}^{n\times n}$. Assume that $A$ is positive definite, so $A = A^{\top}$, and $A = R^{\top}R$ for a unique upper triangular matrix $R$. We have,
        \begin{align*}
            A &= \begin{bmatrix} a_{11} & a_{12} & \cdots & a_{1n} \\ a_{21} & a_{22} & \cdots & a_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ a_{n1} & a_{n2} & \cdots & a_{nn} \end{bmatrix} = 
            \begin{bmatrix}
                r_{11} & 0  & \cdots & 0\\
                r_{12} & r_{22}  &  \cdots & 0 \\
                \vdots & \vdots & \ddots & \vdots\\
                r_{1n} & r_{2n} & \cdots & r_{nn}
            \end{bmatrix}
            \begin{bmatrix}
                r_{11} & r_{12} & \cdots & r_{1n} \\
                0 & r_{22} & \cdots & r_{2n} \\
                0 & 0 & \ddots & \vdots \\
                0 & 0  & \cdots & r_{nn}
            \end{bmatrix}
        \end{align*}
        We then perform a matrix decomposition 
        \begin{align*}
            \begin{bmatrix}
                a_{11} & a^{\top} \\
                a & \hat{A}
            \end{bmatrix}
            =
            \begin{bmatrix}
                r_{11} & 0^{\top} \\
                r & \hat{R}^{\top}
            \end{bmatrix}
            \begin{bmatrix}
                r_{11} & r^{\top} \\
                0 & \hat{R}
            \end{bmatrix}
        .\end{align*}
        Where $\hat{A} = \hat{A}^{\top} \in \mathbb{R}^{n-1 \times n-1}$, $a \in \mathbb{R}^{n-1}$, $\hat{R}^{\top} \in \mathbb{R}^{n-1\times n-1}$ lower triangular, and $\hat{R} \in \mathbb{R}^{n-1\times n-1}$ upper triangular. Further,
        \bigbreak \noindent 
        The recursive column oriented algorithm to compute the Cholesky factor $R$ is given by the following steps
        \begin{enumerate}
            \item $r_{11} = \sqrt{a_{11}}$
            \item $r = \frac{a}{r_{11}} $
            \item $\tilde{A} = \hat{A} - rr^{\top} $
            \item $\text{Alg}(\tilde{A}) = \hat{R} $
        \end{enumerate}
        \bigbreak \noindent 
        The recursive column oriented algorithm to compute the Cholesky factor $R$ requires $\mathcal{O}(n^{3})$ flops.
    \end{remark}
    \bigbreak \noindent 
    Recall the given system 
    \[
        \begin{bmatrix}
            16 & 4 & 8 & 4 \\
            4 & 10 & 8 & 4 \\
            8 & 8 & 12 & 10 \\
            4 & 4 & 10 & 12
        \end{bmatrix}
        \begin{bmatrix}
            x_{1} \\ x_{2} \\ x_{3} \\ x_{4}
        \end{bmatrix}
        =
        \begin{bmatrix}
            32 \\
            26 \\
            38 \\
            30
        \end{bmatrix}.
    \]
    $\text{Alg}(A)$: Notice that 
    \begin{align*}
        \hat{A} &= \begin{bmatrix} 10 & 8 & 4 \\ 8 & 12 & 10 \\ 4 & 10 & 12 \end{bmatrix}, \\
        a &= \begin{bmatrix} 4 \\ 8 \\ 4 \end{bmatrix}.
    \end{align*}
    \bigbreak \noindent 
    By the first step, $r_{11} = \sqrt{16} = 4$.
    \bigbreak \noindent 
    By the second step, 
    \begin{align*}
        r &= \frac{1}{4}\begin{bmatrix} 4 \\ 8 \\ 4 \end{bmatrix} = \begin{bmatrix} 1 \\ 2\\ 1 \end{bmatrix}
    \end{align*}
    By the third step, 
    \begin{align*}
        \tilde{A} &= \begin{bmatrix} 10 & 8 & 4 \\ 8 & 12 & 10 \\ 4 & 10 & 12 \end{bmatrix} - \begin{bmatrix} 1 \\ 2 \\1 \end{bmatrix} \begin{bmatrix} 1 & 2 & 1 \end{bmatrix} \\
                  &= \begin{bmatrix} 10 & 8 & 4 \\ 8 & 12 & 10 \\ 4 & 10 & 12 \end{bmatrix} - \begin{bmatrix} 1 & 2 & 1 \\ 2 & 4 & 2 \\ 1 & 2 & 1 \end{bmatrix} \\
                  &= 
                  \begin{bmatrix}
                      9 & 6  &3 \\
                      6 & 8 & 8 \\
                      3 & 8 & 11
                  \end{bmatrix}
    \end{align*}
    $\text{Alg}(\hat{A})$: We have
    \begin{align*}
        A &=
                  \begin{bmatrix}
                      9 & 6  &3 \\
                      6 & 8 & 8 \\
                      3 & 8 & 11
                  \end{bmatrix}
    .\end{align*}
    So,
    \begin{align*}
        \hat{A} &= \begin{bmatrix} 8 & 8 \\ 8 & 11 \end{bmatrix}, \\
        a &= \begin{bmatrix} 6 \\ 3 \end{bmatrix} 
    .\end{align*}
    By the first step, $r_{11} = \sqrt{9} = 3 $
    \bigbreak \noindent 
    By the second step, 
    \begin{align*}
         r = \frac{1}{3}\begin{bmatrix} 6 \\ 3 \end{bmatrix} = \begin{bmatrix} 2 \\ 1 \end{bmatrix}
    \end{align*}
    By the third step, 
    \begin{align*}
        \tilde{A} &= \begin{bmatrix} 9 & 6  &3 \\ 6 & 8 & 8 \\ 3 & 8 & 11 \end{bmatrix} - \begin{bmatrix} 2 \\ 1 \end{bmatrix} \begin{bmatrix} 2 & 1 \end{bmatrix} \\
                  &= \begin{bmatrix}  8 & 8 \\  8 & 11 \end{bmatrix} - \begin{bmatrix} 4 & 2 \\ 2 & 1 \end{bmatrix} = \begin{bmatrix} 4 & 6 \\ 6 & 10 \end{bmatrix}
    \end{align*}
    $\text{Alg}(\tilde{A})$: We have
    \begin{align*}
        A = \begin{bmatrix}
            4 & 6 \\
            6 & 10
        \end{bmatrix}
   .\end{align*}
    So,
    \begin{align*}
        \hat{A} &= \begin{bmatrix} 10 \end{bmatrix}, \\
        a &= \begin{bmatrix} 6 \end{bmatrix}
    .\end{align*}
    By the first step, 
    \begin{align*}
        r_{11} &= \sqrt{4} = 2
    \end{align*}
    By the second step, 
    \begin{align*}
        r &= \frac{1}{2}\begin{bmatrix}6 \end{bmatrix}  = \begin{bmatrix} 3 \end{bmatrix}
    .\end{align*}
    By the third step,
    \begin{align*}
        \tilde{A} &= \begin{bmatrix} 10\end{bmatrix} - \begin{bmatrix} 3 \end{bmatrix} \begin{bmatrix} 3 \end{bmatrix}^{\top} \\
                  &= \begin{bmatrix} 10 \end{bmatrix} - \begin{bmatrix} 9 \end{bmatrix} = \begin{bmatrix} 1 \end{bmatrix}
    \end{align*}
    $\text{Alg}(\tilde{A})$: We have 
    \begin{align*}
        A = \begin{bmatrix} 1 \end{bmatrix}
    \end{align*}
    So, $r_{11} = \sqrt{1} = 1$. Thus, the Cholesky factor $R $ is 
    \begin{align*}
        R = \begin{bmatrix}
            4 & 1 & 2 & 1 \\
            0 & 3 & 2 & 1  \\
            0 & 0 & 2 & 3 \\
            0 & 0 & 0 & 1
        \end{bmatrix}
    \end{align*}
    Which is precisely the factor that was computer in the row-oriented approach.

    

    \pagebreak \bigbreak \noindent 
    \begin{mdframed}
        1.4.33. Write a nonrecursive algorithm that implements the outer-product formulation of Cholesky's algorithm (1.4.28). Your algorithm should exploit the symmetry of $A$ by referencing only the main diagonal and upper part of $A$, and it should store $R$ over $A$. Be sure to put in the necessary check before taking the square root.
    \end{mdframed}
    \bigbreak \noindent 
    \begin{jlcode}
        for |$i = 1,...,n$|
            if |$a_{ii} \leq 0$|, set error flag, exit
            |$a_{ii} = \sqrt{a_{ii}}$|
            for |$j = i+1,...,n$|
                |$a_{ij} = a_{ij} / a_{ii}$|
            end

            for |$k = i+1,...,n$|
                for |$\ell = k,...,n$|
                    |$a_{k\ell} = a_{k\ell} - a_{ik} \cdot a_{i\ell}$|
                end
            end
        end
    \end{jlcode}
    \bigbreak \noindent 
    At the end of the above algorithm, $R$ is stored in the upper triangular part of $A$. We could zero out the lower triangular part of $A$ so that $A$ stores only the matrix $R$.
    \bigbreak \noindent 
    \begin{jlcode}
    for |$i=1,...,n$|
        for |$j = 1,...,i-1$|
            |$a_{ij} = 0$|
        end
    end
    \end{jlcode}

    \pagebreak \bigbreak \noindent 
    \begin{mdframed}
        1.4.40. Use the bordered form to work part (a) of Exercise 1.4.21. 
    \end{mdframed}
    \bigbreak \noindent 
    \begin{remark}
        \textit{(Bordered form of Choleskys method)}. Suppose $A \in \mathbb{R}^{n\times n}$ is positive definite. Then, $A$ admits a decomposition $A = R^{\top}R$, for a unique upper triangular matrix $R$ called the Cholesky factor, with $r_{ii} > 0$ for $i =1,2,...,n$. So,
        \begin{align*}
            A &= \begin{bmatrix} a_{11} & a_{12} & \cdots & a_{1n} \\ a_{21} & a_{22} & \cdots & a_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ a_{n1} & a_{n2} & \cdots & a_{nn} \end{bmatrix} = 
            \begin{bmatrix}
                r_{11} & 0  & \cdots & 0\\
                r_{12} & r_{22}  &  \cdots & 0 \\
                \vdots & \vdots & \ddots & \vdots\\
                r_{1n} & r_{2n} & \cdots & r_{nn}
            \end{bmatrix}
            \begin{bmatrix}
                r_{11} & r_{12} & \cdots & r_{1n} \\
                0 & r_{22} & \cdots & r_{2n} \\
                0 & 0 & \ddots & \vdots \\
                0 & 0  & \cdots & r_{nn}
            \end{bmatrix}
        \end{align*}
    \end{align*}
    We then perform a matrix decomposition 
    \begin{align*}
        \begin{bmatrix}
            \hat{A} & a \\
            a^{\top} & a_{nn}
        \end{bmatrix}
        =
        \begin{bmatrix}
            \hat{R}^{\top} & 0 \\
            r^{\top} & r_{nn}
        \end{bmatrix}
        \begin{bmatrix}
            \hat{R} & r \\
            0 & r_{nn}
        \end{bmatrix}
    .\end{align*}
    So, 
    \begin{align*}
        \hat{A} &= \hat{R}^{\top}\hat{R}, \\
        a &= \hat{R}^{\top}r, \\
        a_{nn} &= r^{\top}r + r_{nn}^{2} \implies r_{nn} = \sqrt{a_{nn} - r^{\top}r}
    .\end{align*}
    So, the steps for the algorithm are 
    \begin{enumerate}
        \item Recurse $\hat{A}$ until $A \in \mathbb{R}^{1\times 1}$
        \item Solve the lower triangular system $\hat{R}^{\top}r = a$ by forward substitution
        \item Compute $r_{nn} = \sqrt{a_{nn} - r^{\top}r} $
        \item Return the step two on the previous call
    \end{enumerate}
    The calls to the algorithm are
    \begin{enumerate}
        \item $\text{Alg}\left( \begin{bmatrix} 16 & 4 & 8 & 4 \\ 4 & 10 & 8 & 4 \\ 8 & 8 & 12 & 10 \\ 4 & 4 & 10 & 12 \end{bmatrix} \right) $
        \item $\text{Alg}\left(\begin{bmatrix} 16 & 4 & 8 \\ 4 & 10 & 8 \\ 8 & 8 & 12 \end{bmatrix}\right) $
        \item $\text{Alg}\left(\begin{bmatrix} 16 & 4 \\ 4 & 10 \end{bmatrix}\right) $
        \item $\text{Alg}\left(\begin{bmatrix} 16 \end{bmatrix}\right) $
    \end{enumerate}
    On the fourth and final call, we have that $A = \begin{bmatrix} 16 \end{bmatrix} $, and so $r_{11} = \sqrt{16} = 4$. 
    \bigbreak \noindent 
    We return to the third call with
    \begin{align*}
        R &= \begin{bmatrix} 4 & r_{12} & r_{13} & r_{14} \\ 0 & r_{22} & r_{23} & r_{24} \\ 0 & 0 & r_{33} & r_{34} \\ 0 & 0 & 0 & r_{44} \end{bmatrix}, \\
        \hat{R} &= \begin{bmatrix} 4 \end{bmatrix}, \\
        a &= \begin{bmatrix} 4 \end{bmatrix}
    .\end{align*}
    So, we solve the system $\hat{R}^{\top}r = a $
    \begin{align*}
        \begin{bmatrix} 4 \end{bmatrix} \begin{bmatrix} r_{12} \end{bmatrix} &= \begin{bmatrix} 4 \end{bmatrix} \\
        \implies r_{12} &= \begin{bmatrix} 1 \end{bmatrix}
    .\end{align*}
    Next, we find $r_{22}$.
    \begin{align*}
        r_{22} &= \sqrt{10 - \begin{bmatrix} 1 \end{bmatrix}^{\top} \begin{bmatrix} 1 \end{bmatrix}}  \\
               &= \sqrt{10 - 1} = \sqrt{9} = 3
    \end{align*}
    So, we return to the second call with
    \begin{align*}
        R &= \begin{bmatrix} 4 & 1 & r_{13} & r_{14} \\ 0 & 3 & r_{23} & r_{24} \\ 0 & 0 & r_{33} & r_{34} \\ 0 & 0 & 0 & r_{44} \end{bmatrix}, \\
        \hat{R} &= \begin{bmatrix} 4 & 1 \\ 0 & 3 \end{bmatrix}, \\
        a &= \begin{bmatrix} 8 \\ 8 \end{bmatrix}
    .\end{align*}
    We solve the system $\hat{R}^{\top}r = a $ with forward substitution.
    \begin{align*}
        \begin{bmatrix}
            4 & 0 \\
            1 & 3
        \end{bmatrix}
        \begin{bmatrix}
            r_{13} \\ r_{23}
        \end{bmatrix}
         = 
         \begin{bmatrix}
             8 \\ 8
         \end{bmatrix}
    \end{align*}
    So,
    \begin{align*}
        4r_{13} &= 8 \implies r_{13} = 2, \\
        r_{13} + 3r_{23} &= 8 \implies r_{23} = \frac{8-2}{3} = 2
    .\end{align*}
    Thus, $r = \begin{bmatrix} 2 \\ 2 \end{bmatrix} $. With this, we find $r_{33}$.
    \begin{align*}
        r_{33} &= \sqrt{a_{33} - r^{\top}r} = \sqrt{12 - 8} = \sqrt{4} = 2 
    .\end{align*}
    We return to the first call with
    \begin{align*}
        R &= \begin{bmatrix} 4 & 1 & 2 & r_{14} \\ 0 & 3 & 2 & r_{24} \\ 0 & 0 & 2 & r_{34} \\ 0 & 0 & 0 & r_{44} \end{bmatrix}, \\
        \hat{R} &= \begin{bmatrix} 4 & 1 & 2 \\ 0 & 3 & 2 \\ 0 & 0 & 2 \end{bmatrix}, \\
        a &= \begin{bmatrix} 4 \\ 4 \\ 10 \end{bmatrix}
    .\end{align*}
    We again use forward substitution to solve $\hat{R}^{\top} r = a$.
    \begin{align*}
        \begin{bmatrix}
            4 & 0 & 0 \\
            1 & 3 & 0 \\
            2 & 2 & 2
        \end{bmatrix}
        \begin{bmatrix}
            r_{14} \\ r_{24} \\ r_{24} 
        \end{bmatrix}
        = \begin{bmatrix}
            4 \\ 4 \\ 10
        \end{bmatrix}
    .\end{align*}
    We have
    \begin{align*}
        4r_{14} &= 4 \implies r_{14} = 1, \\
        r_{14} + 3r_{24} &= 4 \implies r_{24} = \frac{4-1}{3} = 1, \\
        2r_{14} + 2 r_{23} + 2r_{24} &= 10 \implies r_{24} = \frac{10-2(1)-2(1)}{2} = 3
    .\end{align*}
    Lastly, we find $r_{44}$.
    \begin{align*}
        r_{44} &=\sqrt{a_{44} - r^{\top}r}   = \sqrt{12 - (1^{2} + 1^{2} + 3^{2} )} = \sqrt{1} =1
    \end{align*}
    Thus, the Cholesky factor $R$ is 
    \begin{align*}
            R &= \begin{bmatrix} 4 & 1 & 2 & 1 \\ 0 & 3 & 2 & 1 \\ 0 & 0 & 2 & 3 \\ 0 & 0 & 0 &1 \end{bmatrix} 
    \end{align*}
    Which is exactly the factor found previously.
    



    \pagebreak \bigbreak \noindent 
    \begin{mdframed}
        1.4.54. Prove Proposition 1.4.53. 
        \bigbreak \noindent 
        As in the previous exercise, do not use the Cholesky decomposition in your proof; use the fact that $x^{\top} A x > 0$ for all nonzero $x$.
    \end{mdframed}
    \bigbreak \noindent 
    \textbf{Proposition 1.4.53}. Let $A $ be positive definite, and consider a partition
    \begin{align*}
        A = \begin{bmatrix} A_{11} & A_{12} \\ A_{21} & A_{22} \end{bmatrix}
    \end{align*}
    in which $A_{11} $ and $A_{22} $ are square. Then $A_{11}$ and $A_{22}$ are positive definite
    \bigbreak \noindent 
    \textbf{\textit{Proof.}} Assume $A \in \mathbb{R}^{n\times n}$ is positive definite, with the block decomposition
    \begin{align*}
        A = \begin{blockarray}{ccc}
            & n_{1} & n_{2} \\
            \begin{block}{c[cc]}
                m_{1} & A_{11} & A_{12} \\
                m_{2} & A_{21} & A_{22}
            \end{block}
        \end{blockarray}.
    \end{align*}
    where $A_{11}, A_{22}$ are square, $n_{1} + n_{2} = n$, and $m_{1} + m_{2} = n$. Since $A$ is positive definite, the following properties hold.
    \begin{enumerate}
        \item $A = A^{\top} $
        \item $x^{\top}Ax > 0$ for all $x\in \mathbb{R}^{n}$, $x\ne 0$
    \end{enumerate}
    Since $A = A^{\top}$,
    \begin{align*}
        \begin{bmatrix}
            A_{11} & A_{12} \\
            A_{21} & A_{22}
        \end{bmatrix}
        =
        \begin{bmatrix}
            A_{11}^{\top} & A_{12}^{\top} \\
            A_{21}^{\top} & A_{22}^{\top}
        \end{bmatrix}
    .\end{align*}
    So, $A_{11} = A_{11}^{\top}$, and $A_{22} = A_{22}^{\top}$.
    \bigbreak \noindent 
    Next, consider $x = \begin{pmatrix} \bar{x} \\ 0 \end{pmatrix} \in \mathbb{R}^{n}$, where $\bar{x} \ne 0 \in \mathbb{R}^{n_{1}} $, and $y = \begin{pmatrix} 0 \\ \bar{y} \end{pmatrix} $, where $\bar{y} \ne 0\in \mathbb{R}^{n_{2}} $. Note that since $A$ is positive definite, 
    \begin{align*}
        x^{\top}Ax > 0, \\
        y^{\top}Ay >0
    .\end{align*}
    From $x^{\top}Ax$, we see
    \begin{align*}
        &\begin{pmatrix} \bar{x}^{\top} & 0 \end{pmatrix} \begin{bmatrix} A_{11} & A_{12} \\ A_{21} & A_{22} \end{bmatrix} \begin{pmatrix} \bar{x} \\ 0 \end{pmatrix} > 0, \\
        &\implies \begin{pmatrix} \bar{x}^{\top} & 0  \end{pmatrix} \begin{bmatrix} A_{11}\bar{x} \\ A_{21}\bar{x} \end{bmatrix} >0, \\
        &\implies \bar{x}^{\top}A_{11}\bar{x} > 0
    .\end{align*}
    Since $\bar{x} \in \mathbb{R}^{n_{1}} $ is an arbitrary vector different from zero and $A_{11}$ is symmetric, $A_{11}$ is positive definite.
    \bigbreak \noindent 
    Next, we see 
    \begin{align*}
        &\begin{pmatrix} 0 & \bar{y}^{\top} \end{pmatrix} \begin{bmatrix} A_{11} & A_{12} \\ A_{21} & A_{22} \end{bmatrix} \begin{pmatrix} 0 \\ \bar{y} \end{pmatrix} > 0, \\
        &\implies \begin{pmatrix} 0 & \bar{y}^{\top} \end{pmatrix} \begin{bmatrix} A_{12}\bar{y} \\ A_{22}\bar{y} \end{bmatrix} > 0, \\
        &\implies \bar{y}^{\top}A_{22}\bar{y} > 0
    \end{align*}
    Again, since $\bar{y} \in \mathbb{R}^{n_{2}}$ is an arbitrary vector different from zero and $A_{22}$ is symmetric, $A_{22}$ is positive definite.
    \bigbreak \noindent 
    Therefore, both $A_{11}$ and $A_{22}$ are positive definite. $\endpf$




    \pagebreak  \bigbreak \noindent 
    \begin{mdframed}
        1.4.56. Prove Proposition 1.4.55. 
    \end{mdframed}
    \bigbreak \noindent 
    \textbf{Proposition 1.4.55}: If $A$ and $X$ are $n\times n$, $A$ is positive definite, and $X$ is nonsingular,
    then the matrix $B = X^{\top} A X$ is also positive definite.
    \bigbreak \noindent 
    Considering the special case $A = I$ (which is clearly positive definite), we see that
    this proposition is a generalization of Theorem 1.4.4.
    \bigbreak \noindent 
    \textbf{\textit{Proof.}} Assume that $A,X \in \mathbb{R}^{n\times n}$, with $A$ positive definite, and $X$ non-singular. First, we look at $B^{\top}$.
    \begin{align*}
        B^{\top} &= (X^{\top}AX)^{\top} = X^{\top}A^{\top}\left(X^{\top}\right)^{\top} \\
                 &=X^{\top}A^{\top}X
    .\end{align*}
    But, $A $ is positive definite, so $A = A^{\top}$. Thus,
    \begin{align*}
        B^{\top} &= X^{\top}A^{\top}X = X^{\top}AX = B
    .\end{align*}
    So, $B $ is symmetric. Next, we look at $x^{\top}Bx$. Let $x\in \mathbb{R}^{n}$, $x\ne 0$, we have
    \begin{align*}
        x^{\top}Bx &= x^{\top}X^{\top}AXx = (Xx)^{\top}A(Xx)
    .\end{align*}
    Let $y = Xx$. Since $X$ non-singular and $ x \ne 0$, $Xx \ne 0$, so $y \ne 0$. Thus, $y^{\top}Ay \ne 0$. But, since $A$ positive definite, 
    \begin{align*}
        x^{\top}Bx = y^{\top}Ay > 0
    \end{align*}
    Therefore, $B$ is positive definite. $\endpf $

    \pagebreak \bigbreak \noindent 
    \begin{mdframed}
        1.4.58*.
        Let 
        \[
            A = 
            \begin{bmatrix}
                A_{11} & A_{12} \\
                A_{21} & A_{22}
            \end{bmatrix}
        \]
        be positive definite, and suppose $A_{11}$ is $j \times j$ and 
        $A_{22}$ is $k \times k$. By Proposition 1.4.53, $A_{11}$ is positive definite. 
        Let $R_{11}$ be the Cholesky factor of $A_{11}$, let $R_{12} = R_{11}^{-T} A_{12}$, 
        and let $\tilde{A}_{22} = A_{22} - R_{12}^{T} R_{12}$. The matrix $\tilde{A}_{22}$ is 
        called the \emph{Schur complement} of $A_{11}$ in $A$.
        \begin{enumerate}[(a)]
            \item Show that 
                \[
                    \tilde{A}_{22} = A_{22} - A_{21} A_{11}^{-1} A_{12}.
                \]
            \item Establish a decomposition of $A$ that is similar to (1.4.57) and involves $\tilde{A}_{22}$.
            \item Prove that $\tilde{A}_{22}$ is positive definite.
        \end{enumerate}
    \end{mdframed}
    \bigbreak \noindent 
    \textbf{\textit{Proof.}} Assume that 
    \begin{align*}
        A = \begin{bmatrix}
            A_{11} & A_{12} \\
            A_{21} & A_{22}
        \end{bmatrix}
    \end{align*}
    is positive definite, with $A_{11} \in \mathbb{R}^{j\times j},\; A_{22} \in \mathbb{R}^{k\times k}$. By proposition $1.4.53$, $A_{11}$ is positive definite. Let $R_{11}$ be the Cholesky factor of $A_{11}$, let $R_{12} = R_{11}^{-\top}A_{12}$, and let 
    \begin{align*}
        \tilde{A}_{22} = A_{22} - R_{12}^{\top}R_{12}.
    \end{align*}
    \bigbreak \noindent 
    We have
    \begin{align*}
        \begin{bmatrix}
            A_{11} & A_{12} \\
            A_{21} & A_{22}
        \end{bmatrix}
        =
        \begin{bmatrix}
            R_{11}^{\top} & 0 \\
            R_{21}^{\top} & R_{22}^{\top}
        \end{bmatrix}
        \begin{bmatrix}
            R_{11} & R_{12} \\
            0 & R_{22}
        \end{bmatrix}
    .\end{align*}
    By  matrix multiplication, we have
    \begin{align*}
        A_{21} &= R_{12}^{\top}R_{11}, \\
        \implies R_{12}^{\top} &= A_{21}R_{11}^{-1}
    \end{align*}
    since $R_{11}$ is non-singular. So,
    \begin{align*}
        \tilde{A}_{22} &= A_{22} - R_{12}^{\top}R_{12}, \\
                       &= A_{22} - A_{21}R_{11}^{-1}R_{12} \\
                       &= A_{22} - A_{21}R_{11}^{-1}R_{11}^{-\top}A_{12}
    \end{align*}
    Since $R_{11}$ is the Cholesky factor for $A_{11}$, 
    \begin{align*}
        A_{11} &= R_{11}^{\top}R_{11}, \\
        \implies (A_{11})^{-1} &= (R_{11}^{\top}R_{11})^{-1} \\
                               &= R_{11}^{-1}R_{11}^{-\top}
    .\end{align*}
    So,
    \begin{align*}
        \tilde{A}_{22} &= A_{22} - A_{21}R_{11}^{-1}R_{11}^{-\top}A_{12} \\
                       &= A_{22} - A_{21}A_{11}^{-1}A_{12}
    .\end{align*}
    As desired.
    \bigbreak \noindent 
    The decomposition of $A$ similar to (1.4.57) that involves $\tilde{A}_{22}$ is
    \begin{align*}
        \begin{bmatrix}
            A_{11} & A_{12} \\
            A_{21} & A_{22}
        \end{bmatrix}
        =
        \begin{bmatrix}
            R_{11}^{\top} & 0 \\
            R_{12}^{\top} & I
        \end{bmatrix}
        \begin{bmatrix}
            I & 0 \\
            0 & \tilde{A}_{22}
        \end{bmatrix}
        \begin{bmatrix}
            R_{11} & R_{12} \\
            0 & I
        \end{bmatrix}
    \end{align*}



    \bigbreak \noindent 
    We can show that $\tilde{A}_{22}$ is positive definite by showing the following hold
    \begin{enumerate}
        \item $\tilde{A}_{22} = \tilde{A}^{\top}_{22}$
        \item $x^{\top}\tilde{A}x > 0$ for all $x \in \mathbb{R}^{k}$, $x\ne 0$
    \end{enumerate}
    \bigbreak \noindent 
    First, we show that $\tilde{A}_{22} = \tilde{A}^{\top}_{22}$.
    \begin{align*}
        \tilde{A}_{22} &= A_{22} - A_{21}A_{11}^{-1}A_{12}  \\
        \implies \tilde{A}^{\top}_{22} &= (A_{22} - A_{21}A_{11}^{-1}A_{12})^{\top} \\
                                  &= A_{22}^{\top} -(A_{21}A_{11}^{-1}A_{12})^{\top} \\
                                  &=A_{22}^{\top} -A_{12}^{\top}A_{11}^{-\top}A_{21}^{\top}
    \end{align*}
    But, $A$ is symmetric, so
    \begin{align*}
        \begin{bmatrix}
            A_{11} & A_{12} \\
            A_{21} & A_{22}
        \end{bmatrix}
         = 
         \begin{bmatrix}
             A_{11}^{\top} & A_{21}^{\top} \\
             A_{12}^{\top} & A_{22}^{\top}
         \end{bmatrix}
    .\end{align*}
    Which, implies that $A_{12} = A_{21}^{\top}$, $A_{21} = A_{12}^{\top}$, $A_{22}^{\top} = A_{22}$, and $A_{11} = A_{11}^{\top} $. Thus,
    \begin{align*}
        \tilde{A}^{\top}&=A_{22}^{\top} -A_{12}^{\top}A_{11}^{-\top}A_{21}^{\top} \\
                        &= A_{22} - A_{21}A_{11}^{-\top}A_{12}
    .\end{align*}
    But, since $A_{11} = A_{11}^{\top}$, we see 
    \begin{align*}
        A_{11} &= A_{11}^{\top}, \\
        \implies A_{11}^{-1} &= A_{11}^{-\top} 
    .\end{align*}
    So,
    \begin{align*}
        \tilde{A}^{\top}_{22} &= A_{22} - A_{21}A_{11}^{-1}A_{12} = \tilde{A}_{22}
    .\end{align*}
    Thus $\tilde{A}_{22}$ is symmetric.
    \bigbreak \noindent 
    Let $x = \begin{pmatrix} z \\ y \end{pmatrix} \in \mathbb{R}^{n}$, $x \ne 0$ with $z \in \mathbb{R}^{j}$, and $y \in \mathbb{R}^{k}$. Since $A$ is positive definite,
    \begin{align*}
        \begin{pmatrix} z^{\top} y^{\top} \end{pmatrix} \begin{bmatrix}
        A_{11} & A_{12} \\
        A_{21} & A_{22}
        \end{bmatrix}
        \begin{pmatrix} z \\ y \end{pmatrix} > 0
    .\end{align*}
    We aim to show that $y^{\top}(A_{22} - A_{21}A_{11}^{-1}A_{12})y >0 $ for all $y \in \mathbb{R}^{k},\; y\ne 0$ (since $A_{22} - A_{21}A_{11}^{-1}A_{12} \in \mathbb{R}^{k\times k} $). 
    \bigbreak \noindent 
    We have
    \begin{align*}
        \begin{pmatrix} z^{\top} & y^{\top} \end{pmatrix} \begin{bmatrix} A_{11}z + A_{12}y \\ A_{21}z + A_{22}y \end{bmatrix} > 0
    .\end{align*}
    We need 
    \begin{align*}
        A_{21}z + A_{22}y  &= (A_{22} - A_{21}A_{11}^{-1}A_{12})y,
    \end{align*}
    so,
    \begin{align*}
        A_{21}z + A_{22} y &= A_{22}y - A_{21}A_{11}^{-1}A_{12}y,\\
        \implies A_{21}z &= -A_{21}A_{11}^{-1}A_{12}y, \\
        \implies z  &= -A_{11}^{-1}A_{12}y
    .\end{align*}
    So, let $x = \begin{pmatrix} -A_{11}^{-1}A_{12}y \\ y \end{pmatrix} $. Observe that $x \ne 0$ when $y\ne 0$. Thus, since $x \ne 0$, $y\ne 0$. Recall that $y \in \mathbb{R}^{k} $.
    \bigbreak \noindent 
    We have
    \begin{align*}
        \begin{pmatrix} (-A_{11}^{-1}A_{12}y)^{\top} y^{\top} \end{pmatrix} \begin{bmatrix} A_{11} & A_{12} \\ A_{21} & A_{22} \end{bmatrix} \begin{pmatrix} -A_{11}^{-1}A_{12}y \\ y \end{pmatrix} > 0 , \\
        \implies \begin{pmatrix} (-A_{11}A_{12}y)^{\top} y^{\top} \end{pmatrix} \begin{bmatrix} A_{11}(-A_{11}^{-1}A_{12}y) + A_{12}y \\ (A_{22}-A_{21}A_{11}^{-1}A_{12})y \end{bmatrix} >0, \\
        \implies \begin{pmatrix} (-A_{11}A_{12}y)^{\top} y^{\top} \end{pmatrix} \begin{bmatrix} -A_{11}A_{11}^{-1}A_{12}y + A_{12}y \\ (A_{22}-A_{21}A_{11}^{-1}A_{12})y \end{bmatrix} >0 \\
        \implies \begin{pmatrix} (-A_{11}A_{12}y)^{\top} y^{\top} \end{pmatrix} \begin{bmatrix} -A_{12}y + A_{12}y \\ (A_{22}-A_{21}A_{11}^{-1}A_{12})y \end{bmatrix} >0 \\
        \implies \begin{pmatrix} (-A_{11}A_{12}y)^{\top} y^{\top} \end{pmatrix} \begin{bmatrix} 0 \\ (A_{22}-A_{21}A_{11}^{-1}A_{12})y \end{bmatrix} >0, \\
        \implies y^{\top}(A_{22}-A_{21}A_{11}^{-1}A_{12})y >0 \\
        \implies y^{\top}\tilde{A}_{22}y > 0 \\
    .\end{align*}
    So, $\tilde{A}_{22}$ is positive definite. $\endpf $

    



    \pagebreak \bigbreak \noindent 
    \begin{mdframed}
        1.4.62. Prove that if $A$ is positive definite, then $\det(A) > 0$
    \end{mdframed}
    \bigbreak \noindent 
    \textbf{\textit{Proof.}} Assume that $A$ is a positive definite matrix. Since $A$ is positive definite, then $A$ admits a Cholesky decomposition $A = R^{\top}R$, for a unique upper triangular matrix $R$, with $r_{ii}> 0$. So,
    \begin{align*}
        A &= R^{\top}R \\
        \implies \det(A) &= \det(R^{\top}R) = \det(R^{\top})\det(R)
    \end{align*}
    But, since the determinant of a triangular matrix is the product of the main diagonal, and $r_{ii} > 0$ for $i=1,2,...,n $, both $ \det(R^{\top})$ and $\det(R)$ are positive, so their product is positive. Thus,
    \begin{align*}
        \det(A) &= \det(R^{\top})\det(R) > 0
    \end{align*}
    $\endpf $


\end{document}
