 \documentclass{report}
 
 \input{~/dev/latex/template/preamble.tex}
 \input{~/dev/latex/template/macros.tex}
 
 \title{\Huge{}}
 \author{\huge{Nathan Warner}}
 \date{\huge{}}
 \fancyhf{}
 \rhead{}
 \fancyhead[R]{\itshape Warner} % Left header: Section name
 \fancyhead[L]{\itshape\leftmark}  % Right header: Page number
 \cfoot{\thepage}
 \renewcommand{\headrulewidth}{0pt} % Optional: Removes the header line
 %\pagestyle{fancy}
 %\fancyhf{}
 %\lhead{Warner \thepage}
 %\rhead{}
 % \lhead{\leftmark}
 %\cfoot{\thepage}
 %\setborder
 % \usepackage[default]{sourcecodepro}
 % \usepackage[T1]{fontenc}
 
 % Change the title
 \hypersetup{
     pdftitle={}
 }

 \geometry{
  left=1in,
  right=1in,
  top=1in,
  bottom=1in
}
 
 \begin{document}
     % \maketitle
     %     \begin{titlepage}
     %    \begin{center}
     %        \vspace*{1cm}
     % 
     %        \textbf{}
     % 
     %        \vspace{0.5cm}
     %         
     %             
     %        \vspace{1.5cm}
     % 
     %        \textbf{Nathan Warner}
     % 
     %        \vfill
     %             
     %             
     %        \vspace{0.8cm}
     %      
     %        \includegraphics[width=0.4\textwidth]{~/niu/seal.png}
     %             
     %        Computer Science \\
     %        Northern Illinois University\\
     %        United States\\
     %        
     %             
     %    \end{center}
     % \end{titlepage}
     % \tableofcontents
    \pagebreak \bigbreak \noindent
    Nate Warner \ \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad  MATH 434 \quad  \quad \quad \quad \quad \quad \quad \quad \quad \ \ \quad \quad Fall 2025
    \begin{center}
        \textbf{Problem set 2 - Due: Wednesday, October 15}
    \end{center}
    \bigbreak \noindent 
    \begin{mdframed}
        1.7.10. Let 
        \[
            A = 
            \begin{bmatrix}
                2 & 1 & -1 & 3 \\
                -2 & 0 & 0 & 0 \\
                4 & 1 & -2 & 6 \\
                -6 & -1 & 2 & -3
            \end{bmatrix},
            \qquad
            b =
            \begin{bmatrix}
                13 \\
                -2 \\
                24 \\
                -14
            \end{bmatrix}.
        \]
        \begin{enumerate}[label=(\alph*)]
            \item Calculate the appropriate (four) determinants to show that $A$ can be transformed 
                to (nonsingular) upper-triangular form by operations of type 1 only. 
                (By the way, this is strictly an academic exercise. 
                In practice one never calculates these determinants in advance.)

            \item Carry out the row operations of type 1 to transform the system $Ax = b$ 
                to an equivalent system $Ux = y$, where $U$ is upper triangular. 
                Save the multipliers for use in Exercise 1.7.18.

            \item Carry out the back substitution on the system $Ux = y$ 
                to obtain the solution of $Ax = b$. 
                Donâ€™t forget to check your work.
        \end{enumerate}
    \end{mdframed}
    \bigbreak \noindent 
    \begin{remark}
        Let $A \in \mathbb{R}^{n\times n}$. $A$ admits an $LU$ factorization $A = LU$ where $L$ is unit lower triangular and $U$ is upper triangular if and only if all leading principal submatrices are nonsingular. $\endpf $
    \end{remark}
    \bigbreak \noindent 
    a.) So, we check that
    \begin{align*}
        \det\left(\begin{bmatrix} 2 \end{bmatrix}\right),\; \det\left(\begin{bmatrix} 2 & 1 \\ -2 & 0 \end{bmatrix}\right),\; \det\left(\begin{bmatrix} 2 & 1 & -1 \\ -2 & 0 & 0 \\ 4 & 1 & -2 \end{bmatrix}\right),\; \det\left(            \begin{bmatrix} 2 & 1 & -1 & 3 \\ -2 & 0 & 0 & 0 \\ 4 & 1 & -2 & 6 \\ -6 & -1 & 2 & -3 \end{bmatrix} \right)
    \end{align*}
    are all nonzero. We see that
    \begin{align*}
        \det\left(\begin{bmatrix} 2 \end{bmatrix}\right) &= 2 \ne 0, \\
        \det\left(\begin{bmatrix} 2 & 1 \\ -2 & 0 \end{bmatrix}\right) &= 2(0) - (1)(-2) = 2 \ne 0, \\
        \det\left(\begin{bmatrix} 2 & 1 & -1 \\ -2 & 0 & 0 \\ 4 & 1 & -2 \end{bmatrix}\right) &= -1 \cdot -2(1(-2) - (-1)(1)) = -1 \cdot  2(-2+1) = -2 \ne 0, \\
        \det\left(\begin{bmatrix} 2 & 1 & -1 & 3 \\ -2 & 0 & 0 & 0 \\ 4 & 1 & -2 & 6 \\ -6 & -1 & 2 & -3 \end{bmatrix}\right) &= -1 \cdot -2\det\left(\begin{bmatrix} 1 & -1 & 3 \\ 1 & -2 & 6 \\ -1 & 2 & -3 \end{bmatrix}\right) \\
                                    &= 2\left( 1(-2(-3) - 6(2)) - (-1)(1(-3) - 6(-1)) + 3(1(2) - (-2)(-1)) \right) \\
                                    &= 2(-3) = -6 \ne 0
                                .\end{align*}
                                Thus, all leading principal submatrices are nonsingular and $A$ can be transformed to nonsingular upper-triangular form by operations of type 1 only.
                                \bigbreak \noindent 
                                b.) We use Gaussian Elimination on the augmented system $[A|b] \to [U|y]$. We have
                                \begin{align*}
                                    \begin{bmatrix}
                                        \begin{array}{cccc|c}
                                            2 & 1 & -1 & 3 & 13\\
                                            -2 & 0 & 0 & 0 & -2\\
                                            4 & 1 & -2 & 6 & 24\\
                                            -6 & -1 & 2 & -3 &-14
                                        \end{array}
                                    \end{bmatrix}
                                .\end{align*}
                                The opertions to get $a_{21} = a_{31} = a_{41} = 0$ are
                                \begin{align*}
        &-(-1)r_{1} + r_{2} \to r_{2}^{\prime}, \\
        &-2r_{1} + r_{3} \to r_{3}^{\prime}, \\
        &-(-3)r_{1} + r_{4} \to r_{4}^{\prime}
    .\end{align*}
    Thus, $m_{21} = -1$, $m_{31} = 2$, $m_{41} = -3$ and the system becomes
    \begin{align*}
        \begin{bmatrix}
            \begin{array}{cccc|c}
                2 & 1 & -1 & 3 & 13 \\
                0 & 1 & -1 & 3 & 11 \\
                0 & -1 & 0 & 0 & -2 \\
                0 & 2 & -1 & 6 & 25
            \end{array}
        \end{bmatrix}
    .\end{align*}
    Next, we set $a_{22}$ as the pivot element, $r_{2}$ as the pivot row, and perform the following operations to get $a_{23} = a_{24} = 0$. The operations are
    \begin{align*}
        &r_{3}^{\prime} \rto r_{3} - (-1)r_{2} \implies m_{32} = -1, \\
        &r_{4}^{\prime} \rto r_{4} - 2r_{2} \implies m_{42} = 2
    .\end{align*}
    After these operations, the system becomes
    \begin{align*}
        \begin{bmatrix}
            \begin{array}{cccc|c}
                2 & 1 & -1 & 3 & 13 \\
                0 & 1 & -1 & 3 & 11 \\
                0 & 0 & -1 & 3 & 9 \\
                0 & 0 & 1 & 0 & 3
            \end{array}
        \end{bmatrix}
    .\end{align*}
    Next, we set $a_{33}$ as the pivot element, and $r_{3}$ as the pivot row, and perform the operation
    \begin{align*}
        r_{4}^{\prime} \rto r_{4} - (-1)r_{3} \implies m_{43} = -1
    .\end{align*}
    After this operation, the system becomes
    \begin{align*}
        \begin{bmatrix}
            \begin{array}{cccc|c}
                2 & 1 & -1 & 3 & 13 \\
                0 & 1 & -1 & 3 & 11 \\
                0 & 0 & -1 & 3 & 9 \\
                0 & 0 & 0 & 3 & 12
            \end{array}
        \end{bmatrix}
    .\end{align*}
    Thus, the system $Ux = y$ is 
    \begin{align*}
        \begin{bmatrix}
            2 & 1 & -1 & 3  \\
            0 & 1 & -1 & 3 \\
            0 & 0 & -1 & 3\\
            0 & 0 & 0 & 3 
        \end{bmatrix}
        \begin{bmatrix}
            x_{1} \\ x_{2} \\ x_{3} \\ x_{4}
        \end{bmatrix}
        = 
        \begin{bmatrix}
            13 \\ 11 \\ 9 \\ 12
        \end{bmatrix}
    .\end{align*}
    (c) Using back substitution, we can solve the above system.
    \begin{align*}
        3x_{4} &= 12 \implies x_{4} = 4, \\
        -x_{3} + 3x_{4} &= 9 \implies x_{3} = -1(9-3(4)) = 3, \\
        x_{2} - x_{3} + 3x_{4} &= 11 \implies x_{2} = 11 - 3(4)+3 = 2, \\
        2x_{1} + x_{2} - x_{3} + 3x_{4} &= 13 \implies x_{1} = \frac{13-3(4)+3-2}{2} = 1
    .\end{align*}
    So, the solution to $Ax = b$ is 
    \begin{align*}
        x = \begin{pmatrix} 1 \\ 2 \\ 3 \\ 4 \end{pmatrix}
    .\end{align*}
    We can verify this solution by computing $Ax$, and observing that it equals the given $b$. We see that
    \begin{align*}
        \begin{bmatrix}
            2 & 1 & -1 & 3  \\
            -2 & 0 & 0 & 0  \\
            4 & 1 & -2 & 6 \\
            -6 & -1 & 2 & -3
        \end{bmatrix}
        \begin{pmatrix} 1 \\ 2\\ 3\\ 4 \end{pmatrix}
        &= 
        \begin{pmatrix} 2 + 2 -3 + 12 \\ -2 +  0 + 0 + 0 \\ 4 + 2 - 6 + 24 \\ -6 -2 + 6 -12 \end{pmatrix} = 
        \begin{pmatrix} 13 \\ -2 \\ 24 \\ -14 \end{pmatrix}
    .\end{align*}
    Thus, the solution is verified.
    \bigbreak \noindent 
    \textbf{Note:} We can assemble our multipliers to form $L$, we have
    \begin{align*}
        L &= \begin{bmatrix}
            1 & 0& 0 & 0 \\
            -1 & 1 & 0& 0 \\
            2 & -1 & 1 & 0\\
            -3 & 2 & -1 & 1
        \end{bmatrix}
    .\end{align*}




    \bigbreak \noindent 
    \begin{mdframed}
        1.7.18. Solve the linear system $A x = \hat{b}$, where $A$ is as in Exercise 1.7.10 and 
        \[
            \hat{b} = \begin{bmatrix} 12 & -8 & 21 & -26 \end{bmatrix}^{T}.
        \]
        Use the $L$ and $U$ that you calculated in Exercise 1.7.10.
    \end{mdframed}
    \bigbreak \noindent 
    From the previous exercise, we have that
    \begin{align*}
        L &= \begin{bmatrix} 1 & 0& 0 & 0 \\ -1 & 1 & 0& 0 \\ 2 & -1 & 1 & 0\\ -3 & 2 & -1 & 1 \end{bmatrix}, 
        \quad U = \begin{bmatrix} 2 & 1 & -1 & 3  \\ 0 & 1 & -1 & 3 \\ 0 & 0 & -1 & 3\\ 0 & 0 & 0 & 3 \end{bmatrix}
    .\end{align*}
    So, the system $Ax = \hat{b}$ is solved using our $LU$ decomposition for $A$. We have
    \begin{align*}
        \begin{bmatrix} 1 & 0& 0 & 0 \\ -1 & 1 & 0& 0 \\ 2 & -1 & 1 & 0\\ -3 & 2 & -1 & 1 \end{bmatrix} \begin{bmatrix} 2 & 1 & -1 & 3  \\ 0 & 1 & -1 & 3 \\ 0 & 0 & -1 & 3\\ 0 & 0 & 0 & 3 \end{bmatrix} \begin{pmatrix} x_{1} \\ x_{2} \\ x_{3} \\ x_{4} \end{pmatrix} &= \begin{pmatrix} 12 \\ -8 \\ 21 \\ -26 \end{pmatrix}
    .\end{align*}
    Let $ Ux = y$, and $Ly = b$. First, we solve $Ly = b$ for $y$ using forward substitution. 
    \begin{align*}
        \begin{bmatrix} 1 & 0& 0 & 0 \\ -1 & 1 & 0& 0 \\ 2 & -1 & 1 & 0\\ -3 & 2 & -1 & 1 \end{bmatrix} \begin{pmatrix} y_{1} \\ y_{2} \\ y_{3} \\ y_{4}  \end{pmatrix} &= \begin{pmatrix} 12 \\ -8 \\ 21 \\ -26 \end{pmatrix}
    \end{align*}
    implies
    \begin{align*}
        y_{1} &= 12, \\
        -y_{1} + y_{2} &= -8 \implies y_{2} = -8 + 12 = 4, \\
        2y_{1} - y_{2} + y_{3} &= 21 \implies y_{3} = 21 + 4 - 2(12) = 1, \\
        -3y_{1} + 2y_{2} - y_{3} + y_{4} &= -26 \implies y_{4} = -26 +1-2(4)+3(12) = 3
    .\end{align*}
    So,
    \begin{align*}
        y = \begin{pmatrix} 12 \\ 4 \\ 1 \\3 \end{pmatrix}
    .\end{align*}
    Now, we solve $Ux =y$ with backward substitution. We have
    \begin{align*}
        \begin{bmatrix} 2 & 1 & -1 & 3  \\ 0 & 1 & -1 & 3 \\ 0 & 0 & -1 & 3\\ 0 & 0 & 0 & 3 \end{bmatrix} \begin{pmatrix} x_{1} \\ x_{2} \\ x_{3} \\ x_{4} \end{pmatrix} &= \begin{pmatrix} 12 \\ 4\\ 1 \\3 \end{pmatrix}
    .\end{align*}
    Which, implies that
    \begin{align*}
        3x_{4} &= 3 \implies x_{4} = 1, \\
        -x_{3} + 3x_{4} &= 1\implies x_{3} = -1(1-3(1)) = 2, \\
        x_{2} - x_{3} + 3x_{4} &= 4 \implies x_{2} = 4-3(1)+2 = 3, \\
        2x_{1} + x_{2} - x_{3} + 3x_{4} &= 12 \implies x_{1} = \frac{12-3(1)+2-3}{2} = 4
    .\end{align*}
    So,
    \begin{align*}
        x = \begin{pmatrix} 4 \\ 3 \\ 2 \\ 1 \end{pmatrix}
    .\end{align*}
    We verify the result by computing $Ax$, and comparing it against $\hat{b}$. We have
    \begin{align*}
        \begin{bmatrix} 2 & 1 & -1 & 3 \\ -2 & 0 & 0 & 0 \\ 4 & 1 & -2 & 6 \\ -6 & -1 & 2 & -3 \end{bmatrix} \begin{pmatrix} 4 \\ 3 \\ 2 \\ 1 \end{pmatrix} &= \begin{pmatrix} 2(4) + 1(3) - 1(2) + 3(1) \\ -2(4) + 0 + 0 + 0 \\ 4(4)  +1(3) -2(2) +6(1) \\ -6(4) -1(3) + 2(2)-3(1) \end{pmatrix} = \begin{pmatrix} 12 \\ -8 \\ 21 \\ -26 \end{pmatrix}
    .\end{align*}
    The result is verified.

    \bigbreak \noindent 
    \begin{mdframed}
        1.7.26. Use the inner-product formulation to calculate the $LU$ decomposition of the matrix $A$ in Exercise 1.7.10 
    \end{mdframed}
    \bigbreak \noindent 
    \begin{remark}
        The inner-product formulas to compute the $LU$ decomposition are  
        \begin{align*}
            u_{ij} &= a_{ij} - \sum_{k=1}^{i-1}\ell_{ik}u_{kj} \quad j=i,i+1,...,n \tag{1}, \\
            \ell_{ij} &= \frac{a_{ij} - \sum_{k=1}^{j-1}\ell_{ik}u_{kj}}{u_{jj}} \quad i=j+1,j+2,...,n \tag{2}
        .\end{align*}
        \bigbreak \noindent 
        To use these formulas to find each $u_{ij}$ we first need to plug $i=1$ into $(1)$, then after we get the first row of $U$, we can plug in $j=1$ into $(2)$ to get the first column of $L$, and so on. $\endpf $
    \end{remark}
    \bigbreak \noindent 
    Recall that the matrix $A$ is given as
    \begin{align*}
        \begin{bmatrix}
            2 & 1 & -1 & 3 \\
            -2 & 0 & 0 & 0 \\
            4 & 1 & -2 & 6 \\
            -6 & -1 & 2 & -3
       \end{bmatrix} 
    .\end{align*}
    So, we first find the first row of $U$ (set $i=1$), we have
    \begin{align*}
        u_{11} &= a_{11} =  2, \\
        u_{12} &= a_{12} = 1 , \\
        u_{13} &= a_{13} = -1, \\
        u_{14} &= a_{14} = 3
    .\end{align*}
    Next, we find the first column of $L$ (set $j=1$),
    \begin{align*}
        \ell_{11} &= 1, \\
        \ell_{21} &= \frac{a_{21}}{u_{11}} = \frac{-2}{2} = -1, \\
        \ell_{31} &= \frac{a_{31}}{u_{11}} = \frac{4}{2} = 2,\\
        \ell_{41} &= \frac{a_{41}}{u_{11}} = -\frac{6}{2} = -3
    .\end{align*}
    For the second row of $U$ ($i = 2$), 
    \begin{align*}
        u_{22} &= a_{22} - \sum_{k=1}^{1}\ell_{2k}u_{k2} = 0 - (-1)(1) = 1, \\
        u_{23} &= a_{23} - \sum_{k=1}^{1}\ell_{2k}u_{k3} = 0 - (-1)(-1) = -1, \\
        u_{24} &= a_{24} - \sum_{k=1}^{1}\ell_{2k}u_{k4} = 0 - (-1)(3) = 3
    .\end{align*}
    For the second column of $L$ ($j=2$),
    \begin{align*}
        \ell_{22} &= 1, \\
        \ell_{32} &= \frac{a_{32} - \sum_{k=1}^{1}\ell_{3k}u_{k2}}{u_{22}} = \frac{1-2(1)}{1} = -1, \\
        \ell_{42} &= \frac{a_{42}- \sum_{k=1}^{1}\ell_{4k}u_{k2}}{u_{22}} = \frac{-1 - (-3)(1)}{1} = 2
    .\end{align*}
    For the third row of $U$ ($i = 3$),
    \begin{align*}
        u_{33} &= a_{33} - \sum_{k=1}^{2}\ell_{3k}u_{k3} = a_{33} - (\ell_{31}u_{13} + \ell_{32}u_{23}) = -2 - (2(-1) + (-1)(-1)) = -1, \\
        u_{34} &= a_{34} - \sum_{k=1}^{2}\ell_{3k}u_{k4} = a_{34} - (\ell_{31}u_{14} + \ell_{32}u_{24}) = 6 - (2(3) + (-1)(3)) = 3
    .\end{align*}
    For the third column of $L$ ($j=3$), 
    \begin{align*}
        \ell_{33} &= 1, \\
        \ell_{43} &= \frac{a_{43}- \sum_{k=1}^{2}\ell_{4k}u_{k3}}{u_{33}} = \frac{a_{43} - (\ell_{41}u_{13} + \ell_{42}u_{23})}{u_{33}} = \frac{2 - ((-3)(-1) + (2)(-1))}{-1} = -1
    .\end{align*}
    For the fourth row of $U$ ($i =4$), 
    \begin{align*}
        u_{44} &= a_{44} - \sum_{k=1}^{3}\ell_{4k}u_{k4}  = a_{44} - (\ell_{41}u_{14} + \ell_{42}u_{24}  + \ell_{43}u_{34}) = -3 - ((-3)(3) + 2(3) + (-1)(3)) = -3
    .\end{align*}
    For the fourth column of $L$ ($j=4$),
    \begin{align*}
        \ell_{44} &= 1
    .\end{align*}
    So, the $LU$ decomposition according to the inner-product formulas is
    \begin{align*}
        L &= \begin{bmatrix} 1 & 0& 0 & 0 \\ -1 & 1 & 0& 0 \\ 2 & -1 & 1 & 0\\ -3 & 2 & -1 & 1 \end{bmatrix}, 
        \quad U = \begin{bmatrix} 2 & 1 & -1 & 3  \\ 0 & 1 & -1 & 3 \\ 0 & 0 & -1 & 3\\ 0 & 0 & 0 & 3 \end{bmatrix}
    .\end{align*}
    Which is exactly the same decomposition that we got with Gaussian Elimination.
    

    \bigbreak \noindent 
    \begin{mdframed}
        1.7.34.  In this exercise you will show that performing an elementary row operation of type 1 is equivalent to left multiplication by a matrix of a special type. Suppose $\tilde{A}$ is obtained from $A$ by adding $m$ times the $j$th row to the $i$th row.

        \begin{enumerate}[label=(\alph*)]
            \item Show that $\tilde{A} = M A$, where $M$ is the triangular matrix obtained 
                from the identity matrix by replacing the zero by an $m$ in the $(i,j)$ position. 
                For example, when $i > j$, $M$ has the form
                \[
                    M = \begin{bmatrix}
                        1 &        &        &        &   \\
                          & \ddots &        &        &   \\
                          &        & 1      &        &   \\
                          &        & m      & 1      &   \\
                          &        &        &        & \ddots \\
                          &        &        &        &        1
                    \end{bmatrix}.
                \]
                Notice that this is the matrix obtained by applying the type 1 row operation 
                directly to the identity matrix. We call $M$ an \textit{elementary matrix of type 1}.

            \item Show that $\det(M) = 1$ and $\det(\tilde{A}) = \det(A)$. 
                Thus we see (again) that $\tilde{A}$ is nonsingular if and only if $A$ is.

            \item Show that $M^{-1}$ differs from $M$ only in that it has $-m$ instead of $m$ 
                in the $(i,j)$ position. $M^{-1}$ is also an elementary matrix of type 1. 
                To which elementary operation does it correspond?
        \end{enumerate}
    \end{mdframed}

    \bigbreak \noindent 
    \begin{mdframed}
        1.7.36. Suppose $\tilde{A}$ is obtained from $A$ by multiplying the $i$th row by the nonzero constant $c$.
        \begin{enumerate}[(a)]
            \item Find the form of the matrix $M$ (an \textit{elementary matrix of type 3}) 
                such that $\tilde{A} = M A$.

            \item Find $M^{-1}$ and state its function as an elementary matrix.

            \item Find $\det(M)$ and determine the relationship between $\det(\tilde{A})$ 
                and $\det(A)$. Deduce that $\tilde{A}$ is nonsingular if and only if $A$ is.
        \end{enumerate}
    \end{mdframed}

    \bigbreak \noindent 
    \begin{mdframed}
        1.8.4. Let 
        \[
            A = 
            \begin{bmatrix}
                2 & 2 & -4 \\
                1 & 1 & 5 \\
                1 & 3 & 6
            \end{bmatrix},
            \qquad
            b =
            \begin{bmatrix}
                10 \\
                -2 \\
                -5
            \end{bmatrix}.
        \]
        Use Gaussian elimination with partial pivoting (by hand) to find matrices $L$ and $U$ such that $U$ is upper triangular, $L$ is unit lower triangular with $|l_{ij}| \leq 1$ for all $i > j$, and $LU = \tilde{A}$, where $\tilde{A}$ can be obtained from $A$ by making row interchanges. Use your $LU$ decomposition to solve the system $Ax = b$.
    \end{mdframed}

    \bigbreak \noindent 
    \begin{mdframed}
        1.8.9. Let $A$ be the matrix in Exercise 1.8.4. Determine matrices $P$, $L$, and $U$ with the properties stated in Theorem 1.8.8, such that $A = P^{T}LU$
    \end{mdframed}

    \bigbreak \noindent 
    \begin{mdframed}
        1.8.12. Write an algorithm that implements Gaussian elimination with partial pivoting. Store $L$ and $U$ over $A$, and save a record of the row interchanges. 
    \end{mdframed}

    \bigbreak \noindent 
    \begin{mdframed}
        2.1.10. Prove that the 1-norm is a norm. 
    \end{mdframed}


    \bigbreak \noindent 
    \begin{mdframed}
        2.1.13.  Prove that the $\infty$-norm is a norm.
    \end{mdframed}

    \bigbreak \noindent 
    \begin{mdframed}
        2.1.17. \begin{enumerate}[(a)]
            \item Let $A$ be a positive definite matrix, and let $R$ be its Cholesky factor, 
                so that $A = R^{T}R$. Verify that for all $x \in \mathbb{R}^n$, 
                \[
                    \|x\|_{A} = \|Rx\|_{2}.
                \]

            \item Using the fact that the $2$-norm is indeed a norm on $\mathbb{R}^n$, 
                prove that the $A$-norm is a norm on $\mathbb{R}^n$.
        \end{enumerate}
    \end{mdframed}

    \bigbreak \noindent 
    \begin{mdframed}
        2.2.6.
        \begin{enumerate}[label=(\alph*)]
            \item Show that $\kappa(A) = \kappa(A^{-1})$
            \item Show that for any nonzero scalar $c$, $\kappa(cA) = \kappa(A)$
        \end{enumerate}
    \end{mdframed}

    \bigbreak \noindent 
    \begin{mdframed}
        2.2.15. Let us take another look at the ill-conditioned matrices
        \[
            A = 
            \begin{bmatrix}
                1000 & 999 \\
                999  & 998
            \end{bmatrix},
            \qquad
            A^{-1} =
            \begin{bmatrix}
                -998 & 999 \\
                999  & -1000
            \end{bmatrix}
        \]
        from Example 2.2.8. Notice that
        \[
            A
            \begin{bmatrix}
                1 \\
                1
            \end{bmatrix}
            =
            \begin{bmatrix}
                1999 \\
                1997
            \end{bmatrix}.
            \tag{2.2.16}
        \]
        If we use the $\infty$-norm to measure lengths, the magnification factor 
        \[
            \frac{\|Ax\|_{\infty}}{\|x\|_{\infty}}
        \]
        is $1999$, which equals $\|A\|_{\infty}$. Thus 
        \(
        \begin{bmatrix}
            1 \\ 1
        \end{bmatrix}
        \) 
        is a vector that is magnified maximally by $A$. 
        \bigbreak \noindent 
        Since the amount by which a vector is magnified depends only on its direction and not on its length, 
        we say that 
        \(
        \begin{bmatrix}
            1 \\ 1
        \end{bmatrix}
        \) 
        is in a \textit{direction of maximum magnification} by $A$. 
        \bigbreak \noindent 
        Equivalently we can say that 
        \(
        \begin{bmatrix}
            1999 \\ 1997
        \end{bmatrix}
        \) 
        lies in a \textit{direction of minimum magnification}.
        \bigbreak \noindent 
        Looking now at $A^{-1}$, we note that
        \[
            A^{-1} 
            \begin{bmatrix}
                -1 \\
                1
            \end{bmatrix}
            =
            \begin{bmatrix}
                1997 \\
                -1999
            \end{bmatrix}.
        \]
        The magnification factor 
        \(
        \frac{\|A^{-1}x\|_{\infty}}{\|x\|_{\infty}}
        \)
        is $1999$, which equals $\|A^{-1}\|_{\infty}$, so 
        \(
        \begin{bmatrix}
            -1 \\ 1
        \end{bmatrix}
        \)
        is in a direction of maximum magnification by $A^{-1}$. Equivalently
        \[
            A 
            \begin{bmatrix}
                1997 \\
                -1999
            \end{bmatrix}
            =
            \begin{bmatrix}
                -1 \\
                1
            \end{bmatrix},
            \tag{2.2.17}
        \]
        and 
        \(
        \begin{bmatrix}
            1997 \\ -1999
        \end{bmatrix}
        \)
        is in a direction of minimum magnification by $A$.
        \bigbreak \noindent 
        We will use the vectors in (2.2.16) and (2.2.17) to construct a spectacular example.  Suppose we wish to solve the system
        \[
            \begin{bmatrix}
                1000 & 999 \\
                999  & 998
            \end{bmatrix}
            \begin{bmatrix}
                x_{1} \\
                x_{2}
            \end{bmatrix}
            =
            \begin{bmatrix}
                1999 \\
                1997
            \end{bmatrix}.
            \tag{2.2.18}
        \]
        That is, $Ax = b$, where 
        \(
        b =
        \begin{bmatrix}
            1999 \\
            1997
        \end{bmatrix}.
        \) 
        Then by (2.2.16) the unique solution is
        \[
            x =
            \begin{bmatrix}
                1 \\
                1
            \end{bmatrix}.
        \]
        Now suppose that we solve instead the slightly perturbed system
        \[
            \begin{bmatrix}
                1000 & 999 \\
                999  & 998
            \end{bmatrix}
            \begin{bmatrix}
                \hat{x}_{1} \\
                \hat{x}_{2}
            \end{bmatrix}
            =
            \begin{bmatrix}
                1998.99 \\
                1997.01
            \end{bmatrix}.
            \tag{2.2.19}
        \]
        This is $\hat{A}x = b + \delta b$, where 
        \(
        \delta b =
        \begin{bmatrix}
            -0.01 \\
            0.01
        \end{bmatrix}
        = 0.01
        \begin{bmatrix}
            -1 \\
            1
        \end{bmatrix},
        \)
        which is in a direction of maximum magnification by $A^{-1}$. By (2.2.17), $A\delta x = \delta b$, where 
        \(
        \delta x =
        \begin{bmatrix}
            19.97 \\
            -19.99
        \end{bmatrix}.
        \)
        Therefore 
        \(
        \hat{x} = x + \delta x =
        \begin{bmatrix}
            20.97 \\
            -18.99
        \end{bmatrix}.
        \)
        Thus the nearly identical problems (2.2.18) and (2.2.19) have very different solutions.
    \end{mdframed}

    \bigbreak \noindent 
    \begin{mdframed}
        Repeat the proof of Theorem 2.3.3.
    \end{mdframed}
    \bigbreak \noindent 
    \begin{remark}
        \textbf{Theorem 2.3.3}. 
        Let $A$ be nonsingular, let $b \neq 0$, and let $x$ and $\hat{x} = x + \delta x$ be 
        solutions of $Ax = b$ and $(A + \delta A)\hat{x} = b$, respectively. Then,
        \[
            \frac{\|\delta x\|}{\|\hat{x}\|} 
            \leq \kappa(A) \frac{\|\delta A\|}{\|A\|}.
            \tag{2.3.4}
        \]
        \textbf{\textit{Proof.}} Rewriting the equation $(A + \delta A)\hat{x} = b$ as 
        $Ax + A\delta x + \delta A\hat{x} = b$, using the equation $Ax = b$, and reorganizing 
        the resulting equation, we obtain
        \[
            \delta x = -A^{-1} \delta A \hat{x}.
        \]
        Thus
        \[
            \|\delta x\| \leq \|A^{-1}\| \, \|\delta A\| \, \|\hat{x}\|.
            \tag{2.3.5}
        \]
        Dividing through by $\|\hat{x}\|$ and using the definition 
        $\kappa(A) = \|A\| \, \|A^{-1}\|$, we obtain the desired result.
    \end{remark}
    


\end{document}
