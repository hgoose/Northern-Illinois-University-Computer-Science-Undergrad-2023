 \documentclass{report}
 
 \input{~/dev/latex/template/preamble.tex}
 \input{~/dev/latex/template/macros.tex}
 
 \title{\Huge{}}
 \author{\huge{Nathan Warner}}
 \date{\huge{}}
 \fancyhf{}
 \rhead{}
 \fancyhead[R]{\itshape Warner} % Left header: Section name
 \fancyhead[L]{\itshape\leftmark}  % Right header: Page number
 \cfoot{\thepage}
 \renewcommand{\headrulewidth}{0pt} % Optional: Removes the header line
 %\pagestyle{fancy}
 %\fancyhf{}
 %\lhead{Warner \thepage}
 %\rhead{}
 % \lhead{\leftmark}
 %\cfoot{\thepage}
 %\setborder
 % \usepackage[default]{sourcecodepro}
 % \usepackage[T1]{fontenc}
 
 % Change the title
 \hypersetup{
     pdftitle={}
 }

 \geometry{
  left=1in,
  right=1in,
  top=1in,
  bottom=1in
}
 
 \begin{document}
     % \maketitle
     %     \begin{titlepage}
     %    \begin{center}
     %        \vspace*{1cm}
     % 
     %        \textbf{}
     % 
     %        \vspace{0.5cm}
     %         
     %             
     %        \vspace{1.5cm}
     % 
     %        \textbf{Nathan Warner}
     % 
     %        \vfill
     %             
     %             
     %        \vspace{0.8cm}
     %      
     %        \includegraphics[width=0.4\textwidth]{~/niu/seal.png}
     %             
     %        Computer Science \\
     %        Northern Illinois University\\
     %        United States\\
     %        
     %             
     %    \end{center}
     % \end{titlepage}
     % \tableofcontents
    \pagebreak \bigbreak \noindent
    Nate Warner \ \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad  MATH 434 \quad  \quad \quad \quad \quad \quad \quad \quad \quad \ \ \quad \quad Fall 2025
    \begin{center}
        \textbf{Problem set 2 - Due: Wednesday, October 15}
    \end{center}
    \bigbreak \noindent 
    \begin{mdframed}
        1.7.10. Let 
        \[
            A = 
            \begin{bmatrix}
                2 & 1 & -1 & 3 \\
                -2 & 0 & 0 & 0 \\
                4 & 1 & -2 & 6 \\
                -6 & -1 & 2 & -3
            \end{bmatrix},
            \qquad
            b =
            \begin{bmatrix}
                13 \\
                -2 \\
                24 \\
                -14
            \end{bmatrix}.
        \]
        \begin{enumerate}[label=(\alph*)]
            \item Calculate the appropriate (four) determinants to show that $A$ can be transformed 
                to (nonsingular) upper-triangular form by operations of type 1 only. 
                (By the way, this is strictly an academic exercise. 
                In practice one never calculates these determinants in advance.)

            \item Carry out the row operations of type 1 to transform the system $Ax = b$ 
                to an equivalent system $Ux = y$, where $U$ is upper triangular. 
                Save the multipliers for use in Exercise 1.7.18.

            \item Carry out the back substitution on the system $Ux = y$ 
                to obtain the solution of $Ax = b$. 
                Donâ€™t forget to check your work.
        \end{enumerate}
    \end{mdframed}
    \bigbreak \noindent 
    \begin{remark}
        Let $A \in \mathbb{R}^{n\times n}$. $A$ admits an $LU$ factorization $A = LU$ where $L$ is unit lower triangular and $U$ is upper triangular if and only if all leading principal submatrices are nonsingular. $\endpf $
    \end{remark}
    \bigbreak \noindent 
    a.) So, we check that
    \begin{align*}
        \det\left(\begin{bmatrix} 2 \end{bmatrix}\right),\; \det\left(\begin{bmatrix} 2 & 1 \\ -2 & 0 \end{bmatrix}\right),\; \det\left(\begin{bmatrix} 2 & 1 & -1 \\ -2 & 0 & 0 \\ 4 & 1 & -2 \end{bmatrix}\right),\; \det\left(            \begin{bmatrix} 2 & 1 & -1 & 3 \\ -2 & 0 & 0 & 0 \\ 4 & 1 & -2 & 6 \\ -6 & -1 & 2 & -3 \end{bmatrix} \right)
    \end{align*}
    are all nonzero. We see that
    \begin{align*}
        \det\left(\begin{bmatrix} 2 \end{bmatrix}\right) &= 2 \ne 0, \\
        \det\left(\begin{bmatrix} 2 & 1 \\ -2 & 0 \end{bmatrix}\right) &= 2(0) - (1)(-2) = 2 \ne 0, \\
        \det\left(\begin{bmatrix} 2 & 1 & -1 \\ -2 & 0 & 0 \\ 4 & 1 & -2 \end{bmatrix}\right) &= -1 \cdot -2(1(-2) - (-1)(1)) = -1 \cdot  2(-2+1) = -2 \ne 0, \\
        \det\left(\begin{bmatrix} 2 & 1 & -1 & 3 \\ -2 & 0 & 0 & 0 \\ 4 & 1 & -2 & 6 \\ -6 & -1 & 2 & -3 \end{bmatrix}\right) &= -1 \cdot -2\det\left(\begin{bmatrix} 1 & -1 & 3 \\ 1 & -2 & 6 \\ -1 & 2 & -3 \end{bmatrix}\right) \\
                                    &= 2\left( 1(-2(-3) - 6(2)) - (-1)(1(-3) - 6(-1)) + 3(1(2) - (-2)(-1)) \right) \\
                                    &= 2(-3) = -6 \ne 0
                                .\end{align*}
                                Thus, all leading principal submatrices are nonsingular and $A$ can be transformed to nonsingular upper-triangular form by operations of type 1 only.
                                \bigbreak \noindent 
                                b.) We use Gaussian Elimination on the augmented system $[A|b] \to [U|y]$. We have
                                \begin{align*}
                                    \begin{bmatrix}
                                        \begin{array}{cccc|c}
                                            2 & 1 & -1 & 3 & 13\\
                                            -2 & 0 & 0 & 0 & -2\\
                                            4 & 1 & -2 & 6 & 24\\
                                            -6 & -1 & 2 & -3 &-14
                                        \end{array}
                                    \end{bmatrix}
                                .\end{align*}
                                The opertions to get $a_{21} = a_{31} = a_{41} = 0$ are
                                \begin{align*}
        &-(-1)r_{1} + r_{2} \to r_{2}^{\prime}, \\
        &-2r_{1} + r_{3} \to r_{3}^{\prime}, \\
        &-(-3)r_{1} + r_{4} \to r_{4}^{\prime}
    .\end{align*}
    Thus, $m_{21} = -1$, $m_{31} = 2$, $m_{41} = -3$ and the system becomes
    \begin{align*}
        \begin{bmatrix}
            \begin{array}{cccc|c}
                2 & 1 & -1 & 3 & 13 \\
                0 & 1 & -1 & 3 & 11 \\
                0 & -1 & 0 & 0 & -2 \\
                0 & 2 & -1 & 6 & 25
            \end{array}
        \end{bmatrix}
    .\end{align*}
    Next, we set $a_{22}$ as the pivot element, $r_{2}$ as the pivot row, and perform the following operations to get $a_{23} = a_{24} = 0$. The operations are
    \begin{align*}
        &r_{3}^{\prime} \rto r_{3} - (-1)r_{2} \implies m_{32} = -1, \\
        &r_{4}^{\prime} \rto r_{4} - 2r_{2} \implies m_{42} = 2
    .\end{align*}
    After these operations, the system becomes
    \begin{align*}
        \begin{bmatrix}
            \begin{array}{cccc|c}
                2 & 1 & -1 & 3 & 13 \\
                0 & 1 & -1 & 3 & 11 \\
                0 & 0 & -1 & 3 & 9 \\
                0 & 0 & 1 & 0 & 3
            \end{array}
        \end{bmatrix}
    .\end{align*}
    Next, we set $a_{33}$ as the pivot element, and $r_{3}$ as the pivot row, and perform the operation
    \begin{align*}
        r_{4}^{\prime} \rto r_{4} - (-1)r_{3} \implies m_{43} = -1
    .\end{align*}
    After this operation, the system becomes
    \begin{align*}
        \begin{bmatrix}
            \begin{array}{cccc|c}
                2 & 1 & -1 & 3 & 13 \\
                0 & 1 & -1 & 3 & 11 \\
                0 & 0 & -1 & 3 & 9 \\
                0 & 0 & 0 & 3 & 12
            \end{array}
        \end{bmatrix}
    .\end{align*}
    Thus, the system $Ux = y$ is 
    \begin{align*}
        \begin{bmatrix}
            2 & 1 & -1 & 3  \\
            0 & 1 & -1 & 3 \\
            0 & 0 & -1 & 3\\
            0 & 0 & 0 & 3 
        \end{bmatrix}
        \begin{bmatrix}
            x_{1} \\ x_{2} \\ x_{3} \\ x_{4}
        \end{bmatrix}
        = 
        \begin{bmatrix}
            13 \\ 11 \\ 9 \\ 12
        \end{bmatrix}
    .\end{align*}
    (c) Using back substitution, we can solve the above system.
    \begin{align*}
        3x_{4} &= 12 \implies x_{4} = 4, \\
        -x_{3} + 3x_{4} &= 9 \implies x_{3} = -1(9-3(4)) = 3, \\
        x_{2} - x_{3} + 3x_{4} &= 11 \implies x_{2} = 11 - 3(4)+3 = 2, \\
        2x_{1} + x_{2} - x_{3} + 3x_{4} &= 13 \implies x_{1} = \frac{13-3(4)+3-2}{2} = 1
    .\end{align*}
    So, the solution to $Ax = b$ is 
    \begin{align*}
        x = \begin{pmatrix} 1 \\ 2 \\ 3 \\ 4 \end{pmatrix}
    .\end{align*}
    We can verify this solution by computing $Ax$, and observing that it equals the given $b$. We see that
    \begin{align*}
        \begin{bmatrix}
            2 & 1 & -1 & 3  \\
            -2 & 0 & 0 & 0  \\
            4 & 1 & -2 & 6 \\
            -6 & -1 & 2 & -3
        \end{bmatrix}
        \begin{pmatrix} 1 \\ 2\\ 3\\ 4 \end{pmatrix}
        &= 
        \begin{pmatrix} 2 + 2 -3 + 12 \\ -2 +  0 + 0 + 0 \\ 4 + 2 - 6 + 24 \\ -6 -2 + 6 -12 \end{pmatrix} = 
        \begin{pmatrix} 13 \\ -2 \\ 24 \\ -14 \end{pmatrix}
    .\end{align*}
    Thus, the solution is verified.
    \bigbreak \noindent 
    \textbf{Note:} We can assemble our multipliers to form $L$, we have
    \begin{align*}
        L &= \begin{bmatrix}
            1 & 0& 0 & 0 \\
            -1 & 1 & 0& 0 \\
            2 & -1 & 1 & 0\\
            -3 & 2 & -1 & 1
        \end{bmatrix}
    .\end{align*}




    \bigbreak \noindent 
    \begin{mdframed}
        1.7.18. Solve the linear system $A x = \hat{b}$, where $A$ is as in Exercise 1.7.10 and 
        \[
            \hat{b} = \begin{bmatrix} 12 & -8 & 21 & -26 \end{bmatrix}^{T}.
        \]
        Use the $L$ and $U$ that you calculated in Exercise 1.7.10.
    \end{mdframed}
    \bigbreak \noindent 
    From the previous exercise, we have that
    \begin{align*}
        L &= \begin{bmatrix} 1 & 0& 0 & 0 \\ -1 & 1 & 0& 0 \\ 2 & -1 & 1 & 0\\ -3 & 2 & -1 & 1 \end{bmatrix}, 
        \quad U = \begin{bmatrix} 2 & 1 & -1 & 3  \\ 0 & 1 & -1 & 3 \\ 0 & 0 & -1 & 3\\ 0 & 0 & 0 & 3 \end{bmatrix}
    .\end{align*}
    So, the system $Ax = \hat{b}$ is solved using our $LU$ decomposition for $A$. We have
    \begin{align*}
        \begin{bmatrix} 1 & 0& 0 & 0 \\ -1 & 1 & 0& 0 \\ 2 & -1 & 1 & 0\\ -3 & 2 & -1 & 1 \end{bmatrix} \begin{bmatrix} 2 & 1 & -1 & 3  \\ 0 & 1 & -1 & 3 \\ 0 & 0 & -1 & 3\\ 0 & 0 & 0 & 3 \end{bmatrix} \begin{pmatrix} x_{1} \\ x_{2} \\ x_{3} \\ x_{4} \end{pmatrix} &= \begin{pmatrix} 12 \\ -8 \\ 21 \\ -26 \end{pmatrix}
    .\end{align*}
    Let $ Ux = y$, and $Ly = b$. First, we solve $Ly = b$ for $y$ using forward substitution. 
    \begin{align*}
        \begin{bmatrix} 1 & 0& 0 & 0 \\ -1 & 1 & 0& 0 \\ 2 & -1 & 1 & 0\\ -3 & 2 & -1 & 1 \end{bmatrix} \begin{pmatrix} y_{1} \\ y_{2} \\ y_{3} \\ y_{4}  \end{pmatrix} &= \begin{pmatrix} 12 \\ -8 \\ 21 \\ -26 \end{pmatrix}
    \end{align*}
    implies
    \begin{align*}
        y_{1} &= 12, \\
        -y_{1} + y_{2} &= -8 \implies y_{2} = -8 + 12 = 4, \\
        2y_{1} - y_{2} + y_{3} &= 21 \implies y_{3} = 21 + 4 - 2(12) = 1, \\
        -3y_{1} + 2y_{2} - y_{3} + y_{4} &= -26 \implies y_{4} = -26 +1-2(4)+3(12) = 3
    .\end{align*}
    So,
    \begin{align*}
        y = \begin{pmatrix} 12 \\ 4 \\ 1 \\3 \end{pmatrix}
    .\end{align*}
    Now, we solve $Ux =y$ with backward substitution. We have
    \begin{align*}
        \begin{bmatrix} 2 & 1 & -1 & 3  \\ 0 & 1 & -1 & 3 \\ 0 & 0 & -1 & 3\\ 0 & 0 & 0 & 3 \end{bmatrix} \begin{pmatrix} x_{1} \\ x_{2} \\ x_{3} \\ x_{4} \end{pmatrix} &= \begin{pmatrix} 12 \\ 4\\ 1 \\3 \end{pmatrix}
    .\end{align*}
    Which, implies that
    \begin{align*}
        3x_{4} &= 3 \implies x_{4} = 1, \\
        -x_{3} + 3x_{4} &= 1\implies x_{3} = -1(1-3(1)) = 2, \\
        x_{2} - x_{3} + 3x_{4} &= 4 \implies x_{2} = 4-3(1)+2 = 3, \\
        2x_{1} + x_{2} - x_{3} + 3x_{4} &= 12 \implies x_{1} = \frac{12-3(1)+2-3}{2} = 4
    .\end{align*}
    So,
    \begin{align*}
        x = \begin{pmatrix} 4 \\ 3 \\ 2 \\ 1 \end{pmatrix}
    .\end{align*}
    We verify the result by computing $Ax$, and comparing it against $\hat{b}$. We have
    \begin{align*}
        \begin{bmatrix} 2 & 1 & -1 & 3 \\ -2 & 0 & 0 & 0 \\ 4 & 1 & -2 & 6 \\ -6 & -1 & 2 & -3 \end{bmatrix} \begin{pmatrix} 4 \\ 3 \\ 2 \\ 1 \end{pmatrix} &= \begin{pmatrix} 2(4) + 1(3) - 1(2) + 3(1) \\ -2(4) + 0 + 0 + 0 \\ 4(4)  +1(3) -2(2) +6(1) \\ -6(4) -1(3) + 2(2)-3(1) \end{pmatrix} = \begin{pmatrix} 12 \\ -8 \\ 21 \\ -26 \end{pmatrix}
    .\end{align*}
    The result is verified.

    \bigbreak \noindent 
    \begin{mdframed}
        1.7.26. Use the inner-product formulation to calculate the $LU$ decomposition of the matrix $A$ in Exercise 1.7.10 
    \end{mdframed}
    \bigbreak \noindent 
    \begin{remark}
        The inner-product formulas to compute the $LU$ decomposition are  
        \begin{align*}
            u_{ij} &= a_{ij} - \sum_{k=1}^{i-1}\ell_{ik}u_{kj} \quad j=i,i+1,...,n \tag{1}, \\
            \ell_{ij} &= \frac{a_{ij} - \sum_{k=1}^{j-1}\ell_{ik}u_{kj}}{u_{jj}} \quad i=j+1,j+2,...,n \tag{2}
        .\end{align*}
        \bigbreak \noindent 
        To use these formulas to find each $u_{ij}$ we first need to plug $i=1$ into $(1)$, then after we get the first row of $U$, we can plug in $j=1$ into $(2)$ to get the first column of $L$, and so on. $\endpf $
    \end{remark}
    \bigbreak \noindent 
    Recall that the matrix $A$ is given as
    \begin{align*}
        \begin{bmatrix}
            2 & 1 & -1 & 3 \\
            -2 & 0 & 0 & 0 \\
            4 & 1 & -2 & 6 \\
            -6 & -1 & 2 & -3
       \end{bmatrix} 
    .\end{align*}
    So, we first find the first row of $U$ (set $i=1$), we have
    \begin{align*}
        u_{11} &= a_{11} =  2, \\
        u_{12} &= a_{12} = 1 , \\
        u_{13} &= a_{13} = -1, \\
        u_{14} &= a_{14} = 3
    .\end{align*}
    Next, we find the first column of $L$ (set $j=1$),
    \begin{align*}
        \ell_{11} &= 1, \\
        \ell_{21} &= \frac{a_{21}}{u_{11}} = \frac{-2}{2} = -1, \\
        \ell_{31} &= \frac{a_{31}}{u_{11}} = \frac{4}{2} = 2,\\
        \ell_{41} &= \frac{a_{41}}{u_{11}} = -\frac{6}{2} = -3
    .\end{align*}
    For the second row of $U$ ($i = 2$), 
    \begin{align*}
        u_{22} &= a_{22} - \sum_{k=1}^{1}\ell_{2k}u_{k2} = 0 - (-1)(1) = 1, \\
        u_{23} &= a_{23} - \sum_{k=1}^{1}\ell_{2k}u_{k3} = 0 - (-1)(-1) = -1, \\
        u_{24} &= a_{24} - \sum_{k=1}^{1}\ell_{2k}u_{k4} = 0 - (-1)(3) = 3
    .\end{align*}
    For the second column of $L$ ($j=2$),
    \begin{align*}
        \ell_{22} &= 1, \\
        \ell_{32} &= \frac{a_{32} - \sum_{k=1}^{1}\ell_{3k}u_{k2}}{u_{22}} = \frac{1-2(1)}{1} = -1, \\
        \ell_{42} &= \frac{a_{42}- \sum_{k=1}^{1}\ell_{4k}u_{k2}}{u_{22}} = \frac{-1 - (-3)(1)}{1} = 2
    .\end{align*}
    For the third row of $U$ ($i = 3$),
    \begin{align*}
        u_{33} &= a_{33} - \sum_{k=1}^{2}\ell_{3k}u_{k3} = a_{33} - (\ell_{31}u_{13} + \ell_{32}u_{23}) = -2 - (2(-1) + (-1)(-1)) = -1, \\
        u_{34} &= a_{34} - \sum_{k=1}^{2}\ell_{3k}u_{k4} = a_{34} - (\ell_{31}u_{14} + \ell_{32}u_{24}) = 6 - (2(3) + (-1)(3)) = 3
    .\end{align*}
    For the third column of $L$ ($j=3$), 
    \begin{align*}
        \ell_{33} &= 1, \\
        \ell_{43} &= \frac{a_{43}- \sum_{k=1}^{2}\ell_{4k}u_{k3}}{u_{33}} = \frac{a_{43} - (\ell_{41}u_{13} + \ell_{42}u_{23})}{u_{33}} = \frac{2 - ((-3)(-1) + (2)(-1))}{-1} = -1
    .\end{align*}
    For the fourth row of $U$ ($i =4$), 
    \begin{align*}
        u_{44} &= a_{44} - \sum_{k=1}^{3}\ell_{4k}u_{k4}  = a_{44} - (\ell_{41}u_{14} + \ell_{42}u_{24}  + \ell_{43}u_{34}) = -3 - ((-3)(3) + 2(3) + (-1)(3)) = -3
    .\end{align*}
    For the fourth column of $L$ ($j=4$),
    \begin{align*}
        \ell_{44} &= 1
    .\end{align*}
    So, the $LU$ decomposition according to the inner-product formulas is
    \begin{align*}
        L &= \begin{bmatrix} 1 & 0& 0 & 0 \\ -1 & 1 & 0& 0 \\ 2 & -1 & 1 & 0\\ -3 & 2 & -1 & 1 \end{bmatrix}, 
        \quad U = \begin{bmatrix} 2 & 1 & -1 & 3  \\ 0 & 1 & -1 & 3 \\ 0 & 0 & -1 & 3\\ 0 & 0 & 0 & 3 \end{bmatrix}
    .\end{align*}
    Which is exactly the same decomposition that we got with Gaussian Elimination.
    

    \bigbreak \noindent 
    \begin{mdframed}
        1.7.34.  In this exercise you will show that performing an elementary row operation of type 1 is equivalent to left multiplication by a matrix of a special type. Suppose $\tilde{A}$ is obtained from $A$ by adding $m$ times the $j$th row to the $i$th row.

        \begin{enumerate}[label=(\alph*)]
            \item Show that $\tilde{A} = M A$, where $M$ is the triangular matrix obtained 
                from the identity matrix by replacing the zero by an $m$ in the $(i,j)$ position. 
                For example, when $i > j$, $M$ has the form
                \[
                    M = \begin{bmatrix}
                        1 &        &        &        &  & \\
                          & \ddots &        &        &  & \\
                          &        & 1      &        &  & \\
                          &        & m      & 1      &  & \\
                          &        &        &        & \ddots& \\
                          &        &        &        &       & 1
                    \end{bmatrix}.
                \]
                Notice that this is the matrix obtained by applying the type 1 row operation 
                directly to the identity matrix. We call $M$ an \textit{elementary matrix of type 1}.

            \item Show that $\det(M) = 1$ and $\det(\tilde{A}) = \det(A)$. 
                Thus we see (again) that $\tilde{A}$ is nonsingular if and only if $A$ is.

            \item Show that $M^{-1}$ differs from $M$ only in that it has $-m$ instead of $m$ 
                in the $(i,j)$ position. $M^{-1}$ is also an elementary matrix of type 1. 
                To which elementary operation does it correspond?
        \end{enumerate}
    \end{mdframed}
    \bigbreak \noindent 
    \begin{remark}
       The book defines the elementary operations in the following order 
       \begin{enumerate}
           \item Add a multiple of one equation to another equation.
           \item Interchange two equations.
           \item Multiply an equation by a nonzero constant
       \end{enumerate}
    \end{remark}
    \bigbreak \noindent 
    \begin{remark}
     Let $A \in \mathbb{R}^{n\times n}$, let $e_{i}$ be the $i^{\text{th}}$ standard basis vector in $\mathbb{R}^{n}$. That is, a vector of size $n$ with a one in the $i^{\text{th}}$ position, and zeros everywhere else. Then,
    \begin{align*}
        \text{col}_{j}(A) = Ae_{j}
    .\end{align*}
    \bigbreak \noindent 
    If we take the transpose of both sides,
    \begin{align*}
        \left(\text{col}_{j}(A)\right)^{T} = e_{j}^{T}A^{T},
    \end{align*}
    which implies that
    \begin{align*}
        \left(\text{col}_{j}(A^{T})\right)^{T} = e_{j}^{T}A,
    .\end{align*}
    But, we know that $\text{row}_{j}(A) = (\text{ col}_{j}(A^{T}))^{T} $, and $\text{col}_{j}(A) = (\text{row}_{j}(A^{T}))^{T} $. Thus,
    \begin{align*}
        \left(\text{col}_{j}(A^{T})\right)^{T} = e_{j}^{T}A \\
        \implies \text{row}_{j}(A) = e_{j}^{T}A
    .\end{align*}
    \end{remark} 
    \bigbreak \noindent 
    a.) In $\tilde{A}$, we have
    \begin{align*}
        &\text{If } k \ne i,\; \text{row}_{k}(\tilde{A}) = \text{ row}_{k}(A), \\ 
        &\text{If } k = i,\; \text{row}_{k}(\tilde{A}) = \text{ row}_{k}(A) + m\cdot \text{row}_{j}(A)
    .\end{align*}
    \bigbreak \noindent 
    Let $E_{ij}$ be the zero matrix except for a one at $e_{ij}$. Thus, 
    \begin{align*}
        M &= I + mE_{ij}
    .\end{align*}
    Observe that $E_{ij} = e_{i}e_{j}^{T}$, so
    \begin{align*}
        M &= I + me_{i}e_{j}^{T}
    .\end{align*}
    From this fact, we have
    \begin{align*}
        MA &= (I + me_{i}e_{j}^{T})A = A + me_{i}(e_{j}^{T}A)
    .\end{align*}
    Recall that $e_{j}^{T}A$ is the $j^{\text{th}} $ row of $A$, so
    \begin{align*}
        MA &= A + me_{i} \cdot  \text{row}_{j}(A)
    .\end{align*}
    Further observe that $e_{i} \cdot \text{row}_{j}(A)$ is a matrix of size $n\times n$, where the $i^{\text{th}} $ row is $\text{row}_{j}(A)$, and all other rows are zero.
    \bigbreak \noindent 
    So, we see that 
    \begin{align*}
        &\text{If } k \ne i,\; \text{row}_{k}(E_{ij}A) = 0, \; \text{ so } \text{row}_{k}(MA) = \text{ row}_{k}(A), \\
        &\text{If } k = i,\; \text{row}_{k}(E_{ij}A) = \text{row}_{j}(A), \; \text{ so } \text{row}_{k}(MA) = \text{row}_{i}(A) + m \cdot  \text{row}_{j}(A)
    .\end{align*}
    Thus, $\tilde{A} = MA$
    \bigbreak \noindent 
    b.) Since $M$ is triangular, the determinant is
    \begin{align*}
        \det(M) = \prod_{i=1}^{n} m_{ii}
    .\end{align*}
    But, $m_{ii} = 1$ for $i=1,2,...,n$. Thus, $\det(M) = 1$. The determinant of $\tilde{A}$ is 
    \begin{align*}
        \det(\tilde{A}) = \det(MA)= \det(M)\det(A) = 1\det(A) = \det(A)
    .\end{align*}
    \bigbreak \noindent 
    c.) We propose
    \begin{align*}
        M^{-1} = I - mE_{ij}
    .\end{align*}
    We can verify this by showing $MM^{-1} = (I+mE_{ij})(I-mE_{ij}) = I $. We have
    \begin{align*}
        (I +mE_{ij})(I - mE_{ij}) &= II - mE_{ij} + mE_{ij} - m^{2}E_{ij}^{2} \\
                                  &= I - m^{2}E_{ij}^{2}
        .\end{align*}
    But, since $E_{ij}$ is all zeros except for a one at position $i,j$, for $i\ne j$, we have
    \begin{align*}
        E_{ij}^{2} = (e_{i}e_{j}^{T})(e_{i}e_{j}^{T}) = e_{i}(e_{j}^{T}e_{i})e_{j}^{T}
    \end{align*}
    But, $e_{j}^{T}e_{i} \in \mathbb{R}$, so we can commute
    \begin{align*}
        e_{i}(e_{j}^{T}e_{i})e_{j}^{T} &= (e_{j}^{T}e_{i})e_{i}e_{j}^{T} = e_{j}^{T}e_{i}E_{ij}
    .\end{align*}
    But, since $i\ne j$, $e_{j}^{T}e_{i} = 0$, so $E_{ij}^{2} = 0 $. Thus,
    \begin{align*}
        I - m^{2}E_{ij}^{2} = I
    .\end{align*}
    Thus, $M^{-1} = I - mE_{ij} $, which means that $M^{-1} $ is the same as $M$, but with $-m$ in position $(i,j)$ instead of $+m$. $M^{-1}$ corresponds to the operation
    \begin{align*}
        r_{i} \rto r_{i} - mr_{j}
    .\end{align*}


    \bigbreak \noindent 
    \begin{mdframed}
        1.7.36. Suppose $\tilde{A}$ is obtained from $A$ by multiplying the $i$th row by the nonzero constant $c$.
        \begin{enumerate}[label=(\alph*)]
            \item Find the form of the matrix $M$ (an \textit{elementary matrix of type 3}) 
                such that $\tilde{A} = M A$.

            \item Find $M^{-1}$ and state its function as an elementary matrix.

            \item Find $\det(M)$ and determine the relationship between $\det(\tilde{A})$ 
                and $\det(A)$. Deduce that $\tilde{A}$ is nonsingular if and only if $A$ is.
        \end{enumerate}
    \end{mdframed}
    \bigbreak \noindent 
    a.) 
    Suppose we have $A \in \mathbb{R}^{2\times 2}$, where 
    \begin{align*}
        A = \begin{bmatrix}
            \alpha & \beta \\
            \gamma & \varphi
        \end{bmatrix}
    .\end{align*}
    Now, suppose we want to scale the second row by $c$, where $c \in \mathbb{R}$, then
    \begin{align*}
        \tilde{A} = \begin{bmatrix}
            \alpha & \beta \\ 
            c\gamma & c\varphi
        \end{bmatrix}
        &= 
        \begin{bmatrix}
            \alpha & \beta \\
            \gamma & \varphi 
        \end{bmatrix}
         + 
         \begin{bmatrix}
             \alpha & \beta \\
             (c-1)\gamma & (c-1)\varphi
         \end{bmatrix}
         \\
        &= A + (c-1)\begin{bmatrix}
            0 & 0 \\
            0 & 1
        \end{bmatrix}
        A
        \\
        &= A + (c-1)E_{22}A
    .\end{align*}
    So, suppose we wish to scale the $i^{\text{th}}$ row of $A$ by a constant $c$, then
    \begin{align*}
        \tilde{A}= A + (c-1)E_{ii}A = (I + (c-1)E_{ii})A = MA
    .\end{align*} 
    Thus, $M = I + (c-1)E_{ii}$.
    \bigbreak \noindent 
    It seems that the inverse operation is multiplying row $i$ by $\frac{1}{c}$. Thus, we propose that the inverse of $M$ is
    \begin{align*}
        M^{-1} = I + \left(\frac{1}{c} - 1\right)E_{ii}
    .\end{align*}
    We have
    \begin{align*}
        MM^{-1} &= \left(I + (c-1)E_{ii}\right)\left(I + \left(\frac{1}{c}-1\right)E_{ii}\right) \\
                &= I + \left(\frac{1}{c}-1\right)E_{ii} + (c-1)E_{ii} + (c-1)\left(\frac{1}{c}-1\right)E_{ii}^{2}
    .\end{align*}
    But,
    \begin{align*}
        E_{ii}^{2} &= (e_{i}e_{i}^{T})(e_{i}e_{i}^{T}) = e_{i}(e_{i}^{T}e_{i})e_{i}^{T} \\
                   &= (e_{i}^{T}e_{i})e_{i}e_{i}^{T} = (e_{i}^{T}e_{i})E_{ii} \\
                   &= \norm{e_{i}}^{2}E_{ii} = E_{ii}
    .\end{align*}
    So, $E_{ii}^{2} = E_{ii}$, and 
    \begin{align*}
        MM^{-1} &= II + \left(\frac{1}{c}-1\right)E_{ii} + (c-1)E_{ii} + (c-1)\left(\frac{1}{c}-1\right)E_{ii}^{2} \\
        &= I + \left(\frac{1}{c}-1 + c-1\right)E_{ii} + (c-1)\left(\frac{1}{c}-1\right)E_{ii} \\
        &= I + \left(\frac{1}{c}-1 + c-1 + (c-1)\left(\frac{1}{c}-1\right)\right)E_{ii} \\
        &= I + \left(\frac{1}{c}-1+c-1 + 1 -c - \frac{1}{c} + 1\right)E_{ii} \\
        &= I + 0 E_{ii} = I
    .\end{align*}
    Thus, $M^{-1} = I + \left(\frac{1}{c}-1\right) E_{ii}$
    \bigbreak \noindent 
    Observe that the determinant of $M$ is   
    \begin{align*}
        \det(M) = \det(I + (c-1)E_{ii}) = \prod_{k=1}^{n} m_{kk}
    .\end{align*}
    But, notice that $m_{kk} =1$, except at $k=i$, where we have $m_{ii}=  1 + (c-1) = c$. Thus, $\det(M) = c $, and 
    \begin{align*}
        \det(\tilde{A}) = \det(MA) =  \det(M)\det(A) = c\det(A)
    .\end{align*}
    Since $c\ne 0$, $\det(\tilde{A}) = 0 \iff \det(A) = 0$. Hence, $\tilde{A}$ is nonsingular if and only if $A$ is. $\endpf$


    \bigbreak \noindent 
    \begin{mdframed}
        1.8.4. Let 
        \[
            A = 
            \begin{bmatrix}
                2 & 2 & -4 \\
                1 & 1 & 5 \\
                1 & 3 & 6
            \end{bmatrix},
            \qquad
            b =
            \begin{bmatrix}
                10 \\
                -2 \\
                -5
            \end{bmatrix}.
        \]
        Use Gaussian elimination with partial pivoting (by hand) to find matrices $L$ and $U$ such that $U$ is upper triangular, $L$ is unit lower triangular with $|l_{ij}| \leq 1$ for all $i > j$, and $LU = \tilde{A}$, where $\tilde{A}$ can be obtained from $A$ by making row interchanges. Use your $LU$ decomposition to solve the system $Ax = b$.
    \end{mdframed}
    \bigbreak \noindent 
    We begin by initializing our permutation matrix $P$ as the identity matrix $I$. That is, $P = I$. We set row one as the pivot row, and $a_{11}$ as the pivot element. We look to the first column of $A$ and see that $a_{11}$ has the maximum absolute value, so no partial pivoting at this stage.
    \bigbreak \noindent 
    Let $r_{i}$ denote the $i^{\text{th}}$ row of $A$. We perform the operations
    \begin{align*}
        &r_{2} \rto r_{2} - m_{21}r_{1}, \quad m_{21} = \frac{1}{2}, \\
        &r_{3} \rto r_{3} - m_{31}r_{1}, \quad m_{31} = \frac{1}{2}
    .\end{align*}
    Thus,
    \begin{align*}
        \begin{bmatrix}
            \begin{array}{ccc|c}
                2 & 2 & -4 & 10 \\
                1 & 1 & 5 & -2 \\
                1 & 3 & 6 & -5
            \end{array}
        \end{bmatrix}
        \sim
        \begin{bmatrix}
            \begin{array}{ccc|c}
                2 & 2 & -4 & 10 \\
                \boxed{\frac{1}{2}} & 0 & 7 & -7 \\
                \boxed{\frac{1}{2}}& 2 & 8 & -10
           \end{array}
        \end{bmatrix}
    .\end{align*}
    Note that the boxed numbers are entries of $L$. Next, row two is the pivot row, and $a_{22}$ is the pivot element. Using partial pivoting, we swap rows two and three. We make the same swap in $P$. So,
    \begin{align*}
        \begin{bmatrix}
            \begin{array}{ccc|c}
                2 & 2 & -4 & 10 \\
                \boxed{\frac{1}{2}} & 0 & 7 & -7 \\
                \boxed{\frac{1}{2}}& 2 & 8 & -10
            \end{array}
        \end{bmatrix}
        \sim
        \begin{bmatrix}
            \begin{array}{ccc|c}
                2 & 2 & -4 & 10 \\
                \boxed{\frac{1}{2}}& 2 & 8 & -10 \\
                \boxed{\frac{1}{2}} & 0 & 7 & -7 
            \end{array}
        \end{bmatrix}
        ,\quad
        P = \begin{pmatrix} 1 & 0 & 0 \\ 0 & 0 & 1 \\ 0 & 1 & 0 \end{pmatrix}
    .\end{align*}
    Next, we perform the operation
    \begin{align*}
        r_{3} \rto r_{3} - 0r_{2}
    .\end{align*}
    So,
    \begin{align*}
        \begin{bmatrix}
            \begin{array}{ccc|c}
                2 & 2 & -4 & 10 \\
                \boxed{\frac{1}{2}}& 2 & 8 & -10 \\
                \boxed{\frac{1}{2}} & 0 & 7 & -7 
            \end{array}
        \end{bmatrix}
        \sim
                \begin{bmatrix}
            \begin{array}{ccc|c}
                2 & 2 & -4 & 10 \\
                \boxed{\frac{1}{2}}& 2 & 8 & -10 \\
                \boxed{\frac{1}{2}} & \boxed{0} & 7 & -7 
            \end{array}
        \end{bmatrix}
    .\end{align*}
    Thus,
    \begin{align*}
        L &= \begin{bmatrix}
            1 & 0 & 0 \\
            \frac{1}{2} & 1 & 0 \\
            \frac{1}{2} & 0  & 1
        \end{bmatrix},\;
        U = 
        \begin{bmatrix}
            2 & 2 & -4 \\
            0 & 2 & 8 \\
            0 & 0 & 7
        \end{bmatrix},\;
            y = \begin{pmatrix} 10 \\ -10 \\ -7 \end{pmatrix},\;
            P = \begin{bmatrix}
                1 & 0 & 0 \\
                0 & 0 & 1 \\
                0 & 1 & 0
            \end{bmatrix}, 
    \end{align*}
    and
    \begin{align*}
        Pb = \begin{pmatrix} 10 \\ -5 \\ -2 \end{pmatrix}
    .\end{align*}
    We solve the system $Ax = b$, by splitting into two triangular systems
    \begin{align*}
        Ax = b \implies PAx = Pb \implies \tilde{A} = Pb &\implies LUx = Pb \\
                                                         &\implies 
        \begin{cases}
            Ly &= Pb \\
            Ux &= y 
        \end{cases}
    .\end{align*}
    So, we solve $Ux = y$ with backward substitution
    \begin{align*}
        Ux &= y \implies 
        \begin{bmatrix}
            2 & 2 & -4 \\
            0 & 2 & 8 \\
            0 & 0 & 7
        \end{bmatrix}
        \begin{pmatrix} x_{1} \\ x_{2} \\ x_{3} \end{pmatrix}
        = \begin{pmatrix} 10 \\ -10 \\ -7 \end{pmatrix}
    .\end{align*}
    Which, implies that
    \begin{align*}
        7x_{3} &= - 7 \implies x_{3} = -1, \\
        2x_{2} + 8x_{3} &= -10 \implies x_{2} = \frac{-10 - 2(-1)}{2} = -1, \\
        2x_{1} + 2x_{2} - 4x_{3} &= 10 \implies x_{1} = \frac{10+4(-1)-2(-1)}{2} = 4
    .\end{align*}
    So,
    \begin{align*}
        x = \begin{pmatrix} 4 \\ -1 \\ -1 \end{pmatrix}
    .\end{align*}
    We can verify this solution by checking that $Ax = b$, we have
    \begin{align*}
        \begin{bmatrix}
            2 & 2 & -4 \\
            1 & 1 & 5 \\
            1 & 3 & 6
        \end{bmatrix}
        \begin{pmatrix} 4 \\ -1 \\ -1 \end{pmatrix}
        =
        \begin{pmatrix} 8 - 2 + 4 \\ 4-1 -5 \\ 4-3-6 \end{pmatrix} = \begin{pmatrix} 10 \\ -2 \\ -5 \end{pmatrix}
    .\end{align*}
    Therefore the solution is verified.

    \bigbreak \noindent 
    \begin{mdframed}
        1.8.9. Let $A$ be the matrix in Exercise 1.8.4. Determine matrices $P$, $L$, and $U$ with the properties stated in Theorem 1.8.8, such that $A = P^{T}LU$
    \end{mdframed}
    \bigbreak \noindent 
    The matrices $P,L,U$ are precisely the matrices obtained in the previous exercise, since $A$ is unchanged. The matrices are
    \begin{align*}
        P &= \begin{bmatrix}
            1 & 0 & 0 \\
            0 & 0 & 1 \\
            0 & 1 & 0
        \end{bmatrix},\;
        L = \begin{bmatrix}
            1 & 0 & 0 \\
            \frac{1}{2} & 1 & 0 \\
            \frac{1}{2} & 0  & 1
        \end{bmatrix},\;
        U = 
        \begin{bmatrix}
            2 & 2 & -4 \\
            0 & 2 & 8 \\
            0 & 0 & 7
        \end{bmatrix}
    .\end{align*}
    We have
    \begin{align*}
        \begin{bmatrix}
            1 & 0 & 0 \\
            0 & 0 &  1 \\
            0 & 1 & 0 
        \end{bmatrix}
        \begin{bmatrix}
            1 & 0 & 0 \\
            \frac{1}{2} & 1 & 0 \\
            \frac{1}{2} & 0 & 1
        \end{bmatrix}
        \begin{bmatrix}
            2 & 2 & -4 \\
            0 & 2 & 8 \\
            0 & 0 & 7 
        \end{bmatrix}
        &= 
        \begin{bmatrix}
            1 & 0 & 0 \\
            0 & 0 & 1 \\
            0 & 1 & 0
        \end{bmatrix}
        \begin{bmatrix}
            2 & 2 & -4 \\
            \frac{1}{2}(2) & \frac{1}{2}(2) + 2 & \frac{1}{2}(-4) + 8 \\
            \frac{1}{2}(2) & \frac{1}{2}(2)  &\frac{1}{2}(-4) + 7 
        \end{bmatrix} \\
        &=
        \begin{bmatrix}
        1 & 0 & 0 \\
        0 & 0 & 1\\
        0 & 1 & 0
        \end{bmatrix}
        \begin{bmatrix}
            2 & 2 & -4 \\
            1 & 3 & 6 \\
            1 & 1 & 5
        \end{bmatrix}
        =
        \begin{bmatrix}
            2 & 2 & -4 \\
            1 & 1 & 5 \\
            1 & 3 & 6 
        \end{bmatrix} = A
    .\end{align*}

    \pagebreak \bigbreak \noindent 
    \begin{mdframed}
        1.8.12. Write an algorithm that implements Gaussian elimination with partial pivoting. Store $L$ and $U$ over $A$, and save a record of the row interchanges. 
    \end{mdframed}
    \bigbreak \noindent 
    \begin{cppcode}
    proc row_swap(|$A$|, |$i$|, |$k$|)
        if (|$i = k$|) return
        for |$\ell = 1,...,n$|
            tmp = |$a_{i\ell}$|  
            |$a_{i\ell} = a_{k\ell}$|
            |$a_{k\ell} = tmp$|
        end
    endproc

    proc partial_pivot(|$A$|, |$P$|, |$K$|)
        max = |$\lvert a_{kk} \rvert $|, max_i = |$k$|
        for |$i=k+1,...,n$|
            if (|$\lvert a_{ik}\rvert > \text{max}$|)
                max = |$\lvert a_{ik} \rvert $|, max_i = |$i$|
            end
        end

        if (|$\text{max} = 0$|) set error flag, exit

        row_swap(|$A,k$|, max_i)
        row_swap(|$P,k$|, max_i)
    endproc

    proc gaussian(|$A$|)
        |$P = I$|
        for |$k=1,...,n$|
            partial_pivot(|$A,P,k$|)
            for |$i=k+1,...,n$|
                |$m = a_{ik} / a_{kk}$|
                for |$j=k,...,n$|
                    |$a_{ij} = a_{ij} - m \cdot a_{kj}$|
                end
                |$a_{ik} = m$|
            end
        end
        
        return |$P$|
    endproc
    \end{cppcode}

    \bigbreak \noindent 
    \begin{mdframed}
        2.1.10. Prove that the 1-norm is a norm. 
    \end{mdframed}
    \bigbreak \noindent 
    \begin{remark}
        $\norm{\cdot}$ is a norm if and only if the following properties are satisfied
        \begin{enumerate}
            \item $\norm{x} \geq 0 \text{ and } \norm{x} = 0 \iff x= 0 $ 
            \item $ \norm{\alpha x} = \left\lvert \alpha \right\rvert \norm{x}$
            \item $\norm{x+y} \leq \norm{x} + \norm{y} $ (triangle inequality)
        \end{enumerate}
    \end{remark}
    \bigbreak \noindent 
    The 1-norm for a vector $x \in \mathbb{R}^{n}$ is $\norm{x}_{1} = \sum_{i=1}^{n}\left\lvert x_{i} \right\rvert $.
    \bigbreak \noindent 
    1.) Suppose $\norm{x}_1 = 0$, then
    \begin{align*}
        \sum_{i=1}^{n}\left\lvert x_{i} \right\rvert &= 0 \implies \left\lvert x_{1} \right\rvert + \left\lvert x_{2} \right\rvert + ... + \left\lvert x_{n} \right\rvert = 0 \\
        \implies x_{1} &= x_{2} = ... = x_{n} = 0
    .\end{align*}
    Suppose $x=0$, then $x_{1} = x_{2} = ... = x_{n}= 0$, and
    \begin{align*}
        \sum_{i=1}^{n} \left\lvert x_{i} \right\rvert = \sum_{i=1}^{n} 0 = 0 + 0 + ... + 0  = 0
    .\end{align*}
    2.) 
    \begin{align*}
        \norm{\alpha x}_1 &= \sum_{i=1}^{n}\left\lvert \alpha x_{i} \right\rvert = \left\lvert \alpha x_{1} \right\rvert + \left\lvert \alpha x_{2} \right\rvert + ... + \left\lvert \alpha x_{n} \right\rvert \\
                        &= \left\lvert \alpha \right\rvert \left\lvert x_{1} \right\rvert + \left\lvert \alpha \right\rvert \left\lvert x_{2} \right\rvert + ... + \left\lvert \alpha \right\rvert \left\lvert x_{n} \right\rvert \\
                        &= \left\lvert \alpha \right\rvert \left(\left\lvert x_{1} \right\rvert + \left\lvert x_{2} \right\rvert + ... + \left\lvert x_{n} \right\rvert\right) = \left\lvert \alpha \right\rvert \norm{x}_1
    .\end{align*}
    3.) We can use the triangular inequality for absolute value,
    \begin{align*}
        \norm{x + y}_1 &= \sum_{i=1}^{n} \left\lvert x_{i} + y_{i} \right\rvert \\
                     &= \left\lvert x_{1} + y_{1} \right\rvert + \left\lvert x_{2} + y_{2} \right\rvert + ... + \left\lvert x_{n} + y_{n} \right\rvert \\
                     &\leq \left\lvert x_{1} \right\rvert + \left\lvert y_{1} \right\rvert + \left\lvert x_{2} \right\rvert + \left\lvert y_{2} \right\rvert + ... + \left\lvert x_{n} \right\rvert + \left\lvert y_{n} \right\rvert \\
                     &= \left\lvert x_{1} \right\rvert + \left\lvert x_{2} \right\rvert + ... + \left\lvert x_{n} \right\rvert + \left\lvert y_{1} \right\rvert + \left\lvert y_{2} \right\rvert + ... + \left\lvert y_{n} \right\rvert \\
                     &= \norm{x}_1 + \norm{y}_1
    .\end{align*}
    $\endpf$

    


    \bigbreak \noindent 
    \begin{mdframed}
        2.1.13.  Prove that the $\infty$-norm is a norm.
    \end{mdframed}
    \bigbreak \noindent 
    The $\infty$-norm for a vector $x\in \mathbb{R}^{n}$ is $\norm{x}_{\infty} = \max_{i=1}^{n}\left\lvert x_{i} \right\rvert  $
    \bigbreak \noindent 
    1.) Suppose $\norm{x}_{\infty} = 0$, then
    \begin{align*}
        \text{max}_{i=1}^{n} \left\lvert x_{i} \right\rvert = 0 
    \end{align*}
    implies that $x_{1} = x_{2} = ... = x_{n} = 0$, since $\left\lvert x_{i} \right\rvert \geq 0$ for all $i$. Next, suppose that $x=0$, then 
    \begin{align*}
       \text{max}_{i=1}^{n} 0  = \text{max}\{0,0,...,0\}  = 0
    .\end{align*}
    Thus $\norm{x}_{\infty} = 0 \iff x = 0$ holds for the $\infty$-norm.
    \bigbreak \noindent 
    2.) If $\alpha = 0$, then $\norm{0 \cdot x}_{\infty} = \norm{0}_{\infty} = 0 $, and $\left\lvert 0 \right\rvert \norm{x}_{\infty} = 0 \norm{x}_{\infty} = 0$, so $\norm{0 x}_{\infty} = \left\lvert 0 \right\rvert \norm{x}_{\infty}$. So, assume that $\alpha \ne 0$. We have
    \begin{align*}
        \norm{\alpha x}_{\infty} = \max\{\left\lvert \alpha x_{1} \right\rvert, \left\lvert \alpha x_{2} \right\rvert, ..., \left\lvert \alpha x_{n} \right\rvert\} = \left\lvert \alpha \right\rvert \max \{\left\lvert x_{1} \right\rvert, \left\lvert x_{2} \right\rvert, ..., \left\lvert x_{n} \right\rvert\}
    .\end{align*}
    This follows from the fact that if $\left\lvert \alpha x_{\ell} \right\rvert \geq \left\lvert \alpha x_{i} \right\rvert$, for all $i\ne \ell$, then 
    \begin{align*}
        \left\lvert \alpha \right\rvert \left\lvert x_{\ell} \right\rvert &\geq \left\lvert \alpha \right\rvert \left\lvert x_{i} \right\rvert \\
        \implies \left\lvert x_{\ell} \right\rvert &\geq \left\lvert x_{i} \right\rvert
    .\end{align*}
    3.) By the triangle inequality for absolute value, for each $i$,
    \begin{align*}
        \left\lvert x_{i} + y_{i} \right\rvert &\leq \left\lvert x_{i} \right\rvert + \left\lvert y_{i} \right\rvert
    .\end{align*}
    If we take the max of both sides,
    \begin{align*}
        \norm{x+y}_{\infty} = \max_{i=1}^{n} \left\lvert x_{i} + y_{i} \right\rvert &\leq \max_{i=1}^{n}(\left\lvert x_{i} \right\rvert + \left\lvert y_{i} \right\rvert) \\
                                                              &\leq \max_{i=1}^{n}\left\lvert x_{i} \right\rvert + \max_{i=1}^{n} \left\lvert y_{i} \right\rvert \\
                                                              &= \norm{x}_{\infty} + \norm{y}_{\infty}
    .\end{align*}
    $\endpf $


    \bigbreak \noindent 
    \begin{mdframed}
        2.1.17. 
        \begin{enumerate}[label=(\alph*)]
            \item Let $A$ be a positive definite matrix, and let $R$ be its Cholesky factor, 
                so that $A = R^{T}R$. Verify that for all $x \in \mathbb{R}^n$, 
                \[
                    \|x\|_{A} = \|Rx\|_{2}.
                \]

            \item Using the fact that the $2$-norm is indeed a norm on $\mathbb{R}^n$, 
                prove that the $A$-norm is a norm on $\mathbb{R}^n$.
        \end{enumerate}
    \end{mdframed}
    \bigbreak \noindent 
    \begin{remark}
       Given a positive definite matrix $A \in \mathbb{R}^{n\times n}$ , define the $A$-norm on $\mathbb{R}^{n}$ by
       \begin{align*}
           \norm{x}_{A} = (x^{T}Ax)^{\frac{1}{2}}
       .\end{align*}
    \end{remark}
    \bigbreak \noindent 
    a.)
    \begin{align*}
        \left(x^{T}Ax\right)^{\frac{1}{2}} &= \left(x^{T}R^{T}Rx\right)^{\frac{1}{2}} = \left(\left(Rx\right)^{T}(Rx)\right)^{\frac{1}{2}} = \left(\norm{Rx}_{2}^{2}\right)^{\frac{1}{2}} = \norm{Rx}_{2}
    .\end{align*}
    \bigbreak \noindent 
    b.) Suppose that $\norm{x}_{A} = 0$, then
    \begin{align*}
        \norm{Rx}_{2} = 0 \implies Rx = 0 \implies x = 0
    \end{align*}
    since $R$ is non-singular. Next, suppose that $x = 0$, then
    \begin{align*}
        \norm{0}_{A} &= \norm{R0}_{2} = \norm{0}_{2} = 0
    .\end{align*}
    So, $\norm{x}_{A} = 0 \implies x=0$, and $x=0 \implies \norm{x}_{A} = 0$. Hence, $\norm{x}_{A} = 0 \iff x = 0 $.
    \bigbreak \noindent 
    Next, consider $\norm{\alpha x}_{A}$, we have
    \begin{align*}
        \norm{\alpha x}_{A} &= \norm{R(\alpha x)}_{2} = \norm{\alpha Rx}_{2} = \left\lvert \alpha \right\rvert \norm{Rx}_{2} = \left\lvert \alpha \right\rvert \norm{x}_{A}
    .\end{align*}
    Last, consider $\norm{x +y }_{A}$,
    \begin{align*}
        \norm{x + y}_{A} &= \norm{R(x+y)}_{2} = \norm{Rx + Ry}_{2} \leq \norm{Rx}_{2} + \norm{Ry}_{2} = \norm{x}_{A} + \norm{y}_{A}
    .\end{align*}
    So, $\norm{x}_{A}$ is indeed a norm on $\mathbb{R}^{n}$.

    

    \bigbreak \noindent 
    \begin{mdframed}
        2.2.6.
        \begin{enumerate}[label=(\alph*)]
            \item Show that $\kappa(A) = \kappa(A^{-1})$
            \item Show that for any nonzero scalar $c$, $\kappa(cA) = \kappa(A)$
        \end{enumerate}
    \end{mdframed}
    \bigbreak \noindent 
    a.)
    \begin{align*}
        \kappa(A) &= \norm{A}\norm{A^{-1}}, \\
        \kappa(A^{-1}) &= \norm{A^{-1}}\norm{\left(A^{-1}\right)^{-1}} = \norm{A^{-1}}\norm{A} = \norm{A}\norm{A^{-1}} = \kappa(A)
    .\end{align*}
    b.) 
    \begin{align*}
        \kappa(\alpha A) &= \norm{\alpha A}\norm{\left(\alpha A\right)^{-1}} \\
                         &= \norm{\alpha A}\norm{\frac{1}{\alpha}A^{-1}} \\
                         &= \left\lvert \alpha \right\rvert \norm{A} \left\lvert \frac{1}{\alpha} \right\rvert\norm{A^{-1}} \\
                         &= \norm{A}\norm{A^{-1}} = \kappa(A)
    .\end{align*}
    $\endpf$

    \bigbreak \noindent 
    \begin{mdframed}
        2.2.15. 
        \begin{enumerate}[label=(\alph*)]
            \item Let $\alpha$ be a positive number, and define
                \begin{align*}
                    A_{\alpha} = \begin{bmatrix} \alpha & 0 \\ 0 & \alpha \end{bmatrix}
                .\end{align*}
                Show that for any induced matrix norm we have $\norm{A_{\alpha}} = \alpha,\; \norm{A_{\alpha}^{-1}} = \frac{1}{\alpha}$, and $\kappa(A_{\alpha}) =1$. Thus, $A_{\alpha} $ is well conditioned. On the other hand, $\det(A_{\alpha}) = \alpha^{2}$, so we can make it as large or small as we please by choosing $\alpha$ appropriately.
            \item More generally, given an nonsingular matrix $A$, discuss the condition number and determinant of $A_{\alpha}$, where $\alpha$ is any positive real number.
        \end{enumerate}
    \end{mdframed}
    \bigbreak \noindent 
    a.) Observe that $A_{\alpha} = \alpha I$. For any induced matrix norm, we have the properties $\norm{I} = 1$, and $\norm{s A} = \left\lvert s \right\rvert \norm{A}$. Thus,
    \begin{align*}
        \norm{A_{\alpha}} = \norm{\alpha I} = \left\lvert \alpha \right\rvert \norm{I} = \left\lvert \alpha \right\rvert \cdot 1 = \alpha,
    \end{align*}
    since $\alpha \in \mathbb{R} \setminus \{0\}$, $\left\lvert \alpha \right\rvert  = \alpha$. Furthermore, $A_{\alpha}^{-1} = \left(\alpha I\right)^{-1} = \alpha^{-1} I^{-1} = \frac{1}{\alpha}I$. Thus,
    \begin{align*}
        \norm{A_{\alpha}^{-1}} = \norm{\frac{1}{\alpha}I} = \left\lvert \frac{1}{\alpha} \right\rvert \norm{I} = \left\lvert \frac{1}{\alpha} \right\rvert \cdot 1 = \frac{1}{\alpha}
    .\end{align*}
    Again, since $\alpha \in \mathbb{R} \setminus \{0\}$, $\left\lvert \frac{1}{\alpha} \right\rvert = \frac{1}{\alpha}$. With these two facts, we have
    \begin{align*}
        \kappa(A_{\alpha}) = \norm{A_{\alpha}}\norm{A_{\alpha}^{-1}} = \alpha \cdot  \frac{1}{\alpha} =1
    .\end{align*}
    \bigbreak \noindent 
    b.)  For any nonsingular matrix $A$, we have $A_{\alpha} = \alpha A$, where $\alpha \in \mathbb{R} \setminus \{0\} $. By exercise 2.2.6, we saw
    \begin{align*}
        \kappa(A_{\alpha}) = \kappa(\alpha A) = \kappa(A)
    .\end{align*}
    So, the condition number stays the same regardless of how big or small $\alpha$ is. On the other hand, 
    \begin{align*}
        \det(\alpha A) = \alpha^{n}\det(A)
    .\end{align*}
    So, the determinant grows as $\alpha$ grows, and shrinks as $\alpha$ shrinks. Thus, the determinant can be arbitrarily large or small depending on $\alpha$ while $\kappa(A_{\alpha})$ remains fixed.


    \bigbreak \noindent 
    \begin{mdframed}
        Repeat the proof of Theorem 2.3.3.
    \end{mdframed}
    \bigbreak \noindent 
    \begin{remark}
        \textbf{Theorem 2.3.3}. 
        Let $A$ be nonsingular, let $b \neq 0$, and let $x$ and $\hat{x} = x + \delta x$ be 
        solutions of $Ax = b$ and $(A + \delta A)\hat{x} = b$, respectively. Then,
        \[
            \frac{\|\delta x\|}{\|\hat{x}\|} 
            \leq \kappa(A) \frac{\|\delta A\|}{\|A\|}.
            \tag{2.3.4}
        \]
    \end{remark}
    \bigbreak \noindent 
    \textbf{\textit{Proof.}}
    We have the two systems,
    \begin{align*}
        Ax = b, \quad A\hat{x} = \hat{b}
    .\end{align*}
    Thus, we have
    \begin{align*}
        Ax &= b\; \tag{1}, \\
        A(x + \delta  x) &= b + \delta  b \; \tag{2}
    .\end{align*}
    Looking at $(1)$, we see
    \begin{align*}
        Ax &= b \implies \norm{b} = \norm{Ax}
    .\end{align*}
    But, by the Cauchy Schwarz inequality, $\norm{b} \leq \norm{A}\norm{x}$. So,
    \begin{align*}
        \norm{b} \leq \norm{A}\norm{x} \tag{1}
    .\end{align*}
    Looking at $(2) $, we see
    \begin{align*}
        A(x + \delta  x) &= b + \delta  b \\
        \implies Ax + A \delta x &= b + \delta  b \\
        \implies A \delta  x &= \delta  b  \\
        \implies \delta x &= A^{-1} \delta  b  \\
        \implies \norm{\delta  x} &= \norm{A^{-1} \delta  b} \\
        \implies \norm{ \delta  x} &\leq \norm{A^{-1}} \norm{\delta  b} \tag{2}
    .\end{align*}
    Notice that we can setup $(1)$ so that dividing $(2)$ by $(1)$ gives the relative error in $x$ on the left, and relative error of $b$ on the right. So,
    \begin{align*}
        \frac{1}{\norm{x}} \leq \frac{\norm{A}}{\norm{b}}
    .\end{align*}
    Now, we divide $(2)$ by $(1)$, we have
    \begin{align*}
        \frac{\norm{\delta x}}{\norm{x}} \leq \norm{A^{-1}}\norm{A} \frac{\norm{ \delta  b}}{\norm{b}} = \kappa(A)\frac{\norm{\delta b}}{\norm{b}}
    .\end{align*}
    $\endpf $

    


\end{document}
